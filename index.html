<!doctype html>
<html>
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <script src="https://cdn.tailwindcss.com"></script>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.4/dist/katex.min.css" integrity="sha384-vKruj+a13U8yHIkAyGgK1J3ArTLzrFGBbBc0tDp4ad/EyewESeXE/Iv67Aj8gKZ0" crossorigin="anonymous">
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.4/dist/katex.min.js" integrity="sha384-PwRUT/YqbnEjkZO0zZxNqcxACrXe+j766U2amXcgMg5457rve2Y7I6ZJSm2A0mS4" crossorigin="anonymous"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.4/dist/contrib/auto-render.min.js" integrity="sha384-+VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4+/RRE05" crossorigin="anonymous"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/@alpinejs/collapse@3.x.x/dist/cdn.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/alpinejs@3.x.x/dist/cdn.min.js"></script>
</head>
<body>
  <div class="relative mx-auto h-full max-w-2xl text-md">
    <table class="table-auto">
      <tbody>
        <tr>
          <td></td>
          <td>
            <h1 class="text-4xl pt-4 font-bold"><span class="underline">Vincent's</span> Arxiv FrontPage</h1>
            <br>
            <p>Generated on 2024-02-15.</p><br/>
            <p class="text-sm text-gray-500 pt-2">This frontpage is made by scraping arxiv and by runnig a sentence-model that detects if the abstract describes a paper about a topic of interest. One cool feature: it all pretty much runs via Github Actions. </p>
            <br>
          </td>
        </tr><tr>
          <td></td>
          <td>
            <h2 class="text-2xl tracking-tight pt-4 font-bold">New Datasets</h2>
          </td>
        </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-02-14</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Tell Me More! Towards Implicit User Intention Understanding of Language Model Driven Agents
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Current language model-driven agents often lack mechanisms for effective user participation, which is crucial given the vagueness commonly found in user instructions.Although adept at devising strategies and performing tasks, these agents struggle with seeking clarification and grasping precise user intentions.To bridge this gap, we introduce Intention-in-Interaction (IN3), a novel benchmark designed to inspect users' implicit intentions through explicit queries.Next, we propose the incorporation of model experts as the upstream in agent designs to enhance user-agent interaction.Employing IN3, we empirically train Mistral-Interact, a powerful model that proactively assesses task vagueness, inquires user intentions, and refines them into actionable goals before starting downstream agent task execution.Integrating it into the XAgent framework, we comprehensively evaluate the enhanced agent system regarding user instruction understanding and execution, revealing that our approach notably excels at identifying vague user tasks, recovering and summarizing critical missing information, setting precise and necessary agent execution goals, and minimizing redundant tool usage, thus boosting overall efficiency.<span class='px-1 mx-1 bg-yellow-200'>All the data and codes are released. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.859</span></span></p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2402.09205v1' target="_blank">
                  link
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-02-14</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                A case study of university student networks and the COVID-19 pandemic using a social network analysis approach in halls of residence
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>The COVID-19 pandemic has meant that young university students have had to adapt their learning and have a reduced relational context.Adversity contexts build models of human behaviour based on relationships.However, there is a lack of studies that analyse the behaviour of university students based on their social structure in the context of a pandemic.This information could be useful in making decisions on how to plan collective responses to adversities.The Social Network Analysis (SNA) method has been chosen to address this structural perspective.The aim of our research is to describe the structural behaviour of students in university residences during the COVID-19 pandemic with a more in-depth analysis of student leaders.A descriptive cross-sectional study was carried out at one Spanish Public University, Le\'on, from 23th October 2020 to 20th November 2020.The participation was of 93 students, from four halls of residence.<span class='px-1 mx-1 bg-yellow-200'>The data were collected from a database created specifically at the university to "track" contacts in the COVID-19 pandemic, SiVeUle. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.848</span></span>We applied the SNA for the analysis of the data.The leadership on the university residence was measured using centrality measures.The top leaders were analyzed using the Egonetwork and an assessment of the key players.Students with higher social reputations experience higher levels of pandemic contagion in relation to COVID-19 infection.The results were statistically significant between the centrality in the network and the results of the COVID-19 infection.The most leading students showed a high degree of Betweenness, and three students had the key player structure in the network.Networking behaviour of university students in halls of residence could be related to contagion in the COVID-19 pandemic.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2402.09219v1' target="_blank">
                  link
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-02-14</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Is my Data in your AI Model? Membership Inference Test with Application to Face Images
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>This paper introduces the Membership Inference Test (MINT), a novel approach that aims to empirically assess if specific data was used during the training of Artificial Intelligence (AI) models.Specifically, we propose two novel MINT architectures designed to learn the distinct activation patterns that emerge when an audited model is exposed to data used during its training process.The first architecture is based on a Multilayer Perceptron (MLP) network and the second one is based on Convolutional Neural Networks (CNNs).The proposed MINT architectures are evaluated on a challenging face recognition task, considering three state-of-the-art face recognition models.<span class='px-1 mx-1 bg-yellow-200'>Experiments are carried out using six publicly available databases, comprising over 22 million face images in total. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.735</span></span>Also, different experimental scenarios are considered depending on the context available of the AI model to test.Promising results, up to 90% accuracy, are achieved using our proposed MINT approach, suggesting that it is possible to recognize if an AI model has been trained with specific data.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2402.09225v1' target="_blank">
                  link
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-02-14</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Synthesizing Knowledge-enhanced Features for Real-world Zero-shot Food Detection
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Food computing brings various perspectives to computer vision like vision-based food analysis for nutrition and health.As a fundamental task in food computing, food detection needs Zero-Shot Detection (ZSD) on novel unseen food objects to support real-world scenarios, such as intelligent kitchens and smart restaurants.Therefore, we first benchmark the task of Zero-Shot Food Detection (ZSFD) by introducing FOWA dataset with rich attribute annotations.Unlike ZSD, fine-grained problems in ZSFD like inter-class similarity make synthesized features inseparable.The complexity of food semantic attributes further makes it more difficult for current ZSD methods to distinguish various food categories.To address these problems, we propose a novel framework ZSFDet to tackle fine-grained problems by exploiting the interaction between complex attributes.Specifically, we model the correlation between food categories and attributes in ZSFDet by multi-source graphs to provide prior knowledge for distinguishing fine-grained features.Within ZSFDet, Knowledge-Enhanced Feature Synthesizer (KEFS) learns knowledge representation from multiple sources (e.g., ingredients correlation from knowledge graph) via the multi-source graph fusion.Conditioned on the fusion of semantic knowledge representation, the region feature diffusion model in KEFS can generate fine-grained features for training the effective zero-shot detector.Extensive evaluations demonstrate the superior performance of our method ZSFDet on FOWA and the widely-used food dataset UECFOOD-256, with significant improvements by 1.8% and 3.7% ZSD mAP compared with the strong baseline RRFS.Further experiments on PASCAL VOC and MS COCO prove that enhancement of the semantic knowledge can also improve the performance on general ZSD.<span class='px-1 mx-1 bg-yellow-200'>Code and dataset are available at https://github.com/LanceZPF/KEFS. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.874</span></span></p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2402.09242v1' target="_blank">
                  link
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-02-14</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Only My Model On My Data: A Privacy Preserving Approach Protecting one Model and Deceiving Unauthorized Black-Box Models
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Deep neural networks are extensively applied to real-world tasks, such as face recognition and medical image classification, where privacy and data protection are critical.Image data, if not protected, can be exploited to infer personal or contextual information.Existing privacy preservation methods, like encryption, generate perturbed images that are unrecognizable to even humans.Adversarial attack approaches prohibit automated inference even for authorized stakeholders, limiting practical incentives for commercial and widespread adaptation.This pioneering study tackles an unexplored practical privacy preservation use case by generating human-perceivable images that maintain accurate inference by an authorized model while evading other unauthorized black-box models of similar or dissimilar objectives, and addresses the previous research gaps.<span class='px-1 mx-1 bg-yellow-200'>The datasets employed are ImageNet, for image classification, Celeba-HQ dataset, for identity classification, and AffectNet, for emotion classification. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.827</span></span>Our results show that the generated images can successfully maintain the accuracy of a protected model and degrade the average accuracy of the unauthorized black-box models to 11.97%, 6.63%, and 55.51% on ImageNet, Celeba-HQ, and AffectNet datasets, respectively.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2402.09316v1' target="_blank">
                  link
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-02-14</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Prediction of Activated Sludge Settling Characteristics from Microscopy Images with Deep Convolutional Neural Networks and Transfer Learning
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Microbial communities play a key role in biological wastewater treatment processes.Activated sludge settling characteristics, for example, are affected by microbial community composition, varying by changes in operating conditions and influent characteristics of wastewater treatment plants (WWTPs).Timely assessment and prediction of changes in microbial composition leading to settling problems, such as filamentous bulking (FB), can prevent operational challenges, reductions in treatment efficiency, and adverse environmental impacts.This study presents an innovative computer vision-based approach to assess activated sludge-settling characteristics based on the morphological properties of flocs and filaments in microscopy images.Implementing the transfer learning of deep convolutional neural network (CNN) models, this approach aims to overcome the limitations of existing quantitative image analysis techniques.<span class='px-1 mx-1 bg-yellow-200'>The offline microscopy image dataset was collected over two years, with weekly sampling at a full-scale industrial WWTP in Belgium. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.731</span></span>Multiple data augmentation techniques were employed to enhance the generalizability of the CNN models.Various CNN architectures, including Inception v3, ResNet18, ResNet152, ConvNeXt-nano, and ConvNeXt-S, were tested to evaluate their performance in predicting sludge settling characteristics.The sludge volume index was used as the final prediction variable, but the method can easily be adjusted to predict any other settling metric of choice.The results showed that the suggested CNN-based approach provides less labour-intensive, objective, and consistent assessments, while transfer learning notably minimises the training phase, resulting in a generalizable system that can be employed in real-time applications.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2402.09367v1' target="_blank">
                  link
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-02-14</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Massively Multi-Cultural Knowledge Acquisition & LM Benchmarking
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Pretrained large language models have revolutionized many applications but still face challenges related to cultural bias and a lack of cultural commonsense knowledge crucial for guiding cross-culture communication and interactions.Recognizing the shortcomings of existing methods in capturing the diverse and rich cultures across the world, this paper introduces a novel approach for massively multicultural knowledge acquisition.Specifically, our method strategically navigates from densely informative Wikipedia documents on cultural topics to an extensive network of linked pages.<span class='px-1 mx-1 bg-yellow-200'>Leveraging this valuable source of data collection, we construct the CultureAtlas dataset, which covers a wide range of sub-country level geographical regions and ethnolinguistic groups, with data cleaning and preprocessing to ensure textual assertion sentence self-containment, as well as fine-grained cultural profile information extraction. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.923</span></span>Our dataset not only facilitates the evaluation of language model performance in culturally diverse contexts but also serves as a foundational tool for the development of culturally sensitive and aware language models.Our work marks an important step towards deeper understanding and bridging the gaps of cultural disparities in AI, to promote a more inclusive and balanced representation of global cultures in the digital domain.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2402.09369v1' target="_blank">
                  link
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-02-13</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Auditing Counterfire: Evaluating Advanced Counterargument Generation with Evidence and Style
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>We present a novel dataset for the controlled composition of counterarguments designed for further applications in argument refining, mining, and evaluation.<span class='px-1 mx-1 bg-yellow-200'>Our dataset constitutes enriched counter-arguments to posts in the Reddit ChangeMyView dataset that are integrated with evidence retrieved from high-quality sources and generated based on user preferences, adjusting the critical attributes of evidence and argument style. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.795</span></span><span class='px-1 mx-1 bg-yellow-200'>The resultant Counterfire corpus comprises arguments generated from GPT-3.5 turbo, Koala, and PaLM 2 models and two of their finetuned variants (N = 32,000). <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.718</span></span>Model evaluation indicates strong paraphrasing abilities with evidence, albeit limited word overlap, while demonstrating high style integration (0.9682 for 'reciprocity'), showing the ability of LLM to assimilate diverse styles.Of all models, GPT-3.5 turbo showed the highest scores in argument quality evaluation, showing consistent accuracy (score >0.8).In further analyses, reciprocity-style counterarguments display higher counts in most categories, possibly indicating a more creatively persuasive use of evidence.In contrast, human-written counterarguments exhibited greater argumentative richness and diversity across categories.Despite human-written arguments being favored as the most persuasive in human evaluation, the 'No Style' generated text surprisingly exhibited the highest score, prompting further exploration and investigation on the trade-offs in generation for facts and style.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2402.08498v1' target="_blank">
                  link
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-02-13</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Captions Are Worth a Thousand Words: Enhancing Product Retrieval with Pretrained Image-to-Text Models
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>This paper explores the usage of multimodal image-to-text models to enhance text-based item retrieval.We propose utilizing pre-trained image captioning and tagging models, such as instructBLIP and CLIP, to generate text-based product descriptions which are combined with existing text descriptions.Our work is particularly impactful for smaller eCommerce businesses who are unable to maintain the high-quality text descriptions necessary to effectively perform item retrieval for search and recommendation use cases.<span class='px-1 mx-1 bg-yellow-200'>We evaluate the searchability of ground-truth text, image-generated text, and combinations of both texts on several subsets of Amazon's publicly available ESCI dataset. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.718</span></span>The results demonstrate the dual capability of our proposed models to enhance the retrieval of existing text and generate highly-searchable standalone descriptions.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2402.08532v1' target="_blank">
                  link
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-02-13</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                BdSLW60: A Word-Level Bangla Sign Language Dataset
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Sign language discourse is an essential mode of daily communication for the deaf and hard-of-hearing people.However, research on Bangla Sign Language (BdSL) faces notable limitations, primarily due to the lack of datasets.Recognizing wordlevel signs in BdSL (WL-BdSL) presents a multitude of challenges, including the need for well-annotated datasets, capturing the dynamic nature of sign gestures from facial or hand landmarks, developing suitable machine learning or deep learning-based models with substantial video samples, and so on.<span class='px-1 mx-1 bg-yellow-200'>In this paper, we address these challenges by creating a comprehensive BdSL word-level dataset named BdSLW60 in an unconstrained and natural setting, allowing positional and temporal variations and allowing sign users to change hand dominance freely. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.77</span></span><span class='px-1 mx-1 bg-yellow-200'>The dataset encompasses 60 Bangla sign words, with a significant scale of 9307 video trials provided by 18 signers under the supervision of a sign language professional. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.845</span></span>The dataset was rigorously annotated and cross-checked by 60 annotators.We also introduced a unique approach of a relative quantization-based key frame encoding technique for landmark based sign gesture recognition.We report the benchmarking of our BdSLW60 dataset using the Support Vector Machine (SVM) with testing accuracy up to 67.6% and an attention-based bi-LSTM with testing accuracy up to 75.1%.<span class='px-1 mx-1 bg-yellow-200'>The dataset is available at https://www.kaggle.com/datasets/hasaniut/bdslw60 and the code base is accessible from https://github.com/hasanssl/BdSLW60_Code. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.892</span></span></p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2402.08635v1' target="_blank">
                  link
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-02-13</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                SemRel2024: A Collection of Semantic Textual Relatedness Datasets for 14 Languages
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Exploring and quantifying semantic relatedness is central to representing language.It holds significant implications across various NLP tasks, including offering insights into the capabilities and performance of Large Language Models (LLMs).While earlier NLP research primarily focused on semantic similarity, often within the English language context, we instead investigate the broader phenomenon of semantic relatedness.<span class='px-1 mx-1 bg-yellow-200'>In this paper, we present SemRel, a new semantic relatedness dataset collection annotated by native speakers across 14 languages:Afrikaans, Algerian Arabic, Amharic, English, Hausa, Hindi, Indonesian, Kinyarwanda, Marathi, Moroccan Arabic, Modern Standard Arabic, Punjabi, Spanish, and Telugu. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.719</span></span>These languages originate from five distinct language families and are predominantly spoken in Africa and Asia -- regions characterised by a relatively limited availability of NLP resources.Each instance in the SemRel datasets is a sentence pair associated with a score that represents the degree of semantic textual relatedness between the two sentences.The scores are obtained using a comparative annotation framework.<span class='px-1 mx-1 bg-yellow-200'>We describe the data collection and annotation processes, related challenges when building the datasets, and their impact and utility in NLP. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.707</span></span>We further report experiments for each language and across the different languages.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2402.08638v1' target="_blank">
                  link
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-02-12</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Interaction-Based Driving Scenario Classification and Labeling
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Scenario data play a vital role in autonomous driving related researches, and it is essential to obtain refined descriptions and labels to extract and index scenarios with different types of interactions.However, existing methods cannot cope well with the problem of scenario classification and comparison with vehicle interactions as the core.In this paper, we propose a framework for interaction-based refined scenario classification and labeling.Based on the summarized basic types of vehicle interactions, we slice scenario data stream into a series of scenario segments via spatiotemporal scenario evolution tree.<span class='px-1 mx-1 bg-yellow-200'>The scenario segment statistics of many published scenario datasets are further analyzed. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.732</span></span>We also propose the scenario metric Graph-DTW based on Graph Computation Tree and Dynamic Time Warping to conduct refined scenario comparison and labeling.The extreme interactive scenarios and corner cases can be efficiently filtered and extracted.Moreover, testing examples on trajectory prediction model demonstrate the effectiveness and advantages of scenario labeling and the proposed metric.The overall framework can provide solid support for the usage and indexing of scenario data.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2402.07720v1' target="_blank">
                  link
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-02-12</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Asking Multimodal Clarifying Questions in Mixed-Initiative Conversational Search
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>In mixed-initiative conversational search systems, clarifying questions are used to help users who struggle to express their intentions in a single query.These questions aim to uncover user's information needs and resolve query ambiguities.We hypothesize that in scenarios where multimodal information is pertinent, the clarification process can be improved by using non-textual information.Therefore, we propose to add images to clarifying questions and formulate the novel task of asking multimodal clarifying questions in open-domain, mixed-initiative conversational search systems.<span class='px-1 mx-1 bg-yellow-200'>To facilitate research into this task, we collect a dataset named Melon that contains over 4k multimodal clarifying questions, enriched with over 14k images. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.895</span></span>We also propose a multimodal query clarification model named Marto and adopt a prompt-based, generative fine-tuning strategy to perform the training of different stages with different prompts.Several analyses are conducted to understand the importance of multimodal contents during the query clarification phase.Experimental results indicate that the addition of images leads to significant improvements of up to 90% in retrieval performance when selecting the relevant images.Extensive analyses are also performed to show the superiority of Marto compared with discriminative baselines in terms of effectiveness and efficiency.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2402.07742v1' target="_blank">
                  link
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-02-12</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Text Detoxification as Style Transfer in English and Hindi
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>This paper focuses on text detoxification, i.e., automatically converting toxic text into non-toxic text.This task contributes to safer and more respectful online communication and can be considered a Text Style Transfer (TST) task, where the text style changes while its content is preserved.We present three approaches: knowledge transfer from a similar task, multi-task learning approach, combining sequence-to-sequence modeling with various toxicity classification tasks, and, delete and reconstruct approach.To support our research, we utilize a dataset provided by Dementieva et al.(2021), which contains multiple versions of detoxified texts corresponding to toxic texts.In our experiments, we selected the best variants through expert human annotators, creating a dataset where each toxic sentence is paired with a single, appropriate detoxified version.<span class='px-1 mx-1 bg-yellow-200'>Additionally, we introduced a small Hindi parallel dataset, aligning with a part of the English dataset, suitable for evaluation purposes. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.772</span></span>Our results demonstrate that our approach effectively balances text detoxication while preserving the actual content and maintaining fluency.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2402.07767v1' target="_blank">
                  link
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-02-12</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                TELLER: A Trustworthy Framework for Explainable, Generalizable and Controllable Fake News Detection
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>The proliferation of fake news has emerged as a severe societal problem, raising significant interest from industry and academia.While existing deep-learning based methods have made progress in detecting fake news accurately, their reliability may be compromised caused by the non-transparent reasoning processes, poor generalization abilities and inherent risks of integration with large language models (LLMs).To address this challenge, we propose {\methodname}, a novel framework for trustworthy fake news detection that prioritizes explainability, generalizability and controllability of models.This is achieved via a dual-system framework that integrates cognition and decision systems, adhering to the principles above.The cognition system harnesses human expertise to generate logical predicates, which guide LLMs in generating human-readable logic atoms.Meanwhile, the decision system deduces generalizable logic rules to aggregate these atoms, enabling the identification of the truthfulness of the input news across diverse domains and enhancing transparency in the decision-making process.<span class='px-1 mx-1 bg-yellow-200'>Finally, we present comprehensive evaluation results on four datasets, demonstrating the feasibility and trustworthiness of our proposed framework. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.703</span></span>Our implementation is available at \url{https://github.com/less-and-less-bugs/Trust_TELLER}.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2402.07776v1' target="_blank">
                  link
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-02-12</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                A Benchmark Grocery Dataset of Realworld Point Clouds From Single View
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Fine-grained grocery object recognition is an important computer vision problem with broad applications in automatic checkout, in-store robotic navigation, and assistive technologies for the visually impaired.<span class='px-1 mx-1 bg-yellow-200'>Existing datasets on groceries are mainly 2D images. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.753</span></span>Models trained on these datasets are limited to learning features from the regular 2D grids.While portable 3D sensors such as Kinect were commonly available for mobile phones, sensors such as LiDAR and TrueDepth, have recently been integrated into mobile phones.Despite the availability of mobile 3D sensors, there are currently no dedicated real-world large-scale benchmark 3D datasets for grocery.In addition, existing 3D datasets lack fine-grained grocery categories and have limited training samples.Furthermore, collecting data by going around the object versus the traditional photo capture makes data collection cumbersome.<span class='px-1 mx-1 bg-yellow-200'>Thus, we introduce a large-scale grocery dataset called 3DGrocery100. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.862</span></span>It constitutes 100 classes, with a total of 87,898 3D point clouds created from 10,755 RGB-D single-view images.We benchmark our dataset on six recent state-of-the-art 3D point cloud classification models.Additionally, we also benchmark the dataset on few-shot and continual learning point cloud classification tasks.Project Page: https://bigdatavision.org/3DGrocery100/.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2402.07819v1' target="_blank">
                  link
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-02-12</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Aya Model: An Instruction Finetuned Open-Access Multilingual Language Model
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Recent breakthroughs in large language models (LLMs) have centered around a handful of data-rich languages.What does it take to broaden access to breakthroughs beyond first-class citizen languages?Our work introduces Aya, a massively multilingual generative language model that follows instructions in 101 languages of which over 50% are considered as lower-resourced.Aya outperforms mT0 and BLOOMZ on the majority of tasks while covering double the number of languages.We introduce extensive new evaluation suites that broaden the state-of-art for multilingual eval across 99 languages -- including discriminative and generative tasks, human evaluation, and simulated win rates that cover both held-out tasks and in-distribution performance.Furthermore, we conduct detailed investigations on the optimal finetuning mixture composition, data pruning, as well as the toxicity, bias, and safety of our models.<span class='px-1 mx-1 bg-yellow-200'>We open-source our instruction datasets and our model at https://hf.co/CohereForAI/aya-101 <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.719</span></span></p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2402.07827v1' target="_blank">
                  link
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-02-12</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Lissard: Long and Simple Sequential Reasoning Datasets
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Language models are now capable of solving tasks that require dealing with long sequences consisting of hundreds of thousands of tokens.However, they often fail on tasks that require repetitive use of simple rules, even on sequences that are much shorter than those seen during training.For example, state-of-the-art LLMs can find common items in two lists with up to 20 items but fail when lists have 80 items.In this paper, we introduce Lissard, a benchmark comprising seven tasks whose goal is to assess the ability of models to process and generate wide-range sequence lengths, requiring repetitive procedural execution.Our evaluation of open-source (Mistral-7B and Mixtral-8x7B) and proprietary models (GPT-3.5 and GPT-4) show a consistent decline in performance across all models as the complexity of the sequence increases.<span class='px-1 mx-1 bg-yellow-200'>The datasets and code are available at https://github.com/unicamp-dl/Lissard <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.862</span></span></p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2402.07859v1' target="_blank">
                  link
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-02-12</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                MODIPHY: Multimodal Obscured Detection for IoT using PHantom Convolution-Enabled Faster YOLO
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Low-light conditionsandoccluded scenarios impede object detection in real-world Internet of Things (IoT) applications like autonomous vehicles and security systems.While advanced machine learning models strive for accuracy, their computational demands clash with the limitations of resource-constrained devices, hampering real-time performance.In our current research, we tackle this challenge, by introducing "YOLO Phantom", one of the smallest YOLO models ever conceived.YOLO Phantom utilizes the novel Phantom Convolution block, achieving comparable accuracy to the latest YOLOv8n model while simultaneously reducing both parameters and model size by 43%, resulting in a significant 19% reduction in Giga Floating Point Operations (GFLOPs).YOLO Phantom leverages transfer learning on our multimodal RGB-infrared dataset to address low-light and occlusion issues, equipping it with robust vision under adverse conditions.Its real-world efficacy is demonstrated on an IoT platform with advanced low-light and RGB cameras, seamlessly connecting to an AWS-based notification endpoint for efficient real-time object detection.Benchmarks reveal a substantial boost of 17% and 14% in frames per second (FPS) for thermal and RGB detection, respectively, compared to the baseline YOLOv8n model.<span class='px-1 mx-1 bg-yellow-200'>For community contribution, both the code and the multimodal dataset are available on GitHub. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.816</span></span></p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2402.07894v1' target="_blank">
                  link
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-02-08</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Editable Scene Simulation for Autonomous Driving via Collaborative LLM-Agents
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Scene simulation in autonomous driving has gained significant attention because of its huge potential for generating customized data.However, existing editable scene simulation approaches face limitations in terms of user interaction efficiency, multi-camera photo-realistic rendering and external digital assets integration.<span class='px-1 mx-1 bg-yellow-200'>To address these challenges, this paper introduces ChatSim, the first system that enables editable photo-realistic 3D driving scene simulations via natural language commands with external digital assets. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.705</span></span>To enable editing with high command flexibility,~ChatSim leverages a large language model (LLM) agent collaboration framework.To generate photo-realistic outcomes, ChatSim employs a novel multi-camera neural radiance field method.Furthermore, to unleash the potential of extensive high-quality digital assets, ChatSim employs a novel multi-camera lighting estimation method to achieve scene-consistent assets' rendering.<span class='px-1 mx-1 bg-yellow-200'>Our experiments on Waymo Open Dataset demonstrate that ChatSim can handle complex language commands and generate corresponding photo-realistic scene videos. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.733</span></span></p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2402.05746v1' target="_blank">
                  link
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-02-08</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Jacquard V2: Refining Datasets using the Human In the Loop Data Correction Method
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>In the context of rapid advancements in industrial automation, vision-based robotic grasping plays an increasingly crucial role.In order to enhance visual recognition accuracy, the utilization of large-scale datasets is imperative for training models to acquire implicit knowledge related to the handling of various objects.<span class='px-1 mx-1 bg-yellow-200'>Creating datasets from scratch is a time and labor-intensive process. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.741</span></span>Moreover, existing datasets often contain errors due to automated annotations aimed at expediency, making the improvement of these datasets a substantial research challenge.Consequently, several issues have been identified in the annotation of grasp bounding boxes within the popular Jacquard Grasp.<span class='px-1 mx-1 bg-yellow-200'>We propose utilizing a Human-In-The-Loop(HIL) method to enhance dataset quality. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.729</span></span>This approach relies on backbone deep learning networks to predict object positions and orientations for robotic grasping.Predictions with Intersection over Union (IOU) values below 0.2 undergo an assessment by human operators.After their evaluation, the data is categorized into False Negatives(FN) and True Negatives(TN).FN are then subcategorized into either missing annotations or catastrophic labeling errors.Images lacking labels are augmented with valid grasp bounding box information, whereas images afflicted by catastrophic labeling errors are completely removed.<span class='px-1 mx-1 bg-yellow-200'>The open-source tool Labelbee was employed for 53,026 iterations of HIL dataset enhancement, leading to the removal of 2,884 images and the incorporation of ground truth information for 30,292 images. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.735</span></span><span class='px-1 mx-1 bg-yellow-200'>The enhanced dataset, named the Jacquard V2 Grasping Dataset, served as the training data for a range of neural networks. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.809</span></span></p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2402.05747v1' target="_blank">
                  link
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-02-08</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                UAV-Rain1k: A Benchmark for Raindrop Removal from UAV Aerial Imagery
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Raindrops adhering to the lens of UAVs can obstruct visibility of the background scene and degrade image quality.Despite recent progress in image deraining methods and datasets, there is a lack of focus on raindrop removal from UAV aerial imagery due to the unique challenges posed by varying angles and rapid movement during drone flight.To fill the gap in this research, we first construct a new benchmark dataset for removing raindrops from UAV images, called UAV-Rain1k.<span class='px-1 mx-1 bg-yellow-200'>In this letter, we provide a dataset generation pipeline, which includes modeling raindrop shapes using Blender, collecting background images from various UAV angles, random sampling of rain masks and etc. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.84</span></span>Based on the proposed benchmark, we further present a comprehensive evaluation of existing representative image deraining algorithms, and reveal future research opportunities worth exploring.<span class='px-1 mx-1 bg-yellow-200'>The proposed dataset will be publicly available at https://github.com/cschenxiang/UAV-Rain1k. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.891</span></span></p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2402.05773v1' target="_blank">
                  link
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-02-08</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Examining Gender and Racial Bias in Large Vision-Language Models Using a Novel Dataset of Parallel Images
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Following on recent advances in large language models (LLMs) and subsequent chat models, a new wave of large vision-language models (LVLMs) has emerged.Such models can incorporate images as input in addition to text, and perform tasks such as visual question answering, image captioning, story generation, etc.Here, we examine potential gender and racial biases in such systems, based on the perceived characteristics of the people in the input images.<span class='px-1 mx-1 bg-yellow-200'>To accomplish this, we present a new dataset PAIRS (PArallel Images for eveRyday Scenarios). <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.875</span></span><span class='px-1 mx-1 bg-yellow-200'>The PAIRS dataset contains sets of AI-generated images of people, such that the images are highly similar in terms of background and visual content, but differ along the dimensions of gender (man, woman) and race (Black, white). <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.767</span></span>By querying the LVLMs with such images, we observe significant differences in the responses according to the perceived gender or race of the person depicted.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2402.05779v1' target="_blank">
                  link
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-02-08</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Visual Harmony: Text-Visual Interplay in Circular Infographics
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Infographics are visual representations designed for efficient and effective communication of data and knowledge.One crucial aspect of infographic design is the interplay between text and visual elements, particularly in circular visualizations where the textual descriptions can either be embedded within the graphics or placed adjacent to the visual representation.While several studies have examined text layout design in visualizations in general, the text-visual interplay in infographics and its subsequent perceptual effects remain underexplored.To address this, our study investigates how varying text placement and descriptiveness impact pleasantness, comprehension and overall memorability in the infographics viewing experience.<span class='px-1 mx-1 bg-yellow-200'>We recruited 30 participants and presented them with a collection of 15 infographics across a diverse set of topics, including media and public events, health and nutrition, science and research, and sustainability. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.704</span></span>The text placement (embed, side-to-side) and descriptiveness (simplistic, normal, descriptive) were systematically manipulated, resulting in a total of six experimental conditions.Our key findings indicate that text placement can significantly influence the memorability of infographics, whereas descriptiveness can significantly impact the pleasantness of the viewing experience.Embedding text placement and simplistic text can potentially contribute to more effective infographic designs.These results offer valuable insights for infographic designers, contributing to the creation of more effective and memorable visual representations.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2402.05798v1' target="_blank">
                  link
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-02-08</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception Uncertainties
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>The availability of a reliable map and a robust localization system is critical for the operation of an autonomous vehicle.In a modern system, both mapping and localization solutions generally employ convolutional neural network (CNN) --based perception.Hence, any algorithm should consider potential errors in perception for safe and robust functioning.In this work, we present uncertainty-aware panoptic Localization and Mapping (uPLAM), which employs perception uncertainty as a bridge to fuse the perception information with classical localization and mapping approaches.We introduce an uncertainty-based map aggregation technique to create a long-term panoptic bird's eye view map and provide an associated mapping uncertainty.Our map consists of surface semantics and landmarks with unique IDs.Moreover, we present panoptic uncertainty-aware particle filter-based localization.To this end, we propose an uncertainty-based particle importance weight calculation for the adaptive incorporation of perception information into localization.We also present a new dataset for evaluating long-term panoptic mapping and map-based localization.Extensive evaluations showcase that our proposed uncertainty incorporation leads to better mapping with reliable uncertainty estimates and accurate localization.<span class='px-1 mx-1 bg-yellow-200'>We make our dataset and code available at: \url{http://uplam.cs.uni-freiburg.de} <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.827</span></span></p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2402.05840v1' target="_blank">
                  link
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-02-08</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                PromptCrypt: Prompt Encryption for Secure Communication with Large Language Models
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Cloud-based large language models (LLMs) such as ChatGPT have increasingly become integral to daily operations, serving as vital tools across various applications.While these models offer substantial benefits in terms of accessibility and functionality, they also introduce significant privacy concerns: the transmission and storage of user data in cloud infrastructures pose substantial risks of data breaches and unauthorized access to sensitive information; even if the transmission and storage of data is encrypted, the LLM service provider itself still knows the real contents of the data, preventing individuals or entities from confidently using such LLM services.To address these concerns, this paper proposes a simple yet effective mechanism PromptCrypt to protect user privacy.It uses Emoji to encrypt the user inputs before sending them to LLM, effectively rendering them indecipherable to human or LLM's examination while retaining the original intent of the prompt, thus ensuring the model's performance remains unaffected.We conduct experiments on three tasks, personalized recommendation, sentiment analysis, and tabular data analysis.Experiment results reveal that PromptCrypt can encrypt personal information within prompts in such a manner that not only prevents the discernment of sensitive data by humans or LLM itself, but also maintains or even improves the precision without further tuning, achieving comparable or even better task accuracy than directly prompting the LLM without prompt encryption.These results highlight the practicality of adopting encryption measures that safeguard user privacy without compromising the functional integrity and performance of LLMs.<span class='px-1 mx-1 bg-yellow-200'>Code and dataset are available at https://github.com/agiresearch/PromptCrypt. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.774</span></span></p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2402.05868v1' target="_blank">
                  link
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-02-08</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                GET-Tok: A GenAI-Enriched Multimodal TikTok Dataset Documenting the 2022 Attempted Coup in Peru
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>TikTok is one of the largest and fastest-growing social media sites in the world.TikTok features, however, such as voice transcripts, are often missing and other important features, such as OCR or video descriptions, do not exist.We introduce the Generative AI Enriched TikTok (GET-Tok) data, a pipeline for collecting TikTok videos and enriched data by augmenting the TikTok Research API with generative AI models.As a case study, we collect videos about the attempted coup in Peru initiated by its former President, Pedro Castillo, and its accompanying protests.<span class='px-1 mx-1 bg-yellow-200'>The data includes information on 43,697 videos published from November 20, 2022 to March 1, 2023 (102 days). <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.868</span></span>Generative AI augments the collected data via transcripts of TikTok videos, text descriptions of what is shown in the videos, what text is displayed within the video, and the stances expressed in the video.Overall, this pipeline will contribute to a better understanding of online discussion in a multimodal setting with applications of Generative AI, especially outlining the utility of this pipeline in non-English-language social media.<span class='px-1 mx-1 bg-yellow-200'>Our code used to produce the pipeline is in a public Github repository: https://github.com/gabbypinto/GET-Tok-Peru. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.703</span></span></p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2402.05882v1' target="_blank">
                  link
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-02-08</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Point-VOS: Pointing Up Video Object Segmentation
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Current state-of-the-art Video Object Segmentation (VOS) methods rely on dense per-object mask annotations both during training and testing.This requires time-consuming and costly video annotation mechanisms.We propose a novel Point-VOS task with a spatio-temporally sparse point-wise annotation scheme that substantially reduces the annotation effort.<span class='px-1 mx-1 bg-yellow-200'>We apply our annotation scheme to two large-scale video datasets with text descriptions and annotate over 19M points across 133K objects in 32K videos. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.832</span></span>Based on our annotations, we propose a new Point-VOS benchmark, and a corresponding point-based training mechanism, which we use to establish strong baseline results.We show that existing VOS methods can easily be adapted to leverage our point annotations during training, and can achieve results close to the fully-supervised performance when trained on pseudo-masks generated from these points.In addition, we show that our data can be used to improve models that connect vision and language, by evaluating it on the Video Narrative Grounding (VNG) task.We will make our code and annotations available at https://pointvos.github.io.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2402.05917v1' target="_blank">
                  link
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-02-08</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                SPHINX-X: Scaling Data and Parameters for a Family of Multi-modal Large Language Models
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>We propose SPHINX-X, an extensive Multimodality Large Language Model (MLLM) series developed upon SPHINX.To improve the architecture and training efficiency, we modify the SPHINX framework by removing redundant visual encoders, bypassing fully-padded sub-images with skip tokens, and simplifying multi-stage training into a one-stage all-in-one paradigm.<span class='px-1 mx-1 bg-yellow-200'>To fully unleash the potential of MLLMs, we assemble a comprehensive multi-domain and multimodal dataset covering publicly available resources in language, vision, and vision-language tasks. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.728</span></span><span class='px-1 mx-1 bg-yellow-200'>We further enrich this collection with our curated OCR intensive and Set-of-Mark datasets, extending the diversity and generality. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.913</span></span>By training over different base LLMs including TinyLlama1.1B, InternLM2-7B, LLaMA2-13B, and Mixtral8x7B, we obtain a spectrum of MLLMs that vary in parameter size and multilingual capabilities.Comprehensive benchmarking reveals a strong correlation between the multi-modal performance with the data and parameter scales.Code and models are released at https://github.com/Alpha-VLLM/LLaMA2-Accessory</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2402.05935v1' target="_blank">
                  link
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-02-07</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                A Longitudinal Study of Italian and French Reddit Conversations Around the Russian Invasion of Ukraine
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Global events like wars and pandemics can intensify online discussions, fostering information sharing and connection among individuals.However, the divisive nature of such events may lead to polarization within online communities, shaping the dynamics of online interactions.Our study delves into the conversations within the largest Italian and French Reddit communities, specifically examining how the Russian invasion of Ukraine affected online interactions.<span class='px-1 mx-1 bg-yellow-200'>We use a dataset with over 3 million posts (i.e., comments and submissions) to (1) describe the patterns of moderation activity and (2) characterize war-related discussions in the subreddits. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.756</span></span>We found changes in moderators' behavior, who became more active during the first month of the war.Moreover, we identified a connection between the daily sentiment of comments and the prevalence of war-related discussions.These discussions were not only more negative and toxic compared to non-war-related ones but also did not involve a specific demographic group.Our research reveals that there is no tendency for users with similar characteristics to interact more.Overall, our study reveals how the war in Ukraine had a negative influence on daily conversations in the analyzed communities.This sheds light on how users responded to this significant event, providing insights into the dynamics of online discussions during events of global relevance.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2402.04999v1' target="_blank">
                  link
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-02-07</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Does the Use of Unusual Combinations of Datasets Contribute to Greater Scientific Impact?
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Scientific datasets play a crucial role in contemporary data-driven research, as they allow for the progress of science by facilitating the discovery of new patterns and phenomena.This mounting demand for empirical research raises important questions on how strategic data utilization in research projects can stimulate scientific advancement.In this study, we examine the hypothesis inspired by the recombination theory, which suggests that innovative combinations of existing knowledge, including the use of unusual combinations of datasets, can lead to high-impact discoveries.<span class='px-1 mx-1 bg-yellow-200'>We investigate the scientific outcomes of such atypical data combinations in more than 30,000 publications that leverage over 6,000 datasets curated within one of the largest social science databases, ICPSR. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.714</span></span>This study offers four important insights.First, combining datasets, particularly those infrequently paired, significantly contributes to both scientific and broader impacts (e.g., dissemination to the general public).Second, the combination of datasets with atypically combined topics has the opposite effect -- the use of such data is associated with fewer citations.Third, younger and less experienced research teams tend to use atypical combinations of datasets in research at a higher frequency than their older and more experienced counterparts.Lastly, despite the benefits of data combination, papers that amalgamate data remain infrequent.This finding suggests that the unconventional combination of datasets is an under-utilized but powerful strategy correlated with the scientific and broader impact of scientific discoveries.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2402.05024v1' target="_blank">
                  link
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-02-07</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                A Survey on Domain Generalization for Medical Image Analysis
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Medical Image Analysis (MedIA) has emerged as a crucial tool in computer-aided diagnosis systems, particularly with the advancement of deep learning (DL) in recent years.However, well-trained deep models often experience significant performance degradation when deployed in different medical sites, modalities, and sequences, known as a domain shift issue.In light of this, Domain Generalization (DG) for MedIA aims to address the domain shift challenge by generalizing effectively and performing robustly across unknown data distributions.This paper presents the a comprehensive review of substantial developments in this area.First, we provide a formal definition of domain shift and domain generalization in medical field, and discuss several related settings.Subsequently, we summarize the recent methods from three viewpoints: data manipulation level, feature representation level, and model training level, and present some algorithms in detail for each viewpoints.<span class='px-1 mx-1 bg-yellow-200'>Furthermore, we introduce the commonly used datasets. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.791</span></span>Finally, we summarize existing literature and present some potential research topics for the future.For this survey, we also created a GitHub project by collecting the supporting resources, at the link: https://github.com/Ziwei-Niu/DG_for_MedIA</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2402.05035v1' target="_blank">
                  link
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-02-06</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                The Use of a Large Language Model for Cyberbullying Detection
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>The dominance of social media has added to the channels of bullying for perpetrators.Unfortunately, cyberbullying (CB) is the most prevalent phenomenon in todays cyber world, and is a severe threat to the mental and physical health of citizens.This opens the need to develop a robust system to prevent bullying content from online forums, blogs, and social media platforms to manage the impact in our society.Several machine learning (ML) algorithms have been proposed for this purpose.However, their performances are not consistent due to high class imbalance and generalisation issues.In recent years, large language models (LLMs) like BERT and RoBERTa have achieved state-of-the-art (SOTA) results in several natural language processing (NLP) tasks.Unfortunately, the LLMs have not been applied extensively for CB detection.In our paper, we explored the use of these models for cyberbullying (CB) detection.<span class='px-1 mx-1 bg-yellow-200'>We have prepared a new dataset (D2) from existing studies (Formspring and Twitter). <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.729</span></span>Our experimental results for dataset D1 and D2 showed that RoBERTa outperformed other models.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2402.04088v1' target="_blank">
                  link
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-02-06</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                An Exploration of Clustering Algorithms for Customer Segmentation in the UK Retail Market
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Recently, peoples awareness of online purchases has significantly risen.This has given rise to online retail platforms and the need for a better understanding of customer purchasing behaviour.Retail companies are pressed with the need to deal with a high volume of customer purchases, which requires sophisticated approaches to perform more accurate and efficient customer segmentation.Customer segmentation is a marketing analytical tool that aids customer-centric service and thus enhances profitability.In this paper, we aim to develop a customer segmentation model to improve decision-making processes in the retail market industry.<span class='px-1 mx-1 bg-yellow-200'>To achieve this, we employed a UK-based online retail dataset obtained from the UCI machine learning repository. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.784</span></span><span class='px-1 mx-1 bg-yellow-200'>The retail dataset consists of 541,909 customer records and eight features. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.904</span></span>Our study adopted the RFM (recency, frequency, and monetary) framework to quantify customer values.Thereafter, we compared several state-of-the-art (SOTA) clustering algorithms, namely, K-means clustering, the Gaussian mixture model (GMM), density-based spatial clustering of applications with noise (DBSCAN), agglomerative clustering, and balanced iterative reducing and clustering using hierarchies (BIRCH).The results showed the GMM outperformed other approaches, with a Silhouette Score of 0.80.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2402.04103v1' target="_blank">
                  link
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-02-06</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                EVA-CLIP-18B: Scaling CLIP to 18 Billion Parameters
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Scaling up contrastive language-image pretraining (CLIP) is critical for empowering both vision and multimodal models.We present EVA-CLIP-18B, the largest and most powerful open-source CLIP model to date, with 18-billion parameters.With only 6-billion training samples seen, EVA-CLIP-18B achieves an exceptional 80.7% zero-shot top-1 accuracy averaged across 27 widely recognized image classification benchmarks, outperforming its forerunner EVA-CLIP (5-billion parameters) and other open-source CLIP models by a large margin.Remarkably, we observe a consistent performance improvement with the model size scaling of EVA-CLIP, despite maintaining a constant training dataset of 2-billion image-text pairs from LAION-2B and COYO-700M.<span class='px-1 mx-1 bg-yellow-200'>This dataset is openly available and much smaller than the in-house datasets (e.g., DFN-5B, WebLI-10B) employed in other state-of-the-art CLIP models. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.899</span></span>EVA-CLIP-18B demonstrates the potential of EVA-style weak-to-strong visual model scaling.With our model weights made publicly available, we hope to facilitate future research in vision and multimodal foundation models.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2402.04252v1' target="_blank">
                  link
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-02-05</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                JOBSKAPE: A Framework for Generating Synthetic Job Postings to Enhance Skill Matching
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Recent approaches in skill matching, employing synthetic training data for classification or similarity model training, have shown promising results, reducing the need for time-consuming and expensive annotations.However, previous synthetic datasets have limitations, such as featuring only one skill per sentence and generally comprising short sentences.In this paper, we introduce JobSkape, a framework to generate synthetic data that tackles these limitations, specifically designed to enhance skill-to-taxonomy matching.<span class='px-1 mx-1 bg-yellow-200'>Within this framework, we create SkillSkape, a comprehensive open-source synthetic dataset of job postings tailored for skill-matching tasks. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.785</span></span>We introduce several offline metrics that show that our dataset resembles real-world data.Additionally, we present a multi-step pipeline for skill extraction and matching tasks using large language models (LLMs), benchmarking against known supervised methodologies.We outline that the downstream evaluation results on real-world data can beat baselines, underscoring its efficacy and adaptability.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2402.03242v1' target="_blank">
                  link
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td></td>
          <td>
            <h2 class="text-2xl tracking-tight pt-4 font-bold">Data Quality</h2>
          </td>
        </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-02-08</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Jacquard V2: Refining Datasets using the Human In the Loop Data Correction Method
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>In the context of rapid advancements in industrial automation, vision-based robotic grasping plays an increasingly crucial role.In order to enhance visual recognition accuracy, the utilization of large-scale datasets is imperative for training models to acquire implicit knowledge related to the handling of various objects.Creating datasets from scratch is a time and labor-intensive process.Moreover, existing datasets often contain errors due to automated annotations aimed at expediency, making the improvement of these datasets a substantial research challenge.Consequently, several issues have been identified in the annotation of grasp bounding boxes within the popular Jacquard Grasp.We propose utilizing a Human-In-The-Loop(HIL) method to enhance dataset quality.This approach relies on backbone deep learning networks to predict object positions and orientations for robotic grasping.Predictions with Intersection over Union (IOU) values below 0.2 undergo an assessment by human operators.After their evaluation, the data is categorized into False Negatives(FN) and True Negatives(TN).<span class='px-1 mx-1 bg-yellow-200'>FN are then subcategorized into either missing annotations or catastrophic labeling errors. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.629</span></span>Images lacking labels are augmented with valid grasp bounding box information, whereas images afflicted by catastrophic labeling errors are completely removed.The open-source tool Labelbee was employed for 53,026 iterations of HIL dataset enhancement, leading to the removal of 2,884 images and the incorporation of ground truth information for 30,292 images.The enhanced dataset, named the Jacquard V2 Grasping Dataset, served as the training data for a range of neural networks.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2402.05747v1' target="_blank">
                  link
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-02-07</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Channel-Selective Normalization for Label-Shift Robust Test-Time Adaptation
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Deep neural networks have useful applications in many different tasks, however their performance can be severely affected by changes in the data distribution.For example, in the biomedical field, their performance can be affected by changes in the data (different machines, populations) between training and test datasets.To ensure robustness and generalization to real-world scenarios, test-time adaptation has been recently studied as an approach to adjust models to a new data distribution during inference.Test-time batch normalization is a simple and popular method that achieved compelling performance on domain shift benchmarks.It is implemented by recalculating batch normalization statistics on test batches.Prior work has focused on analysis with test data that has the same label distribution as the training data.<span class='px-1 mx-1 bg-yellow-200'>However, in many practical applications this technique is vulnerable to label distribution shifts, sometimes producing catastrophic failure. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.66</span></span>This presents a risk in applying test time adaptation methods in deployment.We propose to tackle this challenge by only selectively adapting channels in a deep network, minimizing drastic adaptation that is sensitive to label shifts.Our selection scheme is based on two principles that we empirically motivate: (1) later layers of networks are more sensitive to label shift (2) individual features can be sensitive to specific classes.We apply the proposed technique to three classification tasks, including CIFAR10-C, Imagenet-C, and diagnosis of fatty liver, where we explore both covariate and label distribution shifts.We find that our method allows to bring the benefits of TTA while significantly reducing the risk of failure common in other methods, while being robust to choice in hyperparameters.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2402.04958v1' target="_blank">
                  link
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-02-05</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Constrained Decoding for Cross-lingual Label Projection
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Zero-shot cross-lingual transfer utilizing multilingual LLMs has become a popular learning paradigm for low-resource languages with no labeled training data.However, for NLP tasks that involve fine-grained predictions on words and phrases, the performance of zero-shot cross-lingual transfer learning lags far behind supervised fine-tuning methods.Therefore, it is common to exploit translation and label projection to further improve the performance by (1) translating training data that is available in a high-resource language (e.g., English) together with the gold labels into low-resource languages, and/or (2) translating test data in low-resource languages to a high-source language to run inference on, then projecting the predicted span-level labels back onto the original test data.<span class='px-1 mx-1 bg-yellow-200'>However, state-of-the-art marker-based label projection methods suffer from translation quality degradation due to the extra label markers injected in the input to the translation model. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.618</span></span>In this work, we explore a new direction that leverages constrained decoding for label projection to overcome the aforementioned issues.Our new method not only can preserve the quality of translated texts but also has the versatility of being applicable to both translating training and translating test data strategies.This versatility is crucial as our experiments reveal that translating test data can lead to a considerable boost in performance compared to translating only training data.We evaluate on two cross-lingual transfer tasks, namely Named Entity Recognition and Event Argument Extraction, spanning 20 languages.The results demonstrate that our approach outperforms the state-of-the-art marker-based method by a large margin and also shows better performance than other label projection methods that rely on external word alignment.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2402.03131v1' target="_blank">
                  link
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-01-30</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                CPR++: Object Localization via Single Coarse Point Supervision
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Point-based object localization (POL), which pursues high-performance object sensing under low-cost data annotation, has attracted increased attention.<span class='px-1 mx-1 bg-yellow-200'>However, the point annotation mode inevitably introduces semantic variance due to the inconsistency of annotated points. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.68</span></span>Existing POL heavily rely on strict annotation rules, which are difficult to define and apply, to handle the problem.In this study, we propose coarse point refinement (CPR), which to our best knowledge is the first attempt to alleviate semantic variance from an algorithmic perspective.CPR reduces the semantic variance by selecting a semantic centre point in a neighbourhood region to replace the initial annotated point.Furthermore, We design a sampling region estimation module to dynamically compute a sampling region for each object and use a cascaded structure to achieve end-to-end optimization.We further integrate a variance regularization into the structure to concentrate the predicted scores, yielding CPR++.We observe that CPR++ can obtain scale information and further reduce the semantic variance in a global region, thus guaranteeing high-performance object localization.Extensive experiments on four challenging datasets validate the effectiveness of both CPR and CPR++.We hope our work can inspire more research on designing algorithms rather than annotation rules to address the semantic variance problem in POL.The dataset and code will be public at github.com/ucas-vg/PointTinyBenchmark.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2401.17203v1' target="_blank">
                  link
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-01-29</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Capturing Pertinent Symbolic Features for Enhanced Content-Based Misinformation Detection
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Preventing the spread of misinformation is challenging.<span class='px-1 mx-1 bg-yellow-200'>The detection of misleading content presents a significant hurdle due to its extreme linguistic and domain variability. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.642</span></span>Content-based models have managed to identify deceptive language by learning representations from textual data such as social media posts and web articles.However, aggregating representative samples of this heterogeneous phenomenon and implementing effective real-world applications is still elusive.<span class='px-1 mx-1 bg-yellow-200'>Based on analytical work on the language of misinformation, this paper analyzes the linguistic attributes that characterize this phenomenon and how representative of such features some of the most popular misinformation datasets are. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.697</span></span>We demonstrate that the appropriate use of pertinent symbolic knowledge in combination with neural language models is helpful in detecting misleading content.Our results achieve state-of-the-art performance in misinformation datasets across the board, showing that our approach offers a valid and robust alternative to multi-task transfer learning without requiring any additional training data.Furthermore, our results show evidence that structured knowledge can provide the extra boost required to address a complex and unpredictable real-world problem like misinformation detection, not only in terms of accuracy but also time efficiency and resource utilization.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2401.16285v1' target="_blank">
                  link
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-01-25</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Class-attribute Priors: Adapting Optimization to Heterogeneity and Fairness Objective
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Modern classification problems exhibit heterogeneities across individual classes: Each class may have unique attributes, such as sample size, label quality, or predictability (easy vs difficult), and variable importance at test-time.Without care, these heterogeneities impede the learning process, most notably, when optimizing fairness objectives.Confirming this, under a gaussian mixture setting, we show that the optimal SVM classifier for balanced accuracy needs to be adaptive to the class attributes.This motivates us to propose CAP: An effective and general method that generates a class-specific learning strategy (e.g. hyperparameter) based on the attributes of that class.This way, optimization process better adapts to heterogeneities.CAP leads to substantial improvements over the naive approach of assigning separate hyperparameters to each class.We instantiate CAP for loss function design and post-hoc logit adjustment, with emphasis on label-imbalanced problems.We show that CAP is competitive with prior art and its flexibility unlocks clear benefits for fairness objectives beyond balanced accuracy.<span class='px-1 mx-1 bg-yellow-200'>Finally, we evaluate CAP on problems with label noise as well as weighted test objectives to showcase how CAP can jointly adapt to different heterogeneities. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.604</span></span></p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2401.14343v1' target="_blank">
                  link
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-01-25</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                A Comparative Analysis of Noise Reduction Methods in Sentiment Analysis on Noisy Bengali Texts
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>While Bengali is considered a language with limited resources, sentiment analysis has been a subject of extensive research in the literature.Nevertheless, there is a scarcity of exploration into sentiment analysis specifically in the realm of noisy Bengali texts.In this paper, we introduce a dataset (NC-SentNoB) that we annotated manually to identify ten different types of noise found in a pre-existing sentiment analysis dataset comprising of around 15K noisy Bengali texts.<span class='px-1 mx-1 bg-yellow-200'>At first, given an input noisy text, we identify the noise type, addressing this as a multi-label classification task. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.697</span></span>Then, we introduce baseline noise reduction methods to alleviate noise prior to conducting sentiment analysis.Finally, we assess the performance of fine-tuned sentiment analysis models with both noisy and noise-reduced texts to make comparisons.The experimental findings indicate that the noise reduction methods utilized are not satisfactory, highlighting the need for more suitable noise reduction methods in future research endeavors.We have made the implementation and dataset presented in this paper publicly available at https://github.com/ktoufiquee/A-Comparative-Analysis-of-Noise-Reduction-Methods-in-Sentiment-Analysis-on-Noisy-Bengali-Texts</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2401.14360v1' target="_blank">
                  link
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-01-25</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Inconsistency Masks: Removing the Uncertainty from Input-Pseudo-Label Pairs
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Generating sufficient labeled data is a significant hurdle in the efficient execution of deep learning projects, especially in uncharted territories of image segmentation where labeling demands extensive time, unlike classification tasks.Our study confronts this challenge, operating in an environment constrained by limited hardware resources and the lack of extensive datasets or pre-trained models.<span class='px-1 mx-1 bg-yellow-200'>We introduce the novel use of Inconsistency Masks (IM) to effectively filter uncertainty in image-pseudo-label pairs, substantially elevating segmentation quality beyond traditional semi-supervised learning techniques. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.722</span></span>By integrating IM with other methods, we demonstrate remarkable binary segmentation performance on the ISIC 2018 dataset, starting with just 10% labeled data.Notably, three of our hybrid models outperform those trained on the fully labeled dataset.Our approach consistently achieves exceptional results across three additional datasets and shows further improvement when combined with other techniques.For comprehensive and robust evaluation, this paper includes an extensive analysis of prevalent semi-supervised learning strategies, all trained under identical starting conditions.The full code is available at: https://github.com/MichaelVorndran/InconsistencyMasks</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2401.14387v1' target="_blank">
                  link
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td></td>
          <td>
            <h2 class="text-2xl tracking-tight pt-4 font-bold">Benchmarks</h2>
          </td>
        </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-02-14</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Domain-adaptive and Subgroup-specific Cascaded Temperature Regression for Out-of-distribution Calibration
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Although deep neural networks yield high classification accuracy given sufficient training data, their predictions are typically overconfident or under-confident, i.e., the prediction confidences cannot truly reflect the accuracy.Post-hoc calibration tackles this problem by calibrating the prediction confidences without re-training the classification model.However, current approaches assume congruence between test and validation data distributions, limiting their applicability to out-of-distribution scenarios.To this end, we propose a novel meta-set-based cascaded temperature regression method for post-hoc calibration.<span class='px-1 mx-1 bg-yellow-200'>Our method tailors fine-grained scaling functions to distinct test sets by simulating various domain shifts through data augmentation on the validation set. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.634</span></span>We partition each meta-set into subgroups based on predicted category and confidence level, capturing diverse uncertainties.A regression network is then trained to derive category-specific and confidence-level-specific scaling, achieving calibration across meta-sets.Extensive experimental results on MNIST, CIFAR-10, and TinyImageNet demonstrate the effectiveness of the proposed method.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2402.09204v1' target="_blank">
                  link
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-02-14</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Integrating ytopt and libEnsemble to Autotune OpenMC
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>ytopt is a Python machine-learning-based autotuning software package developed within the ECP PROTEAS-TUNE project.The ytopt software adopts an asynchronous search framework that consists of sampling a small number of input parameter configurations and progressively fitting a surrogate model over the input-output space until exhausting the user-defined maximum number of evaluations or the wall-clock time.libEnsemble is a Python toolkit for coordinating workflows of asynchronous and dynamic ensembles of calculations across massively parallel resources developed within the ECP PETSc/TAO project.libEnsemble helps users take advantage of massively parallel resources to solve design, decision, and inference problems and expands the class of problems that can benefit from increased parallelism.In this paper we present our methodology and framework to integrate ytopt and libEnsemble to take advantage of massively parallel resources to accelerate the autotuning process.Specifically, we focus on using the proposed framework to autotune the ECP ExaSMR application OpenMC, an open source Monte Carlo particle transport code.OpenMC has seven tunable parameters some of which have large ranges such as the number of particles in-flight, which is in the range of 100,000 to 8 million, with its default setting of 1 million.<span class='px-1 mx-1 bg-yellow-200'>Setting the proper combination of these parameter values to achieve the best performance is extremely time-consuming. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.602</span></span>Therefore, we apply the proposed framework to autotune the MPI/OpenMP offload version of OpenMC based on a user-defined metric such as the figure of merit (FoM) (particles/s) or energy efficiency energy-delay product (EDF) on the OLCF Frontier TDS system Crusher.<span class='px-1 mx-1 bg-yellow-200'>The experimental results show that we achieve improvement up to 29.49% in FoM and up to 30.44% in EDP. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.65</span></span></p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2402.09222v1' target="_blank">
                  link
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-02-14</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Efficient One-stage Video Object Detection by Exploiting Temporal Consistency
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Recently, one-stage detectors have achieved competitive accuracy and faster speed compared with traditional two-stage detectors on image data.However, in the field of video object detection (VOD), most existing VOD methods are still based on two-stage detectors.Moreover, directly adapting existing VOD methods to one-stage detectors introduces unaffordable computational costs.In this paper, we first analyse the computational bottlenecks of using one-stage detectors for VOD.Based on the analysis, we present a simple yet efficient framework to address the computational bottlenecks and achieve efficient one-stage VOD by exploiting the temporal consistency in video frames.Specifically, our method consists of a location-prior network to filter out background regions and a size-prior network to skip unnecessary computations on low-level feature maps for specific frames.We test our method on various modern one-stage detectors and conduct extensive experiments on the ImageNet VID dataset.<span class='px-1 mx-1 bg-yellow-200'>Excellent experimental results demonstrate the superior effectiveness, efficiency, and compatibility of our method. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.8</span></span>The code is available at https://github.com/guanxiongsun/vfe.pytorch.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2402.09241v1' target="_blank">
                  link
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-02-14</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                TDViT: Temporal Dilated Video Transformer for Dense Video Tasks
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Deep video models, for example, 3D CNNs or video transformers, have achieved promising performance on sparse video tasks, i.e., predicting one result per video.However, challenges arise when adapting existing deep video models to dense video tasks, i.e., predicting one result per frame.Specifically, these models are expensive for deployment, less effective when handling redundant frames, and difficult to capture long-range temporal correlations.To overcome these issues, we propose a Temporal Dilated Video Transformer (TDViT) that consists of carefully designed temporal dilated transformer blocks (TDTB).TDTB can efficiently extract spatiotemporal representations and effectively alleviate the negative effect of temporal redundancy.Furthermore, by using hierarchical TDTBs, our approach obtains an exponentially expanded temporal receptive field and therefore can model long-range dynamics.Extensive experiments are conducted on two different dense video benchmarks, i.e., ImageNet VID for video object detection and YouTube VIS for video instance segmentation.<span class='px-1 mx-1 bg-yellow-200'>Excellent experimental results demonstrate the superior efficiency, effectiveness, and compatibility of our method. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.809</span></span>The code is available at https://github.com/guanxiongsun/vfe.pytorch.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2402.09257v1' target="_blank">
                  link
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-02-14</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Machine Learning in management of precautionary closures caused by lipophilic biotoxins
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Mussel farming is one of the most important aquaculture industries.The main risk to mussel farming is harmful algal blooms (HABs), which pose a risk to human consumption.In Galicia, the Spanish main producer of cultivated mussels, the opening and closing of the production areas is controlled by a monitoring program.In addition to the closures resulting from the presence of toxicity exceeding the legal threshold, in the absence of a confirmatory sampling and the existence of risk factors, precautionary closures may be applied.These decisions are made by experts without the support or formalisation of the experience on which they are based.Therefore, this work proposes a predictive model capable of supporting the application of precautionary closures.<span class='px-1 mx-1 bg-yellow-200'>Achieving sensitivity, accuracy and kappa index values of 97.34%, 91.83% and 0.75 respectively, the kNN algorithm has provided the best results. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.686</span></span>This allows the creation of a system capable of helping in complex situations where forecast errors are more common.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2402.09266v1' target="_blank">
                  link
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-02-14</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Synergistic eigenanalysis of covariance and Hessian matrices for enhanced binary classification
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Covariance and Hessian matrices have been analyzed separately in the literature for classification problems.However, integrating these matrices has the potential to enhance their combined power in improving classification performance.We present a novel approach that combines the eigenanalysis of a covariance matrix evaluated on a training set with a Hessian matrix evaluated on a deep learning model to achieve optimal class separability in binary classification tasks.Our approach is substantiated by formal proofs that establish its capability to maximize between-class mean distance and minimize within-class variances.By projecting data into the combined space of the most relevant eigendirections from both matrices, we achieve optimal class separability as per the linear discriminant analysis (LDA) criteria.Empirical validation across neural and health datasets consistently supports our theoretical framework and demonstrates that our method outperforms established methods.<span class='px-1 mx-1 bg-yellow-200'>Our method stands out by addressing both LDA criteria, unlike PCA and the Hessian method, which predominantly emphasize one criterion each. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.702</span></span>This comprehensive approach captures intricate patterns and relationships, enhancing classification performance.Furthermore, through the utilization of both LDA criteria, our method outperforms LDA itself by leveraging higher-dimensional feature spaces, in accordance with Cover's theorem, which favors linear separability in higher dimensions.Our method also surpasses kernel-based methods and manifold learning techniques in performance.Additionally, our approach sheds light on complex DNN decision-making, rendering them comprehensible within a 2D space.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2402.09281v1' target="_blank">
                  link
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-02-14</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                ICDPO: Effectively Borrowing Alignment Capability of Others via In-context Direct Preference Optimization
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Large Language Models (LLMs) rely on Human Preference Alignment (HPA) to ensure the generation of safe content.Due to the heavy cost associated with fine-tuning, fine-tuning-free methods have emerged, typically modifying LLM decoding with external auxiliary methods.However, these methods do not essentially enhance the LLM itself.In this paper, we rethink the derivation procedures of DPO, based on which we conversely build an instant scorer using the states of the LLM before and after In-context Learning (ICL).Accordingly, we propose a novel approach called In-Context Direct Preference Optimization (ICDPO).It enables LLMs to borrow the HPA capabilities from superior LLMs with ICL, generating well-aligned responses as estimated by the aforementioned instant scorer, thereby enhancing the final performance.ICDPO can be further enhanced with a two-stage retriever and an upgraded scorer, both offering benefits.<span class='px-1 mx-1 bg-yellow-200'>Extensive experiments show its effectiveness, particularly in outperforming two fine-tuning-free baselines, and it exhibits competitiveness with SFT + LoRA. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.646</span></span>We also conduct detailed analyses to offer comprehensive insights into ICDPO.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2402.09320v1' target="_blank">
                  link
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-02-14</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                DoRA: Weight-Decomposed Low-Rank Adaptation
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Among the widely used parameter-efficient finetuning (PEFT) methods, LoRA and its variants have gained considerable popularity because of avoiding additional inference costs.<span class='px-1 mx-1 bg-yellow-200'>However, there still often exists an accuracy gap between these methods and full fine-tuning (FT). <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.67</span></span>In this work, we first introduce a novel weight decomposition analysis to investigate the inherent differences between FT and LoRA.Aiming to resemble the learning capacity of FT from the findings, we propose Weight-Decomposed LowRank Adaptation (DoRA).DoRA decomposes the pre-trained weight into two components, magnitude and direction, for fine-tuning, specifically employing LoRA for directional updates to efficiently minimize the number of trainable parameters.By employing DoRA, we enhance both the learning capacity and training stability of LoRA while avoiding any additional inference overhead.DoRA consistently outperforms LoRA on fine-tuning LLaMA, LLaVA, and VL-BART on various downstream tasks, such as commonsense reasoning, visual instruction tuning, and image/video-text understanding.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2402.09353v1' target="_blank">
                  link
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-02-14</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Single-Reset Divide & Conquer Imitation Learning
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Demonstrations are commonly used to speed up the learning process of Deep Reinforcement Learning algorithms.To cope with the difficulty of accessing multiple demonstrations, some algorithms have been developed to learn from a single demonstration.In particular, the Divide & Conquer Imitation Learning algorithms leverage a sequential bias to learn a control policy for complex robotic tasks using a single state-based demonstration.<span class='px-1 mx-1 bg-yellow-200'>The latest version, DCIL-II demonstrates remarkable sample efficiency. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.643</span></span>This novel method operates within an extended Goal-Conditioned Reinforcement Learning framework, ensuring compatibility between intermediate and subsequent goals extracted from the demonstration.However, a fundamental limitation arises from the assumption that the system can be reset to specific states along the demonstrated trajectory, confining the application to simulated systems.In response, we introduce an extension called Single-Reset DCIL (SR-DCIL), designed to overcome this constraint by relying on a single initial state reset rather than sequential resets.To address this more challenging setting, we integrate two mechanisms inspired by the Learning from Demonstrations literature, including a Demo-Buffer and Value Cloning, to guide the agent toward compatible success states.In addition, we introduce Approximate Goal Switching to facilitate training to reach goals distant from the reset state.Our paper makes several contributions, highlighting the importance of the reset assumption in DCIL-II, presenting the mechanisms of SR-DCIL variants and evaluating their performance in challenging robotic tasks compared to DCIL-II.In summary, this work offers insights into the significance of reset assumptions in the framework of DCIL and proposes SR-DCIL, a first step toward a versatile algorithm capable of learning control policies under a weaker reset assumption.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2402.09355v1' target="_blank">
                  link
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-02-14</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Loss Shaping Constraints for Long-Term Time Series Forecasting
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Several applications in time series forecasting require predicting multiple steps ahead.Despite the vast amount of literature in the topic, both classical and recent deep learning based approaches have mostly focused on minimising performance averaged over the predicted window.We observe that this can lead to disparate distributions of errors across forecasting steps, especially for recent transformer architectures trained on popular forecasting benchmarks.That is, optimising performance on average can lead to undesirably large errors at specific time-steps.In this work, we present a Constrained Learning approach for long-term time series forecasting that aims to find the best model in terms of average performance that respects a user-defined upper bound on the loss at each time-step.We call our approach loss shaping constraints because it imposes constraints on the loss at each time step, and leverage recent duality results to show that despite its non-convexity, the resulting problem has a bounded duality gap.<span class='px-1 mx-1 bg-yellow-200'>We propose a practical Primal-Dual algorithm to tackle it, and demonstrate that the proposed approach exhibits competitive average performance in time series forecasting benchmarks, while shaping the distribution of errors across the predicted window. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.671</span></span></p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2402.09373v1' target="_blank">
                  link
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-02-14</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Limitless FaaS: Overcoming serverless functions execution time limits with invoke driven architecture and memory checkpoints
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Function-as-a-Service (FaaS) allows to directly submit function code to a cloud provider without the burden of managing infrastructure resources.Each cloud provider establishes execution time limits to their FaaS offerings, which impose the risk of spending computation time without achieving partial results.In this work, a framework that enables limitless execution time in FaaS, with little to no modifications to the user-provided function code, is presented.After a thorough literature and theoretical framework review, Apache OpenWhisk Actions and the DMCTP checkpoint-and-restore (CR) tool were selected.With these, dependent successive serverless same-function invocations that exploit the persistence of partial results were implemented.<span class='px-1 mx-1 bg-yellow-200'>The solution was submitted to the FaaSDom benchmark and time metrics were collected. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.668</span></span>Additionally, the solution was characterized in terms of the Serverless Trilemma.The resultant system, even at this proof-of-concept state, offers a lot of value to companies that rely heavily on serverless architecture.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2402.09377v1' target="_blank">
                  link
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-02-14</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                AQA-Bench: An Interactive Benchmark for Evaluating LLMs' Sequential Reasoning Ability
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>This paper introduces AQA-Bench, a novel benchmark to assess the sequential reasoning capabilities of large language models (LLMs) in algorithmic contexts, such as depth-first search (DFS).The key feature of our evaluation benchmark lies in its interactive evaluation protocol -- for example, in DFS, the availability of each node's connected edge is contingent upon the model's traversal to that node, thereby necessitating the LLM's ability to effectively remember visited nodes and strategize subsequent moves.We comprehensively build AQA-Bench with three different algorithms, namely binary search, depth-first search, and breadth-first search, and to evaluate the sequential reasoning ability of 12 different LLMs.Our investigations reveal several interesting findings: (1) Closed-source models like GPT-4 and Gemini generally show strong sequential reasoning ability, significantly outperforming open-source LLMs.(2) Naively providing interactive examples may inadvertently hurt few-shot performance.(3) A very limited number of predecessor steps following the optimal policy can substantially boost small models' performance.(4) The scaling correlation between performance and model size is not always significant, sometimes even showcasing an inverse trend.We hope our study can catalyze future work on advancing the understanding and enhancement of LLMs' capabilities in sequential reasoning.<span class='px-1 mx-1 bg-yellow-200'>The code is available at https://github.com/UCSC-VLAA/AQA-Bench. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.606</span></span></p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2402.09404v1' target="_blank">
                  link
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-02-13</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Intriguing Differences Between Zero-Shot and Systematic Evaluations of Vision-Language Transformer Models
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Transformer-based models have dominated natural language processing and other areas in the last few years due to their superior (zero-shot) performance on benchmark datasets.However, these models are poorly understood due to their complexity and size.While probing-based methods are widely used to understand specific properties, the structures of the representation space are not systematically characterized; consequently, it is unclear how such models generalize and overgeneralize to new inputs beyond datasets.In this paper, based on a new gradient descent optimization method, we are able to explore the embedding space of a commonly used vision-language model.Using the Imagenette dataset, we show that while the model achieves over 99\% zero-shot classification performance, it fails systematic evaluations completely.<span class='px-1 mx-1 bg-yellow-200'>Using a linear approximation, we provide a framework to explain the striking differences. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.602</span></span>We have also obtained similar results using a different model to support that our results are applicable to other transformer models with continuous inputs.We also propose a robust way to detect the modified images.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2402.08473v1' target="_blank">
                  link
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-02-13</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Migration to Microservices: A Comparative Study of Decomposition Strategies and Analysis Metrics
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>The microservices architectural style is widely favored for its scalability, reusability, and easy maintainability, prompting increased adoption by developers.However, transitioning from a monolithic to a microservices-based architecture is intricate and costly.In response, we present a novel method utilizing clustering to identify potential microservices in a given monolithic application.Our approach employs a density-based clustering algorithm considering static analysis, structural, and semantic relationships between classes, ensuring a functionally and contextually coherent partitioning.To assess the reliability of our microservice suggestion approach, we conducted an in-depth analysis of hyperparameter sensitivity and compared it with two established clustering algorithms.A comprehensive comparative analysis involved seven applications, evaluating against six baselines, utilizing a dataset of four open-source Java projects.Metrics assessed the quality of generated microservices.Furthermore, we meticulously compared our suggested microservices with manually identified ones in three microservices-based applications.<span class='px-1 mx-1 bg-yellow-200'>This comparison provided a nuanced understanding of our approach's efficacy and reliability. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.655</span></span>Our methodology demonstrated promising outcomes, showcasing remarkable effectiveness and commendable stability.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2402.08481v1' target="_blank">
                  link
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-02-13</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                The Application of ChatGPT in Responding to Questions Related to the Boston Bowel Preparation Scale
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Background: Colonoscopy, a crucial diagnostic tool in gastroenterology, depends heavily on superior bowel preparation.ChatGPT, a large language model with emergent intelligence which also exhibits potential in medical applications.This study aims to assess the accuracy and consistency of ChatGPT in using the Boston Bowel Preparation Scale (BBPS) for colonoscopy assessment.Methods: We retrospectively collected 233 colonoscopy images from 2020 to 2023.These images were evaluated using the BBPS by 3 senior endoscopists and 3 novice endoscopists.Additionally, ChatGPT also assessed these images, having been divided into three groups and undergone specific Fine-tuning.<span class='px-1 mx-1 bg-yellow-200'>Consistency was evaluated through two rounds of testing. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.613</span></span>Results:In the initial round, ChatGPT's accuracy varied between 48.93% and 62.66%, trailing the endoscopists' accuracy of 76.68% to 77.83%.Kappa values for ChatGPT was between 0.52 and 0.53, compared to 0.75 to 0.87 for the endoscopists.Conclusion: While ChatGPT shows promise in bowel preparation scoring, it currently does not match the accuracy and consistency of experienced endoscopists.Future research should focus on in-depth Fine-tuning.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2402.08492v1' target="_blank">
                  link
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-02-13</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Competitive Revenue Extraction from Time-Discounted Transactions in the Semi-Myopic Regime
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Decentralized cryptocurrencies are payment systems that rely on aligning the incentives of users and miners to operate correctly and offer a high quality of service to users.Recent literature studies the mechanism design problem of the auction serving as a cryptocurrency's transaction fee mechanism (TFM).We find that a non-myopic modelling of miners falls close to another well-known problem: that of online buffer management for packet switching.The main difference is that unlike packets which are of a fixed size throughout their lifetime, in a financial environment, user preferences (and therefore revenue extraction) may be time-dependent.We study the competitive ratio guarantees given a certain discount rate, and show how existing methods from packet scheduling, which we call "the undiscounted case", perform suboptimally in the more general discounted setting.Most notably, we find a novel, simple, memoryless, and optimal deterministic algorithm for the semi-myopic case, when the discount factor is up to ~0.770018.<span class='px-1 mx-1 bg-yellow-200'>We also present a randomized algorithm that achieves better performance than the best possible deterministic algorithm, for any discount rate. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.624</span></span></p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2402.08549v1' target="_blank">
                  link
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-02-13</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Higher Layers Need More LoRA Experts
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Parameter-efficient tuning (PEFT) techniques like low-rank adaptation (LoRA) offer training efficiency on Large Language Models, but their impact on model performance remains limited.Recent efforts integrate LoRA and Mixture-of-Experts (MoE) to improve the performance of PEFT methods.Despite promising results, research on improving the efficiency of LoRA with MoE is still in its early stages.Recent studies have shown that experts in the MoE architecture have different strengths and also exhibit some redundancy.Does this statement also apply to parameter-efficient MoE?In this paper, we introduce a novel parameter-efficient MoE method, \textit{\textbf{M}oE-L\textbf{o}RA with \textbf{L}ayer-wise Expert \textbf{A}llocation (MoLA)} for Transformer-based models, where each model layer has the flexibility to employ a varying number of LoRA experts.We investigate several architectures with varying layer-wise expert configurations.Experiments on six well-known NLP and commonsense QA benchmarks demonstrate that MoLA achieves equal or superior performance compared to all baselines.We find that allocating more LoRA experts to higher layers further enhances the effectiveness of models with a certain number of experts in total.With much fewer parameters, this allocation strategy outperforms the setting with the same number of experts in every layer.<span class='px-1 mx-1 bg-yellow-200'>This work can be widely used as a plug-and-play parameter-efficient tuning approach for various applications. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.65</span></span>The code is available at https://github.com/GCYZSL/MoLA.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2402.08562v1' target="_blank">
                  link
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-02-13</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Denoising Diffusion Restoration Tackles Forward and Inverse Problems for the Laplace Operator
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Diffusion models have emerged as a promising class of generative models that map noisy inputs to realistic images.More recently, they have been employed to generate solutions to partial differential equations (PDEs).However, they still struggle with inverse problems in the Laplacian operator, for instance, the Poisson equation, because the eigenvalues that are large in magnitude amplify the measurement noise.This paper presents a novel approach for the inverse and forward solution of PDEs through the use of denoising diffusion restoration models (DDRM).DDRMs were used in linear inverse problems to restore original clean signals by exploiting the singular value decomposition (SVD) of the linear operator.Equivalently, we present an approach to restore the solution and the parameters in the Poisson equation by exploiting the eigenvalues and the eigenfunctions of the Laplacian operator.<span class='px-1 mx-1 bg-yellow-200'>Our results show that using denoising diffusion restoration significantly improves the estimation of the solution and parameters. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.62</span></span>Our research, as a result, pioneers the integration of diffusion models with the principles of underlying physics to solve PDEs.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2402.08563v1' target="_blank">
                  link
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-02-13</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Two Tales of Single-Phase Contrastive Hebbian Learning
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>The search for "biologically plausible" learning algorithms has converged on the idea of representing gradients as activity differences.However, most approaches require a high degree of synchronization (distinct phases during learning) and introduce substantial computational overhead, which raises doubts regarding their biological plausibility as well as their potential utility for neuromorphic computing.Furthermore, they commonly rely on applying infinitesimal perturbations (nudges) to output units, which is impractical in noisy environments.Recently it has been shown that by modelling artificial neurons as dyads with two oppositely nudged compartments, it is possible for a fully local learning algorithm named ``dual propagation'' to bridge the performance gap to backpropagation, without requiring separate learning phases or infinitesimal nudging.<span class='px-1 mx-1 bg-yellow-200'>However, the algorithm has the drawback that its numerical stability relies on symmetric nudging, which may be restrictive in biological and analog implementations. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.617</span></span>In this work we first provide a solid foundation for the objective underlying the dual propagation method, which also reveals a surprising connection with adversarial robustness.Second, we demonstrate how dual propagation is related to a particular adjoint state method, which is stable regardless of asymmetric nudging.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2402.08573v1' target="_blank">
                  link
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-02-13</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Improving Factual Error Correction for Abstractive Summarization via Data Distillation and Conditional-generation Cloze
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Improving factual consistency in abstractive summarization has been a focus of current research.One promising approach is the post-editing method.However, previous works have yet to make sufficient use of factual factors in summaries and suffers from the negative effect of the training datasets.In this paper, we first propose a novel factual error correction model FactCloze based on a conditional-generation cloze task.FactCloze can construct the causality among factual factors while being able to determine whether the blank can be answered or not.Then, we propose a data distillation method to generate a more faithful summarization dataset SummDSC via multiple-dimensional evaluation.<span class='px-1 mx-1 bg-yellow-200'>We experimentally validate the effectiveness of our approach, which leads to an improvement in multiple factual consistency metrics compared to baselines. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.697</span></span></p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2402.08581v1' target="_blank">
                  link
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-02-13</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Sampling Space-Saving Set Sketches
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Large, distributed data streams are now ubiquitous.High-accuracy sketches with low memory overhead have become the de facto method for analyzing this data.For instance, if we wish to group data by some label and report the largest counts using fixed memory, we need to turn to mergeable heavy hitter sketches that can provide highly accurate approximate counts.Similarly, if we wish to keep track of the number of distinct items in a single set spread across several streams using fixed memory, we can turn to mergeable count distinct sketches that can provide highly accurate set cardinalities.   If we were to try to keep track of the cardinality of multiple sets and report only on the largest ones, maintaining individual count distinct sketches for each set can grow unwieldy, especially if the number of sets is not known in advance.We consider the natural combination of the heavy hitters problem with the count distinct problem, the heavy distinct hitters problem: given a stream of $(\ell, x)$ pairs, find all the labels $\ell$ that are paired with a large number of distinct items $x$ using only constant memory.   No previous work on heavy distinct hitters has managed to be of practical use in the large, distributed data stream setting.We propose a new algorithm, the Sampling Space-Saving Set Sketch, which combines sketching and sampling techniques and has all the desired properties for size, speed, accuracy, mergeability, and invertibility.<span class='px-1 mx-1 bg-yellow-200'>We compare our algorithm to several existing solutions to the heavy distinct hitters problem, and provide experimental results across several data sets showing the superiority of the new sketch. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.609</span></span></p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2402.08604v1' target="_blank">
                  link
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-02-13</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                A Generalized Approach to Online Convex Optimization
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>In this paper, we analyze the problem of online convex optimization in different settings.We show that any algorithm for online linear optimization with fully adaptive adversaries is an algorithm for online convex optimization.We also show that any such algorithm that requires full-information feedback may be transformed to an algorithm with semi-bandit feedback with comparable regret bound.We further show that algorithms that are designed for fully adaptive adversaries using deterministic semi-bandit feedback can obtain similar bounds using only stochastic semi-bandit feedback when facing oblivious adversaries.We use this to describe general meta-algorithms to convert first order algorithms to zeroth order algorithms with comparable regret bounds.Our framework allows us to analyze online optimization in various settings, such full-information feedback, bandit feedback, stochastic regret, adversarial regret and various forms of non-stationary regret.<span class='px-1 mx-1 bg-yellow-200'>Using our analysis, we provide the first efficient projection-free online convex optimization algorithm using linear optimization oracles. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.637</span></span></p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2402.08621v1' target="_blank">
                  link
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-02-13</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                NeRF Analogies: Example-Based Visual Attribute Transfer for NeRFs
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>A Neural Radiance Field (NeRF) encodes the specific relation of 3D geometry and appearance of a scene.We here ask the question whether we can transfer the appearance from a source NeRF onto a target 3D geometry in a semantically meaningful way, such that the resulting new NeRF retains the target geometry but has an appearance that is an analogy to the source NeRF.To this end, we generalize classic image analogies from 2D images to NeRFs.We leverage correspondence transfer along semantic affinity that is driven by semantic features from large, pre-trained 2D image models to achieve multi-view consistent appearance transfer.Our method allows exploring the mix-and-match product space of 3D geometry and appearance.<span class='px-1 mx-1 bg-yellow-200'>We show that our method outperforms traditional stylization-based methods and that a large majority of users prefer our method over several typical baselines. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.614</span></span></p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2402.08622v1' target="_blank">
                  link
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-02-13</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Knowledge Editing on Black-box Large Language Models
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Knowledge editing (KE) aims to efficiently and precisely modify the behavior of large language models (LLMs) to update specific knowledge without negatively influencing other knowledge.Current research primarily focuses on white-box LLMs editing, overlooking an important scenario: black-box LLMs editing, where LLMs are accessed through interfaces and only textual output is available.To address the limitations of existing evaluations that are not inapplicable to black-box LLM editing and lack comprehensiveness, we propose a multi-perspective evaluation framework, incorporating the assessment of style retention for the first time.To tackle privacy leaks of editing data and style over-editing in current methods, we introduce a novel postEdit framework, resolving privacy concerns through downstream post-processing and maintaining textual style consistency via fine-grained editing to original responses.<span class='px-1 mx-1 bg-yellow-200'>Experiments and analysis on two benchmarks demonstrate that postEdit outperforms all baselines and achieves strong generalization, especially with huge improvements on style retention (average $+20.82\%\uparrow$). <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.669</span></span></p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2402.08631v1' target="_blank">
                  link
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-02-13</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                SemRel2024: A Collection of Semantic Textual Relatedness Datasets for 14 Languages
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Exploring and quantifying semantic relatedness is central to representing language.It holds significant implications across various NLP tasks, including offering insights into the capabilities and performance of Large Language Models (LLMs).While earlier NLP research primarily focused on semantic similarity, often within the English language context, we instead investigate the broader phenomenon of semantic relatedness.In this paper, we present SemRel, a new semantic relatedness dataset collection annotated by native speakers across 14 languages:Afrikaans, Algerian Arabic, Amharic, English, Hausa, Hindi, Indonesian, Kinyarwanda, Marathi, Moroccan Arabic, Modern Standard Arabic, Punjabi, Spanish, and Telugu.These languages originate from five distinct language families and are predominantly spoken in Africa and Asia -- regions characterised by a relatively limited availability of NLP resources.Each instance in the SemRel datasets is a sentence pair associated with a score that represents the degree of semantic textual relatedness between the two sentences.<span class='px-1 mx-1 bg-yellow-200'>The scores are obtained using a comparative annotation framework. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.63</span></span>We describe the data collection and annotation processes, related challenges when building the datasets, and their impact and utility in NLP.We further report experiments for each language and across the different languages.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2402.08638v1' target="_blank">
                  link
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-02-13</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Target Score Matching
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p><span class='px-1 mx-1 bg-yellow-200'>Denoising Score Matching estimates the score of a noised version of a target distribution by minimizing a regression loss and is widely used to train the popular class of Denoising Diffusion Models. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.615</span></span><span class='px-1 mx-1 bg-yellow-200'>A well known limitation of Denoising Score Matching, however, is that it yields poor estimates of the score at low noise levels. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.64</span></span>This issue is particularly unfavourable for problems in the physical sciences and for Monte Carlo sampling tasks for which the score of the clean original target is known.<span class='px-1 mx-1 bg-yellow-200'>Intuitively, estimating the score of a slightly noised version of the target should be a simple task in such cases. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.601</span></span>In this paper, we address this shortcoming and show that it is indeed possible to leverage knowledge of the target score.<span class='px-1 mx-1 bg-yellow-200'>We present a Target Score Identity and corresponding Target Score Matching regression loss which allows us to obtain score estimates admitting favourable properties at low noise levels. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.619</span></span></p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2402.08667v1' target="_blank">
                  link
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-02-13</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Model Assessment and Selection under Temporal Distribution Shift
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>We investigate model assessment and selection in a changing environment, by synthesizing datasets from both the current time period and historical epochs.To tackle unknown and potentially arbitrary temporal distribution shift, we develop an adaptive rolling window approach to estimate the generalization error of a given model.<span class='px-1 mx-1 bg-yellow-200'>This strategy also facilitates the comparison between any two candidate models by estimating the difference of their generalization errors. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.644</span></span>We further integrate pairwise comparisons into a single-elimination tournament, achieving near-optimal model selection from a collection of candidates.Theoretical analyses and numerical experiments demonstrate the adaptivity of our proposed methods to the non-stationarity in data.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2402.08672v1' target="_blank">
                  link
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-02-12</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Online Sequential Decision-Making with Unknown Delays
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>In the field of online sequential decision-making, we address the problem with delays utilizing the framework of online convex optimization (OCO), where the feedback of a decision can arrive with an unknown delay.Unlike previous research that is limited to Euclidean norm and gradient information, we propose three families of delayed algorithms based on approximate solutions to handle different types of received feedback.Our proposed algorithms are versatile and applicable to universal norms.Specifically, we introduce a family of Follow the Delayed Regularized Leader algorithms for feedback with full information on the loss function, a family of Delayed Mirror Descent algorithms for feedback with gradient information on the loss function and a family of Simplified Delayed Mirror Descent algorithms for feedback with the value information of the loss function's gradients at corresponding decision points.For each type of algorithm, we provide corresponding regret bounds under cases of general convexity and relative strong convexity, respectively.<span class='px-1 mx-1 bg-yellow-200'>We also demonstrate the efficiency of each algorithm under different norms through concrete examples. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.688</span></span>Furthermore, our theoretical results are consistent with the current best bounds when degenerated to standard settings.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2402.07703v1' target="_blank">
                  link
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-02-12</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Local Centrality Minimization with Quality Guarantees
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Centrality measures, quantifying the importance of vertices or edges, play a fundamental role in network analysis.To date, triggered by some positive approximability results, a large body of work has been devoted to studying centrality maximization, where the goal is to maximize the centrality score of a target vertex by manipulating the structure of a given network.On the other hand, due to the lack of such results, only very little attention has been paid to centrality minimization, despite its practical usefulness.   In this study, we introduce a novel optimization model for local centrality minimization, where the manipulation is allowed only around the target vertex.We prove the NP-hardness of our model and that the most intuitive greedy algorithm has a quite limited performance in terms of approximation ratio.<span class='px-1 mx-1 bg-yellow-200'>Then we design two effective approximation algorithms: The first algorithm is a highly-scalable algorithm that has an approximation ratio unachievable by the greedy algorithm, while the second algorithm is a bicriteria approximation algorithm that solves a continuous relaxation based on the Lov\'asz extension, using a projected subgradient method. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.633</span></span>To the best of our knowledge, ours are the first polynomial-time algorithms with provable approximation guarantees for centrality minimization.Experiments using a variety of real-world networks demonstrate the effectiveness of our proposed algorithms: Our first algorithm is applicable to million-scale graphs and obtains much better solutions than those of scalable baselines, while our second algorithm is rather strong against adversarial instances.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2402.07718v1' target="_blank">
                  link
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-02-12</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Engineering Weighted Connectivity Augmentation Algorithms
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Increasing the connectivity of a graph is a pivotal challenge in robust network design.The weighted connectivity augmentation problem is a common version of the problem that takes link costs into consideration.The problem is then to find a minimum cost subset of a given set of weighted links that increases the connectivity of a graph by one when the links are added to the edge set of the input instance.<span class='px-1 mx-1 bg-yellow-200'>In this work, we give a first implementation of recently discovered better-than-2 approximations. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.686</span></span>Furthermore, we propose three new heuristic and one exact approach.These include a greedy algorithm considering link costs and the number of unique cuts covered, an approach based on minimum spanning trees and a local search algorithm that may improve a given solution by swapping links of paths.Our exact approach uses an ILP formulation with efficient cut enumeration as well as a fast initialization routine.<span class='px-1 mx-1 bg-yellow-200'>We then perform an extensive experimental evaluation which shows that our algorithms are faster and yield the best solutions compared to the current state-of-the-art as well as the recently discovered better-than-2 approximation algorithms. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.724</span></span>Our novel local search algorithm can improve solution quality even further.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2402.07753v1' target="_blank">
                  link
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-02-12</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Insights into $(k,ρ)$-shortcutting algorithms
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>A graph is called a $(k,\rho)$-graph iff every node can reach $\rho$ of its nearest neighbors in at most k hops.This property proved useful in the analysis and design of parallel shortest-path algorithms.Any graph can be transformed into a $(k,\rho)$-graph by adding shortcuts.Formally, the $(k,\rho)$-Minimum-Shortcut problem asks to find an appropriate shortcut set of minimal cardinality.   We show that the $(k,\rho)$-Minimum-Shortcut problem is NP-complete in the practical regime of $k \ge 3$ and $\rho = \Theta(n^\epsilon)$ for $\epsilon > 0$. With a related construction, we bound the approximation factor of known $(k,\rho)$-Minimum-Shortcut problem heuristics from below and propose algorithmic countermeasures improving the approximation quality.Further, we describe an integer linear problem (ILP) solving the $(k,\rho)$-Minimum-Shortcut problem optimally.<span class='px-1 mx-1 bg-yellow-200'>Finally, we compare the practical performance and quality of all algorithms in an empirical campaign. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.709</span></span></p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2402.07771v1' target="_blank">
                  link
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-02-12</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Solving parameter-dependent semi-algebraic systems
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>We consider systems of polynomial equations and inequalities in $\mathbb{Q}[\boldsymbol{y}][\boldsymbol{x}]$ where $\boldsymbol{x} = (x_1, \ldots, x_n)$ and $\boldsymbol{y} = (y_1, \ldots,y_t)$. The $\boldsymbol{y}$ indeterminates are considered as parameters and we assume that when specialising them generically, the set of common complex solutions, to the obtained equations, is finite.We consider the problem of real root classification for such parameter-dependent problems, i.e. identifying the possible number of real solutions depending on the values of the parameters and computing a description of the regions of the space of parameters over which the number of real roots remains invariant.   We design an algorithm for solving this problem.The formulas it outputs enjoy a determinantal structure.Under genericity assumptions, we show that its arithmetic complexity is polynomial in both the maximum degree $d$ and the number $s$ of the input inequalities and exponential in $nt+t^2$. The output formulas consist of polynomials of degree bounded by $(2s+n)d^{n+1}$. This is the first algorithm with such a singly exponential complexity.<span class='px-1 mx-1 bg-yellow-200'>We report on practical experiments showing that a first implementation of this algorithm can tackle examples which were previously out of reach. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.658</span></span></p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2402.07782v1' target="_blank">
                  link
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-02-12</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                HYPO: Hyperspherical Out-of-Distribution Generalization
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Out-of-distribution (OOD) generalization is critical for machine learning models deployed in the real world.However, achieving this can be fundamentally challenging, as it requires the ability to learn invariant features across different domains or environments.In this paper, we propose a novel framework HYPO (HYPerspherical OOD generalization) that provably learns domain-invariant representations in a hyperspherical space.In particular, our hyperspherical learning algorithm is guided by intra-class variation and inter-class separation principles -- ensuring that features from the same class (across different training domains) are closely aligned with their class prototypes, while different class prototypes are maximally separated.We further provide theoretical justifications on how our prototypical learning objective improves the OOD generalization bound.<span class='px-1 mx-1 bg-yellow-200'>Through extensive experiments on challenging OOD benchmarks, we demonstrate that our approach outperforms competitive baselines and achieves superior performance. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.688</span></span>Code is available at https://github.com/deeplearning-wisc/hypo.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2402.07785v1' target="_blank">
                  link
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-02-12</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                From Uncertainty to Precision: Enhancing Binary Classifier Performance through Calibration
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>The assessment of binary classifier performance traditionally centers on discriminative ability using metrics, such as accuracy.However, these metrics often disregard the model's inherent uncertainty, especially when dealing with sensitive decision-making domains, such as finance or healthcare.Given that model-predicted scores are commonly seen as event probabilities, calibration is crucial for accurate interpretation.<span class='px-1 mx-1 bg-yellow-200'>In our study, we analyze the sensitivity of various calibration measures to score distortions and introduce a refined metric, the Local Calibration Score. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.625</span></span>Comparing recalibration methods, we advocate for local regressions, emphasizing their dual role as effective recalibration tools and facilitators of smoother visualizations.We apply these findings in a real-world scenario using Random Forest classifier and regressor to predict credit default while simultaneously measuring calibration during performance optimization.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2402.07790v1' target="_blank">
                  link
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-02-12</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Injecting Wiktionary to improve token-level contextual representations using contrastive learning
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>While static word embeddings are blind to context, for lexical semantics tasks context is rather too present in contextual word embeddings, vectors of same-meaning occurrences being too different (Ethayarajh, 2019).Fine-tuning pre-trained language models (PLMs) using contrastive learning was proposed, leveraging automatically self-augmented examples (Liu et al., 2021b).In this paper, we investigate how to inject a lexicon as an alternative source of supervision, using the English Wiktionary.We also test how dimensionality reduction impacts the resulting contextual word embeddings.We evaluate our approach on the Word-In-Context (WiC) task, in the unsupervised setting (not using the training set).We achieve new SoTA result on the original WiC test set.<span class='px-1 mx-1 bg-yellow-200'>We also propose two new WiC test sets for which we show that our fine-tuning method achieves substantial improvements. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.655</span></span>We also observe improvements, although modest, for the semantic frame induction task.Although we experimented on English to allow comparison with related work, our method is adaptable to the many languages for which large Wiktionaries exist.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2402.07817v1' target="_blank">
                  link
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-02-12</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Differentially Private Zeroth-Order Methods for Scalable Large Language Model Finetuning
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Finetuning on task-specific datasets is a widely-embraced paradigm of harnessing the powerful capability of pretrained LLMs for various downstream tasks.Due to the popularity of LLMs finetuning and its accompanying privacy concerns, differentially private (DP) finetuning of pretrained LLMs has garnered increasing attention to safeguarding the privacy of task-specific datasets.Lying at the design core of DP LLM finetuning methods is the satisfactory tradeoff between privacy, utility, and scalability.Most existing methods build upon the seminal work of DP-SGD.Despite pushing the scalability of DP-SGD to its limit, DP-SGD-based finetuning methods are unfortunately limited by the inherent inefficiency of SGD.In this paper, we investigate the potential of DP zeroth-order methods for LLM pretraining, which avoids the scalability bottleneck of SGD by approximating the gradient with the more efficient zeroth-order gradient.<span class='px-1 mx-1 bg-yellow-200'>Rather than treating the zeroth-order method as a drop-in replacement for SGD, this paper presents a comprehensive study both theoretically and empirically. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.647</span></span>First, we propose the stagewise DP zeroth-order method that dynamically schedules key hyperparameters.This design is grounded on the synergy between DP random perturbation and the gradient approximation error of the zeroth-order method, and its effect on finetuning trajectory.Second, we further enhance the scalability by reducing the trainable parameters that are identified by repurposing a data-free pruning technique requiring no additional data or extra privacy budget.<span class='px-1 mx-1 bg-yellow-200'>We provide theoretical analysis for both proposed methods. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.647</span></span>We conduct extensive empirical analysis on both encoder-only masked language model and decoder-only autoregressive language model, achieving impressive results in terms of scalability and utility.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2402.07818v1' target="_blank">
                  link
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-02-12</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                An approximation algorithm for Maximum DiCut vs. Cut
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p><span class='px-1 mx-1 bg-yellow-200'>Goemans and Williamson designed a 0.878-approximation algorithm for Max-Cut in undirected graphs [JACM'95]. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.606</span></span><span class='px-1 mx-1 bg-yellow-200'>Khot, Kindler, Mosel, and O'Donnel showed that the approximation ratio of the Goemans-Williamson algorithm is optimal assuming <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.626</span></span>Khot's Unique Games Conjecture[SICOMP'07].In the problem of maximum cuts in directed graphs (Max-DiCut), in which we seek as many edges going from one particular side of the cut to the other, the situation is more complicated but the recent work of Brakensiek, Huang, Potechin, and Zwick showed that their 0.874-approximation algorithm is tight under the Unique Games Conjecture (up to a small delta)[FOCS'23].   We consider a promise version of the problem and design an SDP-based algorithm which, if given a directed graph G that has a directed cut of value rho, finds an undirected cut in G (ignoring edge directions) with value at least \rho.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2402.07863v1' target="_blank">
                  link
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-02-12</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Nesting Particle Filters for Experimental Design in Dynamical Systems
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>In this paper, we propose a novel approach to Bayesian Experimental Design (BED) for non-exchangeable data that formulates it as risk-sensitive policy optimization.We develop the Inside-Out SMC^2 algorithm that uses a nested sequential Monte Carlo (SMC) estimator of the expected information gain and embeds it into a particle Markov chain Monte Carlo (pMCMC) framework to perform gradient-based policy optimization.This is in contrast to recent approaches that rely on biased estimators of the expected information gain (EIG) to amortize the cost of experiments by learning a design policy in advance.<span class='px-1 mx-1 bg-yellow-200'>Numerical validation on a set of dynamical systems showcases the efficacy of our method in comparison to other state-of-the-art strategies. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.602</span></span></p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2402.07868v1' target="_blank">
                  link
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-02-08</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Combining Voting and Abstract Argumentation to Understand Online Discussions
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Online discussion platforms are a vital part of the public discourse in a deliberative democracy.However, how to interpret the outcomes of the discussions on these platforms is often unclear.In this paper, we propose a novel and explainable method for selecting a set of most representative, consistent points of view by combining methods from computational social choice and abstract argumentation.Specifically, we model online discussions as abstract argumentation frameworks combined with information regarding which arguments voters approve of.Based on ideas from approval-based multiwinner voting, we introduce several voting rules for selecting a set of preferred extensions that represents voters' points of view.<span class='px-1 mx-1 bg-yellow-200'>We compare the proposed methods across several dimensions, theoretically and in numerical simulations, and give clear suggestions on which methods to use depending on the specific situation. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.642</span></span></p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2402.05895v1' target="_blank">
                  link
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-02-08</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Point-VOS: Pointing Up Video Object Segmentation
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Current state-of-the-art Video Object Segmentation (VOS) methods rely on dense per-object mask annotations both during training and testing.This requires time-consuming and costly video annotation mechanisms.We propose a novel Point-VOS task with a spatio-temporally sparse point-wise annotation scheme that substantially reduces the annotation effort.We apply our annotation scheme to two large-scale video datasets with text descriptions and annotate over 19M points across 133K objects in 32K videos.<span class='px-1 mx-1 bg-yellow-200'>Based on our annotations, we propose a new Point-VOS benchmark, and a corresponding point-based training mechanism, which we use to establish strong baseline results. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.615</span></span>We show that existing VOS methods can easily be adapted to leverage our point annotations during training, and can achieve results close to the fully-supervised performance when trained on pseudo-masks generated from these points.In addition, we show that our data can be used to improve models that connect vision and language, by evaluating it on the Video Narrative Grounding (VNG) task.We will make our code and annotations available at https://pointvos.github.io.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2402.05917v1' target="_blank">
                  link
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-02-08</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Time Series Diffusion in the Frequency Domain
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Fourier analysis has been an instrumental tool in the development of signal processing.This leads us to wonder whether this framework could similarly benefit generative modelling.In this paper, we explore this question through the scope of time series diffusion models.More specifically, we analyze whether representing time series in the frequency domain is a useful inductive bias for score-based diffusion models.By starting from the canonical SDE formulation of diffusion in the time domain, we show that a dual diffusion process occurs in the frequency domain with an important nuance: Brownian motions are replaced by what we call mirrored Brownian motions, characterized by mirror symmetries among their components.<span class='px-1 mx-1 bg-yellow-200'>Building on this insight, we show how to adapt the denoising score matching approach to implement diffusion models in the frequency domain. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.615</span></span>This results in frequency diffusion models, which we compare to canonical time diffusion models.Our empirical evaluation on real-world datasets, covering various domains like healthcare and finance, shows that frequency diffusion models better capture the training distribution than time diffusion models.We explain this observation by showing that time series from these datasets tend to be more localized in the frequency domain than in the time domain, which makes them easier to model in the former case.All our observations point towards impactful synergies between Fourier analysis and diffusion models.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2402.05933v1' target="_blank">
                  link
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-02-08</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Classifying Nodes in Graphs without GNNs
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Graph neural networks (GNNs) are the dominant paradigm for classifying nodes in a graph, but they have several undesirable attributes stemming from their message passing architecture.Recently, distillation methods succeeded in eliminating the use of GNNs at test time but they still require them during training.We perform a careful analysis of the role that GNNs play in distillation methods.This analysis leads us to propose a fully GNN-free approach for node classification, not requiring them at train or test time.Our method consists of three key components: smoothness constraints, pseudo-labeling iterations and neighborhood-label histograms.<span class='px-1 mx-1 bg-yellow-200'>Our final approach can match the state-of-the-art accuracy on standard popular benchmarks such as citation and co-purchase networks, without training a GNN. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.603</span></span></p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2402.05934v1' target="_blank">
                  link
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-02-08</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                SPHINX-X: Scaling Data and Parameters for a Family of Multi-modal Large Language Models
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>We propose SPHINX-X, an extensive Multimodality Large Language Model (MLLM) series developed upon SPHINX.To improve the architecture and training efficiency, we modify the SPHINX framework by removing redundant visual encoders, bypassing fully-padded sub-images with skip tokens, and simplifying multi-stage training into a one-stage all-in-one paradigm.To fully unleash the potential of MLLMs, we assemble a comprehensive multi-domain and multimodal dataset covering publicly available resources in language, vision, and vision-language tasks.We further enrich this collection with our curated OCR intensive and Set-of-Mark datasets, extending the diversity and generality.By training over different base LLMs including TinyLlama1.1B, InternLM2-7B, LLaMA2-13B, and Mixtral8x7B, we obtain a spectrum of MLLMs that vary in parameter size and multilingual capabilities.<span class='px-1 mx-1 bg-yellow-200'>Comprehensive benchmarking reveals a strong correlation between the multi-modal performance with the data and parameter scales. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.618</span></span>Code and models are released at https://github.com/Alpha-VLLM/LLaMA2-Accessory</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2402.05935v1' target="_blank">
                  link
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td></td>
          <td>
            <h2 class="text-2xl tracking-tight pt-4 font-bold">LLMs</h2>
          </td>
        </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-02-14</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Leveraging Large Language Models for Enhanced NLP Task Performance through Knowledge Distillation and Optimized Training Strategies
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>The integration of Large Language Models (LLMs) like GPT-4 into traditional Natural Language Processing (NLP) tasks has opened new avenues for enhancing model performance while reducing the reliance on extensive human annotations.This paper presents a novel approach that leverages the Chain of Thought (CoT) prompting technique to distill knowledge from GPT-4, subsequently applying it to improve the efficiency and effectiveness of a smaller model, BERT, on Named Entity Recognition (NER) tasks.Our method involves a two-phase training process: initially employing GPT-4 annotated data for pre-training and then refining the model with a combination of distilled and original human-annotated data.The results demonstrate that our mixed-training strategy significantly outperforms models trained solely on human annotations, achieving superior F1-scores and showcasing a cost-effective solution for resource-limited or closed-network settings.<span class='px-1 mx-1 bg-yellow-200'>The study also discusses the challenges encountered, such as LLM output variability and the tendency towards hallucinations, proposing future work directions to enhance prompt design and annotation selection. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.689</span></span><span class='px-1 mx-1 bg-yellow-200'>Our findings indicate a promising synergy between LLM insights and traditional NLP techniques, paving the way for more accessible and robust NLP applications. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.687</span></span></p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2402.09282v1' target="_blank">
                  link
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-02-14</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Attacks, Defenses and Evaluations for LLM Conversation Safety: A Survey
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p><span class='px-1 mx-1 bg-yellow-200'>Large Language Models (LLMs) are now commonplace in conversation applications. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.706</span></span><span class='px-1 mx-1 bg-yellow-200'>However, their risks of misuse for generating harmful responses have raised serious societal concerns and spurred recent research on LLM conversation safety. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.763</span></span><span class='px-1 mx-1 bg-yellow-200'>Therefore, in this survey, we provide a comprehensive overview of recent studies, covering three critical aspects of LLM conversation safety: attacks, defenses, and evaluations. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.753</span></span><span class='px-1 mx-1 bg-yellow-200'>Our goal is to provide a structured summary that enhances understanding of LLM conversation safety and encourages further investigation into this important subject. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.739</span></span>For easy reference, we have categorized all the studies mentioned in this survey according to our taxonomy, available at: https://github.com/niconi19/LLM-conversation-safety.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2402.09283v1' target="_blank">
                  link
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-02-14</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Trained Without My Consent: Detecting Code Inclusion In Language Models Trained on Code
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Code auditing ensures that the developed code adheres to standards, regulations, and copyright protection by verifying that it does not contain code from protected sources.<span class='px-1 mx-1 bg-yellow-200'>The recent advent of Large Language Models (LLMs) as coding assistants in the software development process poses new challenges for code auditing. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.661</span></span>The dataset for training these models is mainly collected from publicly available sources.This raises the issue of intellectual property infringement as developers' codes are already included in the dataset.<span class='px-1 mx-1 bg-yellow-200'>Therefore, auditing code developed using LLMs is challenging, as it is difficult to reliably assert if an LLM used during development has been trained on specific copyrighted codes, given that we do not have access to the training datasets of these models. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.702</span></span>Given the non-disclosure of the training datasets, traditional approaches such as code clone detection are insufficient for asserting copyright infringement.To address this challenge, we propose a new approach, TraWiC; a model-agnostic and interpretable method based on membership inference for detecting code inclusion in an LLM's training dataset.We extract syntactic and semantic identifiers unique to each program to train a classifier for detecting code inclusion.<span class='px-1 mx-1 bg-yellow-200'>In our experiments, we observe that TraWiC is capable of detecting 83.87% of codes that were used to train an LLM. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.635</span></span>In comparison, the prevalent clone detection tool NiCad is only capable of detecting 47.64%.In addition to its remarkable performance, TraWiC has low resource overhead in contrast to pair-wise clone detection that is conducted during the auditing process of tools like CodeWhisperer reference tracker, across thousands of code snippets.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2402.09299v1' target="_blank">
                  link
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-02-14</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                ICDPO: Effectively Borrowing Alignment Capability of Others via In-context Direct Preference Optimization
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p><span class='px-1 mx-1 bg-yellow-200'>Large Language Models (LLMs) rely on Human Preference Alignment (HPA) to ensure the generation of safe content. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.684</span></span><span class='px-1 mx-1 bg-yellow-200'>Due to the heavy cost associated with fine-tuning, fine-tuning-free methods have emerged, typically modifying LLM decoding with external auxiliary methods. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.678</span></span><span class='px-1 mx-1 bg-yellow-200'>However, these methods do not essentially enhance the LLM itself. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.756</span></span>In this paper, we rethink the derivation procedures of DPO, based on which we conversely build an instant scorer using the states of the LLM before and after In-context Learning (ICL).Accordingly, we propose a novel approach called In-Context Direct Preference Optimization (ICDPO).<span class='px-1 mx-1 bg-yellow-200'>It enables LLMs to borrow the HPA capabilities from superior LLMs with ICL, generating well-aligned responses as estimated by the aforementioned instant scorer, thereby enhancing the final performance. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.682</span></span>ICDPO can be further enhanced with a two-stage retriever and an upgraded scorer, both offering benefits.Extensive experiments show its effectiveness, particularly in outperforming two fine-tuning-free baselines, and it exhibits competitiveness with SFT + LoRA.We also conduct detailed analyses to offer comprehensive insights into ICDPO.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2402.09320v1' target="_blank">
                  link
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-02-14</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Collusion-Resilience in Transaction Fee Mechanism Design
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Users bid in a transaction fee mechanism (TFM) to get their transactions included and confirmed by a blockchain protocol.<span class='px-1 mx-1 bg-yellow-200'>Roughgarden (EC'21) initiated the formal treatment of TFMs and proposed three requirements: user incentive compatibility (UIC), miner incentive compatibility (MIC), and a form of collusion-resilience called OCA-proofness. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.61</span></span>Ethereum's EIP-1559 mechanism satisfies all three properties simultaneously when there is no contention between transactions, but loses the UIC property when there are too many eligible transactions to fit in a single block.Chung and Shi (SODA'23) considered an alternative notion of collusion-resilience, called c-side-constract-proofness (c-SCP), and showed that, when there is contention between transactions, no TFM can satisfy UIC, MIC, and c-SCP for any c at least 1.OCA-proofness asserts that the users and a miner should not be able to "steal from the protocol" and is intuitively weaker than the c-SCP condition, which stipulates that a coalition of a miner and a subset of users should not be able to profit through strategic deviations (whether at the expense of the protocol or of the users outside the coalition).   Our main result is the first proof that, when there is contention between transactions, no (possibly randomized) direct-revelation TFM satisfies UIC, MIC, and OCA-proofness.This result resolves the main open question in Roughgarden(EC'21).We also suggest several relaxations of the basic model that allow our impossibility result to be circumvented.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2402.09321v1' target="_blank">
                  link
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-02-14</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                AuditLLM: A Tool for Auditing Large Language Models Using Multiprobe Approach
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p><span class='px-1 mx-1 bg-yellow-200'>As Large Language Models (LLMs) gain wider adoption in various contexts, it becomes crucial to ensure they are reasonably safe, consistent, and reliable for an application at hand. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.748</span></span>This may require probing or auditing them.<span class='px-1 mx-1 bg-yellow-200'>Probing LLMs with varied iterations of a single question could reveal potential inconsistencies in their knowledge or functionality. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.773</span></span>However, a tool for performing such audits with simple workflow and low technical threshold is lacking.<span class='px-1 mx-1 bg-yellow-200'>In this demo, we introduce "AuditLLM," a novel tool designed to evaluate the performance of various LLMs in a methodical way. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.729</span></span><span class='px-1 mx-1 bg-yellow-200'>AuditLLM's core functionality lies in its ability to test a given LLM by auditing it using multiple probes generated from a single question, thereby identifying any inconsistencies in the model's understanding or operation. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.707</span></span><span class='px-1 mx-1 bg-yellow-200'>A reasonably robust, reliable, and consistent LLM should output semantically similar responses for a question asked differently or by different people. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.691</span></span><span class='px-1 mx-1 bg-yellow-200'>Based on this assumption, AuditLLM produces easily interpretable results regarding the LLM's consistencies from a single question that the user enters. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.682</span></span>A certain level of inconsistency has been shown to be an indicator of potential bias, hallucinations, and other issues.<span class='px-1 mx-1 bg-yellow-200'>One could then use the output of AuditLLM to further investigate issues with the aforementioned LLM. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.748</span></span><span class='px-1 mx-1 bg-yellow-200'>To facilitate demonstration and practical uses, AuditLLM offers two key modes: (1) Live mode which allows instant auditing of LLMs by analyzing responses to real-time queries; (2) Batch mode which facilitates comprehensive LLM auditing by processing multiple queries at once for in-depth analysis. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.701</span></span><span class='px-1 mx-1 bg-yellow-200'>This tool is beneficial for both researchers and general users, as it enhances our understanding of LLMs' capabilities in generating responses, using a standardized auditing platform. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.708</span></span></p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2402.09334v1' target="_blank">
                  link
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-02-14</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Developing a Framework for Auditing Large Language Models Using Human-in-the-Loop
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p><span class='px-1 mx-1 bg-yellow-200'>As LLMs become more pervasive across various users and scenarios, identifying potential issues when using these models becomes essential. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.734</span></span>Examples include bias, inconsistencies, and hallucination.<span class='px-1 mx-1 bg-yellow-200'>Although auditing the LLM for these problems is desirable, it is far from being easy or solved. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.672</span></span><span class='px-1 mx-1 bg-yellow-200'>An effective method is to probe the LLM using different versions of the same question. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.772</span></span>This could expose inconsistencies in its knowledge or operation, indicating potential for bias or hallucination.However, to operationalize this auditing method at scale, we need an approach to create those probes reliably and automatically.<span class='px-1 mx-1 bg-yellow-200'>In this paper we propose an automatic and scalable solution, where one uses a different LLM along with human-in-the-loop. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.653</span></span><span class='px-1 mx-1 bg-yellow-200'>This approach offers verifiability and transparency, while avoiding circular reliance on the same LLMs, and increasing scientific rigor and generalizability. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.648</span></span>Specifically, we present a novel methodology with two phases of verification using humans: standardized evaluation criteria to verify responses, and a structured prompt template to generate desired probes.<span class='px-1 mx-1 bg-yellow-200'>Experiments on a set of questions from TruthfulQA dataset show that we can generate a reliable set of probes from one LLM that can be used to audit inconsistencies in a different LLM. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.709</span></span><span class='px-1 mx-1 bg-yellow-200'>The criteria for generating and applying auditing probes is generalizable to various LLMs regardless of the underlying structure or training mechanism. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.722</span></span></p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2402.09346v1' target="_blank">
                  link
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-02-14</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Mechanism Design for Automated Market Makers
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Blockchains have popularized automated market makers (AMMs).<span class='px-1 mx-1 bg-yellow-200'>An AMM exchange is an application running on a blockchain which maintains a pool of crypto-assets and automatically trades assets with users governed by some pricing function that prices the assets based on their relative demand/supply. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.613</span></span>AMMs have created an important challenge commonly known as the Miner Extractable Value (MEV).In particular, the miners who control the contents and ordering of transactions in a block can extract value by front-running and back-running users' transactions, leading to arbitrage opportunities that guarantee them risk-free returns.   <span class='px-1 mx-1 bg-yellow-200'>In this paper, we consider how to design AMM mechanisms that eliminate MEV opportunities. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.628</span></span>Specifically, we propose a new AMM mechanism that processes all transactions contained within a block in a batch.We show that our new mechanism satisfies two tiers of guarantees.First, for legacy blockchains where each block is proposed by a single (possibly rotating) miner, we prove that our mechanism satisfies arbitrage resilience, i.e., a miner cannot gain risk-free profit.Moreover, we also guarantee fair treatment among all transactions within the same block, such that the miner is unable to sell off favorable positions in the block to users or arbitragers.Second, for blockchains where the block proposal process is decentralized and offers sequencing-fairness, we prove a stronger notion called incentive compatibility -- roughly speaking, we guarantee that any individual user's best response is to follow the honest strategy.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2402.09357v1' target="_blank">
                  link
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-02-14</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                HiRE: High Recall Approximate Top-$k$ Estimation for Efficient LLM Inference
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p><span class='px-1 mx-1 bg-yellow-200'>Autoregressive decoding with generative Large Language Models (LLMs) on accelerators (GPUs/TPUs) is often memory-bound where most of the time is spent on transferring model parameters from high bandwidth memory (HBM) to cache. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.667</span></span><span class='px-1 mx-1 bg-yellow-200'>On the other hand, recent works show that LLMs can maintain quality with significant sparsity/redundancy in the feedforward (FFN) layers by appropriately training the model to operate on a top-$k$ fraction of rows/columns (where <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.709</span></span>$k \approx 0.05$), there by suggesting a way to reduce the transfer of model parameters, and hence latency.However, exploiting this sparsity for improving latency is hindered by the fact that identifying top rows/columns is data-dependent and is usually performed using full matrix operations, severely limiting potential gains.To address these issues, we introduce HiRE (High Recall Approximate Top-k Estimation).HiRE comprises of two novel components: (i) a compression scheme to cheaply predict top-$k$ rows/columns with high recall, followed by full computation restricted to the predicted subset, and (ii) DA-TOP-$k$: an efficient multi-device approximate top-$k$ operator.We demonstrate that on a one billion parameter model, HiRE applied to both the softmax as well as feedforward layers, achieves almost matching pretraining and downstream accuracy, and speeds up inference latency by $1.47\times$ on a single TPUv5e device.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2402.09360v1' target="_blank">
                  link
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-02-14</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Copyright Traps for Large Language Models
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p><span class='px-1 mx-1 bg-yellow-200'>Questions of fair use of copyright-protected content to train Large Language Models (LLMs) are being very actively debated. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.719</span></span>Document-level inference has been proposed as a new task: inferring from black-box access to the trained model whether a piece of content has been seen during training.SOTA methods however rely on naturally occurring memorization of (part of) the content.While very effective against models that memorize a lot, we hypothesize--and later confirm--that they will not work against models that do not naturally memorize, e.g. medium-size 1B models.<span class='px-1 mx-1 bg-yellow-200'>We here propose to use copyright traps, the inclusion of fictitious entries in original content, to detect the use of copyrighted materials in LLMs with a focus on models where memorization does not naturally occur. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.705</span></span><span class='px-1 mx-1 bg-yellow-200'>We carefully design an experimental setup, randomly inserting traps into original content (books) and train a 1.3B LLM. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.715</span></span>We first validate that the use of content in our target model would be undetectable using existing methods.We then show, contrary to intuition, that even medium-length trap sentences repeated a significant number of times (100) are not detectable using existing methods.However, we show that longer sequences repeated a large number of times can be reliably detected (AUC=0.75) and used as copyright traps.We further improve these results by studying how the number of times a sequence is seen improves detectability, how sequences with higher perplexity tend to be memorized more, and how taking context into account further improves detectability.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2402.09363v1' target="_blank">
                  link
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-02-14</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                HGOT: Hierarchical Graph of Thoughts for Retrieval-Augmented In-Context Learning in Factuality Evaluation
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p><span class='px-1 mx-1 bg-yellow-200'>With the widespread adoption of large language models (LLMs) in numerous applications, the challenge of factuality and the propensity for hallucinations raises significant concerns. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.669</span></span>To address this issue, particularly in retrieval-augmented in-context learning, we introduce the hierarchical graph of thoughts (HGOT), a structured, multi-layered graph approach designed to enhance the retrieval of pertinent passages during in-context learning.<span class='px-1 mx-1 bg-yellow-200'>The framework utilizes the emergent planning capabilities of LLMs, employing the divide-and-conquer strategy to break down complex queries into manageable sub-queries. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.692</span></span>It refines self-consistency majority voting for answer selection, which incorporates the recently proposed citation recall and precision metrics to assess the quality of thoughts, linking an answer's credibility intrinsically to the thought's quality.This methodology introduces a weighted system in majority voting, prioritizing answers based on the citation quality of their thoughts.Additionally, we propose a scoring mechanism for evaluating retrieved passages, considering factors such as citation frequency and quality, self-consistency confidence, and the retrieval module's ranking.<span class='px-1 mx-1 bg-yellow-200'>Experiments reveal that HGOT outperforms other retrieval-augmented in-context learning methods, including Demonstrate-Search-Predict (DSP), ReAct, Self-Ask, and Retrieve-then-Read on different datasets by as much as $7\%$, demonstrating its efficacy in enhancing the factuality of LLMs. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.659</span></span></p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2402.09390v1' target="_blank">
                  link
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-02-14</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                LlaSMol: Advancing Large Language Models for Chemistry with a Large-Scale, Comprehensive, High-Quality Instruction Tuning Dataset
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Chemistry plays a crucial role in many domains, such as drug discovery and material science.<span class='px-1 mx-1 bg-yellow-200'>While large language models (LLMs) such as GPT-4 exhibit remarkable capabilities on natural language processing tasks, existing work shows their performance on chemistry tasks is discouragingly low. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.621</span></span><span class='px-1 mx-1 bg-yellow-200'>In this paper, however, we demonstrate that our developed LLMs can achieve very strong results on a comprehensive set of chemistry tasks, outperforming the most advanced GPT-4 across all the tasks by a substantial margin and approaching the SoTA task-specific models. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.647</span></span>The key to our success is a large-scale, comprehensive, high-quality dataset for instruction tuning named SMolInstruct.<span class='px-1 mx-1 bg-yellow-200'>It contains 14 meticulously selected chemistry tasks and over three million high-quality samples, laying a solid foundation for training and evaluating LLMs for chemistry. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.717</span></span><span class='px-1 mx-1 bg-yellow-200'>Based on SMolInstruct, we fine-tune a set of open-source LLMs, among which, we find that Mistral serves as the best base model for chemistry tasks. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.716</span></span>We further conduct analysis on the impact of trainable parameters, providing insights for future research.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2402.09391v1' target="_blank">
                  link
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-02-14</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Reinforcement Learning from Human Feedback with Active Queries
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p><span class='px-1 mx-1 bg-yellow-200'>Aligning large language models (LLM) with human preference plays a key role in building modern generative models and can be achieved by reinforcement learning from human feedback (RLHF). <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.626</span></span>Despite their superior performance, current RLHF approaches often require a large amount of human-labelled preference data, which is expensive to collect.In this paper, inspired by the success of active learning, we address this problem by proposing query-efficient RLHF methods.We first formalize the alignment problem as a contextual dueling bandit problem and design an active-query-based proximal policy optimization (APPO) algorithm with an $\tilde{O}(d^2/\Delta)$ regret bound and an $\tilde{O}(d^2/\Delta^2)$ query complexity, where $d$ is the dimension of feature space and $\Delta$ is the sub-optimality gap over all the contexts.<span class='px-1 mx-1 bg-yellow-200'>We then propose ADPO, a practical version of our algorithm based on direct preference optimization (DPO) and apply it to fine-tuning LLMs. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.609</span></span>Our experiments show that ADPO, while only making about half of queries for human preference, matches the performance of the state-of-the-art DPO method.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2402.09401v1' target="_blank">
                  link
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-02-14</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                AQA-Bench: An Interactive Benchmark for Evaluating LLMs' Sequential Reasoning Ability
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>This paper introduces AQA-Bench, a novel benchmark to assess the sequential reasoning capabilities of large language models (LLMs) in algorithmic contexts, such as depth-first search (DFS).<span class='px-1 mx-1 bg-yellow-200'>The key feature of our evaluation benchmark lies in its interactive evaluation protocol -- for example, in DFS, the availability of each node's connected edge is contingent upon the model's traversal to that node, thereby necessitating the LLM's ability to effectively remember visited nodes and strategize subsequent moves. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.65</span></span><span class='px-1 mx-1 bg-yellow-200'>We comprehensively build AQA-Bench with three different algorithms, namely binary search, depth-first search, and breadth-first search, and to evaluate the sequential reasoning ability of 12 different LLMs. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.62</span></span><span class='px-1 mx-1 bg-yellow-200'>Our investigations reveal several interesting findings: (1) Closed-source models like GPT-4 and Gemini generally show strong sequential reasoning ability, significantly outperforming open-source LLMs. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.656</span></span>(2) Naively providing interactive examples may inadvertently hurt few-shot performance.(3) A very limited number of predecessor steps following the optimal policy can substantially boost small models' performance.(4) The scaling correlation between performance and model size is not always significant, sometimes even showcasing an inverse trend.<span class='px-1 mx-1 bg-yellow-200'>We hope our study can catalyze future work on advancing the understanding and enhancement of LLMs' capabilities in sequential reasoning. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.725</span></span>The code is available at https://github.com/UCSC-VLAA/AQA-Bench.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2402.09404v1' target="_blank">
                  link
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td></td>
          <td>
            <h2 class="text-2xl tracking-tight pt-4 font-bold">Developer Research</h2>
          </td>
        </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-02-14</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Context Composing for Full Line Code Completion
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p><span class='px-1 mx-1 bg-yellow-200'>Code Completion is one of the most used Integrated Development Environment (IDE) features, which affects the everyday life of a software developer. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.62</span></span>Modern code completion approaches moved from the composition of several static analysis-based contributors to pipelines that involve neural networks.This change allows the proposal of longer code suggestions while maintaining the relatively short time spent on generation itself.At JetBrains, we put a lot of effort into perfecting the code completion workflow so it can be both helpful and non-distracting for a programmer.We managed to ship the Full Line Code Completion feature to PyCharm Pro IDE and proved its usefulness in A/B testing on hundreds of real Python users.The paper describes our approach to context composing for the Transformer model that is a core of the feature's implementation.In addition to that, we share our next steps to improve the feature and emphasize the importance of several research aspects in the area.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2402.09230v1' target="_blank">
                  link
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-02-13</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Evidence Tetris in the Pixelated World of Validity Threats
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Valid empirical studies build confidence in scientific findings.<span class='px-1 mx-1 bg-yellow-200'>Fortunately, it is now common for software engineering researchers to consider threats to validity when designing their studies and to discuss them as part of their publication. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.662</span></span>Yet, in complex experiments with human participants, there is often an overwhelming number of intuitively plausible threats to validity -- more than a researcher can feasibly cover.Therefore, prioritizing potential threats to validity becomes crucial.We suggest moving away from relying solely on intuition for prioritizing validity threats, and propose that evidence on the actual impact of suspected threats to validity should complement intuition.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2402.08608v1' target="_blank">
                  link
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-02-05</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Charting The Evolution of Solidity Error Handling
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>The usage of error handling in Solidity smart contracts is vital because smart contracts perform transactions that should be verified.Transactions that are not carefully handled, may lead to program crashes and vulnerabilities, implying financial loss and legal consequences.<span class='px-1 mx-1 bg-yellow-200'>While Solidity designers attempt to constantly update the language with new features, including error-handling (EH) features, it is necessary for developers to promptly absorb how to use them. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.667</span></span>We conduct a large-scale empirical study on 283K unique open-source smart contracts to identify patterns regarding the usage of Solidity EH features over time.Overall, the usage of most EH features is limited.However, we observe an upward trend (> 60%) in the usage of a Solidity-tailored EH feature, i.e., require.<span class='px-1 mx-1 bg-yellow-200'>This indicates that designers of modern programming languages may consider making error handling more tailored to the purposes of each language. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.659</span></span>Our analysis on 102 versions of the Solidity documentation indicates the volatile nature of Solidity, as the language changes frequently, i.e., there are changes on EH features once or twice a year.Such frequent releases may confuse smart contract developers, discouraging them to carefully read the Solidity documentation, and correctly adopt EH features.Furthermore, our findings reveal that nearly 70% of the examined smart contracts are exposed to potential failures due to missing error handing, e.g., unchecked external calls.Therefore, the use of EH features should be further supported via a more informative documentation containing (1) representative and meaningful examples and (2) details about the impact of potential EH misuses.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2402.03186v1' target="_blank">
                  link
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-02-01</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                An Investigation of Hardware Security Bug Characteristics in Open-Source Projects
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Hardware security is an important concern of system security as vulnerabilities can arise from design errors introduced throughout the development lifecycle.Recent works have proposed techniques to detect hardware security bugs, such as static analysis, fuzzing, and symbolic execution.However, the fundamental properties of hardware security bugs remain relatively unexplored.To gain a better understanding of hardware security bugs, we perform a deep dive into the popular OpenTitan project, including its bug reports and bug fixes.We manually classify the bugs as relevant to functionality or security and analyze characteristics, such as the impact and location of security bugs, and the size of their bug fixes.<span class='px-1 mx-1 bg-yellow-200'>We also investigate relationships between security impact and bug management during development. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.664</span></span><span class='px-1 mx-1 bg-yellow-200'>Finally, we propose an abstract syntax tree-based analysis to identify the syntactic characteristics of bug fixes. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.644</span></span>Our results show that 53% of the bugs in OpenTitan have potential security implications and that 55% of all bug fixes modify only one file.<span class='px-1 mx-1 bg-yellow-200'>Our findings underscore the importance of security-aware development practices and tools and motivate the development of techniques that leverage the highly localized nature of hardware bugs. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.616</span></span></p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2402.00684v1' target="_blank">
                  link
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-02-01</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                To Search or To Gen? Exploring the Synergy between Generative AI and Web Search in Programming
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>The convergence of generative AI and web search is reshaping problem-solving for programmers.However, the lack of understanding regarding their interplay in the information-seeking process often leads programmers to perceive them as alternatives rather than complementary tools.To analyze this interaction and explore their synergy, we conducted an interview study with eight experienced programmers.Drawing from the results and literature, we have identified three major challenges and proposed three decision-making stages, each with its own relevant factors.<span class='px-1 mx-1 bg-yellow-200'>Additionally, we present a comprehensive process model that captures programmers' interaction patterns. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.631</span></span>This model encompasses decision-making stages, the information-foraging loop, and cognitive activities during system interaction, offering a holistic framework to comprehend and optimize the use of these convergent tools in programming.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2402.00764v1' target="_blank">
                  link
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-01-29</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Security Code Review by LLMs: A Deep Dive into Responses
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p><span class='px-1 mx-1 bg-yellow-200'>Security code review aims to combine automated tools and manual efforts to detect security defects during development. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.676</span></span>The rapid development of Large Language Models (LLMs) has shown promising potential in software development, as well as opening up new possibilities in automated security code review.To explore the challenges of applying LLMs in practical code review for security defect detection, this study compared the detection performance of three state-of-the-art LLMs (Gemini Pro, GPT-4, and GPT-3.5) under five prompts on 549 code files that contain security defects from real-world code reviews.Through analyzing 82 responses generated by the best-performing LLM-prompt combination based on 100 randomly selected code files, we extracted and categorized quality problems present in these responses into 5 themes and 16 categories.Our results indicate that the responses produced by LLMs often suffer from verbosity, vagueness, and incompleteness, highlighting the necessity to enhance their conciseness, understandability, and compliance to security defect detection.This work reveals the deficiencies of LLM-generated responses in security code review and paves the way for future optimization of LLMs towards this task.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2401.16310v1' target="_blank">
                  link
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-01-29</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                The role of library versions in Developer-ChatGPT conversations
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>The latest breakthroughs in large language models (LLM) have empowered software development tools, such as ChatGPT, to aid developers in complex tasks.<span class='px-1 mx-1 bg-yellow-200'>Developers use ChatGPT to write code, review code changes, and even debug their programs. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.673</span></span>In these interactions, ChatGPT often recommends code snippets that depend on external libraries.However, code from libraries changes over time, invalidating a once-correct code snippet and making it difficult to reuse recommended code.   <span class='px-1 mx-1 bg-yellow-200'>In this study, we analyze DevGPT, a dataset of more than 4,000 Developer-ChatGPT interactions, to understand the role of library versions in code-related conversations. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.616</span></span>We quantify how often library version constraints are mentioned in code-related conversations and when ChatGPT recommends the installation of specific libraries.Our findings show that, albeit to constantly recommend and analyze code with external dependencies, library version constraints only appear in 9% of the conversations.In the majority of conversations, the version constraints are prompted by users (as opposed to being specified by ChatGPT) as a method for receiving better quality responses.Moreover, we study how library version constraints are used in the conversation through qualitative methods, identifying several potential problems that warrant further research.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2401.16340v1' target="_blank">
                  link
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-01-24</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                What Makes a Great Software Quality Assurance Engineer?
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Software Quality Assurance (SQA) Engineers are responsible for assessing a product during every phase of the software development process to ensure that the outcomes of each phase and the final product possess the desired qualities.In general, a great SQA engineer needs to have a different set of abilities from development engineers to effectively oversee the entire product development process from beginning to end.<span class='px-1 mx-1 bg-yellow-200'>Recent empirical studies identified important attributes of software engineers and managers, but the quality assurance role is overlooked. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.628</span></span>As software quality aspects have become more of a priority in the life cycle of software development, employers seek professionals that best suit the company's objectives and new graduates desire to make a valuable contribution through their job as an SQA engineer, but what makes them great?We addressed this knowledge gap by conducting 25 semi-structured interviews and 363 survey respondents with software quality assurance engineers from different companies around the world.We use the data collected from these activities to derive a comprehensive set of attributes that are considered important.As a result of the interviews, twenty-five attributes were identified and grouped into five main categories: personal, social, technical, management, and decision-making attributes.Through a rating survey, we confirmed that the distinguishing characteristics of great SQA engineers are curiosity, the ability to communicate effectively, and critical thinking skills.This work will guide further studies with SQA practitioners, by considering contextual factors and providing some implications for research and practice.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2401.13623v1' target="_blank">
                  link
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td></td>
          <td>
            <h2 class="text-2xl tracking-tight pt-4 font-bold">Data Annotation Techniques</h2>
          </td>
        </tr></tbody>
  </table>
  <br><br>
</div>
</div>
<script>
  document.addEventListener("DOMContentLoaded", function() {
    renderMathInElement(document.body, {
      // customised options
      // • auto-render specific keys, e.g.:
      delimiters: [
      {left: '$$', right: '$$', display: true},
      {left: '$', right: '$', display: false},
      {left: '\\(', right: '\\)', display: false},
      {left: '\\[', right: '\\]', display: true}
      ],
      // • rendering keys, e.g.:
      throwOnError : false
    });
  });
</script>
</body>
</html>