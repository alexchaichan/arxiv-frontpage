{"created":"2024-03-25 13:19:09","title":"Multilevel Modeling as a Methodology for the Simulation of Human Mobility","abstract":"Multilevel modeling is increasingly relevant in the context of modelling and simulation since it leads to several potential benefits, such as software reuse and integration, the split of semantically separated levels into sub-models, the possibility to employ different levels of detail, and the potential for parallel execution. The coupling that inevitably exists between the sub-models, however, implies the need for maintaining consistency between the various components, more so when different simulation paradigms are employed (e.g., sequential vs parallel, discrete vs continuous). In this paper we argue that multilevel modelling is well suited for the simulation of human mobility, since it naturally leads to the decomposition of the model into two layers, the \"micro\" and \"macro\" layer, where individual entities (micro) and long-range interactions (macro) are described. In this paper we investigate the challenges of multilevel modeling, and describe some preliminary results using prototype implementations of multilayer simulators in the context of epidemic diffusion and vehicle pollution.","sentences":["Multilevel modeling is increasingly relevant in the context of modelling and simulation since it leads to several potential benefits, such as software reuse and integration, the split of semantically separated levels into sub-models, the possibility to employ different levels of detail, and the potential for parallel execution.","The coupling that inevitably exists between the sub-models, however, implies the need for maintaining consistency between the various components, more so when different simulation paradigms are employed (e.g., sequential vs parallel, discrete vs continuous).","In this paper we argue that multilevel modelling is well suited for the simulation of human mobility, since it naturally leads to the decomposition of the model into two layers, the \"micro\" and \"macro\" layer, where individual entities (micro) and long-range interactions (macro) are described.","In this paper we investigate the challenges of multilevel modeling, and describe some preliminary results using prototype implementations of multilayer simulators in the context of epidemic diffusion and vehicle pollution."],"url":"http://arxiv.org/abs/2403.16745v1"}
{"created":"2024-03-25 13:12:39","title":"Looking back and forward: A retrospective and future directions on Software Engineering for systems-of-systems","abstract":"Modern systems are increasingly connected and more integrated with other existing systems, giving rise to systems-of-systems (SoS). An SoS consists of a set of independent, heterogeneous systems that interact to provide new functionalities and accomplish global missions through emergent behavior manifested at runtime. The distinctive characteristics of SoS, when contrasted to traditional systems, pose significant research challenges within Software Engineering. These challenges motivate the need for a paradigm shift and the exploration of novel approaches for designing, developing, deploying, and evolving these systems. The International Workshop on Software Engineering for Systems-of-Systems (SESoS) series started in 2013 to fill a gap in scientific forums addressing SoS from the Software Engineering perspective, becoming the first venue for this purpose. This article presents a study aimed at outlining the evolution and future trajectory of Software Engineering for SoS based on the examination of 57 papers spanning the 11 editions of the SESoS workshop (2013-2023). The study combined scoping review and scientometric analysis methods to categorize and analyze the research contributions concerning temporal and geographic distribution, topics of interest, research methodologies employed, application domains, and research impact. Based on such a comprehensive overview, this article discusses current and future directions in Software Engineering for SoS.","sentences":["Modern systems are increasingly connected and more integrated with other existing systems, giving rise to systems-of-systems (SoS).","An SoS consists of a set of independent, heterogeneous systems that interact to provide new functionalities and accomplish global missions through emergent behavior manifested at runtime.","The distinctive characteristics of SoS, when contrasted to traditional systems, pose significant research challenges within Software Engineering.","These challenges motivate the need for a paradigm shift and the exploration of novel approaches for designing, developing, deploying, and evolving these systems.","The International Workshop on Software Engineering for Systems-of-Systems (SESoS) series started in 2013 to fill a gap in scientific forums addressing SoS from the Software Engineering perspective, becoming the first venue for this purpose.","This article presents a study aimed at outlining the evolution and future trajectory of Software Engineering for SoS based on the examination of 57 papers spanning the 11 editions of the SESoS workshop (2013-2023).","The study combined scoping review and scientometric analysis methods to categorize and analyze the research contributions concerning temporal and geographic distribution, topics of interest, research methodologies employed, application domains, and research impact.","Based on such a comprehensive overview, this article discusses current and future directions in Software Engineering for SoS."],"url":"http://arxiv.org/abs/2403.16740v1"}
{"created":"2024-03-25 13:09:40","title":"Creating a Digital Twin of Spinal Surgery: A Proof of Concept","abstract":"Surgery digitalization is the process of creating a virtual replica of real-world surgery, also referred to as a surgical digital twin (SDT). It has significant applications in various fields such as education and training, surgical planning, and automation of surgical tasks. Given their detailed representations of surgical procedures, SDTs are an ideal foundation for machine learning methods, enabling automatic generation of training data. In robotic surgery, SDTs can provide realistic virtual environments in which robots may learn through trial and error. In this paper, we present a proof of concept (PoC) for surgery digitalization that is applied to an ex-vivo spinal surgery performed in realistic conditions. The proposed digitalization focuses on the acquisition and modelling of the geometry and appearance of the entire surgical scene. We employ five RGB-D cameras for dynamic 3D reconstruction of the surgeon, a high-end camera for 3D reconstruction of the anatomy, an infrared stereo camera for surgical instrument tracking, and a laser scanner for 3D reconstruction of the operating room and data fusion. We justify the proposed methodology, discuss the challenges faced and further extensions of our prototype. While our PoC partially relies on manual data curation, its high quality and great potential motivate the development of automated methods for the creation of SDTs. The quality of our SDT can be assessed in a rendered video available at https://youtu.be/LqVaWGgaTMY .","sentences":["Surgery digitalization is the process of creating a virtual replica of real-world surgery, also referred to as a surgical digital twin (SDT).","It has significant applications in various fields such as education and training, surgical planning, and automation of surgical tasks.","Given their detailed representations of surgical procedures, SDTs are an ideal foundation for machine learning methods, enabling automatic generation of training data.","In robotic surgery, SDTs can provide realistic virtual environments in which robots may learn through trial and error.","In this paper, we present a proof of concept (PoC) for surgery digitalization that is applied to an ex-vivo spinal surgery performed in realistic conditions.","The proposed digitalization focuses on the acquisition and modelling of the geometry and appearance of the entire surgical scene.","We employ five RGB-D cameras for dynamic 3D reconstruction of the surgeon, a high-end camera for 3D reconstruction of the anatomy, an infrared stereo camera for surgical instrument tracking, and a laser scanner for 3D reconstruction of the operating room and data fusion.","We justify the proposed methodology, discuss the challenges faced and further extensions of our prototype.","While our PoC partially relies on manual data curation, its high quality and great potential motivate the development of automated methods for the creation of SDTs.","The quality of our SDT can be assessed in a rendered video available at https://youtu.be/LqVaWGgaTMY ."],"url":"http://arxiv.org/abs/2403.16736v1"}
{"created":"2024-03-25 13:06:31","title":"Enabling Uncertainty Estimation in Iterative Neural Networks","abstract":"Turning pass-through network architectures into iterative ones, which use their own output as input, is a well-known approach for boosting performance. In this paper, we argue that such architectures offer an additional benefit: The convergence rate of their successive outputs is highly correlated with the accuracy of the value to which they converge. Thus, we can use the convergence rate as a useful proxy for uncertainty. This results in an approach to uncertainty estimation that provides state-of-the-art estimates at a much lower computational cost than techniques like Ensembles, and without requiring any modifications to the original iterative model. We demonstrate its practical value by embedding it in two application domains: road detection in aerial images and the estimation of aerodynamic properties of 2D and 3D shapes.","sentences":["Turning pass-through network architectures into iterative ones, which use their own output as input, is a well-known approach for boosting performance.","In this paper, we argue that such architectures offer an additional benefit: The convergence rate of their successive outputs is highly correlated with the accuracy of the value to which they converge.","Thus, we can use the convergence rate as a useful proxy for uncertainty.","This results in an approach to uncertainty estimation that provides state-of-the-art estimates at a much lower computational cost than techniques like Ensembles, and without requiring any modifications to the original iterative model.","We demonstrate its practical value by embedding it in two application domains: road detection in aerial images and the estimation of aerodynamic properties of 2D and 3D shapes."],"url":"http://arxiv.org/abs/2403.16732v1"}
{"created":"2024-03-25 13:04:20","title":"A Robotic Skill Learning System Built Upon Diffusion Policies and Foundation Models","abstract":"In this paper, we build upon two major recent developments in the field, Diffusion Policies for visuomotor manipulation and large pre-trained multimodal foundational models to obtain a robotic skill learning system. The system can obtain new skills via the behavioral cloning approach of visuomotor diffusion policies given teleoperated demonstrations. Foundational models are being used to perform skill selection given the user's prompt in natural language. Before executing a skill the foundational model performs a precondition check given an observation of the workspace. We compare the performance of different foundational models to this end as well as give a detailed experimental evaluation of the skills taught by the user in simulation and the real world. Finally, we showcase the combined system on a challenging food serving scenario in the real world. Videos of all experimental executions, as well as the process of teaching new skills in simulation and the real world, are available on the project's website.","sentences":["In this paper, we build upon two major recent developments in the field, Diffusion Policies for visuomotor manipulation and large pre-trained multimodal foundational models to obtain a robotic skill learning system.","The system can obtain new skills via the behavioral cloning approach of visuomotor diffusion policies given teleoperated demonstrations.","Foundational models are being used to perform skill selection given the user's prompt in natural language.","Before executing a skill the foundational model performs a precondition check given an observation of the workspace.","We compare the performance of different foundational models to this end as well as give a detailed experimental evaluation of the skills taught by the user in simulation and the real world.","Finally, we showcase the combined system on a challenging food serving scenario in the real world.","Videos of all experimental executions, as well as the process of teaching new skills in simulation and the real world, are available on the project's website."],"url":"http://arxiv.org/abs/2403.16730v1"}
{"created":"2024-03-25 13:02:43","title":"Improving Diffusion Models's Data-Corruption Resistance using Scheduled Pseudo-Huber Loss","abstract":"Diffusion models are known to be vulnerable to outliers in training data. In this paper we study an alternative diffusion loss function, which can preserve the high quality of generated data like the original squared $L_{2}$ loss while at the same time being robust to outliers. We propose to use pseudo-Huber loss function with a time-dependent parameter to allow for the trade-off between robustness on the most vulnerable early reverse-diffusion steps and fine details restoration on the final steps. We show that pseudo-Huber loss with the time-dependent parameter exhibits better performance on corrupted datasets in both image and audio domains. In addition, the loss function we propose can potentially help diffusion models to resist dataset corruption while not requiring data filtering or purification compared to conventional training algorithms.","sentences":["Diffusion models are known to be vulnerable to outliers in training data.","In this paper we study an alternative diffusion loss function, which can preserve the high quality of generated data like the original squared $L_{2}$ loss while at the same time being robust to outliers.","We propose to use pseudo-Huber loss function with a time-dependent parameter to allow for the trade-off between robustness on the most vulnerable early reverse-diffusion steps and fine details restoration on the final steps.","We show that pseudo-Huber loss with the time-dependent parameter exhibits better performance on corrupted datasets in both image and audio domains.","In addition, the loss function we propose can potentially help diffusion models to resist dataset corruption while not requiring data filtering or purification compared to conventional training algorithms."],"url":"http://arxiv.org/abs/2403.16728v1"}
{"created":"2024-03-25 12:56:48","title":"Towards a Formalisation of Value-based Actions and Consequentialist Ethics","abstract":"Agents act to bring about a state of the world that is more compatible with their personal or institutional values. To formalise this intuition, the paper proposes an action framework based on the STRIPS formalisation. Technically, the contribution expresses actions in terms of Value-based Formal Reasoning (VFR), which provides a set of propositions derived from an Agent's value profile and the Agent's assessment of propositions with respect to the profile. Conceptually, the contribution provides a computational framework for a form of consequentialist ethics which is satisficing, luralistic, act-based, and preferential.","sentences":["Agents act to bring about a state of the world that is more compatible with their personal or institutional values.","To formalise this intuition, the paper proposes an action framework based on the STRIPS formalisation.","Technically, the contribution expresses actions in terms of Value-based Formal Reasoning (VFR), which provides a set of propositions derived from an Agent's value profile and the Agent's assessment of propositions with respect to the profile.","Conceptually, the contribution provides a computational framework for a form of consequentialist ethics which is satisficing, luralistic, act-based, and preferential."],"url":"http://arxiv.org/abs/2403.16719v1"}
{"created":"2024-03-25 12:51:22","title":"Design Patterns for Multilevel Modeling and Simulation","abstract":"Multilevel modeling and simulation (M&S) is becoming increasingly relevant due to the benefits that this methodology offers. Multilevel models allow users to describe a system at multiple levels of detail. From one side, this can make better use of computational resources, since the more detailed and time-consuming models can be executed only when/where required. From the other side, multilevel models can be assembled from existing components, cutting down development and verification/validation time. A downside of multilevel M&S is that the development process becomes more complex due to some recurrent issues caused by the very nature of multilevel models: how to make sub-models interoperate, how to orchestrate execution, how state variables are to be updated when changing scale, and so on. In this paper, we address some of these issues by presenting a set of design patterns that provide a systematic approach for designing and implementing multilevel models. The proposed design patterns cover multiple aspects, including how to represent different levels of detail, how to combine incompatible models, how to exchange data across models, and so on. Some of the patterns are derived from the general software engineering literature, while others are specific to the multilevel M&S application area.","sentences":["Multilevel modeling and simulation (M&S) is becoming increasingly relevant due to the benefits that this methodology offers.","Multilevel models allow users to describe a system at multiple levels of detail.","From one side, this can make better use of computational resources, since the more detailed and time-consuming models can be executed only when/where required.","From the other side, multilevel models can be assembled from existing components, cutting down development and verification/validation time.","A downside of multilevel M&S is that the development process becomes more complex due to some recurrent issues caused by the very nature of multilevel models: how to make sub-models interoperate, how to orchestrate execution, how state variables are to be updated when changing scale, and so on.","In this paper, we address some of these issues by presenting a set of design patterns that provide a systematic approach for designing and implementing multilevel models.","The proposed design patterns cover multiple aspects, including how to represent different levels of detail, how to combine incompatible models, how to exchange data across models, and so on.","Some of the patterns are derived from the general software engineering literature, while others are specific to the multilevel M&S application area."],"url":"http://arxiv.org/abs/2403.16713v1"}
{"created":"2024-03-25 12:49:40","title":"Chase Termination Beyond Polynomial Time","abstract":"The chase is a widely implemented approach to reason with tuple-generating dependencies (tgds), used in data exchange, data integration, and ontology-based query answering. However, it is merely a semi-decision procedure, which may fail to terminate. Many decidable conditions have been proposed for tgds to ensure chase termination, typically by forbidding some kind of \"cycle\" in the chase process. We propose a new criterion that explicitly allows some such cycles, and yet ensures termination of the standard chase under reasonable conditions. This leads to new decidable fragments of tgds that are not only syntactically more general but also strictly more expressive than the fragments defined by prior acyclicity conditions. Indeed, while known terminating fragments are restricted to PTime data complexity, our conditions yield decidable languages for any k-ExpTime. We further refine our syntactic conditions to obtain fragments of tgds for which an optimised chase procedure decides query entailment in PSpace or k-ExpSpace, respectively.","sentences":["The chase is a widely implemented approach to reason with tuple-generating dependencies (tgds), used in data exchange, data integration, and ontology-based query answering.","However, it is merely a semi-decision procedure, which may fail to terminate.","Many decidable conditions have been proposed for tgds to ensure chase termination, typically by forbidding some kind of \"cycle\" in the chase process.","We propose a new criterion that explicitly allows some such cycles, and yet ensures termination of the standard chase under reasonable conditions.","This leads to new decidable fragments of tgds that are not only syntactically more general but also strictly more expressive than the fragments defined by prior acyclicity conditions.","Indeed, while known terminating fragments are restricted to PTime data complexity, our conditions yield decidable languages for any k-ExpTime.","We further refine our syntactic conditions to obtain fragments of tgds for which an optimised chase procedure decides query entailment in PSpace or k-ExpSpace, respectively."],"url":"http://arxiv.org/abs/2403.16712v1"}
{"created":"2024-03-25 12:44:52","title":"One-Shot Domain Incremental Learning","abstract":"Domain incremental learning (DIL) has been discussed in previous studies on deep neural network models for classification. In DIL, we assume that samples on new domains are observed over time. The models must classify inputs on all domains. In practice, however, we may encounter a situation where we need to perform DIL under the constraint that the samples on the new domain are observed only infrequently. Therefore, in this study, we consider the extreme case where we have only one sample from the new domain, which we call one-shot DIL. We first empirically show that existing DIL methods do not work well in one-shot DIL. We have analyzed the reason for this failure through various investigations. According to our analysis, we clarify that the difficulty of one-shot DIL is caused by the statistics in the batch normalization layers. Therefore, we propose a technique regarding these statistics and demonstrate the effectiveness of our technique through experiments on open datasets.","sentences":["Domain incremental learning (DIL) has been discussed in previous studies on deep neural network models for classification.","In DIL, we assume that samples on new domains are observed over time.","The models must classify inputs on all domains.","In practice, however, we may encounter a situation where we need to perform DIL under the constraint that the samples on the new domain are observed only infrequently.","Therefore, in this study, we consider the extreme case where we have only one sample from the new domain, which we call one-shot DIL.","We first empirically show that existing DIL methods do not work well in one-shot DIL.","We have analyzed the reason for this failure through various investigations.","According to our analysis, we clarify that the difficulty of one-shot DIL is caused by the statistics in the batch normalization layers.","Therefore, we propose a technique regarding these statistics and demonstrate the effectiveness of our technique through experiments on open datasets."],"url":"http://arxiv.org/abs/2403.16707v1"}
{"created":"2024-03-25 12:34:33","title":"ProCQA: A Large-scale Community-based Programming Question Answering Dataset for Code Search","abstract":"Retrieval-based code question answering seeks to match user queries in natural language to relevant code snippets. Previous approaches typically rely on pretraining models using crafted bi-modal and uni-modal datasets to align text and code representations. In this paper, we introduce ProCQA, a large-scale programming question answering dataset extracted from the StackOverflow community, offering naturally structured mixed-modal QA pairs. To validate its effectiveness, we propose a modality-agnostic contrastive pre-training approach to improve the alignment of text and code representations of current code language models. Compared to previous models that primarily employ bimodal and unimodal pairs extracted from CodeSearchNet for pre-training, our model exhibits significant performance improvements across a wide range of code retrieval benchmarks.","sentences":["Retrieval-based code question answering seeks to match user queries in natural language to relevant code snippets.","Previous approaches typically rely on pretraining models using crafted bi-modal and uni-modal datasets to align text and code representations.","In this paper, we introduce ProCQA, a large-scale programming question answering dataset extracted from the StackOverflow community, offering naturally structured mixed-modal QA pairs.","To validate its effectiveness, we propose a modality-agnostic contrastive pre-training approach to improve the alignment of text and code representations of current code language models.","Compared to previous models that primarily employ bimodal and unimodal pairs extracted from CodeSearchNet for pre-training, our model exhibits significant performance improvements across a wide range of code retrieval benchmarks."],"url":"http://arxiv.org/abs/2403.16702v1"}
{"created":"2024-03-25 12:33:10","title":"Resonant Beam Communications: A New Design Paradigm and Challenges","abstract":"Resonant beam communications (RBCom), which adopt oscillating photons between two separate retroreflectors for information transmission, exhibit potential advantages over other types of wireless optical communications (WOC). However, echo interference generated by the modulated beam reflected from the receiver affects the transmission of the desired information. To tackle this challenge, a synchronization-based point-to-point RBCom system is proposed to eliminate the echo interference, and the design for the transmitter and receiver is discussed. Subsequently, the performance of the proposed RBCom is evaluated and compared with that of visible light communications (VLC) and free space optical communications (FOC). Finally, future research directions are outlined and several implementation challenges of RBCom systems are highlighted.","sentences":["Resonant beam communications (RBCom), which adopt oscillating photons between two separate retroreflectors for information transmission, exhibit potential advantages over other types of wireless optical communications (WOC).","However, echo interference generated by the modulated beam reflected from the receiver affects the transmission of the desired information.","To tackle this challenge, a synchronization-based point-to-point RBCom system is proposed to eliminate the echo interference, and the design for the transmitter and receiver is discussed.","Subsequently, the performance of the proposed RBCom is evaluated and compared with that of visible light communications (VLC) and free space optical communications (FOC).","Finally, future research directions are outlined and several implementation challenges of RBCom systems are highlighted."],"url":"http://arxiv.org/abs/2403.16699v1"}
{"created":"2024-03-25 12:31:01","title":"DPStyler: Dynamic PromptStyler for Source-Free Domain Generalization","abstract":"Source-Free Domain Generalization (SFDG) aims to develop a model that works for unseen target domains without relying on any source domain. Recent work, PromptStyler, employs text prompts to simulate different distribution shifts in the joint vision-language space, allowing the model to generalize effectively to unseen domains without using any images. However, 1) PromptStyler's style generation strategy has limitations, as all style patterns are fixed after the first training phase. This leads to the training set in the second training phase being restricted to a limited set of styles. Additionally, 2) the frozen text encoder in PromptStyler result in the encoder's output varying with the style of the input text prompts, making it difficult for the model to learn domain-invariant features. In this paper, we introduce Dynamic PromptStyler (DPStyler), comprising Style Generation and Style Removal modules to address these issues. The Style Generation module refreshes all styles at every training epoch, while the Style Removal module eliminates variations in the encoder's output features caused by input styles. Moreover, since the Style Generation module, responsible for generating style word vectors using random sampling or style mixing, makes the model sensitive to input text prompts, we introduce a model ensemble method to mitigate this sensitivity. Extensive experiments demonstrate that our framework outperforms state-of-the-art methods on benchmark datasets.","sentences":["Source-Free Domain Generalization (SFDG) aims to develop a model that works for unseen target domains without relying on any source domain.","Recent work, PromptStyler, employs text prompts to simulate different distribution shifts in the joint vision-language space, allowing the model to generalize effectively to unseen domains without using any images.","However, 1) PromptStyler's style generation strategy has limitations, as all style patterns are fixed after the first training phase.","This leads to the training set in the second training phase being restricted to a limited set of styles.","Additionally, 2) the frozen text encoder in PromptStyler result in the encoder's output varying with the style of the input text prompts, making it difficult for the model to learn domain-invariant features.","In this paper, we introduce Dynamic PromptStyler (DPStyler), comprising Style Generation and Style Removal modules to address these issues.","The Style Generation module refreshes all styles at every training epoch, while the Style Removal module eliminates variations in the encoder's output features caused by input styles.","Moreover, since the Style Generation module, responsible for generating style word vectors using random sampling or style mixing, makes the model sensitive to input text prompts, we introduce a model ensemble method to mitigate this sensitivity.","Extensive experiments demonstrate that our framework outperforms state-of-the-art methods on benchmark datasets."],"url":"http://arxiv.org/abs/2403.16697v1"}
{"created":"2024-03-25 12:27:24","title":"BatDeck: Advancing Nano-drone Navigation with Low-power Ultrasound-based Obstacle Avoidance","abstract":"Nano-drones, distinguished by their agility, minimal weight, and cost-effectiveness, are particularly well-suited for exploration in confined, cluttered and narrow spaces. Recognizing transparent, highly reflective or absorbing materials, such as glass and metallic surfaces is challenging, as classical sensors, such as cameras or laser rangers, often do not detect them. Inspired by bats, which can fly at high speeds in complete darkness with the help of ultrasound, this paper introduces \\textit{BatDeck}, a pioneering sensor-deck employing a lightweight and low-power ultrasonic sensor for nano-drone autonomous navigation. This paper first provides insights about sensor characteristics, highlighting the influence of motor noise on the ultrasound readings, then it introduces the results of extensive experimental tests for obstacle avoidance (OA) in a diverse environment. Results show that \\textit{BatDeck} allows exploration for a flight time of 8 minutes while covering 136m on average before crash in a challenging environment with transparent and reflective obstacles, proving the effectiveness of ultrasonic sensors for OA on nano-drones.","sentences":["Nano-drones, distinguished by their agility, minimal weight, and cost-effectiveness, are particularly well-suited for exploration in confined, cluttered and narrow spaces.","Recognizing transparent, highly reflective or absorbing materials, such as glass and metallic surfaces is challenging, as classical sensors, such as cameras or laser rangers, often do not detect them.","Inspired by bats, which can fly at high speeds in complete darkness with the help of ultrasound, this paper introduces \\textit{BatDeck}, a pioneering sensor-deck employing a lightweight and low-power ultrasonic sensor for nano-drone autonomous navigation.","This paper first provides insights about sensor characteristics, highlighting the influence of motor noise on the ultrasound readings, then it introduces the results of extensive experimental tests for obstacle avoidance (OA) in a diverse environment.","Results show that \\textit{BatDeck} allows exploration for a flight time of 8 minutes while covering 136m on average before crash in a challenging environment with transparent and reflective obstacles, proving the effectiveness of ultrasonic sensors for OA on nano-drones."],"url":"http://arxiv.org/abs/2403.16696v1"}
{"created":"2024-03-25 12:26:18","title":"Design and Performance of Resonant Beam Communications -- Part II: Mobile Scenario","abstract":"This two-part paper focuses on the system design and performance analysis for a point-to-point resonant beam communication (RBCom) system under both the quasi-static and mobile scenarios. Part I of this paper proposes a synchronization-based information transmission scheme and derives the capacity upper and lower bounds for the quasi-static channel case. In Part II, we address the mobile scenario, where the receiver is in relative motion to the transmitter, and derive a mobile RBCom channel model that jointly considers the Doppler effect, channel variation, and echo interference. With the obtained channel model, we prove that the channel gain of the mobile RBCom decreases as the number of transmitted frames increases, and thus show that the considered mobile RBCom terminates after the transmitter sends a certain number of frames without frequency compensation. By deriving an upper bound on the number of successfully transmitted frames, we formulate the throughput maximization problem for the considered mobile RBCom system, and solve it via a sequential parametric convex approximation (SPCA) method. Finally, simulation results validate the analysis of our proposed method in some typical scenarios.","sentences":["This two-part paper focuses on the system design and performance analysis for a point-to-point resonant beam communication (RBCom) system under both the quasi-static and mobile scenarios.","Part I of this paper proposes a synchronization-based information transmission scheme and derives the capacity upper and lower bounds for the quasi-static channel case.","In Part II, we address the mobile scenario, where the receiver is in relative motion to the transmitter, and derive a mobile RBCom channel model that jointly considers the Doppler effect, channel variation, and echo interference.","With the obtained channel model, we prove that the channel gain of the mobile RBCom decreases as the number of transmitted frames increases, and thus show that the considered mobile RBCom terminates after the transmitter sends a certain number of frames without frequency compensation.","By deriving an upper bound on the number of successfully transmitted frames, we formulate the throughput maximization problem for the considered mobile RBCom system, and solve it via a sequential parametric convex approximation (SPCA) method.","Finally, simulation results validate the analysis of our proposed method in some typical scenarios."],"url":"http://arxiv.org/abs/2403.16694v1"}
{"created":"2024-03-25 12:23:39","title":"Synapse: Learning Preferential Concepts from Visual Demonstrations","abstract":"This paper addresses the problem of preference learning, which aims to learn user-specific preferences (e.g., \"good parking spot\", \"convenient drop-off location\") from visual input. Despite its similarity to learning factual concepts (e.g., \"red cube\"), preference learning is a fundamentally harder problem due to its subjective nature and the paucity of person-specific training data. We address this problem using a new framework called Synapse, which is a neuro-symbolic approach designed to efficiently learn preferential concepts from limited demonstrations. Synapse represents preferences as neuro-symbolic programs in a domain-specific language (DSL) that operates over images, and leverages a novel combination of visual parsing, large language models, and program synthesis to learn programs representing individual preferences. We evaluate Synapse through extensive experimentation including a user case study focusing on mobility-related concepts in mobile robotics and autonomous driving. Our evaluation demonstrates that Synapse significantly outperforms existing baselines as well as its own ablations. The code and other details can be found on the project website https://amrl.cs.utexas.edu/synapse .","sentences":["This paper addresses the problem of preference learning, which aims to learn user-specific preferences (e.g., \"good parking spot\", \"convenient drop-off location\") from visual input.","Despite its similarity to learning factual concepts (e.g., \"red cube\"), preference learning is a fundamentally harder problem due to its subjective nature and the paucity of person-specific training data.","We address this problem using a new framework called Synapse, which is a neuro-symbolic approach designed to efficiently learn preferential concepts from limited demonstrations.","Synapse represents preferences as neuro-symbolic programs in a domain-specific language (DSL) that operates over images, and leverages a novel combination of visual parsing, large language models, and program synthesis to learn programs representing individual preferences.","We evaluate Synapse through extensive experimentation including a user case study focusing on mobility-related concepts in mobile robotics and autonomous driving.","Our evaluation demonstrates that Synapse significantly outperforms existing baselines as well as its own ablations.","The code and other details can be found on the project website https://amrl.cs.utexas.edu/synapse ."],"url":"http://arxiv.org/abs/2403.16689v1"}
{"created":"2024-03-25 12:23:12","title":"Investigation of the effectiveness of applying ChatGPT in Dialogic Teaching Using Electroencephalography","abstract":"In recent years, the rapid development of artificial intelligence technology, especially the emergence of large language models (LLMs) such as ChatGPT, has presented significant prospects for application in the field of education. LLMs possess the capability to interpret knowledge, answer questions, and consider context, thus providing support for dialogic teaching to students. Therefore, an examination of the capacity of LLMs to effectively fulfill instructional roles, thereby facilitating student learning akin to human educators within dialogic teaching scenarios, is an exceptionally valuable research topic. This research recruited 34 undergraduate students as participants, who were randomly divided into two groups. The experimental group engaged in dialogic teaching using ChatGPT, while the control group interacted with human teachers. Both groups learned the histogram equalization unit in the information-related course \"Digital Image Processing\". The research findings show comparable scores between the two groups on the retention test. However, students who engaged in dialogue with ChatGPT exhibited lower performance on the transfer test. Electroencephalography data revealed that students who interacted with ChatGPT exhibited higher levels of cognitive activity, suggesting that ChatGPT could help students establish a knowledge foundation and stimulate cognitive activity. However, its strengths on promoting students. knowledge application and creativity were insignificant. Based upon the research findings, it is evident that ChatGPT cannot fully excel in fulfilling teaching tasks in the dialogue teaching in information related courses. Combining ChatGPT with traditional human teachers might be a more ideal approach. The synergistic use of both can provide students with more comprehensive learning support, thus contributing to enhancing the quality of teaching.","sentences":["In recent years, the rapid development of artificial intelligence technology, especially the emergence of large language models (LLMs) such as ChatGPT, has presented significant prospects for application in the field of education.","LLMs possess the capability to interpret knowledge, answer questions, and consider context, thus providing support for dialogic teaching to students.","Therefore, an examination of the capacity of LLMs to effectively fulfill instructional roles, thereby facilitating student learning akin to human educators within dialogic teaching scenarios, is an exceptionally valuable research topic.","This research recruited 34 undergraduate students as participants, who were randomly divided into two groups.","The experimental group engaged in dialogic teaching using ChatGPT, while the control group interacted with human teachers.","Both groups learned the histogram equalization unit in the information-related course \"Digital Image Processing\".","The research findings show comparable scores between the two groups on the retention test.","However, students who engaged in dialogue with ChatGPT exhibited lower performance on the transfer test.","Electroencephalography data revealed that students who interacted with ChatGPT exhibited higher levels of cognitive activity, suggesting that ChatGPT could help students establish a knowledge foundation and stimulate cognitive activity.","However, its strengths on promoting students.","knowledge application and creativity were insignificant.","Based upon the research findings, it is evident that ChatGPT cannot fully excel in fulfilling teaching tasks in the dialogue teaching in information related courses.","Combining ChatGPT with traditional human teachers might be a more ideal approach.","The synergistic use of both can provide students with more comprehensive learning support, thus contributing to enhancing the quality of teaching."],"url":"http://arxiv.org/abs/2403.16687v1"}
{"created":"2024-03-25 12:21:38","title":"ToXCL: A Unified Framework for Toxic Speech Detection and Explanation","abstract":"The proliferation of online toxic speech is a pertinent problem posing threats to demographic groups. While explicit toxic speech contains offensive lexical signals, implicit one consists of coded or indirect language. Therefore, it is crucial for models not only to detect implicit toxic speech but also to explain its toxicity. This draws a unique need for unified frameworks that can effectively detect and explain implicit toxic speech. Prior works mainly formulated the task of toxic speech detection and explanation as a text generation problem. Nonetheless, models trained using this strategy can be prone to suffer from the consequent error propagation problem. Moreover, our experiments reveal that the detection results of such models are much lower than those that focus only on the detection task. To bridge these gaps, we introduce ToXCL, a unified framework for the detection and explanation of implicit toxic speech. Our model consists of three modules: a (i) Target Group Generator to generate the targeted demographic group(s) of a given post; an (ii) Encoder-Decoder Model in which the encoder focuses on detecting implicit toxic speech and is boosted by a (iii) Teacher Classifier via knowledge distillation, and the decoder generates the necessary explanation. ToXCL achieves new state-of-the-art effectiveness, and outperforms baselines significantly.","sentences":["The proliferation of online toxic speech is a pertinent problem posing threats to demographic groups.","While explicit toxic speech contains offensive lexical signals, implicit one consists of coded or indirect language.","Therefore, it is crucial for models not only to detect implicit toxic speech but also to explain its toxicity.","This draws a unique need for unified frameworks that can effectively detect and explain implicit toxic speech.","Prior works mainly formulated the task of toxic speech detection and explanation as a text generation problem.","Nonetheless, models trained using this strategy can be prone to suffer from the consequent error propagation problem.","Moreover, our experiments reveal that the detection results of such models are much lower than those that focus only on the detection task.","To bridge these gaps, we introduce ToXCL, a unified framework for the detection and explanation of implicit toxic speech.","Our model consists of three modules: a (i) Target Group Generator to generate the targeted demographic group(s) of a given post; an (ii) Encoder-Decoder Model in which the encoder focuses on detecting implicit toxic speech and is boosted by a (iii) Teacher Classifier via knowledge distillation, and the decoder generates the necessary explanation.","ToXCL achieves new state-of-the-art effectiveness, and outperforms baselines significantly."],"url":"http://arxiv.org/abs/2403.16685v1"}
{"created":"2024-03-25 12:15:47","title":"Symmetric Basis Convolutions for Learning Lagrangian Fluid Mechanics","abstract":"Learning physical simulations has been an essential and central aspect of many recent research efforts in machine learning, particularly for Navier-Stokes-based fluid mechanics. Classic numerical solvers have traditionally been computationally expensive and challenging to use in inverse problems, whereas Neural solvers aim to address both concerns through machine learning. We propose a general formulation for continuous convolutions using separable basis functions as a superset of existing methods and evaluate a large set of basis functions in the context of (a) a compressible 1D SPH simulation, (b) a weakly compressible 2D SPH simulation, and (c) an incompressible 2D SPH Simulation. We demonstrate that even and odd symmetries included in the basis functions are key aspects of stability and accuracy. Our broad evaluation shows that Fourier-based continuous convolutions outperform all other architectures regarding accuracy and generalization. Finally, using these Fourier-based networks, we show that prior inductive biases, such as window functions, are no longer necessary. An implementation of our approach, as well as complete datasets and solver implementations, is available at https://github.com/tum-pbs/SFBC.","sentences":["Learning physical simulations has been an essential and central aspect of many recent research efforts in machine learning, particularly for Navier-Stokes-based fluid mechanics.","Classic numerical solvers have traditionally been computationally expensive and challenging to use in inverse problems, whereas Neural solvers aim to address both concerns through machine learning.","We propose a general formulation for continuous convolutions using separable basis functions as a superset of existing methods and evaluate a large set of basis functions in the context of (a) a compressible 1D SPH simulation, (b) a weakly compressible 2D SPH simulation, and (c) an incompressible 2D SPH Simulation.","We demonstrate that even and odd symmetries included in the basis functions are key aspects of stability and accuracy.","Our broad evaluation shows that Fourier-based continuous convolutions outperform all other architectures regarding accuracy and generalization.","Finally, using these Fourier-based networks, we show that prior inductive biases, such as window functions, are no longer necessary.","An implementation of our approach, as well as complete datasets and solver implementations, is available at https://github.com/tum-pbs/SFBC."],"url":"http://arxiv.org/abs/2403.16680v1"}
{"created":"2024-03-25 12:14:48","title":"FOOL: Addressing the Downlink Bottleneck in Satellite Computing with Neural Feature Compression","abstract":"Nanosatellite constellations equipped with sensors capturing large geographic regions provide unprecedented opportunities for Earth observation. As constellation sizes increase, network contention poses a downlink bottleneck. Orbital Edge Computing (OEC) leverages limited onboard compute resources to reduce transfer costs by processing the raw captures at the source. However, current solutions have limited practicability due to reliance on crude filtering methods or over-prioritizing particular downstream tasks.   This work presents FOOL, an OEC-native and task-agnostic feature compression method that preserves prediction performance. FOOL partitions high-resolution satellite imagery to maximize throughput. Further, it embeds context and leverages inter-tile dependencies to lower transfer costs with negligible overhead. While FOOL is a feature compressor, it can recover images with competitive scores on perceptual quality measures at lower bitrates. We extensively evaluate transfer cost reduction by including the peculiarity of intermittently available network connections in low earth orbit. Lastly, we test the feasibility of our system for standardized nanosatellite form factors. We demonstrate that FOOL permits downlinking over 100x the data volume without relying on prior information on the downstream tasks.","sentences":["Nanosatellite constellations equipped with sensors capturing large geographic regions provide unprecedented opportunities for Earth observation.","As constellation sizes increase, network contention poses a downlink bottleneck.","Orbital Edge Computing (OEC) leverages limited onboard compute resources to reduce transfer costs by processing the raw captures at the source.","However, current solutions have limited practicability due to reliance on crude filtering methods or over-prioritizing particular downstream tasks.   ","This work presents FOOL, an OEC-native and task-agnostic feature compression method that preserves prediction performance.","FOOL partitions high-resolution satellite imagery to maximize throughput.","Further, it embeds context and leverages inter-tile dependencies to lower transfer costs with negligible overhead.","While FOOL is a feature compressor, it can recover images with competitive scores on perceptual quality measures at lower bitrates.","We extensively evaluate transfer cost reduction by including the peculiarity of intermittently available network connections in low earth orbit.","Lastly, we test the feasibility of our system for standardized nanosatellite form factors.","We demonstrate that FOOL permits downlinking over 100x the data volume without relying on prior information on the downstream tasks."],"url":"http://arxiv.org/abs/2403.16677v1"}
{"created":"2024-03-25 12:14:34","title":"Design and Performance of Resonant Beam Communications -- Part I: Quasi-Static Scenario","abstract":"This two-part paper studies a point-to-point resonant beam communication (RBCom) system, where two separately deployed retroreflectors are adopted to generate the resonant beam between the transmitter and the receiver, and analyzes the transmission rate of the considered system under both the quasi-static and mobile scenarios. Part I of this paper focuses on the quasi-static scenario where the locations of the transmitter and the receiver are relatively fixed. Specifically, we propose a new information-bearing scheme which adopts a synchronization-based amplitude modulation method to mitigate the echo interference caused by the reflected resonant beam. With this scheme, we show that the quasi-static RBCom channel is equivalent to a Markov channel and can be further simplified as an amplitude-constrained additive white Gaussian noise channel. Moreover, we develop an algorithm that jointly employs the bisection and exhaustive search to maximize its capacity upper and lower bounds. Finally, numerical results validate our analysis. Part II of this paper discusses the performance of the RBCom system under the mobile scenario.","sentences":["This two-part paper studies a point-to-point resonant beam communication (RBCom) system, where two separately deployed retroreflectors are adopted to generate the resonant beam between the transmitter and the receiver, and analyzes the transmission rate of the considered system under both the quasi-static and mobile scenarios.","Part I of this paper focuses on the quasi-static scenario where the locations of the transmitter and the receiver are relatively fixed.","Specifically, we propose a new information-bearing scheme which adopts a synchronization-based amplitude modulation method to mitigate the echo interference caused by the reflected resonant beam.","With this scheme, we show that the quasi-static RBCom channel is equivalent to a Markov channel and can be further simplified as an amplitude-constrained additive white Gaussian noise channel.","Moreover, we develop an algorithm that jointly employs the bisection and exhaustive search to maximize its capacity upper and lower bounds.","Finally, numerical results validate our analysis.","Part II of this paper discusses the performance of the RBCom system under the mobile scenario."],"url":"http://arxiv.org/abs/2403.16676v1"}
{"created":"2024-03-25 12:13:20","title":"Understanding the Functional Roles of Modelling Components in Spiking Neural Networks","abstract":"Spiking neural networks (SNNs), inspired by the neural circuits of the brain, are promising in achieving high computational efficiency with biological fidelity. Nevertheless, it is quite difficult to optimize SNNs because the functional roles of their modelling components remain unclear. By designing and evaluating several variants of the classic model, we systematically investigate the functional roles of key modelling components, leakage, reset, and recurrence, in leaky integrate-and-fire (LIF) based SNNs. Through extensive experiments, we demonstrate how these components influence the accuracy, generalization, and robustness of SNNs. Specifically, we find that the leakage plays a crucial role in balancing memory retention and robustness, the reset mechanism is essential for uninterrupted temporal processing and computational efficiency, and the recurrence enriches the capability to model complex dynamics at a cost of robustness degradation. With these interesting observations, we provide optimization suggestions for enhancing the performance of SNNs in different scenarios. This work deepens the understanding of how SNNs work, which offers valuable guidance for the development of more effective and robust neuromorphic models.","sentences":["Spiking neural networks (SNNs), inspired by the neural circuits of the brain, are promising in achieving high computational efficiency with biological fidelity.","Nevertheless, it is quite difficult to optimize SNNs because the functional roles of their modelling components remain unclear.","By designing and evaluating several variants of the classic model, we systematically investigate the functional roles of key modelling components, leakage, reset, and recurrence, in leaky integrate-and-fire (LIF) based SNNs.","Through extensive experiments, we demonstrate how these components influence the accuracy, generalization, and robustness of SNNs.","Specifically, we find that the leakage plays a crucial role in balancing memory retention and robustness, the reset mechanism is essential for uninterrupted temporal processing and computational efficiency, and the recurrence enriches the capability to model complex dynamics at a cost of robustness degradation.","With these interesting observations, we provide optimization suggestions for enhancing the performance of SNNs in different scenarios.","This work deepens the understanding of how SNNs work, which offers valuable guidance for the development of more effective and robust neuromorphic models."],"url":"http://arxiv.org/abs/2403.16674v1"}
{"created":"2024-03-25 12:07:24","title":"Domain Adaptive Detection of MAVs: A Benchmark and Noise Suppression Network","abstract":"Visual detection of Micro Air Vehicles (MAVs) has attracted increasing attention in recent years due to its important application in various tasks. The existing methods for MAV detection assume that the training set and testing set have the same distribution. As a result, when deployed in new domains, the detectors would have a significant performance degradation due to domain discrepancy. In this paper, we study the problem of cross-domain MAV detection. The contributions of this paper are threefold. 1) We propose a Multi-MAV-Multi-Domain (M3D) dataset consisting of both simulation and realistic images. Compared to other existing datasets, the proposed one is more comprehensive in the sense that it covers rich scenes, diverse MAV types, and various viewing angles. A new benchmark for cross-domain MAV detection is proposed based on the proposed dataset. 2) We propose a Noise Suppression Network (NSN) based on the framework of pseudo-labeling and a large-to-small training procedure. To reduce the challenging pseudo-label noises, two novel modules are designed in this network. The first is a prior-based curriculum learning module for allocating adaptive thresholds for pseudo labels with different difficulties. The second is a masked copy-paste augmentation module for pasting truly-labeled MAVs on unlabeled target images and thus decreasing pseudo-label noises. 3) Extensive experimental results verify the superior performance of the proposed method compared to the state-of-the-art ones. In particular, it achieves mAP of 46.9%(+5.8%), 50.5%(+3.7%), and 61.5%(+11.3%) on the tasks of simulation-to-real adaptation, cross-scene adaptation, and cross-camera adaptation, respectively.","sentences":["Visual detection of Micro Air Vehicles (MAVs) has attracted increasing attention in recent years due to its important application in various tasks.","The existing methods for MAV detection assume that the training set and testing set have the same distribution.","As a result, when deployed in new domains, the detectors would have a significant performance degradation due to domain discrepancy.","In this paper, we study the problem of cross-domain MAV detection.","The contributions of this paper are threefold.","1) We propose a Multi-MAV-Multi-Domain (M3D) dataset consisting of both simulation and realistic images.","Compared to other existing datasets, the proposed one is more comprehensive in the sense that it covers rich scenes, diverse MAV types, and various viewing angles.","A new benchmark for cross-domain MAV detection is proposed based on the proposed dataset.","2) We propose a Noise Suppression Network (NSN) based on the framework of pseudo-labeling and a large-to-small training procedure.","To reduce the challenging pseudo-label noises, two novel modules are designed in this network.","The first is a prior-based curriculum learning module for allocating adaptive thresholds for pseudo labels with different difficulties.","The second is a masked copy-paste augmentation module for pasting truly-labeled MAVs on unlabeled target images and thus decreasing pseudo-label noises.","3) Extensive experimental results verify the superior performance of the proposed method compared to the state-of-the-art ones.","In particular, it achieves mAP of 46.9%(+5.8%), 50.5%(+3.7%), and 61.5%(+11.3%) on the tasks of simulation-to-real adaptation, cross-scene adaptation, and cross-camera adaptation, respectively."],"url":"http://arxiv.org/abs/2403.16669v1"}
{"created":"2024-03-25 12:07:21","title":"Who is bragging more online? A large scale analysis of bragging in social media","abstract":"Bragging is the act of uttering statements that are likely to be positively viewed by others and it is extensively employed in human communication with the aim to build a positive self-image of oneself. Social media is a natural platform for users to employ bragging in order to gain admiration, respect, attention and followers from their audiences. Yet, little is known about the scale of bragging online and its characteristics. This paper employs computational sociolinguistics methods to conduct the first large scale study of bragging behavior on Twitter (U.S.) by focusing on its overall prevalence, temporal dynamics and impact of demographic factors. Our study shows that the prevalence of bragging decreases over time within the same population of users. In addition, younger, more educated and popular users in the U.S. are more likely to brag. Finally, we conduct an extensive linguistics analysis to unveil specific bragging themes associated with different user traits.","sentences":["Bragging is the act of uttering statements that are likely to be positively viewed by others and it is extensively employed in human communication with the aim to build a positive self-image of oneself.","Social media is a natural platform for users to employ bragging in order to gain admiration, respect, attention and followers from their audiences.","Yet, little is known about the scale of bragging online and its characteristics.","This paper employs computational sociolinguistics methods to conduct the first large scale study of bragging behavior on Twitter (U.S.) by focusing on its overall prevalence, temporal dynamics and impact of demographic factors.","Our study shows that the prevalence of bragging decreases over time within the same population of users.","In addition, younger, more educated and popular users in the U.S. are more likely to brag.","Finally, we conduct an extensive linguistics analysis to unveil specific bragging themes associated with different user traits."],"url":"http://arxiv.org/abs/2403.16668v1"}
{"created":"2024-03-25 12:04:03","title":"Deep Reinforcement Learning and Mean-Variance Strategies for Responsible Portfolio Optimization","abstract":"Portfolio optimization involves determining the optimal allocation of portfolio assets in order to maximize a given investment objective. Traditionally, some form of mean-variance optimization is used with the aim of maximizing returns while minimizing risk, however, more recently, deep reinforcement learning formulations have been explored. Increasingly, investors have demonstrated an interest in incorporating ESG objectives when making investment decisions, and modifications to the classical mean-variance optimization framework have been developed. In this work, we study the use of deep reinforcement learning for responsible portfolio optimization, by incorporating ESG states and objectives, and provide comparisons against modified mean-variance approaches. Our results show that deep reinforcement learning policies can provide competitive performance against mean-variance approaches for responsible portfolio allocation across additive and multiplicative utility functions of financial and ESG responsibility objectives.","sentences":["Portfolio optimization involves determining the optimal allocation of portfolio assets in order to maximize a given investment objective.","Traditionally, some form of mean-variance optimization is used with the aim of maximizing returns while minimizing risk, however, more recently, deep reinforcement learning formulations have been explored.","Increasingly, investors have demonstrated an interest in incorporating ESG objectives when making investment decisions, and modifications to the classical mean-variance optimization framework have been developed.","In this work, we study the use of deep reinforcement learning for responsible portfolio optimization, by incorporating ESG states and objectives, and provide comparisons against modified mean-variance approaches.","Our results show that deep reinforcement learning policies can provide competitive performance against mean-variance approaches for responsible portfolio allocation across additive and multiplicative utility functions of financial and ESG responsibility objectives."],"url":"http://arxiv.org/abs/2403.16667v1"}
{"created":"2024-03-25 12:00:57","title":"Adaptive Frequency Bin Interval in FFT via Dense Sampling Factor $\u03b1$","abstract":"The Fast Fourier Transform (FFT) is a fundamental tool for signal analysis, widely used across various fields. However, traditional FFT methods encounter challenges in adjusting the frequency bin interval, which may impede accurate spectral analysis. In this study, we propose a method for adjusting the frequency bin interval in FFT by introducing a parameter $\\alpha$. We elucidate the underlying principles of the proposed method and discuss its potential applications across various contexts. Our findings suggest that the proposed method offers a promising approach to overcome the limitations of traditional FFT methods and enhance spectral analysis accuracy.","sentences":["The Fast Fourier Transform (FFT) is a fundamental tool for signal analysis, widely used across various fields.","However, traditional FFT methods encounter challenges in adjusting the frequency bin interval, which may impede accurate spectral analysis.","In this study, we propose a method for adjusting the frequency bin interval in FFT by introducing a parameter $\\alpha$. We elucidate the underlying principles of the proposed method and discuss its potential applications across various contexts.","Our findings suggest that the proposed method offers a promising approach to overcome the limitations of traditional FFT methods and enhance spectral analysis accuracy."],"url":"http://arxiv.org/abs/2403.16665v1"}
{"created":"2024-03-25 11:57:30","title":"Skill Q-Network: Learning Adaptive Skill Ensemble for Mapless Navigation in Unknown Environments","abstract":"This paper focuses on the acquisition of mapless navigation skills within unknown environments. We introduce the Skill Q-Network (SQN), a novel reinforcement learning method featuring an adaptive skill ensemble mechanism. Unlike existing methods, our model concurrently learns a high-level skill decision process alongside multiple low-level navigation skills, all without the need for prior knowledge. Leveraging a tailored reward function for mapless navigation, the SQN is capable of learning adaptive maneuvers that incorporate both exploration and goal-directed skills, enabling effective navigation in new environments. Our experiments demonstrate that our SQN can effectively navigate complex environments, exhibiting a 40% higher performance compared to baseline models. Without explicit guidance, SQN discovers how to combine low-level skill policies, showcasing both goal-directed navigations to reach destinations and exploration maneuvers to escape from local minimum regions in challenging scenarios. Remarkably, our adaptive skill ensemble method enables zero-shot transfer to out-of-distribution domains, characterized by unseen observations from non-convex obstacles or uneven, subterranean-like environments.","sentences":["This paper focuses on the acquisition of mapless navigation skills within unknown environments.","We introduce the Skill Q-Network (SQN), a novel reinforcement learning method featuring an adaptive skill ensemble mechanism.","Unlike existing methods, our model concurrently learns a high-level skill decision process alongside multiple low-level navigation skills, all without the need for prior knowledge.","Leveraging a tailored reward function for mapless navigation, the SQN is capable of learning adaptive maneuvers that incorporate both exploration and goal-directed skills, enabling effective navigation in new environments.","Our experiments demonstrate that our SQN can effectively navigate complex environments, exhibiting a 40% higher performance compared to baseline models.","Without explicit guidance, SQN discovers how to combine low-level skill policies, showcasing both goal-directed navigations to reach destinations and exploration maneuvers to escape from local minimum regions in challenging scenarios.","Remarkably, our adaptive skill ensemble method enables zero-shot transfer to out-of-distribution domains, characterized by unseen observations from non-convex obstacles or uneven, subterranean-like environments."],"url":"http://arxiv.org/abs/2403.16664v1"}
{"created":"2024-03-25 11:56:29","title":"RU22Fact: Optimizing Evidence for Multilingual Explainable Fact-Checking on Russia-Ukraine Conflict","abstract":"Fact-checking is the task of verifying the factuality of a given claim by examining the available evidence. High-quality evidence plays a vital role in enhancing fact-checking systems and facilitating the generation of explanations that are understandable to humans. However, the provision of both sufficient and relevant evidence for explainable fact-checking systems poses a challenge. To tackle this challenge, we propose a method based on a Large Language Model to automatically retrieve and summarize evidence from the Web. Furthermore, we construct RU22Fact, a novel multilingual explainable fact-checking dataset on the Russia-Ukraine conflict in 2022 of 16K samples, each containing real-world claims, optimized evidence, and referenced explanation. To establish a baseline for our dataset, we also develop an end-to-end explainable fact-checking system to verify claims and generate explanations. Experimental results demonstrate the prospect of optimized evidence in increasing fact-checking performance and also indicate the possibility of further progress in the end-to-end claim verification and explanation generation tasks.","sentences":["Fact-checking is the task of verifying the factuality of a given claim by examining the available evidence.","High-quality evidence plays a vital role in enhancing fact-checking systems and facilitating the generation of explanations that are understandable to humans.","However, the provision of both sufficient and relevant evidence for explainable fact-checking systems poses a challenge.","To tackle this challenge, we propose a method based on a Large Language Model to automatically retrieve and summarize evidence from the Web.","Furthermore, we construct RU22Fact, a novel multilingual explainable fact-checking dataset on the Russia-Ukraine conflict in 2022 of 16K samples, each containing real-world claims, optimized evidence, and referenced explanation.","To establish a baseline for our dataset, we also develop an end-to-end explainable fact-checking system to verify claims and generate explanations.","Experimental results demonstrate the prospect of optimized evidence in increasing fact-checking performance and also indicate the possibility of further progress in the end-to-end claim verification and explanation generation tasks."],"url":"http://arxiv.org/abs/2403.16662v1"}
{"created":"2024-03-25 11:47:53","title":"Graph Augmentation for Recommendation","abstract":"Graph augmentation with contrastive learning has gained significant attention in the field of recommendation systems due to its ability to learn expressive user representations, even when labeled data is limited. However, directly applying existing GCL models to real-world recommendation environments poses challenges. There are two primary issues to address. Firstly, the lack of consideration for data noise in contrastive learning can result in noisy self-supervised signals, leading to degraded performance. Secondly, many existing GCL approaches rely on graph neural network (GNN) architectures, which can suffer from over-smoothing problems due to non-adaptive message passing. To address these challenges, we propose a principled framework called GraphAug. This framework introduces a robust data augmentor that generates denoised self-supervised signals, enhancing recommender systems. The GraphAug framework incorporates a graph information bottleneck (GIB)-regularized augmentation paradigm, which automatically distills informative self-supervision information and adaptively adjusts contrastive view generation. Through rigorous experimentation on real-world datasets, we thoroughly assessed the performance of our novel GraphAug model. The outcomes consistently unveil its superiority over existing baseline methods. The source code for our model is publicly available at: https://github.com/HKUDS/GraphAug.","sentences":["Graph augmentation with contrastive learning has gained significant attention in the field of recommendation systems due to its ability to learn expressive user representations, even when labeled data is limited.","However, directly applying existing GCL models to real-world recommendation environments poses challenges.","There are two primary issues to address.","Firstly, the lack of consideration for data noise in contrastive learning can result in noisy self-supervised signals, leading to degraded performance.","Secondly, many existing GCL approaches rely on graph neural network (GNN) architectures, which can suffer from over-smoothing problems due to non-adaptive message passing.","To address these challenges, we propose a principled framework called GraphAug.","This framework introduces a robust data augmentor that generates denoised self-supervised signals, enhancing recommender systems.","The GraphAug framework incorporates a graph information bottleneck (GIB)-regularized augmentation paradigm, which automatically distills informative self-supervision information and adaptively adjusts contrastive view generation.","Through rigorous experimentation on real-world datasets, we thoroughly assessed the performance of our novel GraphAug model.","The outcomes consistently unveil its superiority over existing baseline methods.","The source code for our model is publicly available at: https://github.com/HKUDS/GraphAug."],"url":"http://arxiv.org/abs/2403.16656v1"}
{"created":"2024-03-25 11:45:21","title":"Grammatical vs Spelling Error Correction: An Investigation into the Responsiveness of Transformer-based Language Models using BART and MarianMT","abstract":"Text continues to remain a relevant form of representation for information. Text documents are created either in digital native platforms or through the conversion of other media files such as images and speech. While the digital native text is invariably obtained through physical or virtual keyboards, technologies such as OCR and speech recognition are utilized to transform the images and speech signals into text content. All these variety of mechanisms of text generation also introduce errors into the captured text.   This project aims at analyzing different kinds of error that occurs in text documents. The work employs two of the advanced deep neural network-based language models, namely, BART and MarianMT, to rectify the anomalies present in the text. Transfer learning of these models with available dataset is performed to finetune their capacity for error correction. A comparative study is conducted to investigate the effectiveness of these models in handling each of the defined error categories. It is observed that while both models can bring down the erroneous sentences by 20+%, BART can handle spelling errors far better (24.6%) than grammatical errors (8.8%).","sentences":["Text continues to remain a relevant form of representation for information.","Text documents are created either in digital native platforms or through the conversion of other media files such as images and speech.","While the digital native text is invariably obtained through physical or virtual keyboards, technologies such as OCR and speech recognition are utilized to transform the images and speech signals into text content.","All these variety of mechanisms of text generation also introduce errors into the captured text.   ","This project aims at analyzing different kinds of error that occurs in text documents.","The work employs two of the advanced deep neural network-based language models, namely, BART and MarianMT, to rectify the anomalies present in the text.","Transfer learning of these models with available dataset is performed to finetune their capacity for error correction.","A comparative study is conducted to investigate the effectiveness of these models in handling each of the defined error categories.","It is observed that while both models can bring down the erroneous sentences by 20+%, BART can handle spelling errors far better (24.6%) than grammatical errors (8.8%)."],"url":"http://arxiv.org/abs/2403.16655v1"}
{"created":"2024-03-25 11:42:01","title":"A Novel Loss Function-based Support Vector Machine for Binary Classification","abstract":"The previous support vector machine(SVM) including $0/1$ loss SVM, hinge loss SVM, ramp loss SVM, truncated pinball loss SVM, and others, overlooked the degree of penalty for the correctly classified samples within the margin. This oversight affects the generalization ability of the SVM classifier to some extent. To address this limitation, from the perspective of confidence margin, we propose a novel Slide loss function ($\\ell_s$) to construct the support vector machine classifier($\\ell_s$-SVM). By introducing the concept of proximal stationary point, and utilizing the property of Lipschitz continuity, we derive the first-order optimality conditions for $\\ell_s$-SVM. Based on this, we define the $\\ell_s$ support vectors and working set of $\\ell_s$-SVM. To efficiently handle $\\ell_s$-SVM, we devise a fast alternating direction method of multipliers with the working set ($\\ell_s$-ADMM), and provide the convergence analysis. The numerical experiments on real world datasets confirm the robustness and effectiveness of the proposed method.","sentences":["The previous support vector machine(SVM) including $0/1$ loss SVM, hinge loss SVM, ramp loss SVM, truncated pinball loss SVM, and others, overlooked the degree of penalty for the correctly classified samples within the margin.","This oversight affects the generalization ability of the SVM classifier to some extent.","To address this limitation, from the perspective of confidence margin, we propose a novel Slide loss function ($\\ell_s$) to construct the support vector machine classifier($\\ell_s$-SVM).","By introducing the concept of proximal stationary point, and utilizing the property of Lipschitz continuity, we derive the first-order optimality conditions for $\\ell_s$-SVM.","Based on this, we define the $\\ell_s$ support vectors and working set of $\\ell_s$-SVM.","To efficiently handle $\\ell_s$-SVM, we devise a fast alternating direction method of multipliers with the working set ($\\ell_s$-ADMM), and provide the convergence analysis.","The numerical experiments on real world datasets confirm the robustness and effectiveness of the proposed method."],"url":"http://arxiv.org/abs/2403.16654v1"}
{"created":"2024-03-25 11:40:32","title":"Trajectory Planning of Robotic Manipulator in Dynamic Environment Exploiting DRL","abstract":"This study is about the implementation of a reinforcement learning algorithm in the trajectory planning of manipulators. We have a 7-DOF robotic arm to pick and place the randomly placed block at a random target point in an unknown environment. The obstacle is randomly moving which creates a hurdle in picking the object. The objective of the robot is to avoid the obstacle and pick the block with constraints to a fixed timestamp. In this literature, we have applied a deep deterministic policy gradient (DDPG) algorithm and compared the model's efficiency with dense and sparse rewards.","sentences":["This study is about the implementation of a reinforcement learning algorithm in the trajectory planning of manipulators.","We have a 7-DOF robotic arm to pick and place the randomly placed block at a random target point in an unknown environment.","The obstacle is randomly moving which creates a hurdle in picking the object.","The objective of the robot is to avoid the obstacle and pick the block with constraints to a fixed timestamp.","In this literature, we have applied a deep deterministic policy gradient (DDPG) algorithm and compared the model's efficiency with dense and sparse rewards."],"url":"http://arxiv.org/abs/2403.16652v1"}
{"created":"2024-03-25 11:37:15","title":"CLHA: A Simple yet Effective Contrastive Learning Framework for Human Alignment","abstract":"Reinforcement learning from human feedback (RLHF) is a crucial technique in aligning large language models (LLMs) with human preferences, ensuring these LLMs behave in beneficial and comprehensible ways to users. However, a longstanding challenge in human alignment techniques based on reinforcement learning lies in their inherent complexity and difficulty in training. To address this challenge, we present a simple yet effective Contrastive Learning Framework for Human Alignment (CLHA) to align LLMs with human preferences directly. CLHA employs a novel rescoring strategy to evaluate the noise within the data by considering its inherent quality and dynamically adjusting the training process. Simultaneously, CLHA utilizes pairwise contrastive loss and adaptive supervised fine-tuning loss to adaptively modify the likelihood of generating responses, ensuring enhanced alignment with human preferences. Using advanced methods, CLHA surpasses other algorithms, showcasing superior performance in terms of reward model scores, automatic evaluations, and human assessments on the widely used ``\\textit{Helpful and Harmless}'' dataset.","sentences":["Reinforcement learning from human feedback (RLHF) is a crucial technique in aligning large language models (LLMs) with human preferences, ensuring these LLMs behave in beneficial and comprehensible ways to users.","However, a longstanding challenge in human alignment techniques based on reinforcement learning lies in their inherent complexity and difficulty in training.","To address this challenge, we present a simple yet effective Contrastive Learning Framework for Human Alignment (CLHA) to align LLMs with human preferences directly.","CLHA employs a novel rescoring strategy to evaluate the noise within the data by considering its inherent quality and dynamically adjusting the training process.","Simultaneously, CLHA utilizes pairwise contrastive loss and adaptive supervised fine-tuning loss to adaptively modify the likelihood of generating responses, ensuring enhanced alignment with human preferences.","Using advanced methods, CLHA surpasses other algorithms, showcasing superior performance in terms of reward model scores, automatic evaluations, and human assessments on the widely used ``\\textit{Helpful and Harmless}'' dataset."],"url":"http://arxiv.org/abs/2403.16649v1"}
{"created":"2024-03-25 11:32:05","title":"Clustering Propagation for Universal Medical Image Segmentation","abstract":"Prominent solutions for medical image segmentation are typically tailored for automatic or interactive setups, posing challenges in facilitating progress achieved in one task to another.$_{\\!}$ This$_{\\!}$ also$_{\\!}$ necessitates$_{\\!}$ separate$_{\\!}$ models for each task, duplicating both training time and parameters.$_{\\!}$ To$_{\\!}$ address$_{\\!}$ above$_{\\!}$ issues,$_{\\!}$ we$_{\\!}$ introduce$_{\\!}$ S2VNet,$_{\\!}$ a$_{\\!}$ universal$_{\\!}$ framework$_{\\!}$ that$_{\\!}$ leverages$_{\\!}$ Slice-to-Volume$_{\\!}$ propagation$_{\\!}$ to$_{\\!}$ unify automatic/interactive segmentation within a single model and one training session. Inspired by clustering-based segmentation techniques, S2VNet makes full use of the slice-wise structure of volumetric data by initializing cluster centers from the cluster$_{\\!}$ results$_{\\!}$ of$_{\\!}$ previous$_{\\!}$ slice.$_{\\!}$ This enables knowledge acquired from prior slices to assist in the segmentation of the current slice, further efficiently bridging the communication between remote slices using mere 2D networks. Moreover, such a framework readily accommodates interactive segmentation with no architectural change, simply by initializing centroids from user inputs. S2VNet distinguishes itself by swift inference speeds and reduced memory consumption compared to prevailing 3D solutions. It can also handle multi-class interactions with each of them serving to initialize different centroids. Experiments on three benchmarks demonstrate S2VNet surpasses task-specified solutions on both automatic/interactive setups.","sentences":["Prominent solutions for medical image segmentation are typically tailored for automatic or interactive setups, posing challenges in facilitating progress achieved in one task to another.$_{\\!}$ This$_{\\!}$ also$_{\\!}$ necessitates$_{\\!}$ separate$_{\\!}$ models for each task, duplicating both training time and parameters.$_{\\!}$ To$_{\\!}$ address$_{\\!}$ above$_{\\!}$ issues,$_{\\!}$ we$_{\\!}$ introduce$_{\\!}$ S2VNet,$_{\\!}$ a$_{\\!}$ universal$_{\\!}$ framework$_{\\!}$ that$_{\\!}$ leverages$_{\\!}$ Slice-to-Volume$_{\\!}$ propagation$_{\\!}$ to$_{\\!}$ unify automatic/interactive segmentation within a single model and one training session.","Inspired by clustering-based segmentation techniques, S2VNet makes full use of the slice-wise structure of volumetric data by initializing cluster centers from the cluster$_{\\!}$ results$_{\\!}$ of$_{\\!}$ previous$_{\\!}$ slice.$_{\\!}$","This enables knowledge acquired from prior slices to assist in the segmentation of the current slice, further efficiently bridging the communication between remote slices using mere 2D networks.","Moreover, such a framework readily accommodates interactive segmentation with no architectural change, simply by initializing centroids from user inputs.","S2VNet distinguishes itself by swift inference speeds and reduced memory consumption compared to prevailing 3D solutions.","It can also handle multi-class interactions with each of them serving to initialize different centroids.","Experiments on three benchmarks demonstrate S2VNet surpasses task-specified solutions on both automatic/interactive setups."],"url":"http://arxiv.org/abs/2403.16646v1"}
{"created":"2024-03-25 11:31:45","title":"Virtual Co-Pilot: Multimodal Large Language Model-enabled Quick-access Procedures for Single Pilot Operations","abstract":"Advancements in technology, pilot shortages, and cost pressures are driving a trend towards single-pilot and even remote operations in aviation. Considering the extensive workload and huge risks associated with single-pilot operations, the development of a Virtual Co-Pilot (V-CoP) is expected to be a potential way to ensure aviation safety. This study proposes a V-CoP concept and explores how humans and virtual assistants can effectively collaborate. A preliminary case study is conducted to explore a critical role of V-CoP, namely automated quick procedures searching, using the multimodal large language model (LLM). The LLM-enabled V-CoP integrates the pilot instruction and real-time cockpit instrumental data to prompt applicable aviation manuals and operation procedures. The results showed that the LLM-enabled V-CoP achieved high accuracy in situational analysis and effective retrieval of procedure information. The results showed that the LLM-enabled V-CoP achieved high accuracy in situational analysis (90.5%) and effective retrieval of procedure information (86.5%). The proposed V-CoP is expected to provide a foundation for future virtual intelligent assistant development, improve the performance of single pilots, and reduce the risk of human errors in aviation.","sentences":["Advancements in technology, pilot shortages, and cost pressures are driving a trend towards single-pilot and even remote operations in aviation.","Considering the extensive workload and huge risks associated with single-pilot operations, the development of a Virtual Co-Pilot (V-CoP) is expected to be a potential way to ensure aviation safety.","This study proposes a V-CoP concept and explores how humans and virtual assistants can effectively collaborate.","A preliminary case study is conducted to explore a critical role of V-CoP, namely automated quick procedures searching, using the multimodal large language model (LLM).","The LLM-enabled V-CoP integrates the pilot instruction and real-time cockpit instrumental data to prompt applicable aviation manuals and operation procedures.","The results showed that the LLM-enabled V-CoP achieved high accuracy in situational analysis and effective retrieval of procedure information.","The results showed that the LLM-enabled V-CoP achieved high accuracy in situational analysis (90.5%) and effective retrieval of procedure information (86.5%).","The proposed V-CoP is expected to provide a foundation for future virtual intelligent assistant development, improve the performance of single pilots, and reduce the risk of human errors in aviation."],"url":"http://arxiv.org/abs/2403.16645v1"}
{"created":"2024-03-25 11:29:32","title":"Bridging the Sim-to-Real Gap with Bayesian Inference","abstract":"We present SIM-FSVGD for learning robot dynamics from data. As opposed to traditional methods, SIM-FSVGD leverages low-fidelity physical priors, e.g., in the form of simulators, to regularize the training of neural network models. While learning accurate dynamics already in the low data regime, SIM-FSVGD scales and excels also when more data is available. We empirically show that learning with implicit physical priors results in accurate mean model estimation as well as precise uncertainty quantification. We demonstrate the effectiveness of SIM-FSVGD in bridging the sim-to-real gap on a high-performance RC racecar system. Using model-based RL, we demonstrate a highly dynamic parking maneuver with drifting, using less than half the data compared to the state of the art.","sentences":["We present SIM-FSVGD for learning robot dynamics from data.","As opposed to traditional methods, SIM-FSVGD leverages low-fidelity physical priors, e.g., in the form of simulators, to regularize the training of neural network models.","While learning accurate dynamics already in the low data regime, SIM-FSVGD scales and excels also when more data is available.","We empirically show that learning with implicit physical priors results in accurate mean model estimation as well as precise uncertainty quantification.","We demonstrate the effectiveness of SIM-FSVGD in bridging the sim-to-real gap on a high-performance RC racecar system.","Using model-based RL, we demonstrate a highly dynamic parking maneuver with drifting, using less than half the data compared to the state of the art."],"url":"http://arxiv.org/abs/2403.16644v1"}
{"created":"2024-03-25 11:28:37","title":"Investigating the Readability of Test Code: Combining Scientific and Practical Views","abstract":"The readability of source code is key for understanding and maintaining software systems and tests. Several studies investigate the readability of source code, but there is limited research on the readability of test code and related influence factors. We investigate the factors that influence the readability of test code from an academic perspective complemented by practical views. First, we perform a Systematic Mapping Study (SMS) with a focus on scientific literature. Second, we extend this study by reviewing grey literature sources for practical aspects on test code readability and understandability. Finally, we conduct a controlled experiment on the readability of a selected set of test cases to collect additional knowledge on influence factors discussed in practice. The result set of the SMS includes 19 primary studies from the scientific literature. The grey literature search reveals 62 sources for information on test code readability. Based on an analysis of these sources, we identified a combined set of 14 factors that influence the readability of test code. 7 of these factors were found in scientific and grey literature, while some factors were mainly discussed in academia (2) or industry (5) with limited overlap. The controlled experiment on practically relevant influence factors showed that the investigated factors have a significant impact on readability for half of the selected test cases. Our review of scientific and grey literature showed that test code readability is of interest for academia and industry with a consensus on key influence factors. However, we also found factors only discussed by practitioners. For some of these factors we were able to confirm an impact on readability in a first experiment. Therefore, we see the need to bring together academic and industry viewpoints to achieve a common view on the readability of software test code.","sentences":["The readability of source code is key for understanding and maintaining software systems and tests.","Several studies investigate the readability of source code, but there is limited research on the readability of test code and related influence factors.","We investigate the factors that influence the readability of test code from an academic perspective complemented by practical views.","First, we perform a Systematic Mapping Study (SMS) with a focus on scientific literature.","Second, we extend this study by reviewing grey literature sources for practical aspects on test code readability and understandability.","Finally, we conduct a controlled experiment on the readability of a selected set of test cases to collect additional knowledge on influence factors discussed in practice.","The result set of the SMS includes 19 primary studies from the scientific literature.","The grey literature search reveals 62 sources for information on test code readability.","Based on an analysis of these sources, we identified a combined set of 14 factors that influence the readability of test code.","7 of these factors were found in scientific and grey literature, while some factors were mainly discussed in academia (2) or industry (5) with limited overlap.","The controlled experiment on practically relevant influence factors showed that the investigated factors have a significant impact on readability for half of the selected test cases.","Our review of scientific and grey literature showed that test code readability is of interest for academia and industry with a consensus on key influence factors.","However, we also found factors only discussed by practitioners.","For some of these factors we were able to confirm an impact on readability in a first experiment.","Therefore, we see the need to bring together academic and industry viewpoints to achieve a common view on the readability of software test code."],"url":"http://arxiv.org/abs/2403.16639v1"}
{"created":"2024-03-25 11:26:18","title":"AI-Generated Video Detection via Spatio-Temporal Anomaly Learning","abstract":"The advancement of generation models has led to the emergence of highly realistic artificial intelligence (AI)-generated videos. Malicious users can easily create non-existent videos to spread false information. This letter proposes an effective AI-generated video detection (AIGVDet) scheme by capturing the forensic traces with a two-branch spatio-temporal convolutional neural network (CNN). Specifically, two ResNet sub-detectors are learned separately for identifying the anomalies in spatical and optical flow domains, respectively. Results of such sub-detectors are fused to further enhance the discrimination ability. A large-scale generated video dataset (GVD) is constructed as a benchmark for model training and evaluation. Extensive experimental results verify the high generalization and robustness of our AIGVDet scheme. Code and dataset will be available at https://github.com/multimediaFor/AIGVDet.","sentences":["The advancement of generation models has led to the emergence of highly realistic artificial intelligence (AI)-generated videos.","Malicious users can easily create non-existent videos to spread false information.","This letter proposes an effective AI-generated video detection (AIGVDet) scheme by capturing the forensic traces with a two-branch spatio-temporal convolutional neural network (CNN).","Specifically, two ResNet sub-detectors are learned separately for identifying the anomalies in spatical and optical flow domains, respectively.","Results of such sub-detectors are fused to further enhance the discrimination ability.","A large-scale generated video dataset (GVD) is constructed as a benchmark for model training and evaluation.","Extensive experimental results verify the high generalization and robustness of our AIGVDet scheme.","Code and dataset will be available at https://github.com/multimediaFor/AIGVDet."],"url":"http://arxiv.org/abs/2403.16638v1"}
{"created":"2024-03-25 11:25:52","title":"Formally Verifying the Safety of Pipelined Moonshot Consensus Protocol","abstract":"Decentralized Finance (DeFi) has emerged as a contemporary competitive as well as complementary to traditional centralized finance systems. As of 23rd January 2024, per Defillama approximately USD 55 billion is the total value locked on the DeFi applications on all blockchains put together.   A Byzantine Fault Tolerant (BFT) State Machine Replication (SMR) protocol, popularly known as the consensus protocol, is the central component of a blockchain. If forks are possible in a consensus protocol, they can be misused to carry out double spending attacks and can be catastrophic given high volumes of finance that are transacted on blockchains. Formal verification of the safety of consensus protocols is the golden standard for guaranteeing that forks are not possible. However, it is considered complex and challenging to do. This is reflected by the fact that not many complex consensus protocols are formally verified except for Tendermint and QBFT.   We focus on Supra's Pipelined Moonshot consensus protocol. Similar to Tendermint's formal verification, we too model Pipelined Moonshot using IVy and formally prove that for all network sizes, as long as the number of Byzantine validators is less than one thirds, the protocol does not allow forks, thus proving that Pipelined Moonshot is safe and double spending cannot be done using forks. The IVy model and proof of safety is available on Github.","sentences":["Decentralized Finance (DeFi) has emerged as a contemporary competitive as well as complementary to traditional centralized finance systems.","As of 23rd January 2024, per Defillama approximately USD 55 billion is the total value locked on the DeFi applications on all blockchains put together.   ","A Byzantine Fault Tolerant (BFT) State Machine Replication (SMR) protocol, popularly known as the consensus protocol, is the central component of a blockchain.","If forks are possible in a consensus protocol, they can be misused to carry out double spending attacks and can be catastrophic given high volumes of finance that are transacted on blockchains.","Formal verification of the safety of consensus protocols is the golden standard for guaranteeing that forks are not possible.","However, it is considered complex and challenging to do.","This is reflected by the fact that not many complex consensus protocols are formally verified except for Tendermint and QBFT.   ","We focus on Supra's Pipelined Moonshot consensus protocol.","Similar to Tendermint's formal verification, we too model Pipelined Moonshot using IVy and formally prove that for all network sizes, as long as the number of Byzantine validators is less than one thirds, the protocol does not allow forks, thus proving that Pipelined Moonshot is safe and double spending cannot be done using forks.","The IVy model and proof of safety is available on Github."],"url":"http://arxiv.org/abs/2403.16637v1"}
{"created":"2024-03-25 11:24:02","title":"V2X-PC: Vehicle-to-everything Collaborative Perception via Point Cluster","abstract":"The objective of the collaborative vehicle-to-everything perception task is to enhance the individual vehicle's perception capability through message communication among neighboring traffic agents. Previous methods focus on achieving optimal performance within bandwidth limitations and typically adopt BEV maps as the basic collaborative message units. However, we demonstrate that collaboration with dense representations is plagued by object feature destruction during message packing, inefficient message aggregation for long-range collaboration, and implicit structure representation communication. To tackle these issues, we introduce a brand new message unit, namely point cluster, designed to represent the scene sparsely with a combination of low-level structure information and high-level semantic information. The point cluster inherently preserves object information while packing messages, with weak relevance to the collaboration range, and supports explicit structure modeling. Building upon this representation, we propose a novel framework V2X-PC for collaborative perception. This framework includes a Point Cluster Packing (PCP) module to keep object feature and manage bandwidth through the manipulation of cluster point numbers. As for effective message aggregation, we propose a Point Cluster Aggregation (PCA) module to match and merge point clusters associated with the same object. To further handle time latency and pose errors encountered in real-world scenarios, we propose parameter-free solutions that can adapt to different noisy levels without finetuning. Experiments on two widely recognized collaborative perception benchmarks showcase the superior performance of our method compared to the previous state-of-the-art approaches relying on BEV maps.","sentences":["The objective of the collaborative vehicle-to-everything perception task is to enhance the individual vehicle's perception capability through message communication among neighboring traffic agents.","Previous methods focus on achieving optimal performance within bandwidth limitations and typically adopt BEV maps as the basic collaborative message units.","However, we demonstrate that collaboration with dense representations is plagued by object feature destruction during message packing, inefficient message aggregation for long-range collaboration, and implicit structure representation communication.","To tackle these issues, we introduce a brand new message unit, namely point cluster, designed to represent the scene sparsely with a combination of low-level structure information and high-level semantic information.","The point cluster inherently preserves object information while packing messages, with weak relevance to the collaboration range, and supports explicit structure modeling.","Building upon this representation, we propose a novel framework V2X-PC for collaborative perception.","This framework includes a Point Cluster Packing (PCP) module to keep object feature and manage bandwidth through the manipulation of cluster point numbers.","As for effective message aggregation, we propose a Point Cluster Aggregation (PCA) module to match and merge point clusters associated with the same object.","To further handle time latency and pose errors encountered in real-world scenarios, we propose parameter-free solutions that can adapt to different noisy levels without finetuning.","Experiments on two widely recognized collaborative perception benchmarks showcase the superior performance of our method compared to the previous state-of-the-art approaches relying on BEV maps."],"url":"http://arxiv.org/abs/2403.16635v1"}
{"created":"2024-03-25 11:22:38","title":"Symbolic and User-friendly Geometric Algebra Routines (SUGAR) for Computations in Matlab","abstract":"Geometric algebra (GA) is a mathematical tool for geometric computing, providing a framework that allows a unified and compact approach to geometric relations which in other mathematical systems are typically described using different more complicated elements. This fact has led to an increasing adoption of GA in applied mathematics and engineering problems. However, the scarcity of symbolic implementations of GA and its inherent complexity, requiring a specific mathematical background, make it challenging and less intuitive for engineers to work with. This prevents wider adoption among more applied professionals. To address this challenge, this paper introduces SUGAR (Symbolic and User-friendly Geometric Algebra Routines), an open-source toolbox designed for Matlab and licensed under the MIT License. SUGAR facilitates the translation of GA concepts into Matlab and provides a collection of user-friendly functions tailored for GA computations, including support for symbolic operations. It supports both numeric and symbolic computations in high-dimensional GAs. Specifically tailored for applied mathematics and engineering applications, SUGAR has been meticulously engineered to represent geometric elements and transformations within two and three-dimensional projective and conformal geometric algebras, aligning with established computational methodologies in the literature. Furthermore, SUGAR efficiently handles functions of multivectors, such as exponential, logarithmic, sinusoidal, and cosine functions, enhancing its applicability across various engineering domains, including robotics, control systems, and power electronics. Finally, this work includes four distinct validation examples, demonstrating SUGAR's capabilities across the above-mentioned fields and its practical utility in addressing real-world applied mathematics and engineering problems.","sentences":["Geometric algebra (GA) is a mathematical tool for geometric computing, providing a framework that allows a unified and compact approach to geometric relations which in other mathematical systems are typically described using different more complicated elements.","This fact has led to an increasing adoption of GA in applied mathematics and engineering problems.","However, the scarcity of symbolic implementations of GA and its inherent complexity, requiring a specific mathematical background, make it challenging and less intuitive for engineers to work with.","This prevents wider adoption among more applied professionals.","To address this challenge, this paper introduces SUGAR (Symbolic and User-friendly Geometric Algebra Routines), an open-source toolbox designed for Matlab and licensed under the MIT License.","SUGAR facilitates the translation of GA concepts into Matlab and provides a collection of user-friendly functions tailored for GA computations, including support for symbolic operations.","It supports both numeric and symbolic computations in high-dimensional GAs.","Specifically tailored for applied mathematics and engineering applications, SUGAR has been meticulously engineered to represent geometric elements and transformations within two and three-dimensional projective and conformal geometric algebras, aligning with established computational methodologies in the literature.","Furthermore, SUGAR efficiently handles functions of multivectors, such as exponential, logarithmic, sinusoidal, and cosine functions, enhancing its applicability across various engineering domains, including robotics, control systems, and power electronics.","Finally, this work includes four distinct validation examples, demonstrating SUGAR's capabilities across the above-mentioned fields and its practical utility in addressing real-world applied mathematics and engineering problems."],"url":"http://arxiv.org/abs/2403.16634v1"}
{"created":"2024-03-25 11:20:23","title":"A comparative analysis of embedding models for patent similarity","abstract":"This paper makes two contributions to the field of text-based patent similarity. First, it compares the performance of different kinds of patent-specific pretrained embedding models, namely static word embeddings (such as word2vec and doc2vec models) and contextual word embeddings (such as transformers based models), on the task of patent similarity calculation. Second, it compares specifically the performance of Sentence Transformers (SBERT) architectures with different training phases on the patent similarity task. To assess the models' performance, we use information about patent interferences, a phenomenon in which two or more patent claims belonging to different patent applications are proven to be overlapping by patent examiners. Therefore, we use these interferences cases as a proxy for maximum similarity between two patents, treating them as ground-truth to evaluate the performance of the different embedding models. Our results point out that, first, Patent SBERT-adapt-ub, the domain adaptation of the pretrained Sentence Transformer architecture proposed in this research, outperforms the current state-of-the-art in patent similarity. Second, they show that, in some cases, large static models performances are still comparable to contextual ones when trained on extensive data; thus, we believe that the superiority in the performance of contextual embeddings may not be related to the actual architecture but rather to the way the training phase is performed.","sentences":["This paper makes two contributions to the field of text-based patent similarity.","First, it compares the performance of different kinds of patent-specific pretrained embedding models, namely static word embeddings (such as word2vec and doc2vec models) and contextual word embeddings (such as transformers based models), on the task of patent similarity calculation.","Second, it compares specifically the performance of Sentence Transformers (SBERT) architectures with different training phases on the patent similarity task.","To assess the models' performance, we use information about patent interferences, a phenomenon in which two or more patent claims belonging to different patent applications are proven to be overlapping by patent examiners.","Therefore, we use these interferences cases as a proxy for maximum similarity between two patents, treating them as ground-truth to evaluate the performance of the different embedding models.","Our results point out that, first, Patent SBERT-adapt-ub, the domain adaptation of the pretrained Sentence Transformer architecture proposed in this research, outperforms the current state-of-the-art in patent similarity.","Second, they show that, in some cases, large static models performances are still comparable to contextual ones when trained on extensive data; thus, we believe that the superiority in the performance of contextual embeddings may not be related to the actual architecture but rather to the way the training phase is performed."],"url":"http://arxiv.org/abs/2403.16630v1"}
{"created":"2024-03-25 11:16:23","title":"SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions","abstract":"Recent advancements in diffusion models have positioned them at the forefront of image generation. Despite their superior performance, diffusion models are not without drawbacks; they are characterized by complex architectures and substantial computational demands, resulting in significant latency due to their iterative sampling process. To mitigate these limitations, we introduce a dual approach involving model miniaturization and a reduction in sampling steps, aimed at significantly decreasing model latency. Our methodology leverages knowledge distillation to streamline the U-Net and image decoder architectures, and introduces an innovative one-step DM training technique that utilizes feature matching and score distillation. We present two models, SDXS-512 and SDXS-1024, achieving inference speeds of approximately 100 FPS (30x faster than SD v1.5) and 30 FP (60x faster than SDXL) on a single GPU, respectively. Moreover, our training approach offers promising applications in image-conditioned control, facilitating efficient image-to-image translation.","sentences":["Recent advancements in diffusion models have positioned them at the forefront of image generation.","Despite their superior performance, diffusion models are not without drawbacks; they are characterized by complex architectures and substantial computational demands, resulting in significant latency due to their iterative sampling process.","To mitigate these limitations, we introduce a dual approach involving model miniaturization and a reduction in sampling steps, aimed at significantly decreasing model latency.","Our methodology leverages knowledge distillation to streamline the U-Net and image decoder architectures, and introduces an innovative one-step DM training technique that utilizes feature matching and score distillation.","We present two models, SDXS-512 and SDXS-1024, achieving inference speeds of approximately 100 FPS (30x faster than SD v1.5) and 30 FP (60x faster than SDXL) on a single GPU, respectively.","Moreover, our training approach offers promising applications in image-conditioned control, facilitating efficient image-to-image translation."],"url":"http://arxiv.org/abs/2403.16627v1"}
{"created":"2024-03-25 11:15:02","title":"Presenting Interval Pomsets with Interfaces","abstract":"Interval-order partially ordered multisets with interfaces (ipomsets) have shown to be a versatile model for executions of concurrent systems in which both precedence and concurrency need to be taken into account. In this paper, we develop a presentation of ipomsets as generated by a graph of certain discrete ipomsets (starters and terminators) under the relation which composes subsequent starters and subsequent terminators. Using this presentation, we show that also subsumptions are generated by elementary relations. We develop a similar correspondence on the automata side, relating higher-dimensional automata, which generate ipomsets, and ST-automata, which generate step sequences, and their respective languages.","sentences":["Interval-order partially ordered multisets with interfaces (ipomsets) have shown to be a versatile model for executions of concurrent systems in which both precedence and concurrency need to be taken into account.","In this paper, we develop a presentation of ipomsets as generated by a graph of certain discrete ipomsets (starters and terminators) under the relation which composes subsequent starters and subsequent terminators.","Using this presentation, we show that also subsumptions are generated by elementary relations.","We develop a similar correspondence on the automata side, relating higher-dimensional automata, which generate ipomsets, and ST-automata, which generate step sequences, and their respective languages."],"url":"http://arxiv.org/abs/2403.16626v1"}
{"created":"2024-03-25 10:44:38","title":"Semantically Enriched Cross-Lingual Sentence Embeddings for Crisis-related Social Media Texts","abstract":"Tasks such as semantic search and clustering on crisis-related social media texts enhance our comprehension of crisis discourse, aiding decision-making and targeted interventions. Pre-trained language models have advanced performance in crisis informatics, but their contextual embeddings lack semantic meaningfulness. Although the CrisisTransformers family includes a sentence encoder to address the semanticity issue, it remains monolingual, processing only English texts. Furthermore, employing separate models for different languages leads to embeddings in distinct vector spaces, introducing challenges when comparing semantic similarities between multi-lingual texts. Therefore, we propose multi-lingual sentence encoders (CT-XLMR-SE and CT-mBERT-SE) that embed crisis-related social media texts for over 50 languages, such that texts with similar meanings are in close proximity within the same vector space, irrespective of language diversity. Results in sentence encoding and sentence matching tasks are promising, suggesting these models could serve as robust baselines when embedding multi-lingual crisis-related social media texts. The models are publicly available at: https://huggingface.co/crisistransformers.","sentences":["Tasks such as semantic search and clustering on crisis-related social media texts enhance our comprehension of crisis discourse, aiding decision-making and targeted interventions.","Pre-trained language models have advanced performance in crisis informatics, but their contextual embeddings lack semantic meaningfulness.","Although the CrisisTransformers family includes a sentence encoder to address the semanticity issue, it remains monolingual, processing only English texts.","Furthermore, employing separate models for different languages leads to embeddings in distinct vector spaces, introducing challenges when comparing semantic similarities between multi-lingual texts.","Therefore, we propose multi-lingual sentence encoders (CT-XLMR-SE and CT-mBERT-SE) that embed crisis-related social media texts for over 50 languages, such that texts with similar meanings are in close proximity within the same vector space, irrespective of language diversity.","Results in sentence encoding and sentence matching tasks are promising, suggesting these models could serve as robust baselines when embedding multi-lingual crisis-related social media texts.","The models are publicly available at: https://huggingface.co/crisistransformers."],"url":"http://arxiv.org/abs/2403.16614v1"}
{"created":"2024-03-25 10:43:47","title":"Technical Development of a Semi-Autonomous Robotic Partition","abstract":"This technical description details the design and engineering process of a semi-autonomous robotic partition. This robotic partition prototype was subsequently employed in a longer-term evaluation in-the-wild study conducted by the authors in a real-world office setting.","sentences":["This technical description details the design and engineering process of a semi-autonomous robotic partition.","This robotic partition prototype was subsequently employed in a longer-term evaluation in-the-wild study conducted by the authors in a real-world office setting."],"url":"http://arxiv.org/abs/2403.16613v1"}
{"created":"2024-03-25 10:42:48","title":"Calibrating Bayesian UNet++ for Sub-Seasonal Forecasting","abstract":"Seasonal forecasting is a crucial task when it comes to detecting the extreme heat and colds that occur due to climate change. Confidence in the predictions should be reliable since a small increase in the temperatures in a year has a big impact on the world. Calibration of the neural networks provides a way to ensure our confidence in the predictions. However, calibrating regression models is an under-researched topic, especially in forecasters. We calibrate a UNet++ based architecture, which was shown to outperform physics-based models in temperature anomalies. We show that with a slight trade-off between prediction error and calibration error, it is possible to get more reliable and sharper forecasts. We believe that calibration should be an important part of safety-critical machine learning applications such as weather forecasters.","sentences":["Seasonal forecasting is a crucial task when it comes to detecting the extreme heat and colds that occur due to climate change.","Confidence in the predictions should be reliable since a small increase in the temperatures in a year has a big impact on the world.","Calibration of the neural networks provides a way to ensure our confidence in the predictions.","However, calibrating regression models is an under-researched topic, especially in forecasters.","We calibrate a UNet++ based architecture, which was shown to outperform physics-based models in temperature anomalies.","We show that with a slight trade-off between prediction error and calibration error, it is possible to get more reliable and sharper forecasts.","We believe that calibration should be an important part of safety-critical machine learning applications such as weather forecasters."],"url":"http://arxiv.org/abs/2403.16612v1"}
{"created":"2024-03-25 10:39:18","title":"Conversational Grounding: Annotation and Analysis of Grounding Acts and Grounding Units","abstract":"Successful conversations often rest on common understanding, where all parties are on the same page about the information being shared. This process, known as conversational grounding, is crucial for building trustworthy dialog systems that can accurately keep track of and recall the shared information. The proficiencies of an agent in grounding the conveyed information significantly contribute to building a reliable dialog system. Despite recent advancements in dialog systems, there exists a noticeable deficit in their grounding capabilities. Traum provided a framework for conversational grounding introducing Grounding Acts and Grounding Units, but substantial progress, especially in the realm of Large Language Models, remains lacking. To bridge this gap, we present the annotation of two dialog corpora employing Grounding Acts, Grounding Units, and a measure of their degree of grounding. We discuss our key findings during the annotation and also provide a baseline model to test the performance of current Language Models in categorizing the grounding acts of the dialogs. Our work aims to provide a useful resource for further research in making conversations with machines better understood and more reliable in natural day-to-day collaborative dialogs.","sentences":["Successful conversations often rest on common understanding, where all parties are on the same page about the information being shared.","This process, known as conversational grounding, is crucial for building trustworthy dialog systems that can accurately keep track of and recall the shared information.","The proficiencies of an agent in grounding the conveyed information significantly contribute to building a reliable dialog system.","Despite recent advancements in dialog systems, there exists a noticeable deficit in their grounding capabilities.","Traum provided a framework for conversational grounding introducing Grounding Acts and Grounding Units, but substantial progress, especially in the realm of Large Language Models, remains lacking.","To bridge this gap, we present the annotation of two dialog corpora employing Grounding Acts, Grounding Units, and a measure of their degree of grounding.","We discuss our key findings during the annotation and also provide a baseline model to test the performance of current Language Models in categorizing the grounding acts of the dialogs.","Our work aims to provide a useful resource for further research in making conversations with machines better understood and more reliable in natural day-to-day collaborative dialogs."],"url":"http://arxiv.org/abs/2403.16609v1"}
{"created":"2024-03-25 10:38:17","title":"Enhancing Industrial Transfer Learning with Style Filter: Cost Reduction and Defect-Focus","abstract":"Addressing the challenge of data scarcity in industrial domains, transfer learning emerges as a pivotal paradigm. This work introduces Style Filter, a tailored methodology for industrial contexts. By selectively filtering source domain data before knowledge transfer, Style Filter reduces the quantity of data while maintaining or even enhancing the performance of transfer learning strategy. Offering label-free operation, minimal reliance on prior knowledge, independence from specific models, and re-utilization, Style Filter is evaluated on authentic industrial datasets, highlighting its effectiveness when employed before conventional transfer strategies in the deep learning domain. The results underscore the effectiveness of Style Filter in real-world industrial applications.","sentences":["Addressing the challenge of data scarcity in industrial domains, transfer learning emerges as a pivotal paradigm.","This work introduces Style Filter, a tailored methodology for industrial contexts.","By selectively filtering source domain data before knowledge transfer, Style Filter reduces the quantity of data while maintaining or even enhancing the performance of transfer learning strategy.","Offering label-free operation, minimal reliance on prior knowledge, independence from specific models, and re-utilization, Style Filter is evaluated on authentic industrial datasets, highlighting its effectiveness when employed before conventional transfer strategies in the deep learning domain.","The results underscore the effectiveness of Style Filter in real-world industrial applications."],"url":"http://arxiv.org/abs/2403.16607v1"}
{"created":"2024-03-25 10:33:20","title":"ROXIE: Defining a Robotic eXplanation and Interpretability Engine","abstract":"In an era where autonomous robots increasingly inhabit public spaces, the imperative for transparency and interpretability in their decision-making processes becomes paramount. This paper presents the overview of a Robotic eXplanation and Interpretability Engine (ROXIE), which addresses this critical need, aiming to demystify the opaque nature of complex robotic behaviors. This paper elucidates the key features and requirements needed for providing information and explanations about robot decision-making processes. It also overviews the suite of software components and libraries available for deployment with ROS 2, empowering users to provide comprehensive explanations and interpretations of robot processes and behaviors, thereby fostering trust and collaboration in human-robot interactions.","sentences":["In an era where autonomous robots increasingly inhabit public spaces, the imperative for transparency and interpretability in their decision-making processes becomes paramount.","This paper presents the overview of a Robotic eXplanation and Interpretability Engine (ROXIE), which addresses this critical need, aiming to demystify the opaque nature of complex robotic behaviors.","This paper elucidates the key features and requirements needed for providing information and explanations about robot decision-making processes.","It also overviews the suite of software components and libraries available for deployment with ROS 2, empowering users to provide comprehensive explanations and interpretations of robot processes and behaviors, thereby fostering trust and collaboration in human-robot interactions."],"url":"http://arxiv.org/abs/2403.16606v1"}
{"created":"2024-03-25 10:30:22","title":"SatSynth: Augmenting Image-Mask Pairs through Diffusion Models for Aerial Semantic Segmentation","abstract":"In recent years, semantic segmentation has become a pivotal tool in processing and interpreting satellite imagery. Yet, a prevalent limitation of supervised learning techniques remains the need for extensive manual annotations by experts. In this work, we explore the potential of generative image diffusion to address the scarcity of annotated data in earth observation tasks. The main idea is to learn the joint data manifold of images and labels, leveraging recent advancements in denoising diffusion probabilistic models. To the best of our knowledge, we are the first to generate both images and corresponding masks for satellite segmentation. We find that the obtained pairs not only display high quality in fine-scale features but also ensure a wide sampling diversity. Both aspects are crucial for earth observation data, where semantic classes can vary severely in scale and occurrence frequency. We employ the novel data instances for downstream segmentation, as a form of data augmentation. In our experiments, we provide comparisons to prior works based on discriminative diffusion models or GANs. We demonstrate that integrating generated samples yields significant quantitative improvements for satellite semantic segmentation -- both compared to baselines and when training only on the original data.","sentences":["In recent years, semantic segmentation has become a pivotal tool in processing and interpreting satellite imagery.","Yet, a prevalent limitation of supervised learning techniques remains the need for extensive manual annotations by experts.","In this work, we explore the potential of generative image diffusion to address the scarcity of annotated data in earth observation tasks.","The main idea is to learn the joint data manifold of images and labels, leveraging recent advancements in denoising diffusion probabilistic models.","To the best of our knowledge, we are the first to generate both images and corresponding masks for satellite segmentation.","We find that the obtained pairs not only display high quality in fine-scale features but also ensure a wide sampling diversity.","Both aspects are crucial for earth observation data, where semantic classes can vary severely in scale and occurrence frequency.","We employ the novel data instances for downstream segmentation, as a form of data augmentation.","In our experiments, we provide comparisons to prior works based on discriminative diffusion models or GANs.","We demonstrate that integrating generated samples yields significant quantitative improvements for satellite semantic segmentation -- both compared to baselines and when training only on the original data."],"url":"http://arxiv.org/abs/2403.16605v1"}
{"created":"2024-03-25 10:20:50","title":"Research Challenges for Adaptive Architecture: Empowering Occupants of Multi-Occupancy Buildings","abstract":"This positional paper outlines our vision of 'adaptive architecture', which involves the integration of robotic technology to physically change an architectural space in supporting the changing needs of its occupants, in response to the CHI'24 workshop \"HabiTech - Inhabiting Buildings, Data & Technology\" call on \"How do new technologies enable and empower the inhabitants of multi-occupancy buildings?\". Specifically, while adaptive architecture holds promise for enhancing occupant satisfaction, comfort, and overall health and well-being, there remains a range of research challenges of (1) how it can effectively support individual occupants, while (2) mediating the conflicting needs of collocated others, and (3) integrating meaningfully into the sociocultural characteristics of their building community.","sentences":["This positional paper outlines our vision of 'adaptive architecture', which involves the integration of robotic technology to physically change an architectural space in supporting the changing needs of its occupants, in response to the CHI'24 workshop \"HabiTech - Inhabiting Buildings, Data & Technology\" call on \"How do new technologies enable and empower the inhabitants of multi-occupancy buildings?\".","Specifically, while adaptive architecture holds promise for enhancing occupant satisfaction, comfort, and overall health and well-being, there remains a range of research challenges of (1) how it can effectively support individual occupants, while (2) mediating the conflicting needs of collocated others, and (3) integrating meaningfully into the sociocultural characteristics of their building community."],"url":"http://arxiv.org/abs/2403.16600v1"}
{"created":"2024-03-25 10:16:51","title":"The Adaptive Workplace: Orchestrating Architectural Services around the Wellbeing of Individual Occupants","abstract":"As the academic consortia members of the EU Horizon project SONATA (\"Situation-aware OrchestratioN of AdapTive Architecture\"), we respond to the workshop call for \"Office Wellbeing by Design: Don't Stand for Anything Less\" by proposing the \"Adaptive Workplace\" concept. In essence, our vision aims to adapt a workplace to the ever-changing needs of individual occupants, instead of that occupants are expected to adapt to their workplace.","sentences":["As the academic consortia members of the EU Horizon project SONATA (\"Situation-aware OrchestratioN of AdapTive Architecture\"), we respond to the workshop call for \"Office Wellbeing by Design: Don't Stand for Anything Less\" by proposing the \"Adaptive Workplace\" concept.","In essence, our vision aims to adapt a workplace to the ever-changing needs of individual occupants, instead of that occupants are expected to adapt to their workplace."],"url":"http://arxiv.org/abs/2403.16595v1"}
{"created":"2024-03-25 10:09:42","title":"Counter-example guided Imitation Learning of Feedback Controllers from Temporal Logic Specifications","abstract":"We present a novel method for imitation learning for control requirements expressed using Signal Temporal Logic (STL). More concretely we focus on the problem of training a neural network to imitate a complex controller. The learning process is guided by efficient data aggregation based on counter-examples and a coverage measure. Moreover, we introduce a method to evaluate the performance of the learned controller via parameterization and parameter estimation of the STL requirements. We demonstrate our approach with a flying robot case study.","sentences":["We present a novel method for imitation learning for control requirements expressed using Signal Temporal Logic (STL).","More concretely we focus on the problem of training a neural network to imitate a complex controller.","The learning process is guided by efficient data aggregation based on counter-examples and a coverage measure.","Moreover, we introduce a method to evaluate the performance of the learned controller via parameterization and parameter estimation of the STL requirements.","We demonstrate our approach with a flying robot case study."],"url":"http://arxiv.org/abs/2403.16593v1"}
{"created":"2024-03-25 10:09:03","title":"TrustAI at SemEval-2024 Task 8: A Comprehensive Analysis of Multi-domain Machine Generated Text Detection Techniques","abstract":"The Large Language Models (LLMs) exhibit remarkable ability to generate fluent content across a wide spectrum of user queries. However, this capability has raised concerns regarding misinformation and personal information leakage. In this paper, we present our methods for the SemEval2024 Task8, aiming to detect machine-generated text across various domains in both mono-lingual and multi-lingual contexts. Our study comprehensively analyzes various methods to detect machine-generated text, including statistical, neural, and pre-trained model approaches. We also detail our experimental setup and perform a in-depth error analysis to evaluate the effectiveness of these methods. Our methods obtain an accuracy of 86.9\\% on the test set of subtask-A mono and 83.7\\% for subtask-B. Furthermore, we also highlight the challenges and essential factors for consideration in future studies.","sentences":["The Large Language Models (LLMs) exhibit remarkable ability to generate fluent content across a wide spectrum of user queries.","However, this capability has raised concerns regarding misinformation and personal information leakage.","In this paper, we present our methods for the SemEval2024 Task8, aiming to detect machine-generated text across various domains in both mono-lingual and multi-lingual contexts.","Our study comprehensively analyzes various methods to detect machine-generated text, including statistical, neural, and pre-trained model approaches.","We also detail our experimental setup and perform a in-depth error analysis to evaluate the effectiveness of these methods.","Our methods obtain an accuracy of 86.9\\% on the test set of subtask-A mono and 83.7\\% for subtask-B. Furthermore, we also highlight the challenges and essential factors for consideration in future studies."],"url":"http://arxiv.org/abs/2403.16592v1"}
{"created":"2024-03-25 10:06:45","title":"Deciphering the Interplay between Local Differential Privacy, Average Bayesian Privacy, and Maximum Bayesian Privacy","abstract":"The swift evolution of machine learning has led to emergence of various definitions of privacy due to the threats it poses to privacy, including the concept of local differential privacy (LDP). Although widely embraced and utilized across numerous domains, this conventional approach to measure privacy still exhibits certain limitations, spanning from failure to prevent inferential disclosure to lack of consideration for the adversary's background knowledge. In this comprehensive study, we introduce Bayesian privacy and delve into the intricate relationship between local differential privacy and its Bayesian counterparts, unveiling novel insights into utility-privacy trade-offs. We introduce a framework that encapsulates both attack and defense strategies, highlighting their interplay and effectiveness. Our theoretical contributions are anchored in the rigorous definitions and relationships between Average Bayesian Privacy (ABP) and Maximum Bayesian Privacy (MBP), encapsulated by equations $\\epsilon_{p,a} \\leq \\frac{1}{\\sqrt{2}}\\sqrt{(\\epsilon_{p,m} + \\epsilon)\\cdot(e^{\\epsilon_{p,m} + \\epsilon} - 1)}$ and the equivalence between $\\xi$-MBP and $2\\xi$-LDP established under uniform prior distribution. These relationships fortify our understanding of the privacy guarantees provided by various mechanisms, leading to the realization that a mechanism satisfying $\\xi$-LDP also confers $\\xi$-MBP, and vice versa. Our work not only lays the groundwork for future empirical exploration but also promises to enhance the design of privacy-preserving algorithms that do not compromise on utility, thereby fostering the development of trustworthy machine learning solutions.","sentences":["The swift evolution of machine learning has led to emergence of various definitions of privacy due to the threats it poses to privacy, including the concept of local differential privacy (LDP).","Although widely embraced and utilized across numerous domains, this conventional approach to measure privacy still exhibits certain limitations, spanning from failure to prevent inferential disclosure to lack of consideration for the adversary's background knowledge.","In this comprehensive study, we introduce Bayesian privacy and delve into the intricate relationship between local differential privacy and its Bayesian counterparts, unveiling novel insights into utility-privacy trade-offs.","We introduce a framework that encapsulates both attack and defense strategies, highlighting their interplay and effectiveness.","Our theoretical contributions are anchored in the rigorous definitions and relationships between Average Bayesian Privacy (ABP) and Maximum Bayesian Privacy (MBP), encapsulated by equations $\\epsilon_{p,a} \\leq \\frac{1}{\\sqrt{2}}\\sqrt{(\\epsilon_{p,m} + \\epsilon)\\cdot(e^{\\epsilon_{p,m} + \\epsilon} - 1)}$ and the equivalence between $\\xi$-MBP and $2\\xi$-LDP established under uniform prior distribution.","These relationships fortify our understanding of the privacy guarantees provided by various mechanisms, leading to the realization that a mechanism satisfying $\\xi$-LDP also confers $\\xi$-MBP, and vice versa.","Our work not only lays the groundwork for future empirical exploration but also promises to enhance the design of privacy-preserving algorithms that do not compromise on utility, thereby fostering the development of trustworthy machine learning solutions."],"url":"http://arxiv.org/abs/2403.16591v1"}
{"created":"2024-03-25 09:51:54","title":"Can Large Language Models (or Humans) Distill Text?","abstract":"We investigate the potential of large language models (LLMs) to distill text: to remove the textual traces of an undesired forbidden variable. We employ a range of LLMs with varying architectures and training approaches to distill text by identifying and removing information about the target variable while preserving other relevant signals. Our findings shed light on the strengths and limitations of LLMs in addressing the distillation and provide insights into the strategies for leveraging these models in computational social science investigations involving text data. In particular, we show that in the strong test of removing sentiment, the statistical association between the processed text and sentiment is still clearly detectable to machine learning classifiers post-LLM-distillation. Furthermore, we find that human annotators also struggle to distill sentiment while preserving other semantic content. This suggests there may be limited separability between concept variables in some text contexts, highlighting limitations of methods relying on text-level transformations and also raising questions about the robustness of distillation methods that achieve statistical independence in representation space if this is difficult for human coders operating on raw text to attain.","sentences":["We investigate the potential of large language models (LLMs) to distill text: to remove the textual traces of an undesired forbidden variable.","We employ a range of LLMs with varying architectures and training approaches to distill text by identifying and removing information about the target variable while preserving other relevant signals.","Our findings shed light on the strengths and limitations of LLMs in addressing the distillation and provide insights into the strategies for leveraging these models in computational social science investigations involving text data.","In particular, we show that in the strong test of removing sentiment, the statistical association between the processed text and sentiment is still clearly detectable to machine learning classifiers post-LLM-distillation.","Furthermore, we find that human annotators also struggle to distill sentiment while preserving other semantic content.","This suggests there may be limited separability between concept variables in some text contexts, highlighting limitations of methods relying on text-level transformations and also raising questions about the robustness of distillation methods that achieve statistical independence in representation space if this is difficult for human coders operating on raw text to attain."],"url":"http://arxiv.org/abs/2403.16584v1"}
{"created":"2024-03-25 09:49:42","title":"In the Search for Optimal Multi-view Learning Models for Crop Classification with Global Remote Sensing Data","abstract":"Crop classification is of critical importance due to its role in studying crop pattern changes, resource management, and carbon sequestration. When employing data-driven techniques for its prediction, utilizing various temporal data sources is necessary. Deep learning models have proven to be effective for this task by mapping time series data to high-level representation for prediction. However, they face substantial challenges when dealing with multiple input patterns. The literature offers limited guidance for Multi-View Learning (MVL) scenarios, as it has primarily focused on exploring fusion strategies with specific encoders and validating them in local regions. In contrast, we investigate the impact of simultaneous selection of the fusion strategy and the encoder architecture evaluated on a global-scale cropland and crop-type classifications. We use a range of five fusion strategies (Input, Feature, Decision, Ensemble, Hybrid) and five temporal encoder architectures (LSTM, GRU, TempCNN, TAE, L-TAE) as possible MVL model configurations. The validation is on the CropHarvest dataset that provides optical, radar, and weather time series, and topographic information as input data. We found that in scenarios with a limited number of labeled samples, a unique configuration is insufficient for all the cases. Instead, a specialized combination, including encoder and fusion strategy, should be meticulously sought. To streamline this search process, we suggest initially identifying the optimal encoder architecture tailored for a particular fusion strategy, and then determining the most suitable fusion strategy for the classification task. We provide a technical framework for researchers exploring crop classification or related tasks through a MVL approach.","sentences":["Crop classification is of critical importance due to its role in studying crop pattern changes, resource management, and carbon sequestration.","When employing data-driven techniques for its prediction, utilizing various temporal data sources is necessary.","Deep learning models have proven to be effective for this task by mapping time series data to high-level representation for prediction.","However, they face substantial challenges when dealing with multiple input patterns.","The literature offers limited guidance for Multi-View Learning (MVL) scenarios, as it has primarily focused on exploring fusion strategies with specific encoders and validating them in local regions.","In contrast, we investigate the impact of simultaneous selection of the fusion strategy and the encoder architecture evaluated on a global-scale cropland and crop-type classifications.","We use a range of five fusion strategies (Input, Feature, Decision, Ensemble, Hybrid) and five temporal encoder architectures (LSTM, GRU, TempCNN, TAE, L-TAE) as possible MVL model configurations.","The validation is on the CropHarvest dataset that provides optical, radar, and weather time series, and topographic information as input data.","We found that in scenarios with a limited number of labeled samples, a unique configuration is insufficient for all the cases.","Instead, a specialized combination, including encoder and fusion strategy, should be meticulously sought.","To streamline this search process, we suggest initially identifying the optimal encoder architecture tailored for a particular fusion strategy, and then determining the most suitable fusion strategy for the classification task.","We provide a technical framework for researchers exploring crop classification or related tasks through a MVL approach."],"url":"http://arxiv.org/abs/2403.16582v1"}
{"created":"2024-03-25 09:43:56","title":"SegICL: A Universal In-context Learning Framework for Enhanced Segmentation in Medical Imaging","abstract":"Medical image segmentation models adapting to new tasks in a training-free manner through in-context learning is an exciting advancement. Universal segmentation models aim to generalize across the diverse modality of medical images, yet their effectiveness often diminishes when applied to out-of-distribution (OOD) data modalities and tasks, requiring intricate fine-tuning of model for optimal performance. For addressing this challenge, we introduce SegICL, a novel approach leveraging In-Context Learning (ICL) for image segmentation. Unlike existing methods, SegICL has the capability to employ text-guided segmentation and conduct in-context learning with a small set of image-mask pairs, eliminating the need for training the model from scratch or fine-tuning for OOD tasks (including OOD modality and dataset). Extensive experimental validation of SegICL demonstrates a positive correlation between the number of prompt samples and segmentation performance on OOD modalities and tasks. This indicates that SegICL effectively address new segmentation tasks based on contextual information. Additionally, SegICL also exhibits comparable segmentation performance to mainstream models on OOD and in-distribution tasks. Our code will be released soon.","sentences":["Medical image segmentation models adapting to new tasks in a training-free manner through in-context learning is an exciting advancement.","Universal segmentation models aim to generalize across the diverse modality of medical images, yet their effectiveness often diminishes when applied to out-of-distribution (OOD) data modalities and tasks, requiring intricate fine-tuning of model for optimal performance.","For addressing this challenge, we introduce SegICL, a novel approach leveraging In-Context Learning (ICL) for image segmentation.","Unlike existing methods, SegICL has the capability to employ text-guided segmentation and conduct in-context learning with a small set of image-mask pairs, eliminating the need for training the model from scratch or fine-tuning for OOD tasks (including OOD modality and dataset).","Extensive experimental validation of SegICL demonstrates a positive correlation between the number of prompt samples and segmentation performance on OOD modalities and tasks.","This indicates that SegICL effectively address new segmentation tasks based on contextual information.","Additionally, SegICL also exhibits comparable segmentation performance to mainstream models on OOD and in-distribution tasks.","Our code will be released soon."],"url":"http://arxiv.org/abs/2403.16578v1"}
{"created":"2024-03-25 09:43:26","title":"Partially-Precise Computing Paradigm for Efficient Hardware Implementation of Application-Specific Embedded Systems","abstract":"Nowadays, the number of emerging embedded systems rapidly grows in many application domains, due to recent advances in artificial intelligence and internet of things. The main inherent specification of these application-specific systems is that they have not a general nature and are basically developed to only perform a particular task and therefore, deal only with a limited and predefined range of custom input values. Despite this significant feature, these emerging applications are still conventionally implemented using general-purpose and precise digital computational blocks, which are essentially developed to provide the correct result for all possible input values. This highly degrades the physical properties of these applications while does not improve their functionality. To resolve this conflict, a novel computational paradigm named as partially-precise computing is introduced in this paper, based on an inspiration from the brain information reduction hypothesis as a tenet of neuroscience. The main specification of a Partially-Precise Computational (PPC) block is that it provides the precise result only for a desired, limited, and predefined set of input values. This relaxes its internal structure which results in improved physical properties with respect to a conventional precise block. The PPC blocks improve the implementation costs of the embedded applications, with a negligible or even without any output quality degradation with respect to the conventional implementation. The applicability and efficiency of the first instances of PPC adders and multipliers in a Gaussian denoising filter, an image blending and a face recognition neural network are demonstrated by means of a wide range of simulation and synthesis results.","sentences":["Nowadays, the number of emerging embedded systems rapidly grows in many application domains, due to recent advances in artificial intelligence and internet of things.","The main inherent specification of these application-specific systems is that they have not a general nature and are basically developed to only perform a particular task and therefore, deal only with a limited and predefined range of custom input values.","Despite this significant feature, these emerging applications are still conventionally implemented using general-purpose and precise digital computational blocks, which are essentially developed to provide the correct result for all possible input values.","This highly degrades the physical properties of these applications while does not improve their functionality.","To resolve this conflict, a novel computational paradigm named as partially-precise computing is introduced in this paper, based on an inspiration from the brain information reduction hypothesis as a tenet of neuroscience.","The main specification of a Partially-Precise Computational (PPC) block is that it provides the precise result only for a desired, limited, and predefined set of input values.","This relaxes its internal structure which results in improved physical properties with respect to a conventional precise block.","The PPC blocks improve the implementation costs of the embedded applications, with a negligible or even without any output quality degradation with respect to the conventional implementation.","The applicability and efficiency of the first instances of PPC adders and multipliers in a Gaussian denoising filter, an image blending and a face recognition neural network are demonstrated by means of a wide range of simulation and synthesis results."],"url":"http://arxiv.org/abs/2403.16577v1"}
{"created":"2024-03-25 09:40:54","title":"A Measure of Synergy based on Union Information","abstract":"The partial information decomposition (PID) framework is concerned with decomposing the information that a set of (two or more) random variables (the sources) has about another variable (the target) into three types of information: unique, redundant, and synergistic. Classical information theory alone does not provide a unique way to decompose information in this manner and additional assumptions have to be made. One often overlooked way to achieve this decomposition is using a so-called measure of union information - which quantifies the information that is present in at least one of the sources - from which a synergy measure stems. In this paper, we introduce a new measure of union information based on adopting a communication channel perspective, compare it with existing measures, and study some of its properties. We also include a comprehensive critical review of characterizations of union information and synergy measures that have been proposed in the literature.","sentences":["The partial information decomposition (PID) framework is concerned with decomposing the information that a set of (two or more) random variables (the sources) has about another variable (the target) into three types of information: unique, redundant, and synergistic.","Classical information theory alone does not provide a unique way to decompose information in this manner and additional assumptions have to be made.","One often overlooked way to achieve this decomposition is using a so-called measure of union information - which quantifies the information that is present in at least one of the sources - from which a synergy measure stems.","In this paper, we introduce a new measure of union information based on adopting a communication channel perspective, compare it with existing measures, and study some of its properties.","We also include a comprehensive critical review of characterizations of union information and synergy measures that have been proposed in the literature."],"url":"http://arxiv.org/abs/2403.16575v1"}
{"created":"2024-03-25 09:36:51","title":"NSINA: A News Corpus for Sinhala","abstract":"The introduction of large language models (LLMs) has advanced natural language processing (NLP), but their effectiveness is largely dependent on pre-training resources. This is especially evident in low-resource languages, such as Sinhala, which face two primary challenges: the lack of substantial training data and limited benchmarking datasets. In response, this study introduces NSINA, a comprehensive news corpus of over 500,000 articles from popular Sinhala news websites, along with three NLP tasks: news media identification, news category prediction, and news headline generation. The release of NSINA aims to provide a solution to challenges in adapting LLMs to Sinhala, offering valuable resources and benchmarks for improving NLP in the Sinhala language. NSINA is the largest news corpus for Sinhala, available up to date.","sentences":["The introduction of large language models (LLMs) has advanced natural language processing (NLP), but their effectiveness is largely dependent on pre-training resources.","This is especially evident in low-resource languages, such as Sinhala, which face two primary challenges: the lack of substantial training data and limited benchmarking datasets.","In response, this study introduces NSINA, a comprehensive news corpus of over 500,000 articles from popular Sinhala news websites, along with three NLP tasks: news media identification, news category prediction, and news headline generation.","The release of NSINA aims to provide a solution to challenges in adapting LLMs to Sinhala, offering valuable resources and benchmarks for improving NLP in the Sinhala language.","NSINA is the largest news corpus for Sinhala, available up to date."],"url":"http://arxiv.org/abs/2403.16571v1"}
{"created":"2024-03-25 09:36:10","title":"Revealing Vulnerabilities of Neural Networks in Parameter Learning and Defense Against Explanation-Aware Backdoors","abstract":"Explainable Artificial Intelligence (XAI) strategies play a crucial part in increasing the understanding and trustworthiness of neural networks. Nonetheless, these techniques could potentially generate misleading explanations. Blinding attacks can drastically alter a machine learning algorithm's prediction and explanation, providing misleading information by adding visually unnoticeable artifacts into the input, while maintaining the model's accuracy. It poses a serious challenge in ensuring the reliability of XAI methods. To ensure the reliability of XAI methods poses a real challenge, we leverage statistical analysis to highlight the changes in CNN weights within a CNN following blinding attacks. We introduce a method specifically designed to limit the effectiveness of such attacks during the evaluation phase, avoiding the need for extra training. The method we suggest defences against most modern explanation-aware adversarial attacks, achieving an approximate decrease of ~99\\% in the Attack Success Rate (ASR) and a ~91\\% reduction in the Mean Square Error (MSE) between the original explanation and the defended (post-attack) explanation across three unique types of attacks.","sentences":["Explainable Artificial Intelligence (XAI) strategies play a crucial part in increasing the understanding and trustworthiness of neural networks.","Nonetheless, these techniques could potentially generate misleading explanations.","Blinding attacks can drastically alter a machine learning algorithm's prediction and explanation, providing misleading information by adding visually unnoticeable artifacts into the input, while maintaining the model's accuracy.","It poses a serious challenge in ensuring the reliability of XAI methods.","To ensure the reliability of XAI methods poses a real challenge, we leverage statistical analysis to highlight the changes in CNN weights within a CNN following blinding attacks.","We introduce a method specifically designed to limit the effectiveness of such attacks during the evaluation phase, avoiding the need for extra training.","The method we suggest defences against most modern explanation-aware adversarial attacks, achieving an approximate decrease of ~99\\% in the Attack Success Rate (ASR) and a ~91\\% reduction in the Mean Square Error (MSE) between the original explanation and the defended (post-attack) explanation across three unique types of attacks."],"url":"http://arxiv.org/abs/2403.16569v1"}
{"created":"2024-03-25 09:32:09","title":"Molecular Communication-Based Intelligent Dopamine Rate Modulator for Parkinson's Disease Treatment","abstract":"Parkinson's disease (PD) is a progressive neurodegenerative disease, and it is caused by the loss of dopaminergic neurons in the basal ganglia (BG). Currently, there is no definite cure for PD, and available treatments mainly aim to alleviate its symptoms. Due to impaired neurotransmitter-based information transmission in PD, molecular communication-based approaches can be employed as potential solutions to address this issue. Molecular Communications (MC) is a bio-inspired communication method utilizing molecules for carrying information. This mode of communication stands out for developing bio-compatible nanomachines for diagnosing and treating, particularly in addressing neurodegenerative diseases like PD, due to its compatibility with biological systems. This study presents a novel treatment method that introduces an Intelligent Dopamine Rate Modulator (IDRM), which is located in the synaptic gap between the substantia nigra pars compacta (SNc) and striatum to compensate for insufficiency dopamine release in BG caused by PD. For storing dopamine in the IDRM, dopamine compound (DAC) is swallowed and crossed through the digestive system, blood circulatory system, blood-brain barrier (BBB), and brain extracellular matrix uptakes with IDRMs. Here, the DAC concentration is calculated in these regions, revealing that the required exogenous dopamine consistently reaches IDRM. Therefore, the perpetual dopamine insufficiency in BG associated with PD can be compensated. This method reduces drug side effects because dopamine is not released in other brain regions. Unlike other treatments, this approach targets the root cause of PD rather than just reducing symptoms.","sentences":["Parkinson's disease (PD) is a progressive neurodegenerative disease, and it is caused by the loss of dopaminergic neurons in the basal ganglia (BG).","Currently, there is no definite cure for PD, and available treatments mainly aim to alleviate its symptoms.","Due to impaired neurotransmitter-based information transmission in PD, molecular communication-based approaches can be employed as potential solutions to address this issue.","Molecular Communications (MC) is a bio-inspired communication method utilizing molecules for carrying information.","This mode of communication stands out for developing bio-compatible nanomachines for diagnosing and treating, particularly in addressing neurodegenerative diseases like PD, due to its compatibility with biological systems.","This study presents a novel treatment method that introduces an Intelligent Dopamine Rate Modulator (IDRM), which is located in the synaptic gap between the substantia nigra pars compacta (SNc) and striatum to compensate for insufficiency dopamine release in BG caused by PD.","For storing dopamine in the IDRM, dopamine compound (DAC) is swallowed and crossed through the digestive system, blood circulatory system, blood-brain barrier (BBB), and brain extracellular matrix uptakes with IDRMs.","Here, the DAC concentration is calculated in these regions, revealing that the required exogenous dopamine consistently reaches IDRM.","Therefore, the perpetual dopamine insufficiency in BG associated with PD can be compensated.","This method reduces drug side effects because dopamine is not released in other brain regions.","Unlike other treatments, this approach targets the root cause of PD rather than just reducing symptoms."],"url":"http://arxiv.org/abs/2403.16564v1"}
{"created":"2024-03-25 09:24:05","title":"FedFixer: Mitigating Heterogeneous Label Noise in Federated Learning","abstract":"Federated Learning (FL) heavily depends on label quality for its performance. However, the label distribution among individual clients is always both noisy and heterogeneous. The high loss incurred by client-specific samples in heterogeneous label noise poses challenges for distinguishing between client-specific and noisy label samples, impacting the effectiveness of existing label noise learning approaches. To tackle this issue, we propose FedFixer, where the personalized model is introduced to cooperate with the global model to effectively select clean client-specific samples. In the dual models, updating the personalized model solely at a local level can lead to overfitting on noisy data due to limited samples, consequently affecting both the local and global models' performance. To mitigate overfitting, we address this concern from two perspectives. Firstly, we employ a confidence regularizer to alleviate the impact of unconfident predictions caused by label noise. Secondly, a distance regularizer is implemented to constrain the disparity between the personalized and global models. We validate the effectiveness of FedFixer through extensive experiments on benchmark datasets. The results demonstrate that FedFixer can perform well in filtering noisy label samples on different clients, especially in highly heterogeneous label noise scenarios.","sentences":["Federated Learning (FL) heavily depends on label quality for its performance.","However, the label distribution among individual clients is always both noisy and heterogeneous.","The high loss incurred by client-specific samples in heterogeneous label noise poses challenges for distinguishing between client-specific and noisy label samples, impacting the effectiveness of existing label noise learning approaches.","To tackle this issue, we propose FedFixer, where the personalized model is introduced to cooperate with the global model to effectively select clean client-specific samples.","In the dual models, updating the personalized model solely at a local level can lead to overfitting on noisy data due to limited samples, consequently affecting both the local and global models' performance.","To mitigate overfitting, we address this concern from two perspectives.","Firstly, we employ a confidence regularizer to alleviate the impact of unconfident predictions caused by label noise.","Secondly, a distance regularizer is implemented to constrain the disparity between the personalized and global models.","We validate the effectiveness of FedFixer through extensive experiments on benchmark datasets.","The results demonstrate that FedFixer can perform well in filtering noisy label samples on different clients, especially in highly heterogeneous label noise scenarios."],"url":"http://arxiv.org/abs/2403.16561v1"}
{"created":"2024-03-25 09:18:48","title":"Active Admittance Control with Iterative Learning for General-Purpose Contact-Rich Manipulation","abstract":"Force interaction is inevitable when robots face multiple operation scenarios. How to make the robot competent in force control for generalized operations such as multi-tasks still remains a challenging problem. Aiming at the reproducibility of interaction tasks and the lack of a generalized force control framework for multi-task scenarios, this paper proposes a novel hybrid control framework based on active admittance control with iterative learning parameters-tunning mechanism. The method adopts admittance control as the underlying algorithm to ensure flexibility, and iterative learning as the high-level algorithm to regulate the parameters of the admittance model. The whole algorithm has flexibility and learning ability, which is capable of achieving the goal of excellent versatility. Four representative interactive robot manipulation tasks are chosen to investigate the consistency and generalisability of the proposed method. Experiments are designed to verify the effectiveness of the whole framework, and an average of 98.21% and 91.52% improvement of RMSE is obtained relative to the traditional admittance control as well as the model-free adaptive control, respectively.","sentences":["Force interaction is inevitable when robots face multiple operation scenarios.","How to make the robot competent in force control for generalized operations such as multi-tasks still remains a challenging problem.","Aiming at the reproducibility of interaction tasks and the lack of a generalized force control framework for multi-task scenarios, this paper proposes a novel hybrid control framework based on active admittance control with iterative learning parameters-tunning mechanism.","The method adopts admittance control as the underlying algorithm to ensure flexibility, and iterative learning as the high-level algorithm to regulate the parameters of the admittance model.","The whole algorithm has flexibility and learning ability, which is capable of achieving the goal of excellent versatility.","Four representative interactive robot manipulation tasks are chosen to investigate the consistency and generalisability of the proposed method.","Experiments are designed to verify the effectiveness of the whole framework, and an average of 98.21% and 91.52% improvement of RMSE is obtained relative to the traditional admittance control as well as the model-free adaptive control, respectively."],"url":"http://arxiv.org/abs/2403.16560v1"}
{"created":"2024-03-25 09:17:15","title":"Elysium: Exploring Object-level Perception in Videos via MLLM","abstract":"Multi-modal Large Language Models (MLLMs) have demonstrated their ability to perceive objects in still images, but their application in video-related tasks, such as object tracking, remains understudied. This lack of exploration is primarily due to two key challenges. Firstly, extensive pretraining on large-scale video datasets is required to equip MLLMs with the capability to perceive objects across multiple frames and understand inter-frame relationships. Secondly, processing a large number of frames within the context window of Large Language Models (LLMs) can impose a significant computational burden. To address the first challenge, we introduce ElysiumTrack-1M, a large-scale video dataset paired with novel tasks: Referring Single Object Tracking (RSOT) and Video Referring Expression Generation (Video-REG). ElysiumTrack-1M contains 1.27 million annotated video frames with corresponding object boxes and descriptions. Leveraging this dataset, we conduct training of MLLMs and propose a token-compression model T-Selector to tackle the second challenge. Our proposed approach, Elysium: Exploring Object-level Perception in Videos via MLLM, is an end-to-end trainable MLLM that makes the first attempt to conduct object-level tasks in videos without requiring any additional plug-in or expert models.","sentences":["Multi-modal Large Language Models (MLLMs) have demonstrated their ability to perceive objects in still images, but their application in video-related tasks, such as object tracking, remains understudied.","This lack of exploration is primarily due to two key challenges.","Firstly, extensive pretraining on large-scale video datasets is required to equip MLLMs with the capability to perceive objects across multiple frames and understand inter-frame relationships.","Secondly, processing a large number of frames within the context window of Large Language Models (LLMs) can impose a significant computational burden.","To address the first challenge, we introduce ElysiumTrack-1M, a large-scale video dataset paired with novel tasks: Referring Single Object Tracking (RSOT) and Video Referring Expression Generation (Video-REG).","ElysiumTrack-1M contains 1.27 million annotated video frames with corresponding object boxes and descriptions.","Leveraging this dataset, we conduct training of MLLMs and propose a token-compression model T-Selector to tackle the second challenge.","Our proposed approach, Elysium: Exploring Object-level Perception in Videos via MLLM, is an end-to-end trainable MLLM that makes the first attempt to conduct object-level tasks in videos without requiring any additional plug-in or expert models."],"url":"http://arxiv.org/abs/2403.16558v1"}
{"created":"2024-03-25 09:16:59","title":"Accelerating Federated Learning by Selecting Beneficial Herd of Local Gradients","abstract":"Federated Learning (FL) is a distributed machine learning framework in communication network systems. However, the systems' Non-Independent and Identically Distributed (Non-IID) data negatively affect the convergence efficiency of the global model, since only a subset of these data samples are beneficial for model convergence. In pursuit of this subset, a reliable approach involves determining a measure of validity to rank the samples within the dataset. In this paper, We propose the BHerd strategy which selects a beneficial herd of local gradients to accelerate the convergence of the FL model. Specifically, we map the distribution of the local dataset to the local gradients and use the Herding strategy to obtain a permutation of the set of gradients, where the more advanced gradients in the permutation are closer to the average of the set of gradients. These top portion of the gradients will be selected and sent to the server for global aggregation. We conduct experiments on different datasets, models and scenarios by building a prototype system, and experimental results demonstrate that our BHerd strategy is effective in selecting beneficial local gradients to mitigate the effects brought by the Non-IID dataset, thus accelerating model convergence.","sentences":["Federated Learning (FL) is a distributed machine learning framework in communication network systems.","However, the systems' Non-Independent and Identically Distributed (Non-IID) data negatively affect the convergence efficiency of the global model, since only a subset of these data samples are beneficial for model convergence.","In pursuit of this subset, a reliable approach involves determining a measure of validity to rank the samples within the dataset.","In this paper, We propose the BHerd strategy which selects a beneficial herd of local gradients to accelerate the convergence of the FL model.","Specifically, we map the distribution of the local dataset to the local gradients and use the Herding strategy to obtain a permutation of the set of gradients, where the more advanced gradients in the permutation are closer to the average of the set of gradients.","These top portion of the gradients will be selected and sent to the server for global aggregation.","We conduct experiments on different datasets, models and scenarios by building a prototype system, and experimental results demonstrate that our BHerd strategy is effective in selecting beneficial local gradients to mitigate the effects brought by the Non-IID dataset, thus accelerating model convergence."],"url":"http://arxiv.org/abs/2403.16557v1"}
{"created":"2024-03-25 09:04:14","title":"PE: A Poincare Explanation Method for Fast Text Hierarchy Generation","abstract":"The black-box nature of deep learning models in NLP hinders their widespread application. The research focus has shifted to Hierarchical Attribution (HA) for its ability to model feature interactions. Recent works model non-contiguous combinations with a time-costly greedy search in Eculidean spaces, neglecting underlying linguistic information in feature representations. In this work, we introduce a novel method, namely Poincar\\'e Explanation (PE), for modeling feature interactions using hyperbolic spaces in an $O(n^2logn)$ time complexity. Inspired by Poincar\\'e model, we propose a framework to project the embeddings into hyperbolic spaces, which exhibit better inductive biases for syntax and semantic hierarchical structures. Eventually, we prove that the hierarchical clustering process in the projected space could be viewed as building a minimum spanning tree and propose a time efficient algorithm. Experimental results demonstrate the effectiveness of our approach.","sentences":["The black-box nature of deep learning models in NLP hinders their widespread application.","The research focus has shifted to Hierarchical Attribution (HA) for its ability to model feature interactions.","Recent works model non-contiguous combinations with a time-costly greedy search in Eculidean spaces, neglecting underlying linguistic information in feature representations.","In this work, we introduce a novel method, namely Poincar\\'e Explanation (PE), for modeling feature interactions using hyperbolic spaces in an $O(n^2logn)$ time complexity.","Inspired by Poincar\\'e model, we propose a framework to project the embeddings into hyperbolic spaces, which exhibit better inductive biases for syntax and semantic hierarchical structures.","Eventually, we prove that the hierarchical clustering process in the projected space could be viewed as building a minimum spanning tree and propose a time efficient algorithm.","Experimental results demonstrate the effectiveness of our approach."],"url":"http://arxiv.org/abs/2403.16554v1"}
{"created":"2024-03-25 08:57:27","title":"QKFormer: Hierarchical Spiking Transformer using Q-K Attention","abstract":"Spiking Transformers, which integrate Spiking Neural Networks (SNNs) with Transformer architectures, have attracted significant attention due to their potential for energy efficiency and high performance. However, existing models in this domain still suffer from suboptimal performance. We introduce several innovations to improve the performance: i) We propose a novel spike-form Q-K attention mechanism, tailored for SNNs, which efficiently models the importance of token or channel dimensions through binary vectors with linear complexity. ii) We incorporate the hierarchical structure, which significantly benefits the performance of both the brain and artificial neural networks, into spiking transformers to obtain multi-scale spiking representation. iii) We design a versatile and powerful patch embedding module with a deformed shortcut specifically for spiking transformers. Together, we develop QKFormer, a hierarchical spiking transformer based on Q-K attention with direct training. QKFormer shows significantly superior performance over existing state-of-the-art SNN models on various mainstream datasets. Notably, with comparable size to Spikformer (66.34 M, 74.81%), QKFormer (64.96 M) achieves a groundbreaking top-1 accuracy of 85.65% on ImageNet-1k, substantially outperforming Spikformer by 10.84%. To our best knowledge, this is the first time that directly training SNNs have exceeded 85% accuracy on ImageNet-1K. The code and models are publicly available at https://github.com/zhouchenlin2096/QKFormer","sentences":["Spiking Transformers, which integrate Spiking Neural Networks (SNNs) with Transformer architectures, have attracted significant attention due to their potential for energy efficiency and high performance.","However, existing models in this domain still suffer from suboptimal performance.","We introduce several innovations to improve the performance: i) We propose a novel spike-form Q-K attention mechanism, tailored for SNNs, which efficiently models the importance of token or channel dimensions through binary vectors with linear complexity.","ii)","We incorporate the hierarchical structure, which significantly benefits the performance of both the brain and artificial neural networks, into spiking transformers to obtain multi-scale spiking representation.","iii)","We design a versatile and powerful patch embedding module with a deformed shortcut specifically for spiking transformers.","Together, we develop QKFormer, a hierarchical spiking transformer based on Q-K attention with direct training.","QKFormer shows significantly superior performance over existing state-of-the-art SNN models on various mainstream datasets.","Notably, with comparable size to Spikformer (66.34 M, 74.81%), QKFormer (64.96 M) achieves a groundbreaking top-1 accuracy of 85.65% on ImageNet-1k, substantially outperforming Spikformer by 10.84%.","To our best knowledge, this is the first time that directly training SNNs have exceeded 85% accuracy on ImageNet-1K. The code and models are publicly available at https://github.com/zhouchenlin2096/QKFormer"],"url":"http://arxiv.org/abs/2403.16552v1"}
{"created":"2024-03-25 08:40:43","title":"Exposing the hidden layers and interplay in the quantum software stack","abstract":"Current and near-future quantum computers face resource limitations due to noise and low qubit counts. Despite this, effective quantum advantage can still be achieved due to the exponential nature of bit-to-qubit conversion. However, optimizing the software architecture of these systems is essential to utilize available resources efficiently. Unfortunately, the focus on user-friendly quantum computers has obscured critical steps in the software stack, leading to ripple effects into the stack's upper layer induced by limitations in current qubit implementations. This paper unveils the hidden interplay among layers of the quantum software stack.","sentences":["Current and near-future quantum computers face resource limitations due to noise and low qubit counts.","Despite this, effective quantum advantage can still be achieved due to the exponential nature of bit-to-qubit conversion.","However, optimizing the software architecture of these systems is essential to utilize available resources efficiently.","Unfortunately, the focus on user-friendly quantum computers has obscured critical steps in the software stack, leading to ripple effects into the stack's upper layer induced by limitations in current qubit implementations.","This paper unveils the hidden interplay among layers of the quantum software stack."],"url":"http://arxiv.org/abs/2403.16545v1"}
{"created":"2024-03-25 08:36:06","title":"Efficient Information Extraction in Few-Shot Relation Classification through Contrastive Representation Learning","abstract":"Differentiating relationships between entity pairs with limited labeled instances poses a significant challenge in few-shot relation classification. Representations of textual data extract rich information spanning the domain, entities, and relations. In this paper, we introduce a novel approach to enhance information extraction combining multiple sentence representations and contrastive learning. While representations in relation classification are commonly extracted using entity marker tokens, we argue that substantial information within the internal model representations remains untapped. To address this, we propose aligning multiple sentence representations, such as the [CLS] token, the [MASK] token used in prompting, and entity marker tokens. Our method employs contrastive learning to extract complementary discriminative information from these individual representations. This is particularly relevant in low-resource settings where information is scarce. Leveraging multiple sentence representations is especially effective in distilling discriminative information for relation classification when additional information, like relation descriptions, are not available. We validate the adaptability of our approach, maintaining robust performance in scenarios that include relation descriptions, and showcasing its flexibility to adapt to different resource constraints.","sentences":["Differentiating relationships between entity pairs with limited labeled instances poses a significant challenge in few-shot relation classification.","Representations of textual data extract rich information spanning the domain, entities, and relations.","In this paper, we introduce a novel approach to enhance information extraction combining multiple sentence representations and contrastive learning.","While representations in relation classification are commonly extracted using entity marker tokens, we argue that substantial information within the internal model representations remains untapped.","To address this, we propose aligning multiple sentence representations, such as the [CLS] token, the [MASK] token used in prompting, and entity marker tokens.","Our method employs contrastive learning to extract complementary discriminative information from these individual representations.","This is particularly relevant in low-resource settings where information is scarce.","Leveraging multiple sentence representations is especially effective in distilling discriminative information for relation classification when additional information, like relation descriptions, are not available.","We validate the adaptability of our approach, maintaining robust performance in scenarios that include relation descriptions, and showcasing its flexibility to adapt to different resource constraints."],"url":"http://arxiv.org/abs/2403.16543v1"}
{"created":"2024-03-25 08:35:19","title":"Differentially Private Online Federated Learning with Correlated Noise","abstract":"We propose a novel differentially private algorithm for online federated learning that employs temporally correlated noise to improve the utility while ensuring the privacy of the continuously released models. To address challenges stemming from DP noise and local updates with streaming noniid data, we develop a perturbed iterate analysis to control the impact of the DP noise on the utility. Moreover, we demonstrate how the drift errors from local updates can be effectively managed under a quasi-strong convexity condition. Subject to an $(\\epsilon, \\delta)$-DP budget, we establish a dynamic regret bound over the entire time horizon that quantifies the impact of key parameters and the intensity of changes in dynamic environments. Numerical experiments validate the efficacy of the proposed algorithm.","sentences":["We propose a novel differentially private algorithm for online federated learning that employs temporally correlated noise to improve the utility while ensuring the privacy of the continuously released models.","To address challenges stemming from DP noise and local updates with streaming noniid data, we develop a perturbed iterate analysis to control the impact of the DP noise on the utility.","Moreover, we demonstrate how the drift errors from local updates can be effectively managed under a quasi-strong convexity condition.","Subject to an $(\\epsilon, \\delta)$-DP budget, we establish a dynamic regret bound over the entire time horizon that quantifies the impact of key parameters and the intensity of changes in dynamic environments.","Numerical experiments validate the efficacy of the proposed algorithm."],"url":"http://arxiv.org/abs/2403.16542v1"}
