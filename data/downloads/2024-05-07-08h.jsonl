{"created":"2024-05-06 17:59:45","title":"Complex Video Reasoning and Robustness Evaluation Suite for Video-LMMs","abstract":"Recent advancements in Large Language Models (LLMs) have led to the development of Video Large Multi-modal Models (Video-LMMs) that can handle a wide range of video understanding tasks. These models have the potential to be deployed in real-world applications such as robotics, AI assistants, medical imaging, and autonomous vehicles. The widespread adoption of Video-LMMs in our daily lives underscores the importance of ensuring and evaluating their robust performance in mirroring human-like reasoning and interaction capabilities in complex, real-world contexts. However, existing benchmarks for Video-LMMs primarily focus on general video comprehension abilities and neglect assessing their reasoning capabilities over complex videos in the real-world context, and robustness of these models through the lens of user prompts as text queries. In this paper, we present the Complex Video Reasoning and Robustness Evaluation Suite (CVRR-ES), a novel benchmark that comprehensively assesses the performance of Video-LMMs across 11 diverse real-world video dimensions. We evaluate 9 recent models, including both open-source and closed-source variants, and find that most of the Video-LMMs, {especially open-source ones,} struggle with robustness and reasoning when dealing with complex videos. Based on our analysis, we develop a training-free Dual-Step Contextual Prompting (DSCP) technique to enhance the performance of existing Video-LMMs. Our findings provide valuable insights for building the next generation of human-centric AI systems with advanced robustness and reasoning capabilities. Our dataset and code are publicly available at: https://mbzuai-oryx.github.io/CVRR-Evaluation-Suite/.","sentences":["Recent advancements in Large Language Models (LLMs) have led to the development of Video Large Multi-modal Models (Video-LMMs) that can handle a wide range of video understanding tasks.","These models have the potential to be deployed in real-world applications such as robotics, AI assistants, medical imaging, and autonomous vehicles.","The widespread adoption of Video-LMMs in our daily lives underscores the importance of ensuring and evaluating their robust performance in mirroring human-like reasoning and interaction capabilities in complex, real-world contexts.","However, existing benchmarks for Video-LMMs primarily focus on general video comprehension abilities and neglect assessing their reasoning capabilities over complex videos in the real-world context, and robustness of these models through the lens of user prompts as text queries.","In this paper, we present the Complex Video Reasoning and Robustness Evaluation Suite (CVRR-ES), a novel benchmark that comprehensively assesses the performance of Video-LMMs across 11 diverse real-world video dimensions.","We evaluate 9 recent models, including both open-source and closed-source variants, and find that most of the Video-LMMs, {especially open-source ones,} struggle with robustness and reasoning when dealing with complex videos.","Based on our analysis, we develop a training-free Dual-Step Contextual Prompting (DSCP) technique to enhance the performance of existing Video-LMMs.","Our findings provide valuable insights for building the next generation of human-centric AI systems with advanced robustness and reasoning capabilities.","Our dataset and code are publicly available at: https://mbzuai-oryx.github.io/CVRR-Evaluation-Suite/."],"url":"http://arxiv.org/abs/2405.03690v1"}
{"created":"2024-05-06 17:59:36","title":"Pose Priors from Language Models","abstract":"We present a zero-shot pose optimization method that enforces accurate physical contact constraints when estimating the 3D pose of humans. Our central insight is that since language is often used to describe physical interaction, large pretrained text-based models can act as priors on pose estimation.   We can thus leverage this insight to improve pose estimation by converting natural language descriptors, generated by a large multimodal model (LMM), into tractable losses to constrain the 3D pose optimization. Despite its simplicity, our method produces surprisingly compelling pose reconstructions of people in close contact, correctly capturing the semantics of the social and physical interactions. We demonstrate that our method rivals more complex state-of-the-art approaches that require expensive human annotation of contact points and training specialized models. Moreover, unlike previous approaches, our method provides a unified framework for resolving self-contact and person-to-person contact.","sentences":["We present a zero-shot pose optimization method that enforces accurate physical contact constraints when estimating the 3D pose of humans.","Our central insight is that since language is often used to describe physical interaction, large pretrained text-based models can act as priors on pose estimation.   ","We can thus leverage this insight to improve pose estimation by converting natural language descriptors, generated by a large multimodal model (LMM), into tractable losses to constrain the 3D pose optimization.","Despite its simplicity, our method produces surprisingly compelling pose reconstructions of people in close contact, correctly capturing the semantics of the social and physical interactions.","We demonstrate that our method rivals more complex state-of-the-art approaches that require expensive human annotation of contact points and training specialized models.","Moreover, unlike previous approaches, our method provides a unified framework for resolving self-contact and person-to-person contact."],"url":"http://arxiv.org/abs/2405.03689v1"}
{"created":"2024-05-06 17:59:07","title":"Large Language Models Reveal Information Operation Goals, Tactics, and Narrative Frames","abstract":"Adversarial information operations can destabilize societies by undermining fair elections, manipulating public opinions on policies, and promoting scams. Despite their widespread occurrence and potential impacts, our understanding of influence campaigns is limited by manual analysis of messages and subjective interpretation of their observable behavior. In this paper, we explore whether these limitations can be mitigated with large language models (LLMs), using GPT-3.5 as a case-study for coordinated campaign annotation. We first use GPT-3.5 to scrutinize 126 identified information operations spanning over a decade. We utilize a number of metrics to quantify the close (if imperfect) agreement between LLM and ground truth descriptions. We next extract coordinated campaigns from two large multilingual datasets from X (formerly Twitter) that respectively discuss the 2022 French election and 2023 Balikaran Philippine-U.S. military exercise in 2023. For each coordinated campaign, we use GPT-3.5 to analyze posts related to a specific concern and extract goals, tactics, and narrative frames, both before and after critical events (such as the date of an election). While the GPT-3.5 sometimes disagrees with subjective interpretation, its ability to summarize and interpret demonstrates LLMs' potential to extract higher-order indicators from text to provide a more complete picture of the information campaigns compared to previous methods.","sentences":["Adversarial information operations can destabilize societies by undermining fair elections, manipulating public opinions on policies, and promoting scams.","Despite their widespread occurrence and potential impacts, our understanding of influence campaigns is limited by manual analysis of messages and subjective interpretation of their observable behavior.","In this paper, we explore whether these limitations can be mitigated with large language models (LLMs), using GPT-3.5 as a case-study for coordinated campaign annotation.","We first use GPT-3.5 to scrutinize 126 identified information operations spanning over a decade.","We utilize a number of metrics to quantify the close (if imperfect) agreement between LLM and ground truth descriptions.","We next extract coordinated campaigns from two large multilingual datasets from X (formerly Twitter) that respectively discuss the 2022 French election and 2023 Balikaran Philippine-U.S. military exercise in 2023.","For each coordinated campaign, we use GPT-3.5 to analyze posts related to a specific concern and extract goals, tactics, and narrative frames, both before and after critical events (such as the date of an election).","While the GPT-3.5 sometimes disagrees with subjective interpretation, its ability to summarize and interpret demonstrates LLMs' potential to extract higher-order indicators from text to provide a more complete picture of the information campaigns compared to previous methods."],"url":"http://arxiv.org/abs/2405.03688v1"}
{"created":"2024-05-06 17:59:02","title":"Monotone Randomized Apportionment","abstract":"Apportionment is the act of distributing the seats of a legislature among political parties (or states) in proportion to their vote shares (or populations). A famous impossibility by Balinski and Young (2001) shows that no apportionment method can be proportional up to one seat (quota) while also responding monotonically to changes in the votes (population monotonicity). Grimmett (2004) proposed to overcome this impossibility by randomizing the apportionment, which can achieve quota as well as perfect proportionality and monotonicity -- at least in terms of the expected number of seats awarded to each party. Still, the correlations between the seats awarded to different parties may exhibit bizarre non-monotonicities. When parties or voters care about joint events, such as whether a coalition of parties reaches a majority, these non-monotonicities can cause paradoxes, including incentives for strategic voting.   In this paper, we propose monotonicity axioms ruling out these paradoxes, and study which of them can be satisfied jointly with Grimmett's axioms. Essentially, we require that, if a set of parties all receive more votes, the probability of those parties jointly receiving more seats should increase. Our work draws on a rich literature on unequal probability sampling in statistics (studied as dependent randomized rounding in computer science). Our main result shows that a sampling scheme due to Sampford (1967) satisfies Grimmett's axioms and a notion of higher-order correlation monotonicity.","sentences":["Apportionment is the act of distributing the seats of a legislature among political parties (or states) in proportion to their vote shares (or populations).","A famous impossibility by Balinski and Young (2001) shows that no apportionment method can be proportional up to one seat (quota) while also responding monotonically to changes in the votes (population monotonicity).","Grimmett (2004) proposed to overcome this impossibility by randomizing the apportionment, which can achieve quota as well as perfect proportionality and monotonicity -- at least in terms of the expected number of seats awarded to each party.","Still, the correlations between the seats awarded to different parties may exhibit bizarre non-monotonicities.","When parties or voters care about joint events, such as whether a coalition of parties reaches a majority, these non-monotonicities can cause paradoxes, including incentives for strategic voting.   ","In this paper, we propose monotonicity axioms ruling out these paradoxes, and study which of them can be satisfied jointly with Grimmett's axioms.","Essentially, we require that, if a set of parties all receive more votes, the probability of those parties jointly receiving more seats should increase.","Our work draws on a rich literature on unequal probability sampling in statistics (studied as dependent randomized rounding in computer science).","Our main result shows that a sampling scheme due to Sampford (1967) satisfies Grimmett's axioms and a notion of higher-order correlation monotonicity."],"url":"http://arxiv.org/abs/2405.03687v1"}
{"created":"2024-05-06 17:57:27","title":"Language-Image Models with 3D Understanding","abstract":"Multi-modal large language models (MLLMs) have shown incredible capabilities in a variety of 2D vision and language tasks. We extend MLLMs' perceptual capabilities to ground and reason about images in 3-dimensional space. To that end, we first develop a large-scale pre-training dataset for 2D and 3D called LV3D by combining multiple existing 2D and 3D recognition datasets under a common task formulation: as multi-turn question-answering. Next, we introduce a new MLLM named Cube-LLM and pre-train it on LV3D. We show that pure data scaling makes a strong 3D perception capability without 3D specific architectural design or training objective. Cube-LLM exhibits intriguing properties similar to LLMs: (1) Cube-LLM can apply chain-of-thought prompting to improve 3D understanding from 2D context information. (2) Cube-LLM can follow complex and diverse instructions and adapt to versatile input and output formats. (3) Cube-LLM can be visually prompted such as 2D box or a set of candidate 3D boxes from specialists. Our experiments on outdoor benchmarks demonstrate that Cube-LLM significantly outperforms existing baselines by 21.3 points of AP-BEV on the Talk2Car dataset for 3D grounded reasoning and 17.7 points on the DriveLM dataset for complex reasoning about driving scenarios, respectively. Cube-LLM also shows competitive results in general MLLM benchmarks such as refCOCO for 2D grounding with (87.0) average score, as well as visual question answering benchmarks such as VQAv2, GQA, SQA, POPE, etc. for complex reasoning. Our project is available at https://janghyuncho.github.io/Cube-LLM.","sentences":["Multi-modal large language models (MLLMs) have shown incredible capabilities in a variety of 2D vision and language tasks.","We extend MLLMs' perceptual capabilities to ground and reason about images in 3-dimensional space.","To that end, we first develop a large-scale pre-training dataset for 2D and 3D called LV3D by combining multiple existing 2D and 3D recognition datasets under a common task formulation: as multi-turn question-answering.","Next, we introduce a new MLLM named Cube-LLM and pre-train it on LV3D.","We show that pure data scaling makes a strong 3D perception capability without 3D specific architectural design or training objective.","Cube-LLM exhibits intriguing properties similar to LLMs: (1) Cube-LLM can apply chain-of-thought prompting to improve 3D understanding from 2D context information.","(2) Cube-LLM can follow complex and diverse instructions and adapt to versatile input and output formats.","(3) Cube-LLM can be visually prompted such as 2D box or a set of candidate 3D boxes from specialists.","Our experiments on outdoor benchmarks demonstrate that Cube-LLM significantly outperforms existing baselines by 21.3 points of AP-BEV on the Talk2Car dataset for 3D grounded reasoning and 17.7 points on the DriveLM dataset for complex reasoning about driving scenarios, respectively.","Cube-LLM also shows competitive results in general MLLM benchmarks such as refCOCO for 2D grounding with (87.0) average score, as well as visual question answering benchmarks such as VQAv2, GQA, SQA, POPE, etc. for complex reasoning.","Our project is available at https://janghyuncho.github.io/Cube-LLM."],"url":"http://arxiv.org/abs/2405.03685v1"}
{"created":"2024-05-06 17:57:03","title":"An Empty Room is All We Want: Automatic Defurnishing of Indoor Panoramas","abstract":"We propose a pipeline that leverages Stable Diffusion to improve inpainting results in the context of defurnishing -- the removal of furniture items from indoor panorama images. Specifically, we illustrate how increased context, domain-specific model fine-tuning, and improved image blending can produce high-fidelity inpaints that are geometrically plausible without needing to rely on room layout estimation. We demonstrate qualitative and quantitative improvements over other furniture removal techniques.","sentences":["We propose a pipeline that leverages Stable Diffusion to improve inpainting results in the context of defurnishing -- the removal of furniture items from indoor panorama images.","Specifically, we illustrate how increased context, domain-specific model fine-tuning, and improved image blending can produce high-fidelity inpaints that are geometrically plausible without needing to rely on room layout estimation.","We demonstrate qualitative and quantitative improvements over other furniture removal techniques."],"url":"http://arxiv.org/abs/2405.03682v1"}
{"created":"2024-05-06 17:53:33","title":"Towards A Human-in-the-Loop LLM Approach to Collaborative Discourse Analysis","abstract":"LLMs have demonstrated proficiency in contextualizing their outputs using human input, often matching or beating human-level performance on a variety of tasks. However, LLMs have not yet been used to characterize synergistic learning in students' collaborative discourse. In this exploratory work, we take a first step towards adopting a human-in-the-loop prompt engineering approach with GPT-4-Turbo to summarize and categorize students' synergistic learning during collaborative discourse. Our preliminary findings suggest GPT-4-Turbo may be able to characterize students' synergistic learning in a manner comparable to humans and that our approach warrants further investigation.","sentences":["LLMs have demonstrated proficiency in contextualizing their outputs using human input, often matching or beating human-level performance on a variety of tasks.","However, LLMs have not yet been used to characterize synergistic learning in students' collaborative discourse.","In this exploratory work, we take a first step towards adopting a human-in-the-loop prompt engineering approach with GPT-4-Turbo to summarize and categorize students' synergistic learning during collaborative discourse.","Our preliminary findings suggest GPT-4-Turbo may be able to characterize students' synergistic learning in a manner comparable to humans and that our approach warrants further investigation."],"url":"http://arxiv.org/abs/2405.03677v1"}
{"created":"2024-05-06 17:52:04","title":"Why is SAM Robust to Label Noise?","abstract":"Sharpness-Aware Minimization (SAM) is most known for achieving state-of the-art performances on natural image and language tasks. However, its most pronounced improvements (of tens of percent) is rather in the presence of label noise. Understanding SAM's label noise robustness requires a departure from characterizing the robustness of minimas lying in \"flatter\" regions of the loss landscape. In particular, the peak performance under label noise occurs with early stopping, far before the loss converges. We decompose SAM's robustness into two effects: one induced by changes to the logit term and the other induced by changes to the network Jacobian. The first can be observed in linear logistic regression where SAM provably up-weights the gradient contribution from clean examples. Although this explicit up-weighting is also observable in neural networks, when we intervene and modify SAM to remove this effect, surprisingly, we see no visible degradation in performance. We infer that SAM's effect in deeper networks is instead explained entirely by the effect SAM has on the network Jacobian. We theoretically derive the implicit regularization induced by this Jacobian effect in two layer linear networks. Motivated by our analysis, we see that cheaper alternatives to SAM that explicitly induce these regularization effects largely recover the benefits in deep networks trained on real-world datasets.","sentences":["Sharpness-Aware Minimization (SAM) is most known for achieving state-of the-art performances on natural image and language tasks.","However, its most pronounced improvements (of tens of percent) is rather in the presence of label noise.","Understanding SAM's label noise robustness requires a departure from characterizing the robustness of minimas lying in \"flatter\" regions of the loss landscape.","In particular, the peak performance under label noise occurs with early stopping, far before the loss converges.","We decompose SAM's robustness into two effects: one induced by changes to the logit term and the other induced by changes to the network Jacobian.","The first can be observed in linear logistic regression where SAM provably up-weights the gradient contribution from clean examples.","Although this explicit up-weighting is also observable in neural networks, when we intervene and modify SAM to remove this effect, surprisingly, we see no visible degradation in performance.","We infer that SAM's effect in deeper networks is instead explained entirely by the effect SAM has on the network Jacobian.","We theoretically derive the implicit regularization induced by this Jacobian effect in two layer linear networks.","Motivated by our analysis, we see that cheaper alternatives to SAM that explicitly induce these regularization effects largely recover the benefits in deep networks trained on real-world datasets."],"url":"http://arxiv.org/abs/2405.03676v1"}
{"created":"2024-05-06 17:49:32","title":"Anti-Heroes: An Ethics-focused Method for Responsible Designer Intentions","abstract":"HCI and design researchers have designed, adopted, and customized a range of ethics-focused methods to inscribe values and support ethical decision making in a design process. In this work-in-progress, we add to this body of resources, constructing a method that surfaces the designer's intentions in an action-focused way, encouraging consideration of both manipulative and value-centered roles. Anti-Heroes is a card deck that allows a designer to playfully take on pairs of manipulative (Anti-Hero) and value-centered (Hero) roles during design ideation/conceptualization, evaluation, and ethical dialogue. The card deck includes twelve cards with Anti-Hero and Hero faces, along with three action cards that include reflective questions for different play modes. Alongside the creation of the Anti-Hero card deck, we describe the evaluation and iteration of the card deck through playtesting sessions with four groups of three design students. We propose implications of Anti-Heros for technology and design education and practice.","sentences":["HCI and design researchers have designed, adopted, and customized a range of ethics-focused methods to inscribe values and support ethical decision making in a design process.","In this work-in-progress, we add to this body of resources, constructing a method that surfaces the designer's intentions in an action-focused way, encouraging consideration of both manipulative and value-centered roles.","Anti-Heroes is a card deck that allows a designer to playfully take on pairs of manipulative (Anti-Hero) and value-centered (Hero) roles during design ideation/conceptualization, evaluation, and ethical dialogue.","The card deck includes twelve cards with Anti-Hero and Hero faces, along with three action cards that include reflective questions for different play modes.","Alongside the creation of the Anti-Hero card deck, we describe the evaluation and iteration of the card deck through playtesting sessions with four groups of three design students.","We propose implications of Anti-Heros for technology and design education and practice."],"url":"http://arxiv.org/abs/2405.03674v1"}
{"created":"2024-05-06 17:49:31","title":"MemoryMamba: Memory-Augmented State Space Model for Defect Recognition","abstract":"As automation advances in manufacturing, the demand for precise and sophisticated defect detection technologies grows. Existing vision models for defect recognition methods are insufficient for handling the complexities and variations of defects in contemporary manufacturing settings. These models especially struggle in scenarios involving limited or imbalanced defect data. In this work, we introduce MemoryMamba, a novel memory-augmented state space model (SSM), designed to overcome the limitations of existing defect recognition models. MemoryMamba integrates the state space model with the memory augmentation mechanism, enabling the system to maintain and retrieve essential defect-specific information in training. Its architecture is designed to capture dependencies and intricate defect characteristics, which are crucial for effective defect detection. In the experiments, MemoryMamba was evaluated across four industrial datasets with diverse defect types and complexities. The model consistently outperformed other methods, demonstrating its capability to adapt to various defect recognition scenarios.","sentences":["As automation advances in manufacturing, the demand for precise and sophisticated defect detection technologies grows.","Existing vision models for defect recognition methods are insufficient for handling the complexities and variations of defects in contemporary manufacturing settings.","These models especially struggle in scenarios involving limited or imbalanced defect data.","In this work, we introduce MemoryMamba, a novel memory-augmented state space model (SSM), designed to overcome the limitations of existing defect recognition models.","MemoryMamba integrates the state space model with the memory augmentation mechanism, enabling the system to maintain and retrieve essential defect-specific information in training.","Its architecture is designed to capture dependencies and intricate defect characteristics, which are crucial for effective defect detection.","In the experiments, MemoryMamba was evaluated across four industrial datasets with diverse defect types and complexities.","The model consistently outperformed other methods, demonstrating its capability to adapt to various defect recognition scenarios."],"url":"http://arxiv.org/abs/2405.03673v1"}
{"created":"2024-05-06 17:48:24","title":"Cutting through buggy adversarial example defenses: fixing 1 line of code breaks Sabre","abstract":"Sabre is a defense to adversarial examples that was accepted at IEEE S&P 2024. We first reveal significant flaws in the evaluation that point to clear signs of gradient masking. We then show the cause of this gradient masking: a bug in the original evaluation code. By fixing a single line of code in the original repository, we reduce Sabre's robust accuracy to 0%. In response to this, the authors modify the defense and introduce a new defense component not described in the original paper. But this fix contains a second bug; modifying one more line of code reduces robust accuracy to below baseline levels.","sentences":["Sabre is a defense to adversarial examples that was accepted at IEEE S&P 2024.","We first reveal significant flaws in the evaluation that point to clear signs of gradient masking.","We then show the cause of this gradient masking: a bug in the original evaluation code.","By fixing a single line of code in the original repository, we reduce Sabre's robust accuracy to 0%.","In response to this, the authors modify the defense and introduce a new defense component not described in the original paper.","But this fix contains a second bug; modifying one more line of code reduces robust accuracy to below baseline levels."],"url":"http://arxiv.org/abs/2405.03672v1"}
{"created":"2024-05-06 17:48:10","title":"Prompting Task Trees using Gemini: Methodologies and Insights","abstract":"Robots are the future of every technology where every advanced technology eventually will be used to make robots which are more efficient. The major challenge today is to train the robots exactly and empathetically using knowledge representation. This paper gives you insights of how we can use unstructured knowledge representation and convert them to meaningful structured representation with the help of prompt engineering which can be eventually used in the robots to make help them understand how human brain can make wonders with the minimal data or objects can providing to them.","sentences":["Robots are the future of every technology where every advanced technology eventually will be used to make robots which are more efficient.","The major challenge today is to train the robots exactly and empathetically using knowledge representation.","This paper gives you insights of how we can use unstructured knowledge representation and convert them to meaningful structured representation with the help of prompt engineering which can be eventually used in the robots to make help them understand how human brain can make wonders with the minimal data or objects can providing to them."],"url":"http://arxiv.org/abs/2405.03671v1"}
{"created":"2024-05-06 17:45:56","title":"IMELL Cut Elimination with Linear Overhead","abstract":"Recently, Accattoli introduced the Exponential Substitution Calculus (ESC) given by untyped proof terms for Intuitionistic Multiplicative Exponential Linear Logic (IMELL), endowed with rewriting rules at-a-distance for cut elimination. He also introduced a new cut elimination strategy, dubbed the good strategy, and showed that its number of steps is a time cost model with polynomial overhead for the ESC/IMELL, and the first such one.   Here, we refine Accattoli's result by introducing an abstract machine for ESC and proving that it implements the good strategy and computes cut-free terms/proofs within a linear overhead.","sentences":["Recently, Accattoli introduced the Exponential Substitution Calculus (ESC) given by untyped proof terms for Intuitionistic Multiplicative Exponential Linear Logic (IMELL), endowed with rewriting rules at-a-distance for cut elimination.","He also introduced a new cut elimination strategy, dubbed the good strategy, and showed that its number of steps is a time cost model with polynomial overhead for the ESC/IMELL, and the first such one.   ","Here, we refine Accattoli's result by introducing an abstract machine for ESC and proving that it implements the good strategy and computes cut-free terms/proofs within a linear overhead."],"url":"http://arxiv.org/abs/2405.03669v1"}
{"created":"2024-05-06 17:43:34","title":"ScrewMimic: Bimanual Imitation from Human Videos with Screw Space Projection","abstract":"Bimanual manipulation is a longstanding challenge in robotics due to the large number of degrees of freedom and the strict spatial and temporal synchronization required to generate meaningful behavior. Humans learn bimanual manipulation skills by watching other humans and by refining their abilities through play. In this work, we aim to enable robots to learn bimanual manipulation behaviors from human video demonstrations and fine-tune them through interaction. Inspired by seminal work in psychology and biomechanics, we propose modeling the interaction between two hands as a serial kinematic linkage -- as a screw motion, in particular, that we use to define a new action space for bimanual manipulation: screw actions. We introduce ScrewMimic, a framework that leverages this novel action representation to facilitate learning from human demonstration and self-supervised policy fine-tuning. Our experiments demonstrate that ScrewMimic is able to learn several complex bimanual behaviors from a single human video demonstration, and that it outperforms baselines that interpret demonstrations and fine-tune directly in the original space of motion of both arms. For more information and video results, https://robin-lab.cs.utexas.edu/ScrewMimic/","sentences":["Bimanual manipulation is a longstanding challenge in robotics due to the large number of degrees of freedom and the strict spatial and temporal synchronization required to generate meaningful behavior.","Humans learn bimanual manipulation skills by watching other humans and by refining their abilities through play.","In this work, we aim to enable robots to learn bimanual manipulation behaviors from human video demonstrations and fine-tune them through interaction.","Inspired by seminal work in psychology and biomechanics, we propose modeling the interaction between two hands as a serial kinematic linkage -- as a screw motion, in particular, that we use to define a new action space for bimanual manipulation: screw actions.","We introduce ScrewMimic, a framework that leverages this novel action representation to facilitate learning from human demonstration and self-supervised policy fine-tuning.","Our experiments demonstrate that ScrewMimic is able to learn several complex bimanual behaviors from a single human video demonstration, and that it outperforms baselines that interpret demonstrations and fine-tune directly in the original space of motion of both arms.","For more information and video results, https://robin-lab.cs.utexas.edu/ScrewMimic/"],"url":"http://arxiv.org/abs/2405.03666v1"}
{"created":"2024-05-06 17:41:13","title":"A New Robust Partial $p$-Wasserstein-Based Metric for Comparing Distributions","abstract":"The $2$-Wasserstein distance is sensitive to minor geometric differences between distributions, making it a very powerful dissimilarity metric. However, due to this sensitivity, a small outlier mass can also cause a significant increase in the $2$-Wasserstein distance between two similar distributions. Similarly, sampling discrepancy can cause the empirical $2$-Wasserstein distance on $n$ samples in $\\mathbb{R}^2$ to converge to the true distance at a rate of $n^{-1/4}$, which is significantly slower than the rate of $n^{-1/2}$ for $1$-Wasserstein distance.   We introduce a new family of distances parameterized by $k \\ge 0$, called $k$-RPW, that is based on computing the partial $2$-Wasserstein distance. We show that (1) $k$-RPW satisfies the metric properties, (2) $k$-RPW is robust to small outlier mass while retaining the sensitivity of $2$-Wasserstein distance to minor geometric differences, and (3) when $k$ is a constant, $k$-RPW distance between empirical distributions on $n$ samples in $\\mathbb{R}^2$ converges to the true distance at a rate of $n^{-1/3}$, which is faster than the convergence rate of $n^{-1/4}$ for the $2$-Wasserstein distance.   Using the partial $p$-Wasserstein distance, we extend our distance to any $p \\in [1,\\infty]$. By setting parameters $k$ or $p$ appropriately, we can reduce our distance to the total variation, $p$-Wasserstein, and the L\\'evy-Prokhorov distances. Experiments show that our distance function achieves higher accuracy in comparison to the $1$-Wasserstein, $2$-Wasserstein, and TV distances for image retrieval tasks on noisy real-world data sets.","sentences":["The $2$-Wasserstein distance is sensitive to minor geometric differences between distributions, making it a very powerful dissimilarity metric.","However, due to this sensitivity, a small outlier mass can also cause a significant increase in the $2$-Wasserstein distance between two similar distributions.","Similarly, sampling discrepancy can cause the empirical $2$-Wasserstein distance on $n$ samples in $\\mathbb{R}^2$ to converge to the true distance at a rate of $n^{-1/4}$, which is significantly slower than the rate of $n^{-1/2}$ for $1$-Wasserstein distance.   ","We introduce a new family of distances parameterized by $k \\ge 0$, called $k$-RPW, that is based on computing the partial $2$-Wasserstein distance.","We show that (1) $k$-RPW satisfies the metric properties, (2) $k$-RPW is robust to small outlier mass while retaining the sensitivity of $2$-Wasserstein distance to minor geometric differences, and (3) when $k$ is a constant, $k$-RPW distance between empirical distributions on $n$ samples in $\\mathbb{R}^2$ converges to the true distance at a rate of $n^{-1/3}$, which is faster than the convergence rate of $n^{-1/4}$ for the $2$-Wasserstein distance.   ","Using the partial $p$-Wasserstein distance, we extend our distance to any $p \\in [1,\\infty]$. By setting parameters $k$ or $p$ appropriately, we can reduce our distance to the total variation, $p$-Wasserstein, and the L\\'evy-Prokhorov distances.","Experiments show that our distance function achieves higher accuracy in comparison to the $1$-Wasserstein, $2$-Wasserstein, and TV distances for image retrieval tasks on noisy real-world data sets."],"url":"http://arxiv.org/abs/2405.03664v1"}
{"created":"2024-05-06 17:39:53","title":"Diffeomorphic Template Registration for Atmospheric Turbulence Mitigation","abstract":"We describe a method for recovering the irradiance underlying a collection of images corrupted by atmospheric turbulence. Since supervised data is often technically impossible to obtain, assumptions and biases have to be imposed to solve this inverse problem, and we choose to model them explicitly. Rather than initializing a latent irradiance (\"template\") by heuristics to estimate deformation, we select one of the images as a reference, and model the deformation in this image by the aggregation of the optical flow from it to other images, exploiting a prior imposed by Central Limit Theorem. Then with a novel flow inversion module, the model registers each image TO the template but WITHOUT the template, avoiding artifacts related to poor template initialization. To illustrate the robustness of the method, we simply (i) select the first frame as the reference and (ii) use the simplest optical flow to estimate the warpings, yet the improvement in registration is decisive in the final reconstruction, as we achieve state-of-the-art performance despite its simplicity. The method establishes a strong baseline that can be further improved by integrating it seamlessly into more sophisticated pipelines, or with domain-specific methods if so desired.","sentences":["We describe a method for recovering the irradiance underlying a collection of images corrupted by atmospheric turbulence.","Since supervised data is often technically impossible to obtain, assumptions and biases have to be imposed to solve this inverse problem, and we choose to model them explicitly.","Rather than initializing a latent irradiance (\"template\") by heuristics to estimate deformation, we select one of the images as a reference, and model the deformation in this image by the aggregation of the optical flow from it to other images, exploiting a prior imposed by Central Limit Theorem.","Then with a novel flow inversion module, the model registers each image TO the template but WITHOUT the template, avoiding artifacts related to poor template initialization.","To illustrate the robustness of the method, we simply (i) select the first frame as the reference and (ii) use the simplest optical flow to estimate the warpings, yet the improvement in registration is decisive in the final reconstruction, as we achieve state-of-the-art performance despite its simplicity.","The method establishes a strong baseline that can be further improved by integrating it seamlessly into more sophisticated pipelines, or with domain-specific methods if so desired."],"url":"http://arxiv.org/abs/2405.03662v1"}
{"created":"2024-05-06 17:38:20","title":"Competitive strategies to use \"warm start\" algorithms with predictions","abstract":"We consider the problem of learning and using predictions for warm start algorithms with predictions. In this setting, an algorithm is given an instance of a problem, and a prediction of the solution. The runtime of the algorithm is bounded by the distance from the predicted solution to the true solution of the instance. Previous work has shown that when instances are drawn iid from some distribution, it is possible to learn an approximately optimal fixed prediction (Dinitz et al, NeurIPS 2021), and in the adversarial online case, it is possible to compete with the best fixed prediction in hindsight (Khodak et al, NeurIPS 2022).   In this work we give competitive guarantees against stronger benchmarks that consider a set of $k$ predictions $\\mathbf{P}$. That is, the \"optimal offline cost\" to solve an instance with respect to $\\mathbf{P}$ is the distance from the true solution to the closest member of $\\mathbf{P}$. This is analogous to the $k$-medians objective function. In the distributional setting, we show a simple strategy that incurs cost that is at most an $O(k)$ factor worse than the optimal offline cost. We then show a way to leverage learnable coarse information, in the form of partitions of the instance space into groups of \"similar\" instances, that allows us to potentially avoid this $O(k)$ factor.   Finally, we consider an online version of the problem, where we compete against offline strategies that are allowed to maintain a moving set of $k$ predictions or \"trajectories,\" and are charged for how much the predictions move. We give an algorithm that does at most $O(k^4 \\ln^2 k)$ times as much work as any offline strategy of $k$ trajectories. This algorithm is deterministic (robust to an adaptive adversary), and oblivious to the setting of $k$. Thus the guarantee holds for all $k$ simultaneously.","sentences":["We consider the problem of learning and using predictions for warm start algorithms with predictions.","In this setting, an algorithm is given an instance of a problem, and a prediction of the solution.","The runtime of the algorithm is bounded by the distance from the predicted solution to the true solution of the instance.","Previous work has shown that when instances are drawn iid from some distribution, it is possible to learn an approximately optimal fixed prediction (Dinitz et al, NeurIPS 2021), and in the adversarial online case, it is possible to compete with the best fixed prediction in hindsight (Khodak et al, NeurIPS 2022).   ","In this work we give competitive guarantees against stronger benchmarks that consider a set of $k$ predictions $\\mathbf{P}$. That is, the \"optimal offline cost\" to solve an instance with respect to $\\mathbf{P}$ is the distance from the true solution to the closest member of $\\mathbf{P}$. This is analogous to the $k$-medians objective function.","In the distributional setting, we show a simple strategy that incurs cost that is at most an $O(k)$ factor worse than the optimal offline cost.","We then show a way to leverage learnable coarse information, in the form of partitions of the instance space into groups of \"similar\" instances, that allows us to potentially avoid this $O(k)$ factor.   ","Finally, we consider an online version of the problem, where we compete against offline strategies that are allowed to maintain a moving set of $k$ predictions or \"trajectories,\" and are charged for how much the predictions move.","We give an algorithm that does at most $O(k^4 \\ln^2 k)$ times as much work as any offline strategy of $k$ trajectories.","This algorithm is deterministic (robust to an adaptive adversary), and oblivious to the setting of $k$.","Thus the guarantee holds for all $k$ simultaneously."],"url":"http://arxiv.org/abs/2405.03661v1"}
{"created":"2024-05-06 17:37:23","title":"CICA: Content-Injected Contrastive Alignment for Zero-Shot Document Image Classification","abstract":"Zero-shot learning has been extensively investigated in the broader field of visual recognition, attracting significant interest recently. However, the current work on zero-shot learning in document image classification remains scarce. The existing studies either focus exclusively on zero-shot inference, or their evaluation does not align with the established criteria of zero-shot evaluation in the visual recognition domain. We provide a comprehensive document image classification analysis in Zero-Shot Learning (ZSL) and Generalized Zero-Shot Learning (GZSL) settings to address this gap. Our methodology and evaluation align with the established practices of this domain. Additionally, we propose zero-shot splits for the RVL-CDIP dataset. Furthermore, we introduce CICA (pronounced 'ki-ka'), a framework that enhances the zero-shot learning capabilities of CLIP. CICA consists of a novel 'content module' designed to leverage any generic document-related textual information. The discriminative features extracted by this module are aligned with CLIP's text and image features using a novel 'coupled-contrastive' loss. Our module improves CLIP's ZSL top-1 accuracy by 6.7% and GZSL harmonic mean by 24% on the RVL-CDIP dataset. Our module is lightweight and adds only 3.3% more parameters to CLIP. Our work sets the direction for future research in zero-shot document classification.","sentences":["Zero-shot learning has been extensively investigated in the broader field of visual recognition, attracting significant interest recently.","However, the current work on zero-shot learning in document image classification remains scarce.","The existing studies either focus exclusively on zero-shot inference, or their evaluation does not align with the established criteria of zero-shot evaluation in the visual recognition domain.","We provide a comprehensive document image classification analysis in Zero-Shot Learning (ZSL) and Generalized Zero-Shot Learning (GZSL) settings to address this gap.","Our methodology and evaluation align with the established practices of this domain.","Additionally, we propose zero-shot splits for the RVL-CDIP dataset.","Furthermore, we introduce CICA (pronounced 'ki-ka'), a framework that enhances the zero-shot learning capabilities of CLIP.","CICA consists of a novel 'content module' designed to leverage any generic document-related textual information.","The discriminative features extracted by this module are aligned with CLIP's text and image features using a novel 'coupled-contrastive' loss.","Our module improves CLIP's ZSL top-1 accuracy by 6.7% and GZSL harmonic mean by 24% on the RVL-CDIP dataset.","Our module is lightweight and adds only 3.3% more parameters to CLIP.","Our work sets the direction for future research in zero-shot document classification."],"url":"http://arxiv.org/abs/2405.03660v1"}
{"created":"2024-05-06 17:36:44","title":"A Construct-Optimize Approach to Sparse View Synthesis without Camera Pose","abstract":"Novel view synthesis from a sparse set of input images is a challenging problem of great practical interest, especially when camera poses are absent or inaccurate. Direct optimization of camera poses and usage of estimated depths in neural radiance field algorithms usually do not produce good results because of the coupling between poses and depths, and inaccuracies in monocular depth estimation. In this paper, we leverage the recent 3D Gaussian splatting method to develop a novel construct-and-optimize method for sparse view synthesis without camera poses. Specifically, we construct a solution progressively by using monocular depth and projecting pixels back into the 3D world. During construction, we optimize the solution by detecting 2D correspondences between training views and the corresponding rendered images. We develop a unified differentiable pipeline for camera registration and adjustment of both camera poses and depths, followed by back-projection. We also introduce a novel notion of an expected surface in Gaussian splatting, which is critical to our optimization. These steps enable a coarse solution, which can then be low-pass filtered and refined using standard optimization methods. We demonstrate results on the Tanks and Temples and Static Hikes datasets with as few as three widely-spaced views, showing significantly better quality than competing methods, including those with approximate camera pose information. Moreover, our results improve with more views and outperform previous InstantNGP and Gaussian Splatting algorithms even when using half the dataset.","sentences":["Novel view synthesis from a sparse set of input images is a challenging problem of great practical interest, especially when camera poses are absent or inaccurate.","Direct optimization of camera poses and usage of estimated depths in neural radiance field algorithms usually do not produce good results because of the coupling between poses and depths, and inaccuracies in monocular depth estimation.","In this paper, we leverage the recent 3D Gaussian splatting method to develop a novel construct-and-optimize method for sparse view synthesis without camera poses.","Specifically, we construct a solution progressively by using monocular depth and projecting pixels back into the 3D world.","During construction, we optimize the solution by detecting 2D correspondences between training views and the corresponding rendered images.","We develop a unified differentiable pipeline for camera registration and adjustment of both camera poses and depths, followed by back-projection.","We also introduce a novel notion of an expected surface in Gaussian splatting, which is critical to our optimization.","These steps enable a coarse solution, which can then be low-pass filtered and refined using standard optimization methods.","We demonstrate results on the Tanks and Temples and Static Hikes datasets with as few as three widely-spaced views, showing significantly better quality than competing methods, including those with approximate camera pose information.","Moreover, our results improve with more views and outperform previous InstantNGP and Gaussian Splatting algorithms even when using half the dataset."],"url":"http://arxiv.org/abs/2405.03659v1"}
{"created":"2024-05-06 17:33:58","title":"A review on data-driven constitutive laws for solids","abstract":"This review article highlights state-of-the-art data-driven techniques to discover, encode, surrogate, or emulate constitutive laws that describe the path-independent and path-dependent response of solids. Our objective is to provide an organized taxonomy to a large spectrum of methodologies developed in the past decades and to discuss the benefits and drawbacks of the various techniques for interpreting and forecasting mechanics behavior across different scales. Distinguishing between machine-learning-based and model-free methods, we further categorize approaches based on their interpretability and on their learning process/type of required data, while discussing the key problems of generalization and trustworthiness. We attempt to provide a road map of how these can be reconciled in a data-availability-aware context. We also touch upon relevant aspects such as data sampling techniques, design of experiments, verification, and validation.","sentences":["This review article highlights state-of-the-art data-driven techniques to discover, encode, surrogate, or emulate constitutive laws that describe the path-independent and path-dependent response of solids.","Our objective is to provide an organized taxonomy to a large spectrum of methodologies developed in the past decades and to discuss the benefits and drawbacks of the various techniques for interpreting and forecasting mechanics behavior across different scales.","Distinguishing between machine-learning-based and model-free methods, we further categorize approaches based on their interpretability and on their learning process/type of required data, while discussing the key problems of generalization and trustworthiness.","We attempt to provide a road map of how these can be reconciled in a data-availability-aware context.","We also touch upon relevant aspects such as data sampling techniques, design of experiments, verification, and validation."],"url":"http://arxiv.org/abs/2405.03658v1"}
{"created":"2024-05-06 17:26:34","title":"Can LLMs Deeply Detect Complex Malicious Queries? A Framework for Jailbreaking via Obfuscating Intent","abstract":"To demonstrate and address the underlying maliciousness, we propose a theoretical hypothesis and analytical approach, and introduce a new black-box jailbreak attack methodology named IntentObfuscator, exploiting this identified flaw by obfuscating the true intentions behind user prompts.This approach compels LLMs to inadvertently generate restricted content, bypassing their built-in content security measures. We detail two implementations under this framework: \"Obscure Intention\" and \"Create Ambiguity\", which manipulate query complexity and ambiguity to evade malicious intent detection effectively. We empirically validate the effectiveness of the IntentObfuscator method across several models, including ChatGPT-3.5, ChatGPT-4, Qwen and Baichuan, achieving an average jailbreak success rate of 69.21\\%. Notably, our tests on ChatGPT-3.5, which claims 100 million weekly active users, achieved a remarkable success rate of 83.65\\%. We also extend our validation to diverse types of sensitive content like graphic violence, racism, sexism, political sensitivity, cybersecurity threats, and criminal skills, further proving the substantial impact of our findings on enhancing 'Red Team' strategies against LLM content security frameworks.","sentences":["To demonstrate and address the underlying maliciousness, we propose a theoretical hypothesis and analytical approach, and introduce a new black-box jailbreak attack methodology named IntentObfuscator, exploiting this identified flaw by obfuscating the true intentions behind user prompts.","This approach compels LLMs to inadvertently generate restricted content, bypassing their built-in content security measures.","We detail two implementations under this framework: \"Obscure Intention\" and \"Create Ambiguity\", which manipulate query complexity and ambiguity to evade malicious intent detection effectively.","We empirically validate the effectiveness of the IntentObfuscator method across several models, including ChatGPT-3.5, ChatGPT-4, Qwen and Baichuan, achieving an average jailbreak success rate of 69.21\\%.","Notably, our tests on ChatGPT-3.5, which claims 100 million weekly active users, achieved a remarkable success rate of 83.65\\%.","We also extend our validation to diverse types of sensitive content like graphic violence, racism, sexism, political sensitivity, cybersecurity threats, and criminal skills, further proving the substantial impact of our findings on enhancing 'Red Team' strategies against LLM content security frameworks."],"url":"http://arxiv.org/abs/2405.03654v1"}
{"created":"2024-05-06 17:23:42","title":"Field-of-View Extension for Diffusion MRI via Deep Generative Models","abstract":"Purpose: In diffusion MRI (dMRI), the volumetric and bundle analyses of whole-brain tissue microstructure and connectivity can be severely impeded by an incomplete field-of-view (FOV). This work aims to develop a method for imputing the missing slices directly from existing dMRI scans with an incomplete FOV. We hypothesize that the imputed image with complete FOV can improve the whole-brain tractography for corrupted data with incomplete FOV. Therefore, our approach provides a desirable alternative to discarding the valuable dMRI data, enabling subsequent tractography analyses that would otherwise be challenging or unattainable with corrupted data. Approach: We propose a framework based on a deep generative model that estimates the absent brain regions in dMRI scans with incomplete FOV. The model is capable of learning both the diffusion characteristics in diffusion-weighted images (DWI) and the anatomical features evident in the corresponding structural images for efficiently imputing missing slices of DWI outside of incomplete FOV. Results: For evaluating the imputed slices, on the WRAP dataset the proposed framework achieved PSNRb0=22.397, SSIMb0=0.905, PSNRb1300=22.479, SSIMb1300=0.893; on the NACC dataset it achieved PSNRb0=21.304, SSIMb0=0.892, PSNRb1300=21.599, SSIMb1300= 0.877. The proposed framework improved the tractography accuracy, as demonstrated by an increased average Dice score for 72 tracts (p < 0.001) on both the WRAP and NACC datasets. Conclusions: Results suggest that the proposed framework achieved sufficient imputation performance in dMRI data with incomplete FOV for improving whole-brain tractography, thereby repairing the corrupted data. Our approach achieved more accurate whole-brain tractography results with extended and complete FOV and reduced the uncertainty when analyzing bundles associated with Alzheimer's Disease.","sentences":["Purpose: In diffusion MRI (dMRI), the volumetric and bundle analyses of whole-brain tissue microstructure and connectivity can be severely impeded by an incomplete field-of-view (FOV).","This work aims to develop a method for imputing the missing slices directly from existing dMRI scans with an incomplete FOV.","We hypothesize that the imputed image with complete FOV can improve the whole-brain tractography for corrupted data with incomplete FOV.","Therefore, our approach provides a desirable alternative to discarding the valuable dMRI data, enabling subsequent tractography analyses that would otherwise be challenging or unattainable with corrupted data.","Approach:","We propose a framework based on a deep generative model that estimates the absent brain regions in dMRI scans with incomplete FOV.","The model is capable of learning both the diffusion characteristics in diffusion-weighted images (DWI) and the anatomical features evident in the corresponding structural images for efficiently imputing missing slices of DWI outside of incomplete FOV.","Results:","For evaluating the imputed slices, on the WRAP dataset the proposed framework achieved PSNRb0=22.397, SSIMb0=0.905, PSNRb1300=22.479, SSIMb1300=0.893; on the NACC dataset it achieved PSNRb0=21.304, SSIMb0=0.892, PSNRb1300=21.599, SSIMb1300= 0.877.","The proposed framework improved the tractography accuracy, as demonstrated by an increased average Dice score for 72 tracts (p < 0.001) on both the WRAP and NACC datasets.","Conclusions: Results suggest that the proposed framework achieved sufficient imputation performance in dMRI data with incomplete FOV for improving whole-brain tractography, thereby repairing the corrupted data.","Our approach achieved more accurate whole-brain tractography results with extended and complete FOV and reduced the uncertainty when analyzing bundles associated with Alzheimer's Disease."],"url":"http://arxiv.org/abs/2405.03652v1"}
{"created":"2024-05-06 17:14:34","title":"Adaptive Retrieval and Scalable Indexing for k-NN Search with Cross-Encoders","abstract":"Cross-encoder (CE) models which compute similarity by jointly encoding a query-item pair perform better than embedding-based models (dual-encoders) at estimating query-item relevance. Existing approaches perform k-NN search with CE by approximating the CE similarity with a vector embedding space fit either with dual-encoders (DE) or CUR matrix factorization. DE-based retrieve-and-rerank approaches suffer from poor recall on new domains and the retrieval with DE is decoupled from the CE. While CUR-based approaches can be more accurate than the DE-based approach, they require a prohibitively large number of CE calls to compute item embeddings, thus making it impractical for deployment at scale. In this paper, we address these shortcomings with our proposed sparse-matrix factorization based method that efficiently computes latent query and item embeddings to approximate CE scores and performs k-NN search with the approximate CE similarity. We compute item embeddings offline by factorizing a sparse matrix containing query-item CE scores for a set of train queries. Our method produces a high-quality approximation while requiring only a fraction of CE calls as compared to CUR-based methods, and allows for leveraging DE to initialize the embedding space while avoiding compute- and resource-intensive finetuning of DE via distillation. At test time, the item embeddings remain fixed and retrieval occurs over rounds, alternating between a) estimating the test query embedding by minimizing error in approximating CE scores of items retrieved thus far, and b) using the updated test query embedding for retrieving more items. Our k-NN search method improves recall by up to 5% (k=1) and 54% (k=100) over DE-based approaches. Additionally, our indexing approach achieves a speedup of up to 100x over CUR-based and 5x over DE distillation methods, while matching or improving k-NN search recall over baselines.","sentences":["Cross-encoder (CE) models which compute similarity by jointly encoding a query-item pair perform better than embedding-based models (dual-encoders) at estimating query-item relevance.","Existing approaches perform k-NN search with CE by approximating the CE similarity with a vector embedding space fit either with dual-encoders (DE) or CUR matrix factorization.","DE-based retrieve-and-rerank approaches suffer from poor recall on new domains and the retrieval with DE is decoupled from the CE.","While CUR-based approaches can be more accurate than the DE-based approach, they require a prohibitively large number of CE calls to compute item embeddings, thus making it impractical for deployment at scale.","In this paper, we address these shortcomings with our proposed sparse-matrix factorization based method that efficiently computes latent query and item embeddings to approximate CE scores and performs k-NN search with the approximate CE similarity.","We compute item embeddings offline by factorizing a sparse matrix containing query-item CE scores for a set of train queries.","Our method produces a high-quality approximation while requiring only a fraction of CE calls as compared to CUR-based methods, and allows for leveraging DE to initialize the embedding space while avoiding compute- and resource-intensive finetuning of DE via distillation.","At test time, the item embeddings remain fixed and retrieval occurs over rounds, alternating between a) estimating the test query embedding by minimizing error in approximating CE scores of items retrieved thus far, and b) using the updated test query embedding for retrieving more items.","Our k-NN search method improves recall by up to 5% (k=1) and 54% (k=100) over DE-based approaches.","Additionally, our indexing approach achieves a speedup of up to 100x over CUR-based and 5x over DE distillation methods, while matching or improving k-NN search recall over baselines."],"url":"http://arxiv.org/abs/2405.03651v1"}
{"created":"2024-05-06 17:14:09","title":"Generated Contents Enrichment","abstract":"In this paper, we investigate a novel artificial intelligence generation task, termed as generated contents enrichment (GCE). Different from conventional artificial intelligence contents generation task that enriches the given textual description implicitly with limited semantics for generating visually real content, our proposed GCE strives to perform content enrichment explicitly on both the visual and textual domain, from which the enriched contents are visually real, structurally reasonable, and semantically abundant. Towards to solve GCE, we propose a deep end-to-end method that explicitly explores the semantics and inter-semantic relationships during the enrichment. Specifically, we first model the input description as a semantic graph, wherein each node represents an object and each edge corresponds to the inter-object relationship. We then adopt Graph Convolutional Networks on top of the input scene description to predict the enriching objects and their relationships with the input objects. Finally, the enriched graph is fed into an image synthesis model to carry out the visual contents generation. Our experiments conducted on the Visual Genome dataset exhibit promising and visually plausible results.","sentences":["In this paper, we investigate a novel artificial intelligence generation task, termed as generated contents enrichment (GCE).","Different from conventional artificial intelligence contents generation task that enriches the given textual description implicitly with limited semantics for generating visually real content, our proposed GCE strives to perform content enrichment explicitly on both the visual and textual domain, from which the enriched contents are visually real, structurally reasonable, and semantically abundant.","Towards to solve GCE, we propose a deep end-to-end method that explicitly explores the semantics and inter-semantic relationships during the enrichment.","Specifically, we first model the input description as a semantic graph, wherein each node represents an object and each edge corresponds to the inter-object relationship.","We then adopt Graph Convolutional Networks on top of the input scene description to predict the enriching objects and their relationships with the input objects.","Finally, the enriched graph is fed into an image synthesis model to carry out the visual contents generation.","Our experiments conducted on the Visual Genome dataset exhibit promising and visually plausible results."],"url":"http://arxiv.org/abs/2405.03650v1"}
{"created":"2024-05-06 17:12:21","title":"Learning Robust Classifiers with Self-Guided Spurious Correlation Mitigation","abstract":"Deep neural classifiers tend to rely on spurious correlations between spurious attributes of inputs and targets to make predictions, which could jeopardize their generalization capability. Training classifiers robust to spurious correlations typically relies on annotations of spurious correlations in data, which are often expensive to get. In this paper, we tackle an annotation-free setting and propose a self-guided spurious correlation mitigation framework. Our framework automatically constructs fine-grained training labels tailored for a classifier obtained with empirical risk minimization to improve its robustness against spurious correlations. The fine-grained training labels are formulated with different prediction behaviors of the classifier identified in a novel spuriousness embedding space. We construct the space with automatically detected conceptual attributes and a novel spuriousness metric which measures how likely a class-attribute correlation is exploited for predictions. We demonstrate that training the classifier to distinguish different prediction behaviors reduces its reliance on spurious correlations without knowing them a priori and outperforms prior methods on five real-world datasets.","sentences":["Deep neural classifiers tend to rely on spurious correlations between spurious attributes of inputs and targets to make predictions, which could jeopardize their generalization capability.","Training classifiers robust to spurious correlations typically relies on annotations of spurious correlations in data, which are often expensive to get.","In this paper, we tackle an annotation-free setting and propose a self-guided spurious correlation mitigation framework.","Our framework automatically constructs fine-grained training labels tailored for a classifier obtained with empirical risk minimization to improve its robustness against spurious correlations.","The fine-grained training labels are formulated with different prediction behaviors of the classifier identified in a novel spuriousness embedding space.","We construct the space with automatically detected conceptual attributes and a novel spuriousness metric which measures how likely a class-attribute correlation is exploited for predictions.","We demonstrate that training the classifier to distinguish different prediction behaviors reduces its reliance on spurious correlations without knowing them a priori and outperforms prior methods on five real-world datasets."],"url":"http://arxiv.org/abs/2405.03649v1"}
{"created":"2024-05-06 17:09:10","title":"Content-Oblivious Leader Election on Rings","abstract":"In content-oblivious computation, n nodes wish to compute a given task over an asynchronous network that suffers from an extremely harsh type of noise, which corrupts the content of all messages across all channels. In a recent work, Censor-Hillel, Cohen, Gelles, and Sela (Distributed Computing, 2023) showed how to perform arbitrary computations in a content-oblivious way in 2-edge connected networks but only if the network has a distinguished node (called root) to initiate the computation.   Our goal is to remove this assumption, which was conjectured to be necessary. Achieving this goal essentially reduces to performing a content-oblivious leader election since an elected leader can then serve as the root required to perform arbitrary content-oblivious computations. We focus on ring networks, which are the simplest 2-edge connected graphs. On oriented rings, we obtain a leader election algorithm with message complexity O(n*ID_max), where ID_max is the maximal assigned ID. As it turns out, this dependency on $ID_max$ is inherent: we show a lower bound of Omega(n*log(ID_max/n)) messages for content-oblivious leader election algorithms. We also extend our results to non-oriented rings, where nodes cannot tell which channel leads to which neighbor. In this case, however, the algorithm does not terminate but only reaches quiescence.","sentences":["In content-oblivious computation, n nodes wish to compute a given task over an asynchronous network that suffers from an extremely harsh type of noise, which corrupts the content of all messages across all channels.","In a recent work, Censor-Hillel, Cohen, Gelles, and Sela (Distributed Computing, 2023) showed how to perform arbitrary computations in a content-oblivious way in 2-edge connected networks but only if the network has a distinguished node (called root) to initiate the computation.   ","Our goal is to remove this assumption, which was conjectured to be necessary.","Achieving this goal essentially reduces to performing a content-oblivious leader election since an elected leader can then serve as the root required to perform arbitrary content-oblivious computations.","We focus on ring networks, which are the simplest 2-edge connected graphs.","On oriented rings, we obtain a leader election algorithm with message complexity O(n*ID_max), where ID_max is the maximal assigned ID.","As it turns out, this dependency on $ID_max$ is inherent: we show a lower bound of Omega(n*log(ID_max/n)) messages for content-oblivious leader election algorithms.","We also extend our results to non-oriented rings, where nodes cannot tell which channel leads to which neighbor.","In this case, however, the algorithm does not terminate but only reaches quiescence."],"url":"http://arxiv.org/abs/2405.03646v1"}
{"created":"2024-05-06 17:07:28","title":"When LLMs Meet Cybersecurity: A Systematic Literature Review","abstract":"The rapid advancements in large language models (LLMs) have opened new avenues across various fields, including cybersecurity, which faces an ever-evolving threat landscape and need for innovative technologies. Despite initial explorations into the application of LLMs in cybersecurity, there is a lack of a comprehensive overview of this research area. This paper bridge this gap by providing a systematic literature review, encompassing an analysis of over 180 works, spanning across 25 LLMs and more than 10 downstream scenarios. Our comprehensive overview addresses three critical research questions: the construction of cybersecurity-oriented LLMs, LLMs' applications in various cybersecurity tasks, and the existing challenges and further research in this area. This study aims to shed light on the extensive potential of LLMs in enhancing cybersecurity practices, and serve as a valuable resource for applying LLMs in this doamin. We also maintain and regularly updated list of practical guides on LLMs for cybersecurity at https://github.com/tmylla/Awesome-LLM4Cybersecurity.","sentences":["The rapid advancements in large language models (LLMs) have opened new avenues across various fields, including cybersecurity, which faces an ever-evolving threat landscape and need for innovative technologies.","Despite initial explorations into the application of LLMs in cybersecurity, there is a lack of a comprehensive overview of this research area.","This paper bridge this gap by providing a systematic literature review, encompassing an analysis of over 180 works, spanning across 25 LLMs and more than 10 downstream scenarios.","Our comprehensive overview addresses three critical research questions: the construction of cybersecurity-oriented LLMs, LLMs' applications in various cybersecurity tasks, and the existing challenges and further research in this area.","This study aims to shed light on the extensive potential of LLMs in enhancing cybersecurity practices, and serve as a valuable resource for applying LLMs in this doamin.","We also maintain and regularly updated list of practical guides on LLMs for cybersecurity at https://github.com/tmylla/Awesome-LLM4Cybersecurity."],"url":"http://arxiv.org/abs/2405.03644v1"}
{"created":"2024-05-06 17:06:32","title":"Collecting Consistently High Quality Object Tracks with Minimal Human Involvement by Using Self-Supervised Learning to Detect Tracker Errors","abstract":"We propose a hybrid framework for consistently producing high-quality object tracks by combining an automated object tracker with little human input. The key idea is to tailor a module for each dataset to intelligently decide when an object tracker is failing and so humans should be brought in to re-localize an object for continued tracking. Our approach leverages self-supervised learning on unlabeled videos to learn a tailored representation for a target object that is then used to actively monitor its tracked region and decide when the tracker fails. Since labeled data is not needed, our approach can be applied to novel object categories. Experiments on three datasets demonstrate our method outperforms existing approaches, especially for small, fast moving, or occluded objects.","sentences":["We propose a hybrid framework for consistently producing high-quality object tracks by combining an automated object tracker with little human input.","The key idea is to tailor a module for each dataset to intelligently decide when an object tracker is failing and so humans should be brought in to re-localize an object for continued tracking.","Our approach leverages self-supervised learning on unlabeled videos to learn a tailored representation for a target object that is then used to actively monitor its tracked region and decide when the tracker fails.","Since labeled data is not needed, our approach can be applied to novel object categories.","Experiments on three datasets demonstrate our method outperforms existing approaches, especially for small, fast moving, or occluded objects."],"url":"http://arxiv.org/abs/2405.03643v1"}
{"created":"2024-05-06 17:06:11","title":"Classification of Breast Cancer Histopathology Images using a Modified Supervised Contrastive Learning Method","abstract":"Deep neural networks have reached remarkable achievements in medical image processing tasks, specifically classifying and detecting various diseases. However, when confronted with limited data, these networks face a critical vulnerability, often succumbing to overfitting by excessively memorizing the limited information available. This work addresses the challenge mentioned above by improving the supervised contrastive learning method to reduce the impact of false positives. Unlike most existing methods that rely predominantly on fully supervised learning, our approach leverages the advantages of self-supervised learning in conjunction with employing the available labeled data. We evaluate our method on the BreakHis dataset, which consists of breast cancer histopathology images, and demonstrate an increase in classification accuracy by 1.45% at the image level and 1.42% at the patient level compared to the state-of-the-art method. This improvement corresponds to 93.63% absolute accuracy, highlighting our approach's effectiveness in leveraging data properties to learn more appropriate representation space.","sentences":["Deep neural networks have reached remarkable achievements in medical image processing tasks, specifically classifying and detecting various diseases.","However, when confronted with limited data, these networks face a critical vulnerability, often succumbing to overfitting by excessively memorizing the limited information available.","This work addresses the challenge mentioned above by improving the supervised contrastive learning method to reduce the impact of false positives.","Unlike most existing methods that rely predominantly on fully supervised learning, our approach leverages the advantages of self-supervised learning in conjunction with employing the available labeled data.","We evaluate our method on the BreakHis dataset, which consists of breast cancer histopathology images, and demonstrate an increase in classification accuracy by 1.45% at the image level and 1.42% at the patient level compared to the state-of-the-art method.","This improvement corresponds to 93.63% absolute accuracy, highlighting our approach's effectiveness in leveraging data properties to learn more appropriate representation space."],"url":"http://arxiv.org/abs/2405.03642v1"}
{"created":"2024-05-06 16:56:16","title":"Cosine Annealing Optimized Denoising Diffusion Error Correction Codes","abstract":"To address the issue of increased bit error rates during the later stages of linear search in denoising diffusion error correction codes, we propose a novel method that optimizes denoising diffusion error correction codes (ECC) using cosine annealing. In response to the challenge of decoding long codewords, the proposed method employs a variance adjustment strategy during the reverse diffusion process, rather than maintaining a constant variance. By leveraging cosine annealing, this method effectively lowers the bit error rate and enhances decoding effciency. This letter extensively validates the approach through experiments and demonstrates signifcant improvements in bit error rate reduction and iteration effciency compared to existing methods. This advancement offers a promising solution for improving ECC decoding performance, potentially impacting secure digital communication practices.","sentences":["To address the issue of increased bit error rates during the later stages of linear search in denoising diffusion error correction codes, we propose a novel method that optimizes denoising diffusion error correction codes (ECC) using cosine annealing.","In response to the challenge of decoding long codewords, the proposed method employs a variance adjustment strategy during the reverse diffusion process, rather than maintaining a constant variance.","By leveraging cosine annealing, this method effectively lowers the bit error rate and enhances decoding effciency.","This letter extensively validates the approach through experiments and demonstrates signifcant improvements in bit error rate reduction and iteration effciency compared to existing methods.","This advancement offers a promising solution for improving ECC decoding performance, potentially impacting secure digital communication practices."],"url":"http://arxiv.org/abs/2405.03638v1"}
{"created":"2024-05-06 16:55:30","title":"Collage: Light-Weight Low-Precision Strategy for LLM Training","abstract":"Large models training is plagued by the intense compute cost and limited hardware memory. A practical solution is low-precision representation but is troubled by loss in numerical accuracy and unstable training rendering the model less useful. We argue that low-precision floating points can perform well provided the error is properly compensated at the critical locations in the training process. We propose Collage which utilizes multi-component float representation in low-precision to accurately perform operations with numerical errors accounted. To understand the impact of imprecision to training, we propose a simple and novel metric which tracks the lost information during training as well as differentiates various precision strategies. Our method works with commonly used low-precision such as half-precision ($16$-bit floating points) and can be naturally extended to work with even lower precision such as $8$-bit. Experimental results show that pre-training using Collage removes the requirement of using $32$-bit floating-point copies of the model and attains similar/better training performance compared to $(16, 32)$-bit mixed-precision strategy, with up to $3.7\\times$ speedup and $\\sim 15\\%$ to $23\\%$ less memory usage in practice.","sentences":["Large models training is plagued by the intense compute cost and limited hardware memory.","A practical solution is low-precision representation but is troubled by loss in numerical accuracy and unstable training rendering the model less useful.","We argue that low-precision floating points can perform well provided the error is properly compensated at the critical locations in the training process.","We propose Collage which utilizes multi-component float representation in low-precision to accurately perform operations with numerical errors accounted.","To understand the impact of imprecision to training, we propose a simple and novel metric which tracks the lost information during training as well as differentiates various precision strategies.","Our method works with commonly used low-precision such as half-precision ($16$-bit floating points) and can be naturally extended to work with even lower precision such as $8$-bit.","Experimental results show that pre-training using Collage removes the requirement of using $32$-bit floating-point copies of the model and attains similar/better training performance compared to $(16, 32)$-bit mixed-precision strategy, with up to $3.7\\times$ speedup and $\\sim 15\\%$ to $23\\%$ less memory usage in practice."],"url":"http://arxiv.org/abs/2405.03637v1"}
{"created":"2024-05-06 16:55:20","title":"Federated Learning Privacy: Attacks, Defenses, Applications, and Policy Landscape - A Survey","abstract":"Deep learning has shown incredible potential across a vast array of tasks and accompanying this growth has been an insatiable appetite for data. However, a large amount of data needed for enabling deep learning is stored on personal devices and recent concerns on privacy have further highlighted challenges for accessing such data. As a result, federated learning (FL) has emerged as an important privacy-preserving technology enabling collaborative training of machine learning models without the need to send the raw, potentially sensitive, data to a central server. However, the fundamental premise that sending model updates to a server is privacy-preserving only holds if the updates cannot be \"reverse engineered\" to infer information about the private training data. It has been shown under a wide variety of settings that this premise for privacy does {\\em not} hold.   In this survey paper, we provide a comprehensive literature review of the different privacy attacks and defense methods in FL. We identify the current limitations of these attacks and highlight the settings in which FL client privacy can be broken. We dissect some of the successful industry applications of FL and draw lessons for future successful adoption. We survey the emerging landscape of privacy regulation for FL. We conclude with future directions for taking FL toward the cherished goal of generating accurate models while preserving the privacy of the data from its participants.","sentences":["Deep learning has shown incredible potential across a vast array of tasks and accompanying this growth has been an insatiable appetite for data.","However, a large amount of data needed for enabling deep learning is stored on personal devices and recent concerns on privacy have further highlighted challenges for accessing such data.","As a result, federated learning (FL) has emerged as an important privacy-preserving technology enabling collaborative training of machine learning models without the need to send the raw, potentially sensitive, data to a central server.","However, the fundamental premise that sending model updates to a server is privacy-preserving only holds if the updates cannot be \"reverse engineered\" to infer information about the private training data.","It has been shown under a wide variety of settings that this premise for privacy does {\\em not} hold.   ","In this survey paper, we provide a comprehensive literature review of the different privacy attacks and defense methods in FL.","We identify the current limitations of these attacks and highlight the settings in which FL client privacy can be broken.","We dissect some of the successful industry applications of FL and draw lessons for future successful adoption.","We survey the emerging landscape of privacy regulation for FL.","We conclude with future directions for taking FL toward the cherished goal of generating accurate models while preserving the privacy of the data from its participants."],"url":"http://arxiv.org/abs/2405.03636v1"}
{"created":"2024-05-06 16:50:42","title":"Neural Graph Mapping for Dense SLAM with Efficient Loop Closure","abstract":"Existing neural field-based SLAM methods typically employ a single monolithic field as their scene representation. This prevents efficient incorporation of loop closure constraints and limits scalability. To address these shortcomings, we propose a neural mapping framework which anchors lightweight neural fields to the pose graph of a sparse visual SLAM system. Our approach shows the ability to integrate large-scale loop closures, while limiting necessary reintegration. Furthermore, we verify the scalability of our approach by demonstrating successful building-scale mapping taking multiple loop closures into account during the optimization, and show that our method outperforms existing state-of-the-art approaches on large scenes in terms of quality and runtime. Our code is available at https://kth-rpl.github.io/neural_graph_mapping/.","sentences":["Existing neural field-based SLAM methods typically employ a single monolithic field as their scene representation.","This prevents efficient incorporation of loop closure constraints and limits scalability.","To address these shortcomings, we propose a neural mapping framework which anchors lightweight neural fields to the pose graph of a sparse visual SLAM system.","Our approach shows the ability to integrate large-scale loop closures, while limiting necessary reintegration.","Furthermore, we verify the scalability of our approach by demonstrating successful building-scale mapping taking multiple loop closures into account during the optimization, and show that our method outperforms existing state-of-the-art approaches on large scenes in terms of quality and runtime.","Our code is available at https://kth-rpl.github.io/neural_graph_mapping/."],"url":"http://arxiv.org/abs/2405.03633v1"}
{"created":"2024-05-06 16:49:11","title":"LaserEscape: Detecting and Mitigating Optical Probing Attacks","abstract":"The security of integrated circuits (ICs) can be broken by sophisticated physical attacks relying on failure analysis methods. Optical probing is one of the most prominent examples of such attacks, which can be accomplished in a matter of days, even with limited knowledge of the IC under attack. Unfortunately, few countermeasures are proposed in the literature, and none has been fabricated and tested in practice. These countermeasures usually require changing the standard cell libraries and, thus, are incompatible with digital and programmable platforms, such as field programmable gate arrays (FPGAs). In this work, we shift our attention from preventing the attack to detecting and responding to it. We introduce LaserEscape, the first fully digital and FPGA-compatible countermeasure to detect and mitigate optical probing attacks. LaserEscape incorporates digital delay-based sensors to reliably detect the physical alteration on the fabric caused by laser beam irradiations in real time. Furthermore, as a response to the attack, LaserEscape deploys real-time hiding approaches using randomized hardware reconfigurability. It realizes 1) moving target defense (MTD) to physically move the sensitive circuity under attack out of the probing field of focus to protect secret keys and 2) polymorphism to logically obfuscate the functionality of the targeted circuit to counter function extraction and reverse engineering attempts. We demonstrate the effectiveness and resiliency of our approach by performing optical probing attacks on protected and unprotected designs on a 28-nm FPGA. Our results show that optical probing attacks can be reliably detected and mitigated without interrupting the chip's operation.","sentences":["The security of integrated circuits (ICs) can be broken by sophisticated physical attacks relying on failure analysis methods.","Optical probing is one of the most prominent examples of such attacks, which can be accomplished in a matter of days, even with limited knowledge of the IC under attack.","Unfortunately, few countermeasures are proposed in the literature, and none has been fabricated and tested in practice.","These countermeasures usually require changing the standard cell libraries and, thus, are incompatible with digital and programmable platforms, such as field programmable gate arrays (FPGAs).","In this work, we shift our attention from preventing the attack to detecting and responding to it.","We introduce LaserEscape, the first fully digital and FPGA-compatible countermeasure to detect and mitigate optical probing attacks.","LaserEscape incorporates digital delay-based sensors to reliably detect the physical alteration on the fabric caused by laser beam irradiations in real time.","Furthermore, as a response to the attack, LaserEscape deploys real-time hiding approaches using randomized hardware reconfigurability.","It realizes 1) moving target defense (MTD) to physically move the sensitive circuity under attack out of the probing field of focus to protect secret keys and 2) polymorphism to logically obfuscate the functionality of the targeted circuit to counter function extraction and reverse engineering attempts.","We demonstrate the effectiveness and resiliency of our approach by performing optical probing attacks on protected and unprotected designs on a 28-nm FPGA.","Our results show that optical probing attacks can be reliably detected and mitigated without interrupting the chip's operation."],"url":"http://arxiv.org/abs/2405.03632v1"}
{"created":"2024-05-06 16:45:48","title":"State-Aware Timeliness in Energy Harvesting IoT Systems Monitoring a Markovian Source","abstract":"In this study, we investigate the optimal transmission policies within an energy harvesting status update system, where the demand for status updates depends on the state of the source. The system monitors a two-state Markovian source that characterizes a stochastic process, which can be in either a normal state or an alarm state, with a higher demand for fresh updates when the source is in the alarm state. We propose a metric to capture the freshness of status updates for each state of the stochastic process by introducing two Age of Information (AoI) variables, extending the definition of AoI to account for the state changes of the stochastic process. We formulate the problem as a Markov Decision Process (MDP), utilizing a transition cost function that applies linear and non-linear penalties based on AoI and the state of the stochastic process. Through analytical investigation, we delve into the structure of the optimal transmission policy for the resulting MDP problem. Furthermore, we evaluate the derived policies via numerical results and demonstrate their effectiveness in reserving energy in anticipation of forthcoming alarm states.","sentences":["In this study, we investigate the optimal transmission policies within an energy harvesting status update system, where the demand for status updates depends on the state of the source.","The system monitors a two-state Markovian source that characterizes a stochastic process, which can be in either a normal state or an alarm state, with a higher demand for fresh updates when the source is in the alarm state.","We propose a metric to capture the freshness of status updates for each state of the stochastic process by introducing two Age of Information (AoI) variables, extending the definition of AoI to account for the state changes of the stochastic process.","We formulate the problem as a Markov Decision Process (MDP), utilizing a transition cost function that applies linear and non-linear penalties based on AoI and the state of the stochastic process.","Through analytical investigation, we delve into the structure of the optimal transmission policy for the resulting MDP problem.","Furthermore, we evaluate the derived policies via numerical results and demonstrate their effectiveness in reserving energy in anticipation of forthcoming alarm states."],"url":"http://arxiv.org/abs/2405.03628v1"}
{"created":"2024-05-06 16:41:52","title":"$\u03b5$-Policy Gradient for Online Pricing","abstract":"Combining model-based and model-free reinforcement learning approaches, this paper proposes and analyzes an $\\epsilon$-policy gradient algorithm for the online pricing learning task. The algorithm extends $\\epsilon$-greedy algorithm by replacing greedy exploitation with gradient descent step and facilitates learning via model inference. We optimize the regret of the proposed algorithm by quantifying the exploration cost in terms of the exploration probability $\\epsilon$ and the exploitation cost in terms of the gradient descent optimization and gradient estimation errors. The algorithm achieves an expected regret of order $\\mathcal{O}(\\sqrt{T})$ (up to a logarithmic factor) over $T$ trials.","sentences":["Combining model-based and model-free reinforcement learning approaches, this paper proposes and analyzes an $\\epsilon$-policy gradient algorithm for the online pricing learning task.","The algorithm extends $\\epsilon$-greedy algorithm by replacing greedy exploitation with gradient descent step and facilitates learning via model inference.","We optimize the regret of the proposed algorithm by quantifying the exploration cost in terms of the exploration probability $\\epsilon$ and the exploitation cost in terms of the gradient descent optimization and gradient estimation errors.","The algorithm achieves an expected regret of order $\\mathcal{O}(\\sqrt{T})$ (up to a logarithmic factor) over $T$ trials."],"url":"http://arxiv.org/abs/2405.03624v1"}
{"created":"2024-05-06 16:35:56","title":"Detecting Android Malware: From Neural Embeddings to Hands-On Validation with BERTroid","abstract":"As cyber threats and malware attacks increasingly alarm both individuals and businesses, the urgency for proactive malware countermeasures intensifies. This has driven a rising interest in automated machine learning solutions. Transformers, a cutting-edge category of attention-based deep learning methods, have demonstrated remarkable success. In this paper, we present BERTroid, an innovative malware detection model built on the BERT architecture. Overall, BERTroid emerged as a promising solution for combating Android malware. Its ability to outperform state-of-the-art solutions demonstrates its potential as a proactive defense mechanism against malicious software attacks. Additionally, we evaluate BERTroid on multiple datasets to assess its performance across diverse scenarios. In the dynamic landscape of cybersecurity, our approach has demonstrated promising resilience against the rapid evolution of malware on Android systems. While the machine learning model captures broad patterns, we emphasize the role of manual validation for deeper comprehension and insight into these behaviors. This human intervention is critical for discerning intricate and context-specific behaviors, thereby validating and reinforcing the model's findings.","sentences":["As cyber threats and malware attacks increasingly alarm both individuals and businesses, the urgency for proactive malware countermeasures intensifies.","This has driven a rising interest in automated machine learning solutions.","Transformers, a cutting-edge category of attention-based deep learning methods, have demonstrated remarkable success.","In this paper, we present BERTroid, an innovative malware detection model built on the BERT architecture.","Overall, BERTroid emerged as a promising solution for combating Android malware.","Its ability to outperform state-of-the-art solutions demonstrates its potential as a proactive defense mechanism against malicious software attacks.","Additionally, we evaluate BERTroid on multiple datasets to assess its performance across diverse scenarios.","In the dynamic landscape of cybersecurity, our approach has demonstrated promising resilience against the rapid evolution of malware on Android systems.","While the machine learning model captures broad patterns, we emphasize the role of manual validation for deeper comprehension and insight into these behaviors.","This human intervention is critical for discerning intricate and context-specific behaviors, thereby validating and reinforcing the model's findings."],"url":"http://arxiv.org/abs/2405.03620v1"}
{"created":"2024-05-06 16:34:44","title":"The trade-offs between Monolithic vs. Distributed Architectures","abstract":"Software architects frequently engage in trade-off analysis, often confronting sub-optimal solutions due to unforeseen or overlooked disadvantages. Such outcomes can detrimentally affect a company's business operations and resource allocation. This article conducts a critical review of archi- tectural styles, particularly focusing on the strengths and weaknesses of both monolithic and distributed architectures, and their relationship to architectural characteristics. It also explores the role of cloud computing in transitioning from monolithic to distributed-based applications. Utilizing a broad range of sources, including papers and books from both industry and academia, this research provides an overview from theoretical foundations to practical applications. A notable trend observed is a shift back from distributed to monolithic architectures, possibly due to factors such as cost, complexity, and performance.","sentences":["Software architects frequently engage in trade-off analysis, often confronting sub-optimal solutions due to unforeseen or overlooked disadvantages.","Such outcomes can detrimentally affect a company's business operations and resource allocation.","This article conducts a critical review of archi- tectural styles, particularly focusing on the strengths and weaknesses of both monolithic and distributed architectures, and their relationship to architectural characteristics.","It also explores the role of cloud computing in transitioning from monolithic to distributed-based applications.","Utilizing a broad range of sources, including papers and books from both industry and academia, this research provides an overview from theoretical foundations to practical applications.","A notable trend observed is a shift back from distributed to monolithic architectures, possibly due to factors such as cost, complexity, and performance."],"url":"http://arxiv.org/abs/2405.03619v1"}
{"created":"2024-05-06 16:32:29","title":"A Controlled Experiment on the Energy Efficiency of the Source Code Generated by Code Llama","abstract":"Context. Nowadays, 83% of software developers use Large Language Models (LLMs) to generate code. LLMs recently became essential to increase the productivity of software developers and decrease the time and cost of software development. Developers ranging from novices to experts use LLM tools not only to detect and patch bugs, but also to integrate generated code into their software. However, as of today there is no objective assessment of the energy efficiency of the source code generated by LLM tools. Released in August 2023, Code Llama is one of the most recent LLM tools.   Goal. In this paper, we present an empirical study that assesses the energy efficiency of Code Llama with respect to human-written source code.   Method. We design an experiment involving three human-written benchmarks implemented in C++, JavaScript, and Python. We ask Code Llama to generate the code of the benchmarks using different prompts and temperatures. Therefore, we execute both implementations and profile their energy efficiency.   Results. Our study shows that the energy efficiency of code generated by Code Llama is heavily-dependent on the chosen programming language and the specific code problem at hand. Also, human implementations tend to be more energy efficient overall, with generated JavaScript code outperforming its human counterpart. Moreover, explicitly asking Code Llama to generate energy-efficient code results in an equal or worse energy efficiency, as well as using different temperatures seems not to affect the energy efficiency of generated code.   Conclusions. According to our results, code generated using Code Llama does not guarantee energy efficiency, even when prompted to do so. Therefore, software developers should evaluate the energy efficiency of generated code before integrating it into the software system under development.","sentences":["Context.","Nowadays, 83% of software developers use Large Language Models (LLMs) to generate code.","LLMs recently became essential to increase the productivity of software developers and decrease the time and cost of software development.","Developers ranging from novices to experts use LLM tools not only to detect and patch bugs, but also to integrate generated code into their software.","However, as of today there is no objective assessment of the energy efficiency of the source code generated by LLM tools.","Released in August 2023, Code Llama is one of the most recent LLM tools.   Goal.","In this paper, we present an empirical study that assesses the energy efficiency of Code Llama with respect to human-written source code.   ","Method.","We design an experiment involving three human-written benchmarks implemented in C++, JavaScript, and Python.","We ask Code Llama to generate the code of the benchmarks using different prompts and temperatures.","Therefore, we execute both implementations and profile their energy efficiency.   Results.","Our study shows that the energy efficiency of code generated by Code Llama is heavily-dependent on the chosen programming language and the specific code problem at hand.","Also, human implementations tend to be more energy efficient overall, with generated JavaScript code outperforming its human counterpart.","Moreover, explicitly asking Code Llama to generate energy-efficient code results in an equal or worse energy efficiency, as well as using different temperatures seems not to affect the energy efficiency of generated code.   ","Conclusions.","According to our results, code generated using Code Llama does not guarantee energy efficiency, even when prompted to do so.","Therefore, software developers should evaluate the energy efficiency of generated code before integrating it into the software system under development."],"url":"http://arxiv.org/abs/2405.03616v1"}
{"created":"2024-05-06 16:32:01","title":"Nonnegative Matrix Factorization in Dimensionality Reduction: A Survey","abstract":"Dimensionality Reduction plays a pivotal role in improving feature learning accuracy and reducing training time by eliminating redundant features, noise, and irrelevant data. Nonnegative Matrix Factorization (NMF) has emerged as a popular and powerful method for dimensionality reduction. Despite its extensive use, there remains a need for a comprehensive analysis of NMF in the context of dimensionality reduction. To address this gap, this paper presents a comprehensive survey of NMF, focusing on its applications in both feature extraction and feature selection. We introduce a classification of dimensionality reduction, enhancing understanding of the underlying concepts. Subsequently, we delve into a thorough summary of diverse NMF approaches used for feature extraction and selection. Furthermore, we discuss the latest research trends and potential future directions of NMF in dimensionality reduction, aiming to highlight areas that need further exploration and development.","sentences":["Dimensionality Reduction plays a pivotal role in improving feature learning accuracy and reducing training time by eliminating redundant features, noise, and irrelevant data.","Nonnegative Matrix Factorization (NMF) has emerged as a popular and powerful method for dimensionality reduction.","Despite its extensive use, there remains a need for a comprehensive analysis of NMF in the context of dimensionality reduction.","To address this gap, this paper presents a comprehensive survey of NMF, focusing on its applications in both feature extraction and feature selection.","We introduce a classification of dimensionality reduction, enhancing understanding of the underlying concepts.","Subsequently, we delve into a thorough summary of diverse NMF approaches used for feature extraction and selection.","Furthermore, we discuss the latest research trends and potential future directions of NMF in dimensionality reduction, aiming to highlight areas that need further exploration and development."],"url":"http://arxiv.org/abs/2405.03615v1"}
{"created":"2024-05-06 16:31:40","title":"Repairing with Zero Skip Cost","abstract":"To measure repair latency at helper nodes, we introduce a new metric called skip cost that quantifies the number of contiguous sections accessed on a disk. We provide explicit constructions of zigzag codes and fractional repetition codes that incur zero skip cost","sentences":["To measure repair latency at helper nodes, we introduce a new metric called skip cost that quantifies the number of contiguous sections accessed on a disk.","We provide explicit constructions of zigzag codes and fractional repetition codes that incur zero skip cost"],"url":"http://arxiv.org/abs/2405.03614v1"}
{"created":"2024-05-06 16:31:19","title":"Dual Relation Mining Network for Zero-Shot Learning","abstract":"Zero-shot learning (ZSL) aims to recognize novel classes through transferring shared semantic knowledge (e.g., attributes) from seen classes to unseen classes. Recently, attention-based methods have exhibited significant progress which align visual features and attributes via a spatial attention mechanism. However, these methods only explore visual-semantic relationship in the spatial dimension, which can lead to classification ambiguity when different attributes share similar attention regions, and semantic relationship between attributes is rarely discussed. To alleviate the above problems, we propose a Dual Relation Mining Network (DRMN) to enable more effective visual-semantic interactions and learn semantic relationship among attributes for knowledge transfer. Specifically, we introduce a Dual Attention Block (DAB) for visual-semantic relationship mining, which enriches visual information by multi-level feature fusion and conducts spatial attention for visual to semantic embedding. Moreover, an attribute-guided channel attention is utilized to decouple entangled semantic features. For semantic relationship modeling, we utilize a Semantic Interaction Transformer (SIT) to enhance the generalization of attribute representations among images. Additionally, a global classification branch is introduced as a complement to human-defined semantic attributes, and we then combine the results with attribute-based classification. Extensive experiments demonstrate that the proposed DRMN leads to new state-of-the-art performances on three standard ZSL benchmarks, i.e., CUB, SUN, and AwA2.","sentences":["Zero-shot learning (ZSL) aims to recognize novel classes through transferring shared semantic knowledge (e.g., attributes) from seen classes to unseen classes.","Recently, attention-based methods have exhibited significant progress which align visual features and attributes via a spatial attention mechanism.","However, these methods only explore visual-semantic relationship in the spatial dimension, which can lead to classification ambiguity when different attributes share similar attention regions, and semantic relationship between attributes is rarely discussed.","To alleviate the above problems, we propose a Dual Relation Mining Network (DRMN) to enable more effective visual-semantic interactions and learn semantic relationship among attributes for knowledge transfer.","Specifically, we introduce a Dual Attention Block (DAB) for visual-semantic relationship mining, which enriches visual information by multi-level feature fusion and conducts spatial attention for visual to semantic embedding.","Moreover, an attribute-guided channel attention is utilized to decouple entangled semantic features.","For semantic relationship modeling, we utilize a Semantic Interaction Transformer (SIT) to enhance the generalization of attribute representations among images.","Additionally, a global classification branch is introduced as a complement to human-defined semantic attributes, and we then combine the results with attribute-based classification.","Extensive experiments demonstrate that the proposed DRMN leads to new state-of-the-art performances on three standard ZSL benchmarks, i.e., CUB, SUN, and AwA2."],"url":"http://arxiv.org/abs/2405.03613v1"}
{"created":"2024-05-06 16:23:12","title":"Decision algorithms for reversibility of one-dimensional non-linear cellular automata under null boundary conditions","abstract":"The property of reversibility is quite meaningful for the classic theoretical computer science model, cellular automata. For the reversibility problem for a CA under null boundary conditions, while linear rules have been studied a lot, the non-linear rules remain unexplored at present. The paper investigates the reversibility problem of general one-dimensional CA on a finite field $\\mathbb{Z}_p$, and proposes an approach to optimize the Amoroso's infinite CA surjectivity detection algorithm. This paper proposes algorithms for deciding the reversibility of one-dimensional CA under null boundary conditions. We propose a method to decide the strict reversibility of one-dimensional CA under null boundary conditions. We also provide a bucket chain based algorithm for calculating the reversibility function of one-dimensional CA under null boundary conditions. These decision algorithms work for not only linear rules but also non-linear rules. In addition, it has been confirmed that the reversibility function always has a period, and its periodicity is related to the periodicity of the corresponding bucket chain. Some of our experiment results of reversible CA are presented in the paper, complementing and validating the theoretical aspects, and thereby further supporting the research conclusions of this paper.","sentences":["The property of reversibility is quite meaningful for the classic theoretical computer science model, cellular automata.","For the reversibility problem for a CA under null boundary conditions, while linear rules have been studied a lot, the non-linear rules remain unexplored at present.","The paper investigates the reversibility problem of general one-dimensional CA on a finite field $\\mathbb{Z}_p$, and proposes an approach to optimize the Amoroso's infinite CA surjectivity detection algorithm.","This paper proposes algorithms for deciding the reversibility of one-dimensional CA under null boundary conditions.","We propose a method to decide the strict reversibility of one-dimensional CA under null boundary conditions.","We also provide a bucket chain based algorithm for calculating the reversibility function of one-dimensional CA under null boundary conditions.","These decision algorithms work for not only linear rules but also non-linear rules.","In addition, it has been confirmed that the reversibility function always has a period, and its periodicity is related to the periodicity of the corresponding bucket chain.","Some of our experiment results of reversible CA are presented in the paper, complementing and validating the theoretical aspects, and thereby further supporting the research conclusions of this paper."],"url":"http://arxiv.org/abs/2405.03609v1"}
{"created":"2024-05-06 16:17:33","title":"Trackable Island-model Genetic Algorithms at Wafer Scale","abstract":"Emerging ML/AI hardware accelerators, like the 850,000 processor Cerebras Wafer-Scale Engine (WSE), hold great promise to scale up the capabilities of evolutionary computation. However, challenges remain in maintaining visibility into underlying evolutionary processes while efficiently utilizing these platforms' large processor counts. Here, we focus on the problem of extracting phylogenetic information from digital evolution on the WSE platform. We present a tracking-enabled asynchronous island-based genetic algorithm (GA) framework for WSE hardware. Emulated and on-hardware GA benchmarks with a simple tracking-enabled agent model clock upwards of 1 million generations a minute for population sizes reaching 16 million. This pace enables quadrillions of evaluations a day. We validate phylogenetic reconstructions from these trials and demonstrate their suitability for inference of underlying evolutionary conditions. In particular, we demonstrate extraction of clear phylometric signals that differentiate wafer-scale runs with adaptive dynamics enabled versus disabled. Together, these benchmark and validation trials reflect strong potential for highly scalable evolutionary computation that is both efficient and observable. Kernel code implementing the island-model GA supports drop-in customization to support any fixed-length genome content and fitness criteria, allowing it to be leveraged to advance research interests across the community.","sentences":["Emerging ML/AI hardware accelerators, like the 850,000 processor Cerebras Wafer-Scale Engine (WSE), hold great promise to scale up the capabilities of evolutionary computation.","However, challenges remain in maintaining visibility into underlying evolutionary processes while efficiently utilizing these platforms' large processor counts.","Here, we focus on the problem of extracting phylogenetic information from digital evolution on the WSE platform.","We present a tracking-enabled asynchronous island-based genetic algorithm (GA) framework for WSE hardware.","Emulated and on-hardware GA benchmarks with a simple tracking-enabled agent model clock upwards of 1 million generations a minute for population sizes reaching 16 million.","This pace enables quadrillions of evaluations a day.","We validate phylogenetic reconstructions from these trials and demonstrate their suitability for inference of underlying evolutionary conditions.","In particular, we demonstrate extraction of clear phylometric signals that differentiate wafer-scale runs with adaptive dynamics enabled versus disabled.","Together, these benchmark and validation trials reflect strong potential for highly scalable evolutionary computation that is both efficient and observable.","Kernel code implementing the island-model GA supports drop-in customization to support any fixed-length genome content and fitness criteria, allowing it to be leveraged to advance research interests across the community."],"url":"http://arxiv.org/abs/2405.03605v1"}
{"created":"2024-05-06 16:04:03","title":"GREEN: Generative Radiology Report Evaluation and Error Notation","abstract":"Evaluating radiology reports is a challenging problem as factual correctness is extremely important due to the need for accurate medical communication about medical images. Existing automatic evaluation metrics either suffer from failing to consider factual correctness (e.g., BLEU and ROUGE) or are limited in their interpretability (e.g., F1CheXpert and F1RadGraph). In this paper, we introduce GREEN (Generative Radiology Report Evaluation and Error Notation), a radiology report generation metric that leverages the natural language understanding of language models to identify and explain clinically significant errors in candidate reports, both quantitatively and qualitatively. Compared to current metrics, GREEN offers: 1) a score aligned with expert preferences, 2) human interpretable explanations of clinically significant errors, enabling feedback loops with end-users, and 3) a lightweight open-source method that reaches the performance of commercial counterparts. We validate our GREEN metric by comparing it to GPT-4, as well as to error counts of 6 experts and preferences of 2 experts. Our method demonstrates not only higher correlation with expert error counts, but simultaneously higher alignment with expert preferences when compared to previous approaches.\"","sentences":["Evaluating radiology reports is a challenging problem as factual correctness is extremely important due to the need for accurate medical communication about medical images.","Existing automatic evaluation metrics either suffer from failing to consider factual correctness (e.g., BLEU and ROUGE) or are limited in their interpretability (e.g., F1CheXpert and F1RadGraph).","In this paper, we introduce GREEN (Generative Radiology Report Evaluation and Error Notation), a radiology report generation metric that leverages the natural language understanding of language models to identify and explain clinically significant errors in candidate reports, both quantitatively and qualitatively.","Compared to current metrics, GREEN offers: 1) a score aligned with expert preferences, 2) human interpretable explanations of clinically significant errors, enabling feedback loops with end-users, and 3) a lightweight open-source method that reaches the performance of commercial counterparts.","We validate our GREEN metric by comparing it to GPT-4, as well as to error counts of 6 experts and preferences of 2 experts.","Our method demonstrates not only higher correlation with expert error counts, but simultaneously higher alignment with expert preferences when compared to previous approaches.\""],"url":"http://arxiv.org/abs/2405.03595v1"}
{"created":"2024-05-06 16:03:32","title":"Enabling High-Sparsity Foundational Llama Models with Efficient Pretraining and Deployment","abstract":"Large language models (LLMs) have revolutionized Natural Language Processing (NLP), but their size creates computational bottlenecks. We introduce a novel approach to create accurate, sparse foundational versions of performant LLMs that achieve full accuracy recovery for fine-tuning tasks at up to 70% sparsity. We achieve this for the LLaMA-2 7B model by combining the SparseGPT one-shot pruning method and sparse pretraining of those models on a subset of the SlimPajama dataset mixed with a Python subset of The Stack dataset. We exhibit training acceleration due to sparsity on Cerebras CS-3 chips that closely matches theoretical scaling. In addition, we establish inference acceleration of up to 3x on CPUs by utilizing Neural Magic's DeepSparse engine and 1.7x on GPUs through Neural Magic's nm-vllm engine. The above gains are realized via sparsity alone, thus enabling further gains through additional use of quantization. Specifically, we show a total speedup on CPUs for sparse-quantized LLaMA models of up to 8.6x. We demonstrate these results across diverse, challenging tasks, including chat, instruction following, code generation, arithmetic reasoning, and summarization to prove their generality. This work paves the way for rapidly creating smaller and faster LLMs without sacrificing accuracy.","sentences":["Large language models (LLMs) have revolutionized Natural Language Processing (NLP), but their size creates computational bottlenecks.","We introduce a novel approach to create accurate, sparse foundational versions of performant LLMs that achieve full accuracy recovery for fine-tuning tasks at up to 70% sparsity.","We achieve this for the LLaMA-2 7B model by combining the SparseGPT one-shot pruning method and sparse pretraining of those models on a subset of the SlimPajama dataset mixed with a Python subset of The Stack dataset.","We exhibit training acceleration due to sparsity on Cerebras CS-3 chips that closely matches theoretical scaling.","In addition, we establish inference acceleration of up to 3x on CPUs by utilizing Neural Magic's DeepSparse engine and 1.7x on GPUs through Neural Magic's nm-vllm engine.","The above gains are realized via sparsity alone, thus enabling further gains through additional use of quantization.","Specifically, we show a total speedup on CPUs for sparse-quantized LLaMA models of up to 8.6x.","We demonstrate these results across diverse, challenging tasks, including chat, instruction following, code generation, arithmetic reasoning, and summarization to prove their generality.","This work paves the way for rapidly creating smaller and faster LLMs without sacrificing accuracy."],"url":"http://arxiv.org/abs/2405.03594v1"}
{"created":"2024-05-06 15:58:03","title":"The Sociotechnical Stack: Opportunities for Social Computing Research in Non-consensual Intimate Media","abstract":"Non-consensual intimate media (NCIM) involves sharing intimate content without the depicted person's consent, including \"revenge porn\" and sexually explicit deepfakes. While NCIM has received attention in legal, psychological, and communication fields over the past decade, it is not sufficiently addressed in computing scholarship. This paper addresses this gap by linking NCIM harms to the specific technological components that facilitate them. We introduce the sociotechnical stack, a conceptual framework designed to map the technical stack to its corresponding social impacts. The sociotechnical stack allows us to analyze sociotechnical problems like NCIM, and points toward opportunities for computing research. We propose a research roadmap for computing and social computing communities to deter NCIM perpetration and support victim-survivors through building and rebuilding technologies.","sentences":["Non-consensual intimate media (NCIM) involves sharing intimate content without the depicted person's consent, including \"revenge porn\" and sexually explicit deepfakes.","While NCIM has received attention in legal, psychological, and communication fields over the past decade, it is not sufficiently addressed in computing scholarship.","This paper addresses this gap by linking NCIM harms to the specific technological components that facilitate them.","We introduce the sociotechnical stack, a conceptual framework designed to map the technical stack to its corresponding social impacts.","The sociotechnical stack allows us to analyze sociotechnical problems like NCIM, and points toward opportunities for computing research.","We propose a research roadmap for computing and social computing communities to deter NCIM perpetration and support victim-survivors through building and rebuilding technologies."],"url":"http://arxiv.org/abs/2405.03585v1"}
{"created":"2024-05-06 15:53:55","title":"Functional Latent Dynamics for Irregularly Sampled Time Series Forecasting","abstract":"Irregularly sampled time series with missing values are often observed in multiple real-world applications such as healthcare, climate and astronomy. They pose a significant challenge to standard deep learn- ing models that operate only on fully observed and regularly sampled time series. In order to capture the continuous dynamics of the irreg- ular time series, many models rely on solving an Ordinary Differential Equation (ODE) in the hidden state. These ODE-based models tend to perform slow and require large memory due to sequential operations and a complex ODE solver. As an alternative to complex ODE-based mod- els, we propose a family of models called Functional Latent Dynamics (FLD). Instead of solving the ODE, we use simple curves which exist at all time points to specify the continuous latent state in the model. The coefficients of these curves are learned only from the observed values in the time series ignoring the missing values. Through extensive experi- ments, we demonstrate that FLD achieves better performance compared to the best ODE-based model while reducing the runtime and memory overhead. Specifically, FLD requires an order of magnitude less time to infer the forecasts compared to the best performing forecasting model.","sentences":["Irregularly sampled time series with missing values are often observed in multiple real-world applications such as healthcare, climate and astronomy.","They pose a significant challenge to standard deep learn- ing models that operate only on fully observed and regularly sampled time series.","In order to capture the continuous dynamics of the irreg- ular time series, many models rely on solving an Ordinary Differential Equation (ODE) in the hidden state.","These ODE-based models tend to perform slow and require large memory due to sequential operations and a complex ODE solver.","As an alternative to complex ODE-based mod- els, we propose a family of models called Functional Latent Dynamics (FLD).","Instead of solving the ODE, we use simple curves which exist at all time points to specify the continuous latent state in the model.","The coefficients of these curves are learned only from the observed values in the time series ignoring the missing values.","Through extensive experi- ments, we demonstrate that FLD achieves better performance compared to the best ODE-based model while reducing the runtime and memory overhead.","Specifically, FLD requires an order of magnitude less time to infer the forecasts compared to the best performing forecasting model."],"url":"http://arxiv.org/abs/2405.03582v1"}
{"created":"2024-05-06 15:49:46","title":"ILILT: Implicit Learning of Inverse Lithography Technologies","abstract":"Lithography, transferring chip design masks to the silicon wafer, is the most important phase in modern semiconductor manufacturing flow. Due to the limitations of lithography systems, Extensive design optimizations are required to tackle the design and silicon mismatch. Inverse lithography technology (ILT) is one of the promising solutions to perform pre-fabrication optimization, termed mask optimization. Because of mask optimization problems' constrained non-convexity, numerical ILT solvers rely heavily on good initialization to avoid getting stuck on sub-optimal solutions. Machine learning (ML) techniques are hence proposed to generate mask initialization for ILT solvers with one-shot inference, targeting faster and better convergence during ILT. This paper addresses the question of \\textit{whether ML models can directly generate high-quality optimized masks without engaging ILT solvers in the loop}. We propose an implicit learning ILT framework: ILILT, which leverages the implicit layer learning method and lithography-conditioned inputs to ground the model. Trained to understand the ILT optimization procedure, ILILT can outperform the state-of-the-art machine learning solutions, significantly improving efficiency and quality.","sentences":["Lithography, transferring chip design masks to the silicon wafer, is the most important phase in modern semiconductor manufacturing flow.","Due to the limitations of lithography systems, Extensive design optimizations are required to tackle the design and silicon mismatch.","Inverse lithography technology (ILT) is one of the promising solutions to perform pre-fabrication optimization, termed mask optimization.","Because of mask optimization problems' constrained non-convexity, numerical ILT solvers rely heavily on good initialization to avoid getting stuck on sub-optimal solutions.","Machine learning (ML) techniques are hence proposed to generate mask initialization for ILT solvers with one-shot inference, targeting faster and better convergence during ILT.","This paper addresses the question of \\textit{whether ML models can directly generate high-quality optimized masks without engaging ILT solvers in the loop}.","We propose an implicit learning ILT framework: ILILT, which leverages the implicit layer learning method and lithography-conditioned inputs to ground the model.","Trained to understand the ILT optimization procedure, ILILT can outperform the state-of-the-art machine learning solutions, significantly improving efficiency and quality."],"url":"http://arxiv.org/abs/2405.03574v1"}
{"created":"2024-05-06 15:48:22","title":"Demystifying Anonymity: Uncovering the Structure Underlying \"Read-Write Wait-Free Covering\"","abstract":"The study of particular synchronization problems in anonymous shared-memory models -- be it processor anonymity, memory anonymity, or full anonymity -- has produced ad hoc, so-called covering arguments in which processors overwrite each other's writes. Those arguments give us proverbial fish, but they do not teach us how to fish. In this paper, we take a step back to ask more general questions.   First, what does it mean to solve a task under processor anonymity? With tasks such as renaming, the traditional notion obviously does not apply. Instead of restricting ourselves to colorless tasks, we propose using the notion of group solvability, which allows transferring any task to processor-anonymous models.   Second, we consider solving tasks read-write wait-free under full anonymity, and we ask what we call the eventual-pattern question: if anonymous processors forever read and write in anonymous shared-memory, learning about inputs of other processors, what is the structure of the eventually-stable sets of inputs that processors learn? Solving the eventual-pattern question leads us to a group solution to the snapshot task and to M(M-1)/2-renaming, where M is the number of distinct inputs. Finally, using the snapshot solution, we easily obtain a solution to obstruction-free consensus.","sentences":["The study of particular synchronization problems in anonymous shared-memory models -- be it processor anonymity, memory anonymity, or full anonymity -- has produced ad hoc, so-called covering arguments in which processors overwrite each other's writes.","Those arguments give us proverbial fish, but they do not teach us how to fish.","In this paper, we take a step back to ask more general questions.   ","First, what does it mean to solve a task under processor anonymity?","With tasks such as renaming, the traditional notion obviously does not apply.","Instead of restricting ourselves to colorless tasks, we propose using the notion of group solvability, which allows transferring any task to processor-anonymous models.   ","Second, we consider solving tasks read-write wait-free under full anonymity, and we ask what we call the eventual-pattern question: if anonymous processors forever read and write in anonymous shared-memory, learning about inputs of other processors, what is the structure of the eventually-stable sets of inputs that processors learn?","Solving the eventual-pattern question leads us to a group solution to the snapshot task and to M(M-1)/2-renaming, where M is the number of distinct inputs.","Finally, using the snapshot solution, we easily obtain a solution to obstruction-free consensus."],"url":"http://arxiv.org/abs/2405.03573v1"}
{"created":"2024-05-06 15:48:14","title":"RoboCar: A Rapidly Deployable Open-Source Platform for Autonomous Driving Research","abstract":"This paper introduces RoboCar, an open-source research platform for autonomous driving developed at the University of Luxembourg. RoboCar provides a modular, cost-effective framework for the development of experimental Autonomous Driving Systems (ADS), utilizing the 2018 KIA Soul EV. The platform integrates a robust hardware and software architecture that aligns with the vehicle's existing systems, minimizing the need for extensive modifications. It supports various autonomous driving functions and has undergone real-world testing on public roads in Luxembourg City. This paper outlines the platform's architecture, integration challenges, and initial test results, offering insights into its application in advancing autonomous driving research. RoboCar is available to anyone at https://github.com/sntubix/robocar and is released under an open-source MIT license.","sentences":["This paper introduces RoboCar, an open-source research platform for autonomous driving developed at the University of Luxembourg.","RoboCar provides a modular, cost-effective framework for the development of experimental Autonomous Driving Systems (ADS), utilizing the 2018 KIA Soul EV.","The platform integrates a robust hardware and software architecture that aligns with the vehicle's existing systems, minimizing the need for extensive modifications.","It supports various autonomous driving functions and has undergone real-world testing on public roads in Luxembourg City.","This paper outlines the platform's architecture, integration challenges, and initial test results, offering insights into its application in advancing autonomous driving research.","RoboCar is available to anyone at https://github.com/sntubix/robocar and is released under an open-source MIT license."],"url":"http://arxiv.org/abs/2405.03572v1"}
{"created":"2024-05-06 15:42:30","title":"Majority consensus thresholds in competitive Lotka--Volterra populations","abstract":"One of the key challenges in synthetic biology is devising robust signaling primitives for engineered microbial consortia. In such systems, a fundamental signal amplification problem is the majority consensus problem: given a system with two input species with initial difference of $\\Delta$ in population sizes, what is the probability that the system reaches a state in which only the initial majority species is present?   In this work, we consider a discrete and stochastic version of competitive Lotka--Volterra dynamics, a standard model of microbial community dynamics. We identify new threshold properties for majority consensus under different types of interference competition:   - We show that under so-called self-destructive interference competition between the two input species, majority consensus can be reached with high probability if the initial difference satisfies $\\Delta \\in \\Omega(\\log^2 n)$, where $n$ is the initial population size. This gives an exponential improvement compared to the previously known bound of $\\Omega(\\sqrt{n \\log n})$ by Cho et al. [Distributed Computing, 2021] given for a special case of the competitive Lotka--Volterra model. In contrast, we show that an initial gap of $\\Delta \\in \\Omega(\\sqrt{\\log n})$ is necessary.   - On the other hand, we prove that under non-self-destructive interference competition, an initial gap of $\\Omega(\\sqrt{n})$ is necessary to succeed with high probability and that a $\\Omega(\\sqrt{n \\log n})$ gap is sufficient.   This shows a strong qualitative gap between the performance of self-destructive and non-self-destructive interference competition. Moreover, we show that if in addition the populations exhibit interference competition between the individuals of the same species, then majority consensus cannot always be solved with high probability, no matter what the difference in the initial population counts.","sentences":["One of the key challenges in synthetic biology is devising robust signaling primitives for engineered microbial consortia.","In such systems, a fundamental signal amplification problem is the majority consensus problem: given a system with two input species with initial difference of $\\Delta$ in population sizes, what is the probability that the system reaches a state in which only the initial majority species is present?   ","In this work, we consider a discrete and stochastic version of competitive Lotka--Volterra dynamics, a standard model of microbial community dynamics.","We identify new threshold properties for majority consensus under different types of interference competition:   - We show that under so-called self-destructive interference competition between the two input species, majority consensus can be reached with high probability if the initial difference satisfies $\\Delta \\in \\Omega(\\log^2 n)$, where $n$ is the initial population size.","This gives an exponential improvement compared to the previously known bound of $\\Omega(\\sqrt{n \\log n})$ by Cho et al.","[Distributed Computing, 2021] given for a special case of the competitive Lotka--Volterra model.","In contrast, we show that an initial gap of $\\Delta \\in \\Omega(\\sqrt{\\log n})$ is necessary.   -","On the other hand, we prove that under non-self-destructive interference competition, an initial gap of $\\Omega(\\sqrt{n})$ is necessary to succeed with high probability and that a $\\Omega(\\sqrt{n \\log n})$ gap is sufficient.   ","This shows a strong qualitative gap between the performance of self-destructive and non-self-destructive interference competition.","Moreover, we show that if in addition the populations exhibit interference competition between the individuals of the same species, then majority consensus cannot always be solved with high probability, no matter what the difference in the initial population counts."],"url":"http://arxiv.org/abs/2405.03568v1"}
{"created":"2024-05-06 15:41:41","title":"Deep Space Separable Distillation for Lightweight Acoustic Scene Classification","abstract":"Acoustic scene classification (ASC) is highly important in the real world. Recently, deep learning-based methods have been widely employed for acoustic scene classification. However, these methods are currently not lightweight enough as well as their performance is not satisfactory. To solve these problems, we propose a deep space separable distillation network. Firstly, the network performs high-low frequency decomposition on the log-mel spectrogram, significantly reducing computational complexity while maintaining model performance. Secondly, we specially design three lightweight operators for ASC, including Separable Convolution (SC), Orthonormal Separable Convolution (OSC), and Separable Partial Convolution (SPC). These operators exhibit highly efficient feature extraction capabilities in acoustic scene classification tasks. The experimental results demonstrate that the proposed method achieves a performance gain of 9.8% compared to the currently popular deep learning methods, while also having smaller parameter count and computational complexity.","sentences":["Acoustic scene classification (ASC) is highly important in the real world.","Recently, deep learning-based methods have been widely employed for acoustic scene classification.","However, these methods are currently not lightweight enough as well as their performance is not satisfactory.","To solve these problems, we propose a deep space separable distillation network.","Firstly, the network performs high-low frequency decomposition on the log-mel spectrogram, significantly reducing computational complexity while maintaining model performance.","Secondly, we specially design three lightweight operators for ASC, including Separable Convolution (SC), Orthonormal Separable Convolution (OSC), and Separable Partial Convolution (SPC).","These operators exhibit highly efficient feature extraction capabilities in acoustic scene classification tasks.","The experimental results demonstrate that the proposed method achieves a performance gain of 9.8% compared to the currently popular deep learning methods, while also having smaller parameter count and computational complexity."],"url":"http://arxiv.org/abs/2405.03567v1"}
{"created":"2024-05-06 15:38:32","title":"Liberating Seen Classes: Boosting Few-Shot and Zero-Shot Text Classification via Anchor Generation and Classification Reframing","abstract":"Few-shot and zero-shot text classification aim to recognize samples from novel classes with limited labeled samples or no labeled samples at all. While prevailing methods have shown promising performance via transferring knowledge from seen classes to unseen classes, they are still limited by (1) Inherent dissimilarities among classes make the transformation of features learned from seen classes to unseen classes both difficult and inefficient. (2) Rare labeled novel samples usually cannot provide enough supervision signals to enable the model to adjust from the source distribution to the target distribution, especially for complicated scenarios. To alleviate the above issues, we propose a simple and effective strategy for few-shot and zero-shot text classification. We aim to liberate the model from the confines of seen classes, thereby enabling it to predict unseen categories without the necessity of training on seen classes. Specifically, for mining more related unseen category knowledge, we utilize a large pre-trained language model to generate pseudo novel samples, and select the most representative ones as category anchors. After that, we convert the multi-class classification task into a binary classification task and use the similarities of query-anchor pairs for prediction to fully leverage the limited supervision signals. Extensive experiments on six widely used public datasets show that our proposed method can outperform other strong baselines significantly in few-shot and zero-shot tasks, even without using any seen class samples.","sentences":["Few-shot and zero-shot text classification aim to recognize samples from novel classes with limited labeled samples or no labeled samples at all.","While prevailing methods have shown promising performance via transferring knowledge from seen classes to unseen classes, they are still limited by (1) Inherent dissimilarities among classes make the transformation of features learned from seen classes to unseen classes both difficult and inefficient.","(2) Rare labeled novel samples usually cannot provide enough supervision signals to enable the model to adjust from the source distribution to the target distribution, especially for complicated scenarios.","To alleviate the above issues, we propose a simple and effective strategy for few-shot and zero-shot text classification.","We aim to liberate the model from the confines of seen classes, thereby enabling it to predict unseen categories without the necessity of training on seen classes.","Specifically, for mining more related unseen category knowledge, we utilize a large pre-trained language model to generate pseudo novel samples, and select the most representative ones as category anchors.","After that, we convert the multi-class classification task into a binary classification task and use the similarities of query-anchor pairs for prediction to fully leverage the limited supervision signals.","Extensive experiments on six widely used public datasets show that our proposed method can outperform other strong baselines significantly in few-shot and zero-shot tasks, even without using any seen class samples."],"url":"http://arxiv.org/abs/2405.03565v1"}
{"created":"2024-05-06 15:34:31","title":"ID-centric Pre-training for Recommendation","abstract":"Classical sequential recommendation models generally adopt ID embeddings to store knowledge learned from user historical behaviors and represent items. However, these unique IDs are challenging to be transferred to new domains. With the thriving of pre-trained language model (PLM), some pioneer works adopt PLM for pre-trained recommendation, where modality information (e.g., text) is considered universal across domains via PLM. Unfortunately, the behavioral information in ID embeddings is still verified to be dominating in PLM-based recommendation models compared to modality information and thus limits these models' performance. In this work, we propose a novel ID-centric recommendation pre-training paradigm (IDP), which directly transfers informative ID embeddings learned in pre-training domains to item representations in new domains. Specifically, in pre-training stage, besides the ID-based sequential model for recommendation, we also build a Cross-domain ID-matcher (CDIM) learned by both behavioral and modality information. In the tuning stage, modality information of new domain items is regarded as a cross-domain bridge built by CDIM. We first leverage the textual information of downstream domain items to retrieve behaviorally and semantically similar items from pre-training domains using CDIM. Next, these retrieved pre-trained ID embeddings, rather than certain textual embeddings, are directly adopted to generate downstream new items' embeddings. Through extensive experiments on real-world datasets, both in cold and warm settings, we demonstrate that our proposed model significantly outperforms all baselines. Codes will be released upon acceptance.","sentences":["Classical sequential recommendation models generally adopt ID embeddings to store knowledge learned from user historical behaviors and represent items.","However, these unique IDs are challenging to be transferred to new domains.","With the thriving of pre-trained language model (PLM), some pioneer works adopt PLM for pre-trained recommendation, where modality information (e.g., text) is considered universal across domains via PLM.","Unfortunately, the behavioral information in ID embeddings is still verified to be dominating in PLM-based recommendation models compared to modality information and thus limits these models' performance.","In this work, we propose a novel ID-centric recommendation pre-training paradigm (IDP), which directly transfers informative ID embeddings learned in pre-training domains to item representations in new domains.","Specifically, in pre-training stage, besides the ID-based sequential model for recommendation, we also build a Cross-domain ID-matcher (CDIM) learned by both behavioral and modality information.","In the tuning stage, modality information of new domain items is regarded as a cross-domain bridge built by CDIM.","We first leverage the textual information of downstream domain items to retrieve behaviorally and semantically similar items from pre-training domains using CDIM.","Next, these retrieved pre-trained ID embeddings, rather than certain textual embeddings, are directly adopted to generate downstream new items' embeddings.","Through extensive experiments on real-world datasets, both in cold and warm settings, we demonstrate that our proposed model significantly outperforms all baselines.","Codes will be released upon acceptance."],"url":"http://arxiv.org/abs/2405.03562v1"}
{"created":"2024-05-06 15:32:09","title":"Model- and Data-Based Control of Self-Balancing Robots: Practical Educational Approach with LabVIEW and Arduino","abstract":"A two-wheeled self-balancing robot (TWSBR) is non-linear and unstable system. This study compares the performance of model-based and data-based control strategies for TWSBRs, with an explicit practical educational approach. Model-based control (MBC) algorithms such as Lead-Lag and PID control require a proficient dynamic modeling and mathematical manipulation to drive the linearized equations of motions and develop the appropriate controller. On the other side, data-based control (DBC) methods, like fuzzy control, provide a simpler and quicker approach to designing effective controllers without needing in-depth understanding of the system model. In this paper, the advantages and disadvantages of both MBC and DBC using a TWSBR are illustrated. All controllers were implemented and tested on the OSOYOO self-balancing kit, including an Arduino microcontroller, MPU-6050 sensor, and DC motors. The control law and the user interface are constructed using the LabVIEW-LINX toolkit. A real-time hardware-in-loop experiment validates the results, highlighting controllers that can be implemented on a cost-effective platform.","sentences":["A two-wheeled self-balancing robot (TWSBR) is non-linear and unstable system.","This study compares the performance of model-based and data-based control strategies for TWSBRs, with an explicit practical educational approach.","Model-based control (MBC) algorithms such as Lead-Lag and PID control require a proficient dynamic modeling and mathematical manipulation to drive the linearized equations of motions and develop the appropriate controller.","On the other side, data-based control (DBC) methods, like fuzzy control, provide a simpler and quicker approach to designing effective controllers without needing in-depth understanding of the system model.","In this paper, the advantages and disadvantages of both MBC and DBC using a TWSBR are illustrated.","All controllers were implemented and tested on the OSOYOO self-balancing kit, including an Arduino microcontroller, MPU-6050 sensor, and DC motors.","The control law and the user interface are constructed using the LabVIEW-LINX toolkit.","A real-time hardware-in-loop experiment validates the results, highlighting controllers that can be implemented on a cost-effective platform."],"url":"http://arxiv.org/abs/2405.03561v1"}
{"created":"2024-05-06 15:25:48","title":"A Comprehensive Overview and Survey of O-RAN: Exploring Slicing-aware Architecture, Deployment Options, and Use Cases","abstract":"Open-radio access network (O-RAN) seeks to establish principles of openness, programmability, automation, intelligence, and hardware-software disaggregation with interoperable interfaces. It advocates for multi-vendorism and multi-stakeholderism within a cloudified and virtualized wireless infrastructure, aimed at enhancing the deployment, operation, and maintenance of RAN architecture. This enhancement promises increased flexibility, performance optimization, service innovation, energy efficiency, and cost efficiency in fifth-generation (5G), sixth-generation (6G), and future networks. One of the key features of the O-RAN architecture is its support for network slicing, which entails interaction with other slicing domains within a mobile network, notably the transport network (TN) domain and the core network (CN) domain, to realize end-to-end (E2E) network slicing. The study of this feature requires exploring the stances and contributions of diverse standards development organizations (SDOs). In this context, we note that despite the ongoing industrial deployments and standardization efforts, the research and standardization communities have yet to comprehensively address network slicing in O-RAN. To address this gap, this survey paper provides a comprehensive exploration of network slicing in O-RAN through an in-depth review of specification documents from O-RAN Alliance and research papers from leading industry and academic institutions. The paper commences with an overview of the ongoing standardization efforts and open-source contributions associated with O-RAN, subsequently delving into the latest O-RAN architecture with an emphasis on its slicing aspects. Further, the paper explores deployment scenarios for network slicing within O-RAN, examining options for the deployment and orchestration of O-RAN and TN network slice subnets...","sentences":["Open-radio access network (O-RAN) seeks to establish principles of openness, programmability, automation, intelligence, and hardware-software disaggregation with interoperable interfaces.","It advocates for multi-vendorism and multi-stakeholderism within a cloudified and virtualized wireless infrastructure, aimed at enhancing the deployment, operation, and maintenance of RAN architecture.","This enhancement promises increased flexibility, performance optimization, service innovation, energy efficiency, and cost efficiency in fifth-generation (5G), sixth-generation (6G), and future networks.","One of the key features of the O-RAN architecture is its support for network slicing, which entails interaction with other slicing domains within a mobile network, notably the transport network (TN) domain and the core network (CN) domain, to realize end-to-end (E2E) network slicing.","The study of this feature requires exploring the stances and contributions of diverse standards development organizations (SDOs).","In this context, we note that despite the ongoing industrial deployments and standardization efforts, the research and standardization communities have yet to comprehensively address network slicing in O-RAN.","To address this gap, this survey paper provides a comprehensive exploration of network slicing in O-RAN through an in-depth review of specification documents from O-RAN Alliance and research papers from leading industry and academic institutions.","The paper commences with an overview of the ongoing standardization efforts and open-source contributions associated with O-RAN, subsequently delving into the latest O-RAN architecture with an emphasis on its slicing aspects.","Further, the paper explores deployment scenarios for network slicing within O-RAN, examining options for the deployment and orchestration of O-RAN and TN network slice subnets..."],"url":"http://arxiv.org/abs/2405.03555v1"}
{"created":"2024-05-06 15:20:30","title":"AlphaMath Almost Zero: process Supervision without process","abstract":"Recent advancements in large language models (LLMs) have substantially enhanced their mathematical reasoning abilities. However, these models still struggle with complex problems that require multiple reasoning steps, frequently leading to logical or numerical errors. While numerical mistakes can largely be addressed by integrating a code interpreter, identifying logical errors within intermediate steps is more challenging. Moreover, manually annotating these steps for training is not only expensive but also demands specialized expertise. In this study, we introduce an innovative approach that eliminates the need for manual annotation by leveraging the Monte Carlo Tree Search (MCTS) framework to generate both the process supervision and evaluation signals automatically. Essentially, when a LLM is well pre-trained, only the mathematical questions and their final answers are required to generate our training data, without requiring the solutions. We proceed to train a step-level value model designed to improve the LLM's inference process in mathematical domains. Our experiments indicate that using automatically generated solutions by LLMs enhanced with MCTS significantly improves the model's proficiency in dealing with intricate mathematical reasoning tasks.","sentences":["Recent advancements in large language models (LLMs) have substantially enhanced their mathematical reasoning abilities.","However, these models still struggle with complex problems that require multiple reasoning steps, frequently leading to logical or numerical errors.","While numerical mistakes can largely be addressed by integrating a code interpreter, identifying logical errors within intermediate steps is more challenging.","Moreover, manually annotating these steps for training is not only expensive but also demands specialized expertise.","In this study, we introduce an innovative approach that eliminates the need for manual annotation by leveraging the Monte Carlo Tree Search (MCTS) framework to generate both the process supervision and evaluation signals automatically.","Essentially, when a LLM is well pre-trained, only the mathematical questions and their final answers are required to generate our training data, without requiring the solutions.","We proceed to train a step-level value model designed to improve the LLM's inference process in mathematical domains.","Our experiments indicate that using automatically generated solutions by LLMs enhanced with MCTS significantly improves the model's proficiency in dealing with intricate mathematical reasoning tasks."],"url":"http://arxiv.org/abs/2405.03553v1"}
{"created":"2024-05-06 15:11:38","title":"MAmmoTH2: Scaling Instructions from the Web","abstract":"Instruction tuning improves the reasoning abilities of large language models (LLMs), with data quality and scalability being the crucial factors. Most instruction tuning data come from human crowd-sourcing or GPT-4 distillation. We propose a paradigm to efficiently harvest 10 million naturally existing instruction data from the pre-training web corpus to enhance LLM reasoning. Our approach involves (1) recalling relevant documents, (2) extracting instruction-response pairs, and (3) refining the extracted pairs using open-source LLMs. Fine-tuning base LLMs on this dataset, we build MAmmoTH2 models, which significantly boost performance on reasoning benchmarks. Notably, MAmmoTH2-7B's (Mistral) performance increases from 11% to 34% on MATH and from 36% to 67% on GSM8K without training on any in-domain data. Further training MAmmoTH2 on public instruction tuning datasets yields MAmmoTH2-Plus, achieving state-of-the-art performance on several reasoning and chatbot benchmarks. Our work demonstrates how to harvest large-scale, high-quality instruction data without costly human annotation or GPT-4 distillation, providing a new paradigm for building better instruction tuning data.","sentences":["Instruction tuning improves the reasoning abilities of large language models (LLMs), with data quality and scalability being the crucial factors.","Most instruction tuning data come from human crowd-sourcing or GPT-4 distillation.","We propose a paradigm to efficiently harvest 10 million naturally existing instruction data from the pre-training web corpus to enhance LLM reasoning.","Our approach involves (1) recalling relevant documents, (2) extracting instruction-response pairs, and (3) refining the extracted pairs using open-source LLMs.","Fine-tuning base LLMs on this dataset, we build MAmmoTH2 models, which significantly boost performance on reasoning benchmarks.","Notably, MAmmoTH2-7B's (Mistral) performance increases from 11% to 34% on MATH and from 36% to 67% on GSM8K without training on any in-domain data.","Further training MAmmoTH2 on public instruction tuning datasets yields MAmmoTH2-Plus, achieving state-of-the-art performance on several reasoning and chatbot benchmarks.","Our work demonstrates how to harvest large-scale, high-quality instruction data without costly human annotation or GPT-4 distillation, providing a new paradigm for building better instruction tuning data."],"url":"http://arxiv.org/abs/2405.03548v1"}
{"created":"2024-05-06 15:10:46","title":"Position Paper: Leveraging Foundational Models for Black-Box Optimization: Benefits, Challenges, and Future Directions","abstract":"Undeniably, Large Language Models (LLMs) have stirred an extraordinary wave of innovation in the machine learning research domain, resulting in substantial impact across diverse fields such as reinforcement learning, robotics, and computer vision. Their incorporation has been rapid and transformative, marking a significant paradigm shift in the field of machine learning research.   However, the field of experimental design, grounded on black-box optimization, has been much less affected by such a paradigm shift, even though integrating LLMs with optimization presents a unique landscape ripe for exploration. In this position paper, we frame the field of black-box optimization around sequence-based foundation models and organize their relationship with previous literature. We discuss the most promising ways foundational language models can revolutionize optimization, which include harnessing the vast wealth of information encapsulated in free-form text to enrich task comprehension, utilizing highly flexible sequence models such as Transformers to engineer superior optimization strategies, and enhancing performance prediction over previously unseen search spaces.","sentences":["Undeniably, Large Language Models (LLMs) have stirred an extraordinary wave of innovation in the machine learning research domain, resulting in substantial impact across diverse fields such as reinforcement learning, robotics, and computer vision.","Their incorporation has been rapid and transformative, marking a significant paradigm shift in the field of machine learning research.   ","However, the field of experimental design, grounded on black-box optimization, has been much less affected by such a paradigm shift, even though integrating LLMs with optimization presents a unique landscape ripe for exploration.","In this position paper, we frame the field of black-box optimization around sequence-based foundation models and organize their relationship with previous literature.","We discuss the most promising ways foundational language models can revolutionize optimization, which include harnessing the vast wealth of information encapsulated in free-form text to enrich task comprehension, utilizing highly flexible sequence models such as Transformers to engineer superior optimization strategies, and enhancing performance prediction over previously unseen search spaces."],"url":"http://arxiv.org/abs/2405.03547v1"}
{"created":"2024-05-06 15:10:19","title":"CCDM: Continuous Conditional Diffusion Models for Image Generation","abstract":"Continuous Conditional Generative Modeling (CCGM) aims to estimate the distribution of high-dimensional data, typically images, conditioned on scalar continuous variables known as regression labels. While Continuous conditional Generative Adversarial Networks (CcGANs) were initially designed for this task, their adversarial training mechanism remains vulnerable to extremely sparse or imbalanced data, resulting in suboptimal outcomes. To enhance the quality of generated images, a promising alternative is to replace CcGANs with Conditional Diffusion Models (CDMs), renowned for their stable training process and ability to produce more realistic images. However, existing CDMs encounter challenges when applied to CCGM tasks due to several limitations such as inadequate U-Net architectures and deficient model fitting mechanisms for handling regression labels. In this paper, we introduce Continuous Conditional Diffusion Models (CCDMs), the first CDM designed specifically for the CCGM task. CCDMs address the limitations of existing CDMs by introducing specially designed conditional diffusion processes, a modified denoising U-Net with a custom-made conditioning mechanism, a novel hard vicinal loss for model fitting, and an efficient conditional sampling procedure. With comprehensive experiments on four datasets with varying resolutions ranging from 64x64 to 192x192, we demonstrate the superiority of the proposed CCDM over state-of-the-art CCGM models, establishing new benchmarks in CCGM. Extensive ablation studies validate the model design and implementation configuration of the proposed CCDM. Our code is publicly available at https://github.com/UBCDingXin/CCDM.","sentences":["Continuous Conditional Generative Modeling (CCGM) aims to estimate the distribution of high-dimensional data, typically images, conditioned on scalar continuous variables known as regression labels.","While Continuous conditional Generative Adversarial Networks (CcGANs) were initially designed for this task, their adversarial training mechanism remains vulnerable to extremely sparse or imbalanced data, resulting in suboptimal outcomes.","To enhance the quality of generated images, a promising alternative is to replace CcGANs with Conditional Diffusion Models (CDMs), renowned for their stable training process and ability to produce more realistic images.","However, existing CDMs encounter challenges when applied to CCGM tasks due to several limitations such as inadequate U-Net architectures and deficient model fitting mechanisms for handling regression labels.","In this paper, we introduce Continuous Conditional Diffusion Models (CCDMs), the first CDM designed specifically for the CCGM task.","CCDMs address the limitations of existing CDMs by introducing specially designed conditional diffusion processes, a modified denoising U-Net with a custom-made conditioning mechanism, a novel hard vicinal loss for model fitting, and an efficient conditional sampling procedure.","With comprehensive experiments on four datasets with varying resolutions ranging from 64x64 to 192x192, we demonstrate the superiority of the proposed CCDM over state-of-the-art CCGM models, establishing new benchmarks in CCGM.","Extensive ablation studies validate the model design and implementation configuration of the proposed CCDM.","Our code is publicly available at https://github.com/UBCDingXin/CCDM."],"url":"http://arxiv.org/abs/2405.03546v1"}
{"created":"2024-05-06 15:10:16","title":"Optimizing Hand Region Detection in MediaPipe Holistic Full-Body Pose Estimation to Improve Accuracy and Avoid Downstream Errors","abstract":"This paper addresses a critical flaw in MediaPipe Holistic's hand Region of Interest (ROI) prediction, which struggles with non-ideal hand orientations, affecting sign language recognition accuracy. We propose a data-driven approach to enhance ROI estimation, leveraging an enriched feature set including additional hand keypoints and the z-dimension. Our results demonstrate better estimates, with higher Intersection-over-Union compared to the current method. Our code and optimizations are available at https://github.com/sign-language-processing/mediapipe-hand-crop-fix.","sentences":["This paper addresses a critical flaw in MediaPipe Holistic's hand Region of Interest (ROI) prediction, which struggles with non-ideal hand orientations, affecting sign language recognition accuracy.","We propose a data-driven approach to enhance ROI estimation, leveraging an enriched feature set including additional hand keypoints and the z-dimension.","Our results demonstrate better estimates, with higher Intersection-over-Union compared to the current method.","Our code and optimizations are available at https://github.com/sign-language-processing/mediapipe-hand-crop-fix."],"url":"http://arxiv.org/abs/2405.03545v1"}
{"created":"2024-05-06 15:06:56","title":"A Formal Model of Security Controls' Capabilities and Its Applications to Policy Refinement and Incident Management","abstract":"Enforcing security requirements in networked information systems relies on security controls to mitigate the risks from increasingly dangerous threats. Configuring security controls is challenging; even nowadays, administrators must perform it without adequate tool support. Hence, this process is plagued by errors that translate to insecure postures, security incidents, and a lack of promptness in answering threats. This paper presents the Security Capability Model (SCM), a formal model that abstracts the features that security controls offer for enforcing security policies, which includes an Information Model that depicts the basic concepts related to rules (i.e., conditions, actions, events) and policies (i.e., conditions' evaluation, resolution strategies, default actions), and a Data Model that covers the capabilities needed to describe different types of filtering and channel protection controls. Following state-of-the-art design patterns, the model allows for generating abstract versions of the security controls' languages and a model-driven approach for translating abstract policies into device-specific configuration settings. By validating its effectiveness in real-world scenarios, we show that SCM enables the automation of different and complex security tasks, i.e., accurate and granular security control comparison, policy refinement, and incident response. Lastly, we present opportunities for extensions and integration with other frameworks and models.","sentences":["Enforcing security requirements in networked information systems relies on security controls to mitigate the risks from increasingly dangerous threats.","Configuring security controls is challenging; even nowadays, administrators must perform it without adequate tool support.","Hence, this process is plagued by errors that translate to insecure postures, security incidents, and a lack of promptness in answering threats.","This paper presents the Security Capability Model (SCM), a formal model that abstracts the features that security controls offer for enforcing security policies, which includes an Information Model that depicts the basic concepts related to rules (i.e., conditions, actions, events) and policies (i.e., conditions' evaluation, resolution strategies, default actions), and a Data Model that covers the capabilities needed to describe different types of filtering and channel protection controls.","Following state-of-the-art design patterns, the model allows for generating abstract versions of the security controls' languages and a model-driven approach for translating abstract policies into device-specific configuration settings.","By validating its effectiveness in real-world scenarios, we show that SCM enables the automation of different and complex security tasks, i.e., accurate and granular security control comparison, policy refinement, and incident response.","Lastly, we present opportunities for extensions and integration with other frameworks and models."],"url":"http://arxiv.org/abs/2405.03544v1"}
{"created":"2024-05-06 15:02:16","title":"RepVGG-GELAN: Enhanced GELAN with VGG-STYLE ConvNets for Brain Tumour Detection","abstract":"Object detection algorithms particularly those based on YOLO have demonstrated remarkable efficiency in balancing speed and accuracy. However, their application in brain tumour detection remains underexplored. This study proposes RepVGG-GELAN, a novel YOLO architecture enhanced with RepVGG, a reparameterized convolutional approach for object detection tasks particularly focusing on brain tumour detection within medical images. RepVGG-GELAN leverages the RepVGG architecture to improve both speed and accuracy in detecting brain tumours. Integrating RepVGG into the YOLO framework aims to achieve a balance between computational efficiency and detection performance. This study includes a spatial pyramid pooling-based Generalized Efficient Layer Aggregation Network (GELAN) architecture which further enhances the capability of RepVGG. Experimental evaluation conducted on a brain tumour dataset demonstrates the effectiveness of RepVGG-GELAN surpassing existing RCS-YOLO in terms of precision and speed. Specifically, RepVGG-GELAN achieves an increased precision of 4.91% and an increased AP50 of 2.54% over the latest existing approach while operating at 240.7 GFLOPs. The proposed RepVGG-GELAN with GELAN architecture presents promising results establishing itself as a state-of-the-art solution for accurate and efficient brain tumour detection in medical images. The implementation code is publicly available at https://github.com/ThensiB/RepVGG-GELAN.","sentences":["Object detection algorithms particularly those based on YOLO have demonstrated remarkable efficiency in balancing speed and accuracy.","However, their application in brain tumour detection remains underexplored.","This study proposes RepVGG-GELAN, a novel YOLO architecture enhanced with RepVGG, a reparameterized convolutional approach for object detection tasks particularly focusing on brain tumour detection within medical images.","RepVGG-GELAN leverages the RepVGG architecture to improve both speed and accuracy in detecting brain tumours.","Integrating RepVGG into the YOLO framework aims to achieve a balance between computational efficiency and detection performance.","This study includes a spatial pyramid pooling-based Generalized Efficient Layer Aggregation Network (GELAN) architecture which further enhances the capability of RepVGG.","Experimental evaluation conducted on a brain tumour dataset demonstrates the effectiveness of RepVGG-GELAN surpassing existing RCS-YOLO in terms of precision and speed.","Specifically, RepVGG-GELAN achieves an increased precision of 4.91% and an increased AP50 of 2.54% over the latest existing approach while operating at 240.7 GFLOPs.","The proposed RepVGG-GELAN with GELAN architecture presents promising results establishing itself as a state-of-the-art solution for accurate and efficient brain tumour detection in medical images.","The implementation code is publicly available at https://github.com/ThensiB/RepVGG-GELAN."],"url":"http://arxiv.org/abs/2405.03541v1"}
{"created":"2024-05-06 14:55:37","title":"Exploring the Efficacy of Federated-Continual Learning Nodes with Attention-Based Classifier for Robust Web Phishing Detection: An Empirical Investigation","abstract":"Web phishing poses a dynamic threat, requiring detection systems to quickly adapt to the latest tactics. Traditional approaches of accumulating data and periodically retraining models are outpaced. We propose a novel paradigm combining federated learning and continual learning, enabling distributed nodes to continually update models on streams of new phishing data, without accumulating data. These locally adapted models are then aggregated at a central server via federated learning. To enhance detection, we introduce a custom attention-based classifier model with residual connections, tailored for web phishing, leveraging attention mechanisms to capture intricate phishing patterns. We evaluate our hybrid learning paradigm across continual learning strategies (cumulative, replay, MIR, LwF) and model architectures through an empirical investigation. Our main contributions are: (1) a new hybrid federated-continual learning paradigm for robust web phishing detection, and (2) a novel attention + residual connections based model explicitly designed for this task, attaining 0.93 accuracy, 0.90 precision, 0.96 recall and 0.93 f1-score with the LwF strategy, outperforming traditional approaches in detecting emerging phishing threats while retaining past knowledge.","sentences":["Web phishing poses a dynamic threat, requiring detection systems to quickly adapt to the latest tactics.","Traditional approaches of accumulating data and periodically retraining models are outpaced.","We propose a novel paradigm combining federated learning and continual learning, enabling distributed nodes to continually update models on streams of new phishing data, without accumulating data.","These locally adapted models are then aggregated at a central server via federated learning.","To enhance detection, we introduce a custom attention-based classifier model with residual connections, tailored for web phishing, leveraging attention mechanisms to capture intricate phishing patterns.","We evaluate our hybrid learning paradigm across continual learning strategies (cumulative, replay, MIR, LwF) and model architectures through an empirical investigation.","Our main contributions are: (1) a new hybrid federated-continual learning paradigm for robust web phishing detection, and (2) a novel attention + residual connections based model explicitly designed for this task, attaining 0.93 accuracy, 0.90 precision, 0.96 recall and 0.93 f1-score with the LwF strategy, outperforming traditional approaches in detecting emerging phishing threats while retaining past knowledge."],"url":"http://arxiv.org/abs/2405.03537v1"}
{"created":"2024-05-06 14:53:58","title":"Extensional and Non-extensional Functions as Processes","abstract":"Following Milner's seminal paper, the representation of functions as processes has received considerable attention. For pure $\\lambda$-calculus, the process representations yield (at best) non-extensional $\\lambda $-theories (i.e., $\\beta$ rule holds, whereas $\\eta$ does not).   In the paper, we study how to obtain extensional representations, and how to move between extensional and non-extensional representations. Using Internal $\\pi$, $\\mathrm{I}\\pi$ (a subset of the $\\pi$-calculus in which all outputs are bound), we develop a refinement of Milner's original encoding of functions as processes that is parametric on certain abstract components called wires. These are, intuitively, processes whose task is to connect two end-point channels. We show that when a few algebraic properties of wires hold, the encoding yields a $\\lambda$-theory. Exploiting the symmetries and dualities of $\\mathrm{I}\\pi$, we isolate three main classes of wires. The first two have a sequential behaviour and are dual of each other; the third has a parallel behaviour and is the dual of itself. We show the adoption of the parallel wires yields an extensional $\\lambda$-theory; in fact, it yields an equality that coincides with that of B\\\"ohm trees with infinite $\\eta$. In contrast, the other two classes of wires yield non-extensional $\\lambda$-theories whose equalities are those of the L\\'evy-Longo and B\\\"ohm trees.","sentences":["Following Milner's seminal paper, the representation of functions as processes has received considerable attention.","For pure $\\lambda$-calculus, the process representations yield (at best) non-extensional $\\lambda $-theories (i.e., $\\beta$ rule holds, whereas $\\eta$ does not).   ","In the paper, we study how to obtain extensional representations, and how to move between extensional and non-extensional representations.","Using Internal $\\pi$, $\\mathrm{I}\\pi$ (a subset of the $\\pi$-calculus in which all outputs are bound), we develop a refinement of Milner's original encoding of functions as processes that is parametric on certain abstract components called wires.","These are, intuitively, processes whose task is to connect two end-point channels.","We show that when a few algebraic properties of wires hold, the encoding yields a $\\lambda$-theory.","Exploiting the symmetries and dualities of $\\mathrm{I}\\pi$, we isolate three main classes of wires.","The first two have a sequential behaviour and are dual of each other; the third has a parallel behaviour and is the dual of itself.","We show the adoption of the parallel wires yields an extensional $\\lambda$-theory; in fact, it yields an equality that coincides with that of B\\\"ohm trees with infinite $\\eta$. In contrast, the other two classes of wires yield non-extensional $\\lambda$-theories whose equalities are those of the L\\'evy-Longo and B\\\"ohm trees."],"url":"http://arxiv.org/abs/2405.03536v1"}
{"created":"2024-05-06 14:52:23","title":"Meta-Evolve: Continuous Robot Evolution for One-to-many Policy Transfer","abstract":"We investigate the problem of transferring an expert policy from a source robot to multiple different robots. To solve this problem, we propose a method named $Meta$-$Evolve$ that uses continuous robot evolution to efficiently transfer the policy to each target robot through a set of tree-structured evolutionary robot sequences. The robot evolution tree allows the robot evolution paths to be shared, so our approach can significantly outperform naive one-to-one policy transfer. We present a heuristic approach to determine an optimized robot evolution tree. Experiments have shown that our method is able to improve the efficiency of one-to-three transfer of manipulation policy by up to 3.2$\\times$ and one-to-six transfer of agile locomotion policy by 2.4$\\times$ in terms of simulation cost over the baseline of launching multiple independent one-to-one policy transfers.","sentences":["We investigate the problem of transferring an expert policy from a source robot to multiple different robots.","To solve this problem, we propose a method named $Meta$-$Evolve$ that uses continuous robot evolution to efficiently transfer the policy to each target robot through a set of tree-structured evolutionary robot sequences.","The robot evolution tree allows the robot evolution paths to be shared, so our approach can significantly outperform naive one-to-one policy transfer.","We present a heuristic approach to determine an optimized robot evolution tree.","Experiments have shown that our method is able to improve the efficiency of one-to-three transfer of manipulation policy by up to 3.2$\\times$ and one-to-six transfer of agile locomotion policy by 2.4$\\times$ in terms of simulation cost over the baseline of launching multiple independent one-to-one policy transfers."],"url":"http://arxiv.org/abs/2405.03534v1"}
{"created":"2024-05-06 14:47:40","title":"Semi-autonomous Robotic Disassembly Enhanced by Mixed Reality","abstract":"In this study, we introduce \"SARDiM,\" a modular semi-autonomous platform enhanced with mixed reality for industrial disassembly tasks. Through a case study focused on EV battery disassembly, SARDiM integrates Mixed Reality, object segmentation, teleoperation, force feedback, and variable autonomy. Utilising the ROS, Unity, and MATLAB platforms, alongside a joint impedance controller, SARDiM facilitates teleoperated disassembly. The approach combines FastSAM for real-time object segmentation, generating data which is subsequently processed through a cluster analysis algorithm to determine the centroid and orientation of the components, categorizing them by size and disassembly priority. This data guides the MoveIt platform in trajectory planning for the Franka Robot arm. SARDiM provides the capability to switch between two teleoperation modes: manual and semi-autonomous with variable autonomy. Each was evaluated using four different Interface Methods (IM): direct view, monitor feed, mixed reality with monitor feed, and point cloud mixed reality. Evaluations across the eight IMs demonstrated a 40.61% decrease in joint limit violations using Mode 2. Moreover, Mode 2-IM4 outperformed Mode 1-IM1 by achieving a 2.33%-time reduction while considerably increasing safety, making it optimal for operating in hazardous environments at a safe distance, with the same ease of use as teleoperation with a direct view of the environment.","sentences":["In this study, we introduce \"SARDiM,\" a modular semi-autonomous platform enhanced with mixed reality for industrial disassembly tasks.","Through a case study focused on EV battery disassembly, SARDiM integrates Mixed Reality, object segmentation, teleoperation, force feedback, and variable autonomy.","Utilising the ROS, Unity, and MATLAB platforms, alongside a joint impedance controller, SARDiM facilitates teleoperated disassembly.","The approach combines FastSAM for real-time object segmentation, generating data which is subsequently processed through a cluster analysis algorithm to determine the centroid and orientation of the components, categorizing them by size and disassembly priority.","This data guides the MoveIt platform in trajectory planning for the Franka Robot arm.","SARDiM provides the capability to switch between two teleoperation modes: manual and semi-autonomous with variable autonomy.","Each was evaluated using four different Interface Methods (IM): direct view, monitor feed, mixed reality with monitor feed, and point cloud mixed reality.","Evaluations across the eight IMs demonstrated a 40.61% decrease in joint limit violations using Mode 2.","Moreover, Mode 2-IM4 outperformed Mode 1-IM1 by achieving a 2.33%-time reduction while considerably increasing safety, making it optimal for operating in hazardous environments at a safe distance, with the same ease of use as teleoperation with a direct view of the environment."],"url":"http://arxiv.org/abs/2405.03530v1"}
{"created":"2024-05-06 14:44:06","title":"ReinWiFi: A Reinforcement-Learning-Based Framework for the Application-Layer QoS Optimization of WiFi Networks","abstract":"In this paper, a reinforcement-learning-based scheduling framework is proposed and implemented to optimize the application-layer quality-of-service (QoS) of a practical wireless local area network (WLAN) suffering from unknown interference. Particularly, application-layer tasks of file delivery and delay-sensitive communication, e.g., screen projection, in a WLAN with enhanced distributed channel access (EDCA) mechanism, are jointly scheduled by adjusting the contention window sizes and application-layer throughput limitation, such that their QoS, including the throughput of file delivery and the round trip time of the delay-sensitive communication, can be optimized. Due to the unknown interference and vendor-dependent implementation of the network interface card, the relation between the scheduling policy and the system QoS is unknown. Hence, a reinforcement learning method is proposed, in which a novel Q-network is trained to map from the historical scheduling parameters and QoS observations to the current scheduling action. It is demonstrated on a testbed that the proposed framework can achieve a significantly better QoS than the conventional EDCA mechanism.","sentences":["In this paper, a reinforcement-learning-based scheduling framework is proposed and implemented to optimize the application-layer quality-of-service (QoS) of a practical wireless local area network (WLAN) suffering from unknown interference.","Particularly, application-layer tasks of file delivery and delay-sensitive communication, e.g., screen projection, in a WLAN with enhanced distributed channel access (EDCA) mechanism, are jointly scheduled by adjusting the contention window sizes and application-layer throughput limitation, such that their QoS, including the throughput of file delivery and the round trip time of the delay-sensitive communication, can be optimized.","Due to the unknown interference and vendor-dependent implementation of the network interface card, the relation between the scheduling policy and the system QoS is unknown.","Hence, a reinforcement learning method is proposed, in which a novel Q-network is trained to map from the historical scheduling parameters and QoS observations to the current scheduling action.","It is demonstrated on a testbed that the proposed framework can achieve a significantly better QoS than the conventional EDCA mechanism."],"url":"http://arxiv.org/abs/2405.03526v1"}
{"created":"2024-05-06 14:40:50","title":"Exploring knowledge graph-based neural-symbolic system from application perspective","abstract":"The rapid advancement in artificial intelligence (AI), particularly through deep neural networks, has catalyzed significant progress in fields such as vision and text processing. Nonetheless, the pursuit of AI systems that exhibit human-like reasoning and interpretability continues to pose a substantial challenge. The Neural-Symbolic paradigm, which integrates the deep learning prowess of neural networks with the reasoning capabilities of symbolic systems, presents a promising pathway toward developing more transparent and comprehensible AI systems. Within this paradigm, the Knowledge Graph (KG) emerges as a crucial element, offering a structured and dynamic method for representing knowledge through interconnected entities and relationships, predominantly utilizing the triple (subject, predicate, object). This paper explores recent advancements in neural-symbolic integration based on KG, elucidating how KG underpins this integration across three key categories: enhancing the reasoning and interpretability of neural networks through the incorporation of symbolic knowledge (Symbol for Neural), refining the completeness and accuracy of symbolic systems via neural network methodologies (Neural for Symbol), and facilitating their combined application in Hybrid Neural-Symbolic Integration. It highlights current trends and proposes directions for future research in the domain of Neural-Symbolic AI.","sentences":["The rapid advancement in artificial intelligence (AI), particularly through deep neural networks, has catalyzed significant progress in fields such as vision and text processing.","Nonetheless, the pursuit of AI systems that exhibit human-like reasoning and interpretability continues to pose a substantial challenge.","The Neural-Symbolic paradigm, which integrates the deep learning prowess of neural networks with the reasoning capabilities of symbolic systems, presents a promising pathway toward developing more transparent and comprehensible AI systems.","Within this paradigm, the Knowledge Graph (KG) emerges as a crucial element, offering a structured and dynamic method for representing knowledge through interconnected entities and relationships, predominantly utilizing the triple (subject, predicate, object).","This paper explores recent advancements in neural-symbolic integration based on KG, elucidating how KG underpins this integration across three key categories: enhancing the reasoning and interpretability of neural networks through the incorporation of symbolic knowledge (Symbol for Neural), refining the completeness and accuracy of symbolic systems via neural network methodologies (Neural for Symbol), and facilitating their combined application in Hybrid Neural-Symbolic Integration.","It highlights current trends and proposes directions for future research in the domain of Neural-Symbolic AI."],"url":"http://arxiv.org/abs/2405.03524v1"}
{"created":"2024-05-06 14:40:44","title":"Basilisk: Achieving Competitive Performance with Open EDA Tools on an Open-Source Linux-Capable RISC-V SoC","abstract":"We introduce Basilisk, an optimized application-specific integrated circuit (ASIC) implementation and design flow building on the end-to-end open-source Iguana system-on-chip (SoC). We present enhancements to synthesis tools and logic optimization scripts improving quality of results (QoR), as well as an optimized physical design with an improved power grid and cell placement integration enabling a higher core utilization. The tapeout-ready version of Basilisk implemented in IHP's open 130 nm technology achieves an operation frequency of 77 MHz (51 logic levels) under typical conditions, a 2.3x improvement compared to the baseline open-source EDA design flow presented in Iguana, and a higher 55 % core utilization compared to 50 % in the baseline design. Through collaboration with EDA tool developers and domain experts, Basilisk exemplifies a synergistic effort towards competitive open-source electronic design automation (EDA) tools for research and industry applications.","sentences":["We introduce Basilisk, an optimized application-specific integrated circuit (ASIC) implementation and design flow building on the end-to-end open-source Iguana system-on-chip (SoC).","We present enhancements to synthesis tools and logic optimization scripts improving quality of results (QoR), as well as an optimized physical design with an improved power grid and cell placement integration enabling a higher core utilization.","The tapeout-ready version of Basilisk implemented in IHP's open 130 nm technology achieves an operation frequency of 77 MHz (51 logic levels) under typical conditions, a 2.3x improvement compared to the baseline open-source EDA design flow presented in Iguana, and a higher 55 % core utilization compared to 50 % in the baseline design.","Through collaboration with EDA tool developers and domain experts, Basilisk exemplifies a synergistic effort towards competitive open-source electronic design automation (EDA) tools for research and industry applications."],"url":"http://arxiv.org/abs/2405.03523v1"}
{"created":"2024-05-06 14:37:07","title":"Is Sora a World Simulator? A Comprehensive Survey on General World Models and Beyond","abstract":"General world models represent a crucial pathway toward achieving Artificial General Intelligence (AGI), serving as the cornerstone for various applications ranging from virtual environments to decision-making systems. Recently, the emergence of the Sora model has attained significant attention due to its remarkable simulation capabilities, which exhibits an incipient comprehension of physical laws. In this survey, we embark on a comprehensive exploration of the latest advancements in world models. Our analysis navigates through the forefront of generative methodologies in video generation, where world models stand as pivotal constructs facilitating the synthesis of highly realistic visual content. Additionally, we scrutinize the burgeoning field of autonomous-driving world models, meticulously delineating their indispensable role in reshaping transportation and urban mobility. Furthermore, we delve into the intricacies inherent in world models deployed within autonomous agents, shedding light on their profound significance in enabling intelligent interactions within dynamic environmental contexts. At last, we examine challenges and limitations of world models, and discuss their potential future directions. We hope this survey can serve as a foundational reference for the research community and inspire continued innovation. This survey will be regularly updated at: https://github.com/GigaAI-research/General-World-Models-Survey.","sentences":["General world models represent a crucial pathway toward achieving Artificial General Intelligence (AGI), serving as the cornerstone for various applications ranging from virtual environments to decision-making systems.","Recently, the emergence of the Sora model has attained significant attention due to its remarkable simulation capabilities, which exhibits an incipient comprehension of physical laws.","In this survey, we embark on a comprehensive exploration of the latest advancements in world models.","Our analysis navigates through the forefront of generative methodologies in video generation, where world models stand as pivotal constructs facilitating the synthesis of highly realistic visual content.","Additionally, we scrutinize the burgeoning field of autonomous-driving world models, meticulously delineating their indispensable role in reshaping transportation and urban mobility.","Furthermore, we delve into the intricacies inherent in world models deployed within autonomous agents, shedding light on their profound significance in enabling intelligent interactions within dynamic environmental contexts.","At last, we examine challenges and limitations of world models, and discuss their potential future directions.","We hope this survey can serve as a foundational reference for the research community and inspire continued innovation.","This survey will be regularly updated at: https://github.com/GigaAI-research/General-World-Models-Survey."],"url":"http://arxiv.org/abs/2405.03520v1"}
{"created":"2024-05-06 14:36:01","title":"Low-light Object Detection","abstract":"In this competition we employed a model fusion approach to achieve object detection results close to those of real images. Our method is based on the CO-DETR model, which was trained on two sets of data: one containing images under dark conditions and another containing images enhanced with low-light conditions. We used various enhancement techniques on the test data to generate multiple sets of prediction results. Finally, we applied a clustering aggregation method guided by IoU thresholds to select the optimal results.","sentences":["In this competition we employed a model fusion approach to achieve object detection results close to those of real images.","Our method is based on the CO-DETR model, which was trained on two sets of data: one containing images under dark conditions and another containing images enhanced with low-light conditions.","We used various enhancement techniques on the test data to generate multiple sets of prediction results.","Finally, we applied a clustering aggregation method guided by IoU thresholds to select the optimal results."],"url":"http://arxiv.org/abs/2405.03519v1"}
{"created":"2024-05-06 14:33:35","title":"Reinforcement Nash Equilibrium Solver","abstract":"Nash Equilibrium (NE) is the canonical solution concept of game theory, which provides an elegant tool to understand the rationalities. Though mixed strategy NE exists in any game with finite players and actions, computing NE in two- or multi-player general-sum games is PPAD-Complete. Various alternative solutions, e.g., Correlated Equilibrium (CE), and learning methods, e.g., fictitious play (FP), are proposed to approximate NE. For convenience, we call these methods as \"inexact solvers\", or \"solvers\" for short. However, the alternative solutions differ from NE and the learning methods generally fail to converge to NE. Therefore, in this work, we propose REinforcement Nash Equilibrium Solver (RENES), which trains a single policy to modify the games with different sizes and applies the solvers on the modified games where the obtained solution is evaluated on the original games. Specifically, our contributions are threefold. i) We represent the games as $\\alpha$-rank response graphs and leverage graph neural network (GNN) to handle the games with different sizes as inputs; ii) We use tensor decomposition, e.g., canonical polyadic (CP), to make the dimension of modifying actions fixed for games with different sizes; iii) We train the modifying strategy for games with the widely-used proximal policy optimization (PPO) and apply the solvers to solve the modified games, where the obtained solution is evaluated on original games. Extensive experiments on large-scale normal-form games show that our method can further improve the approximation of NE of different solvers, i.e., $\\alpha$-rank, CE, FP and PRD, and can be generalized to unseen games.","sentences":["Nash Equilibrium (NE) is the canonical solution concept of game theory, which provides an elegant tool to understand the rationalities.","Though mixed strategy NE exists in any game with finite players and actions, computing NE in two- or multi-player general-sum games is PPAD-Complete.","Various alternative solutions, e.g., Correlated Equilibrium (CE), and learning methods, e.g., fictitious play (FP), are proposed to approximate NE.","For convenience, we call these methods as \"inexact solvers\", or \"solvers\" for short.","However, the alternative solutions differ from NE and the learning methods generally fail to converge to NE.","Therefore, in this work, we propose REinforcement Nash Equilibrium Solver (RENES), which trains a single policy to modify the games with different sizes and applies the solvers on the modified games where the obtained solution is evaluated on the original games.","Specifically, our contributions are threefold.","i)","We represent the games as $\\alpha$-rank response graphs and leverage graph neural network (GNN) to handle the games with different sizes as inputs; ii) We use tensor decomposition, e.g., canonical polyadic (CP), to make the dimension of modifying actions fixed for games with different sizes; iii)","We train the modifying strategy for games with the widely-used proximal policy optimization (PPO) and apply the solvers to solve the modified games, where the obtained solution is evaluated on original games.","Extensive experiments on large-scale normal-form games show that our method can further improve the approximation of NE of different solvers, i.e., $\\alpha$-rank, CE, FP and PRD, and can be generalized to unseen games."],"url":"http://arxiv.org/abs/2405.03518v1"}
{"created":"2024-05-06 14:29:24","title":"GI-SMN: Gradient Inversion Attack against Federated Learning without Prior Knowledge","abstract":"Federated learning (FL) has emerged as a privacy-preserving machine learning approach where multiple parties share gradient information rather than original user data. Recent work has demonstrated that gradient inversion attacks can exploit the gradients of FL to recreate the original user data, posing significant privacy risks. However, these attacks make strong assumptions about the attacker, such as altering the model structure or parameters, gaining batch normalization statistics, or acquiring prior knowledge of the original training set, etc. Consequently, these attacks are not possible in real-world scenarios. To end it, we propose a novel Gradient Inversion attack based on Style Migration Network (GI-SMN), which breaks through the strong assumptions made by previous gradient inversion attacks. The optimization space is reduced by the refinement of the latent code and the use of regular terms to facilitate gradient matching. GI-SMN enables the reconstruction of user data with high similarity in batches. Experimental results have demonstrated that GI-SMN outperforms state-of-the-art gradient inversion attacks in both visual effect and similarity metrics. Additionally, it also can overcome gradient pruning and differential privacy defenses.","sentences":["Federated learning (FL) has emerged as a privacy-preserving machine learning approach where multiple parties share gradient information rather than original user data.","Recent work has demonstrated that gradient inversion attacks can exploit the gradients of FL to recreate the original user data, posing significant privacy risks.","However, these attacks make strong assumptions about the attacker, such as altering the model structure or parameters, gaining batch normalization statistics, or acquiring prior knowledge of the original training set, etc.","Consequently, these attacks are not possible in real-world scenarios.","To end it, we propose a novel Gradient Inversion attack based on Style Migration Network (GI-SMN), which breaks through the strong assumptions made by previous gradient inversion attacks.","The optimization space is reduced by the refinement of the latent code and the use of regular terms to facilitate gradient matching.","GI-SMN enables the reconstruction of user data with high similarity in batches.","Experimental results have demonstrated that GI-SMN outperforms state-of-the-art gradient inversion attacks in both visual effect and similarity metrics.","Additionally, it also can overcome gradient pruning and differential privacy defenses."],"url":"http://arxiv.org/abs/2405.03516v1"}
{"created":"2024-05-06 14:26:08","title":"Development of Ultra-Portable 3D Mapping Systems for Emergency Services","abstract":"Miniaturization of cameras and LiDAR sensors has enabled the development of wearable 3D mapping systems for emergency responders. These systems have the potential to revolutionize response capabilities by providing real-time, high-fidelity maps of dynamic and hazardous environments. We present our recent efforts towards the development of such ultra-portable 3D mapping systems. We review four different sensor configurations, either helmet-mounted or body-worn, with two different mapping algorithms that were implemented and evaluated during field trials. The paper discusses the experimental results with the aim to stimulate further discussion within the portable 3D mapping research community.","sentences":["Miniaturization of cameras and LiDAR sensors has enabled the development of wearable 3D mapping systems for emergency responders.","These systems have the potential to revolutionize response capabilities by providing real-time, high-fidelity maps of dynamic and hazardous environments.","We present our recent efforts towards the development of such ultra-portable 3D mapping systems.","We review four different sensor configurations, either helmet-mounted or body-worn, with two different mapping algorithms that were implemented and evaluated during field trials.","The paper discusses the experimental results with the aim to stimulate further discussion within the portable 3D mapping research community."],"url":"http://arxiv.org/abs/2405.03514v1"}
{"created":"2024-05-06 14:25:58","title":"QBER: Quantifying Cyber Risks for Strategic Decisions","abstract":"Quantifying cyber risks is essential for organizations to grasp their vulnerability to threats and make informed decisions. However, current approaches still need to work on blending economic viewpoints to provide insightful analysis. To bridge this gap, we introduce QBER approach to offer decision-makers measurable risk metrics. The QBER evaluates losses from cyberattacks, performs detailed risk analyses based on existing cybersecurity measures, and provides thorough cost assessments. Our contributions involve outlining cyberattack probabilities and risks, identifying Technical, Economic, and Legal (TEL) impacts, creating a model to gauge impacts, suggesting risk mitigation strategies, and examining trends and challenges in implementing widespread Cyber Risk Quantification (CRQ). The QBER approach serves as a guided approach for organizations to assess risks and strategically invest in cybersecurity.","sentences":["Quantifying cyber risks is essential for organizations to grasp their vulnerability to threats and make informed decisions.","However, current approaches still need to work on blending economic viewpoints to provide insightful analysis.","To bridge this gap, we introduce QBER approach to offer decision-makers measurable risk metrics.","The QBER evaluates losses from cyberattacks, performs detailed risk analyses based on existing cybersecurity measures, and provides thorough cost assessments.","Our contributions involve outlining cyberattack probabilities and risks, identifying Technical, Economic, and Legal (TEL) impacts, creating a model to gauge impacts, suggesting risk mitigation strategies, and examining trends and challenges in implementing widespread Cyber Risk Quantification (CRQ).","The QBER approach serves as a guided approach for organizations to assess risks and strategically invest in cybersecurity."],"url":"http://arxiv.org/abs/2405.03513v1"}
{"created":"2024-05-06 14:23:33","title":"Extremal Separation Problems for Temporal Instance Queries","abstract":"The separation problem for a class Q of database queries is to find a query in Q that distinguishes between a given set of `positive' and `negative' data examples. Separation provides explanations of examples and underpins the query-by-example paradigm to support database users in constructing and refining queries. As the space of all separating queries can be large, it is helpful to succinctly represent this space by means of its most specific (logically strongest) and general (weakest) members. We investigate this extremal separation problem for classes of instance queries formulated in linear temporal logic LTL with the operators conjunction, next, and eventually. Our results range from tight complexity bounds for verifying and counting extremal separators to algorithms computing them.","sentences":["The separation problem for a class Q of database queries is to find a query in Q that distinguishes between a given set of `positive' and `negative' data examples.","Separation provides explanations of examples and underpins the query-by-example paradigm to support database users in constructing and refining queries.","As the space of all separating queries can be large, it is helpful to succinctly represent this space by means of its most specific (logically strongest) and general (weakest) members.","We investigate this extremal separation problem for classes of instance queries formulated in linear temporal logic LTL with the operators conjunction, next, and eventually.","Our results range from tight complexity bounds for verifying and counting extremal separators to algorithms computing them."],"url":"http://arxiv.org/abs/2405.03511v1"}
{"created":"2024-05-06 14:22:17","title":"Are Human Rules Necessary? Generating Reusable APIs with CoT Reasoning and In-Context Learning","abstract":"Inspired by the great potential of Large Language Models (LLMs) for solving complex coding tasks, in this paper, we propose a novel approach, named Code2API, to automatically perform APIzation for Stack Overflow code snippets. Code2API does not require additional model training or any manual crafting rules and can be easily deployed on personal computers without relying on other external tools. Specifically, Code2API guides the LLMs through well-designed prompts to generate well-formed APIs for given code snippets. To elicit knowledge and logical reasoning from LLMs, we used chain-of-thought (CoT) reasoning and few-shot in-context learning, which can help the LLMs fully understand the APIzation task and solve it step by step in a manner similar to a developer. Our evaluations show that Code2API achieves a remarkable accuracy in identifying method parameters (65%) and return statements (66%) equivalent to human-generated ones, surpassing the current state-of-the-art approach, APIzator, by 15.0% and 16.5% respectively. Moreover, compared with APIzator, our user study demonstrates that Code2API exhibits superior performance in generating meaningful method names, even surpassing the human-level performance, and developers are more willing to use APIs generated by our approach, highlighting the applicability of our tool in practice. Finally, we successfully extend our framework to the Python dataset, achieving a comparable performance with Java, which verifies the generalizability of our tool.","sentences":["Inspired by the great potential of Large Language Models (LLMs) for solving complex coding tasks, in this paper, we propose a novel approach, named Code2API, to automatically perform APIzation for Stack Overflow code snippets.","Code2API does not require additional model training or any manual crafting rules and can be easily deployed on personal computers without relying on other external tools.","Specifically, Code2API guides the LLMs through well-designed prompts to generate well-formed APIs for given code snippets.","To elicit knowledge and logical reasoning from LLMs, we used chain-of-thought (CoT) reasoning and few-shot in-context learning, which can help the LLMs fully understand the APIzation task and solve it step by step in a manner similar to a developer.","Our evaluations show that Code2API achieves a remarkable accuracy in identifying method parameters (65%) and return statements (66%) equivalent to human-generated ones, surpassing the current state-of-the-art approach, APIzator, by 15.0% and 16.5% respectively.","Moreover, compared with APIzator, our user study demonstrates that Code2API exhibits superior performance in generating meaningful method names, even surpassing the human-level performance, and developers are more willing to use APIs generated by our approach, highlighting the applicability of our tool in practice.","Finally, we successfully extend our framework to the Python dataset, achieving a comparable performance with Java, which verifies the generalizability of our tool."],"url":"http://arxiv.org/abs/2405.03509v1"}
{"created":"2024-05-06 14:20:42","title":"Spin-Wave Voices: Sonification of Nanoscale Spin Waves as an Engagement and Research Tool","abstract":"Magnonics is an emerging research field that addresses the use of spin waves (magnons), purely magnetic waves, for information transport and processing. Spin waves are a potential replacement for electric current in modern computational devices that would make them more compact and energy efficient. The field is yet little known, even among physicists. Additionally, with the development of new measuring techniques and computational physics, the obtained magnetic data becomes more complex, in some cases including 3D vector fields and time-resolution. This work presents an approach to the audio-visual representation of the spin waves and discusses its use as a tool for science communication exhibits and possible data analysis tool. The work also details an instance of such an exhibit presented at the annual international digital art exhibition Ars Electronica Festival in 2022.","sentences":["Magnonics is an emerging research field that addresses the use of spin waves (magnons), purely magnetic waves, for information transport and processing.","Spin waves are a potential replacement for electric current in modern computational devices that would make them more compact and energy efficient.","The field is yet little known, even among physicists.","Additionally, with the development of new measuring techniques and computational physics, the obtained magnetic data becomes more complex, in some cases including 3D vector fields and time-resolution.","This work presents an approach to the audio-visual representation of the spin waves and discusses its use as a tool for science communication exhibits and possible data analysis tool.","The work also details an instance of such an exhibit presented at the annual international digital art exhibition Ars Electronica Festival in 2022."],"url":"http://arxiv.org/abs/2405.03506v1"}
{"created":"2024-05-06 14:13:38","title":"Boosting Single Positive Multi-label Classification with Generalized Robust Loss","abstract":"Multi-label learning (MLL) requires comprehensive multi-semantic annotations that is hard to fully obtain, thus often resulting in missing labels scenarios. In this paper, we investigate Single Positive Multi-label Learning (SPML), where each image is associated with merely one positive label. Existing SPML methods only focus on designing losses using mechanisms such as hard pseudo-labeling and robust losses, mostly leading to unacceptable false negatives. To address this issue, we first propose a generalized loss framework based on expected risk minimization to provide soft pseudo labels, and point out that the former losses can be seamlessly converted into our framework. In particular, we design a novel robust loss based on our framework, which enjoys flexible coordination between false positives and false negatives, and can additionally deal with the imbalance between positive and negative samples. Extensive experiments show that our approach can significantly improve SPML performance and outperform the vast majority of state-of-the-art methods on all the four benchmarks.","sentences":["Multi-label learning (MLL) requires comprehensive multi-semantic annotations that is hard to fully obtain, thus often resulting in missing labels scenarios.","In this paper, we investigate Single Positive Multi-label Learning (SPML), where each image is associated with merely one positive label.","Existing SPML methods only focus on designing losses using mechanisms such as hard pseudo-labeling and robust losses, mostly leading to unacceptable false negatives.","To address this issue, we first propose a generalized loss framework based on expected risk minimization to provide soft pseudo labels, and point out that the former losses can be seamlessly converted into our framework.","In particular, we design a novel robust loss based on our framework, which enjoys flexible coordination between false positives and false negatives, and can additionally deal with the imbalance between positive and negative samples.","Extensive experiments show that our approach can significantly improve SPML performance and outperform the vast majority of state-of-the-art methods on all the four benchmarks."],"url":"http://arxiv.org/abs/2405.03501v1"}
{"created":"2024-05-06 14:11:36","title":"A Rate-Distortion-Classification Approach for Lossy Image Compression","abstract":"In lossy image compression, the objective is to achieve minimal signal distortion while compressing images to a specified bit rate. The increasing demand for visual analysis applications, particularly in classification tasks, has emphasized the significance of considering semantic distortion in compressed images. To bridge the gap between image compression and visual analysis, we propose a Rate-Distortion-Classification (RDC) model for lossy image compression, offering a unified framework to optimize the trade-off between rate, distortion, and classification accuracy. The RDC model is extensively analyzed both statistically on a multi-distribution source and experimentally on the widely used MNIST dataset. The findings reveal that the RDC model exhibits desirable properties, including monotonic non-increasing and convex functions, under certain conditions. This work provides insights into the development of human-machine friendly compression methods and Video Coding for Machine (VCM) approaches, paving the way for end-to-end image compression techniques in real-world applications.","sentences":["In lossy image compression, the objective is to achieve minimal signal distortion while compressing images to a specified bit rate.","The increasing demand for visual analysis applications, particularly in classification tasks, has emphasized the significance of considering semantic distortion in compressed images.","To bridge the gap between image compression and visual analysis, we propose a Rate-Distortion-Classification (RDC) model for lossy image compression, offering a unified framework to optimize the trade-off between rate, distortion, and classification accuracy.","The RDC model is extensively analyzed both statistically on a multi-distribution source and experimentally on the widely used MNIST dataset.","The findings reveal that the RDC model exhibits desirable properties, including monotonic non-increasing and convex functions, under certain conditions.","This work provides insights into the development of human-machine friendly compression methods and Video Coding for Machine (VCM) approaches, paving the way for end-to-end image compression techniques in real-world applications."],"url":"http://arxiv.org/abs/2405.03500v1"}
{"created":"2024-05-06 14:02:59","title":"Jointly Learning Cost and Constraints from Demonstrations for Safe Trajectory Generation","abstract":"Learning from Demonstration allows robots to mimic human actions. However, these methods do not model constraints crucial to ensure safety of the learned skill. Moreover, even when explicitly modelling constraints, they rely on the assumption of a known cost function, which limits their practical usability for task with unknown cost. In this work we propose a two-step optimization process that allow to estimate cost and constraints by decoupling the learning of cost functions from the identification of unknown constraints within the demonstrated trajectories. Initially, we identify the cost function by isolating the effect of constraints on parts of the demonstrations. Subsequently, a constraint leaning method is used to identify the unknown constraints. Our approach is validated both on simulated trajectories and a real robotic manipulation task. Our experiments show the impact that incorrect cost estimation has on the learned constraints and illustrate how the proposed method is able to infer unknown constraints, such as obstacles, from demonstrated trajectories without any initial knowledge of the cost.","sentences":["Learning from Demonstration allows robots to mimic human actions.","However, these methods do not model constraints crucial to ensure safety of the learned skill.","Moreover, even when explicitly modelling constraints, they rely on the assumption of a known cost function, which limits their practical usability for task with unknown cost.","In this work we propose a two-step optimization process that allow to estimate cost and constraints by decoupling the learning of cost functions from the identification of unknown constraints within the demonstrated trajectories.","Initially, we identify the cost function by isolating the effect of constraints on parts of the demonstrations.","Subsequently, a constraint leaning method is used to identify the unknown constraints.","Our approach is validated both on simulated trajectories and a real robotic manipulation task.","Our experiments show the impact that incorrect cost estimation has on the learned constraints and illustrate how the proposed method is able to infer unknown constraints, such as obstacles, from demonstrated trajectories without any initial knowledge of the cost."],"url":"http://arxiv.org/abs/2405.03491v1"}
{"created":"2024-05-06 14:01:05","title":"On the Influence of Data Resampling for Deep Learning-Based Log Anomaly Detection: Insights and Recommendations","abstract":"Numerous DL-based approaches have garnered considerable attention in the field of software Log Anomaly Detection. However, a practical challenge persists: the class imbalance in the public data commonly used to train the DL models. This imbalance is characterized by a substantial disparity in the number of abnormal log sequences compared to normal ones, for example, anomalies represent less than 1% of one of the most popular datasets. Previous research has indicated that existing DLLAD approaches may exhibit unsatisfactory performance, particularly when confronted with datasets featuring severe class imbalances. Mitigating class imbalance through data resampling has proven effective for other software engineering tasks, however, it has been unexplored for LAD thus far. This study aims to fill this gap by providing an in-depth analysis of the impact of diverse data resampling methods on existing DLLAD approaches from two distinct perspectives. Firstly, we assess the performance of these DLLAD approaches across three datasets and explore the impact of resampling ratios of normal to abnormal data on ten data resampling methods. Secondly, we evaluate the effectiveness of the data resampling methods when utilizing optimal resampling ratios of normal to abnormal data. Our findings indicate that oversampling methods generally outperform undersampling and hybrid methods. Data resampling on raw data yields superior results compared to data resampling in the feature space. In most cases, certain undersampling and hybrid methods show limited effectiveness. Additionally, by exploring the resampling ratio of normal to abnormal data, we suggest generating more data for minority classes through oversampling while removing less data from majority classes through undersampling. In conclusion, our study provides valuable insights into the intricate relationship between data resampling methods and DLLAD.","sentences":["Numerous DL-based approaches have garnered considerable attention in the field of software Log Anomaly Detection.","However, a practical challenge persists: the class imbalance in the public data commonly used to train the DL models.","This imbalance is characterized by a substantial disparity in the number of abnormal log sequences compared to normal ones, for example, anomalies represent less than 1% of one of the most popular datasets.","Previous research has indicated that existing DLLAD approaches may exhibit unsatisfactory performance, particularly when confronted with datasets featuring severe class imbalances.","Mitigating class imbalance through data resampling has proven effective for other software engineering tasks, however, it has been unexplored for LAD thus far.","This study aims to fill this gap by providing an in-depth analysis of the impact of diverse data resampling methods on existing DLLAD approaches from two distinct perspectives.","Firstly, we assess the performance of these DLLAD approaches across three datasets and explore the impact of resampling ratios of normal to abnormal data on ten data resampling methods.","Secondly, we evaluate the effectiveness of the data resampling methods when utilizing optimal resampling ratios of normal to abnormal data.","Our findings indicate that oversampling methods generally outperform undersampling and hybrid methods.","Data resampling on raw data yields superior results compared to data resampling in the feature space.","In most cases, certain undersampling and hybrid methods show limited effectiveness.","Additionally, by exploring the resampling ratio of normal to abnormal data, we suggest generating more data for minority classes through oversampling while removing less data from majority classes through undersampling.","In conclusion, our study provides valuable insights into the intricate relationship between data resampling methods and DLLAD."],"url":"http://arxiv.org/abs/2405.03489v1"}
{"created":"2024-05-06 13:59:54","title":"Accurate and Fast Approximate Graph Pattern Mining at Scale","abstract":"Approximate graph pattern mining (A-GPM) is an important data analysis tool for many graph-based applications. There exist sampling-based A-GPM systems to provide automation and generalization over a wide variety of use cases. However, there are two major obstacles that prevent existing A-GPM systems being adopted in practice. First, the termination mechanism that decides when to end sampling lacks theoretical backup on confidence, and is unstable and slow in practice. Second, they suffer poor performance when dealing with the \"needle-in-the-hay\" cases, because a huge number of samples are required to converge, given the extremely low hit rate of their fixed sampling schemes. We build ScaleGPM, an accurate and fast A-GPM system that removes the two obstacles. First, we propose a novel on-the-fly convergence detection mechanism to achieve stable termination and provide theoretical guarantee on the confidence, with negligible overhead. Second, we propose two techniques to deal with the \"needle-in-the-hay\" problem, eager-verify and hybrid sampling. Our eager-verify method improves sampling hit rate by pruning unpromising candidates as early as possible. Hybrid sampling improves performance by automatically choosing the better scheme between fine-grained and coarse-grained sampling schemes. Experiments show that our online convergence detection mechanism can detect convergence and results in stable and rapid termination with theoretically guaranteed confidence. We show the effectiveness of eager-verify in improving the hit rate, and the scheme-selection mechanism in correctly choosing the better scheme for various cases. Overall, ScaleGPM achieves a geomean average of 565x (up to 610169x) speedup over the state-of-the-art A-GPM system, Arya. In particular, ScaleGPM handles billion-scale graphs in seconds, where existing systems either run out of memory or fail to complete in hours.","sentences":["Approximate graph pattern mining (A-GPM) is an important data analysis tool for many graph-based applications.","There exist sampling-based A-GPM systems to provide automation and generalization over a wide variety of use cases.","However, there are two major obstacles that prevent existing A-GPM systems being adopted in practice.","First, the termination mechanism that decides when to end sampling lacks theoretical backup on confidence, and is unstable and slow in practice.","Second, they suffer poor performance when dealing with the \"needle-in-the-hay\" cases, because a huge number of samples are required to converge, given the extremely low hit rate of their fixed sampling schemes.","We build ScaleGPM, an accurate and fast A-GPM system that removes the two obstacles.","First, we propose a novel on-the-fly convergence detection mechanism to achieve stable termination and provide theoretical guarantee on the confidence, with negligible overhead.","Second, we propose two techniques to deal with the \"needle-in-the-hay\" problem, eager-verify and hybrid sampling.","Our eager-verify method improves sampling hit rate by pruning unpromising candidates as early as possible.","Hybrid sampling improves performance by automatically choosing the better scheme between fine-grained and coarse-grained sampling schemes.","Experiments show that our online convergence detection mechanism can detect convergence and results in stable and rapid termination with theoretically guaranteed confidence.","We show the effectiveness of eager-verify in improving the hit rate, and the scheme-selection mechanism in correctly choosing the better scheme for various cases.","Overall, ScaleGPM achieves a geomean average of 565x (up to 610169x) speedup over the state-of-the-art A-GPM system, Arya.","In particular, ScaleGPM handles billion-scale graphs in seconds, where existing systems either run out of memory or fail to complete in hours."],"url":"http://arxiv.org/abs/2405.03488v1"}
{"created":"2024-05-06 13:57:03","title":"UnsafeBench: Benchmarking Image Safety Classifiers on Real-World and AI-Generated Images","abstract":"Image safety classifiers play an important role in identifying and mitigating the spread of unsafe images online (e.g., images including violence, hateful rhetoric, etc.). At the same time, with the advent of text-to-image models and increasing concerns about the safety of AI models, developers are increasingly relying on image safety classifiers to safeguard their models. Yet, the performance of current image safety classifiers remains unknown for real-world and AI-generated images. To bridge this research gap, in this work, we propose UnsafeBench, a benchmarking framework that evaluates the effectiveness and robustness of image safety classifiers. First, we curate a large dataset of 10K real-world and AI-generated images that are annotated as safe or unsafe based on a set of 11 unsafe categories of images (sexual, violent, hateful, etc.). Then, we evaluate the effectiveness and robustness of five popular image safety classifiers, as well as three classifiers that are powered by general-purpose visual language models. Our assessment indicates that existing image safety classifiers are not comprehensive and effective enough in mitigating the multifaceted problem of unsafe images. Also, we find that classifiers trained only on real-world images tend to have degraded performance when applied to AI-generated images. Motivated by these findings, we design and implement a comprehensive image moderation tool called PerspectiveVision, which effectively identifies 11 categories of real-world and AI-generated unsafe images. The best PerspectiveVision model achieves an overall F1-Score of 0.810 on six evaluation datasets, which is comparable with closed-source and expensive state-of-the-art models like GPT-4V. UnsafeBench and PerspectiveVision can aid the research community in better understanding the landscape of image safety classification in the era of generative AI.","sentences":["Image safety classifiers play an important role in identifying and mitigating the spread of unsafe images online (e.g., images including violence, hateful rhetoric, etc.).","At the same time, with the advent of text-to-image models and increasing concerns about the safety of AI models, developers are increasingly relying on image safety classifiers to safeguard their models.","Yet, the performance of current image safety classifiers remains unknown for real-world and AI-generated images.","To bridge this research gap, in this work, we propose UnsafeBench, a benchmarking framework that evaluates the effectiveness and robustness of image safety classifiers.","First, we curate a large dataset of 10K real-world and AI-generated images that are annotated as safe or unsafe based on a set of 11 unsafe categories of images (sexual, violent, hateful, etc.).","Then, we evaluate the effectiveness and robustness of five popular image safety classifiers, as well as three classifiers that are powered by general-purpose visual language models.","Our assessment indicates that existing image safety classifiers are not comprehensive and effective enough in mitigating the multifaceted problem of unsafe images.","Also, we find that classifiers trained only on real-world images tend to have degraded performance when applied to AI-generated images.","Motivated by these findings, we design and implement a comprehensive image moderation tool called PerspectiveVision, which effectively identifies 11 categories of real-world and AI-generated unsafe images.","The best PerspectiveVision model achieves an overall F1-Score of 0.810 on six evaluation datasets, which is comparable with closed-source and expensive state-of-the-art models like GPT-4V. UnsafeBench and PerspectiveVision can aid the research community in better understanding the landscape of image safety classification in the era of generative AI."],"url":"http://arxiv.org/abs/2405.03486v1"}
{"created":"2024-05-06 13:56:56","title":"LGTM: Local-to-Global Text-Driven Human Motion Diffusion Model","abstract":"In this paper, we introduce LGTM, a novel Local-to-Global pipeline for Text-to-Motion generation. LGTM utilizes a diffusion-based architecture and aims to address the challenge of accurately translating textual descriptions into semantically coherent human motion in computer animation. Specifically, traditional methods often struggle with semantic discrepancies, particularly in aligning specific motions to the correct body parts. To address this issue, we propose a two-stage pipeline to overcome this challenge: it first employs large language models (LLMs) to decompose global motion descriptions into part-specific narratives, which are then processed by independent body-part motion encoders to ensure precise local semantic alignment. Finally, an attention-based full-body optimizer refines the motion generation results and guarantees the overall coherence. Our experiments demonstrate that LGTM gains significant improvements in generating locally accurate, semantically-aligned human motion, marking a notable advancement in text-to-motion applications. Code and data for this paper are available at https://github.com/L-Sun/LGTM","sentences":["In this paper, we introduce LGTM, a novel Local-to-Global pipeline for Text-to-Motion generation.","LGTM utilizes a diffusion-based architecture and aims to address the challenge of accurately translating textual descriptions into semantically coherent human motion in computer animation.","Specifically, traditional methods often struggle with semantic discrepancies, particularly in aligning specific motions to the correct body parts.","To address this issue, we propose a two-stage pipeline to overcome this challenge: it first employs large language models (LLMs) to decompose global motion descriptions into part-specific narratives, which are then processed by independent body-part motion encoders to ensure precise local semantic alignment.","Finally, an attention-based full-body optimizer refines the motion generation results and guarantees the overall coherence.","Our experiments demonstrate that LGTM gains significant improvements in generating locally accurate, semantically-aligned human motion, marking a notable advancement in text-to-motion applications.","Code and data for this paper are available at https://github.com/L-Sun/LGTM"],"url":"http://arxiv.org/abs/2405.03485v1"}
