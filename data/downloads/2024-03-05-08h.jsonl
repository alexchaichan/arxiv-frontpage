{"created":"2024-03-03 01:33:47","title":"Bandit Profit-maximization for Targeted Marketing","abstract":"We study a sequential profit-maximization problem, optimizing for both price and ancillary variables like marketing expenditures. Specifically, we aim to maximize profit over an arbitrary sequence of multiple demand curves, each dependent on a distinct ancillary variable, but sharing the same price. A prototypical example is targeted marketing, where a firm (seller) wishes to sell a product over multiple markets. The firm may invest different marketing expenditures for different markets to optimize customer acquisition, but must maintain the same price across all markets. Moreover, markets may have heterogeneous demand curves, each responding to prices and marketing expenditures differently. The firm's objective is to maximize its gross profit, the total revenue minus marketing costs.   Our results are near-optimal algorithms for this class of problems in an adversarial bandit setting, where demand curves are arbitrary non-adaptive sequences, and the firm observes only noisy evaluations of chosen points on the demand curves. We prove a regret upper bound of $\\widetilde{\\mathcal{O}}\\big(nT^{3/4}\\big)$ and a lower bound of $\\Omega\\big((nT)^{3/4}\\big)$ for monotonic demand curves, and a regret bound of $\\widetilde{\\Theta}\\big(nT^{2/3}\\big)$ for demands curves that are monotonic in price and concave in the ancillary variables.","sentences":["We study a sequential profit-maximization problem, optimizing for both price and ancillary variables like marketing expenditures.","Specifically, we aim to maximize profit over an arbitrary sequence of multiple demand curves, each dependent on a distinct ancillary variable, but sharing the same price.","A prototypical example is targeted marketing, where a firm (seller) wishes to sell a product over multiple markets.","The firm may invest different marketing expenditures for different markets to optimize customer acquisition, but must maintain the same price across all markets.","Moreover, markets may have heterogeneous demand curves, each responding to prices and marketing expenditures differently.","The firm's objective is to maximize its gross profit, the total revenue minus marketing costs.   ","Our results are near-optimal algorithms for this class of problems in an adversarial bandit setting, where demand curves are arbitrary non-adaptive sequences, and the firm observes only noisy evaluations of chosen points on the demand curves.","We prove a regret upper bound of $\\widetilde{\\mathcal{O}}\\big(nT^{3/4}\\big)$ and a lower bound of $\\Omega\\big((nT)^{3/4}\\big)$ for monotonic demand curves, and a regret bound of $\\widetilde{\\Theta}\\big(nT^{2/3}\\big)$ for demands curves that are monotonic in price and concave in the ancillary variables."],"url":"http://arxiv.org/abs/2403.01361v1"}
{"created":"2024-03-03 01:26:12","title":"ModelWriter: Text & Model-Synchronized Document Engineering Platform","abstract":"The ModelWriter platform provides a generic framework for automated traceability analysis. In this paper, we demonstrate how this framework can be used to trace the consistency and completeness of technical documents that consist of a set of System Installation Design Principles used by Airbus to ensure the correctness of aircraft system installation. We show in particular, how the platform allows the integration of two types of reasoning: reasoning about the meaning of text using semantic parsing and description logic theorem proving; and reasoning about document structure using first-order relational logic and finite model finding for traceability analysis.","sentences":["The ModelWriter platform provides a generic framework for automated traceability analysis.","In this paper, we demonstrate how this framework can be used to trace the consistency and completeness of technical documents that consist of a set of System Installation Design Principles used by Airbus to ensure the correctness of aircraft system installation.","We show in particular, how the platform allows the integration of two types of reasoning: reasoning about the meaning of text using semantic parsing and description logic theorem proving; and reasoning about document structure using first-order relational logic and finite model finding for traceability analysis."],"url":"http://arxiv.org/abs/2403.01359v1"}
{"created":"2024-03-03 01:09:43","title":"Security and Privacy Enhancing in Blockchain-based IoT Environments via Anonym Auditing","abstract":"The integration of blockchain technology in Internet of Things (IoT) environments is a revolutionary step towards ensuring robust security and enhanced privacy. This paper delves into the unique challenges and solutions associated with securing blockchain-based IoT systems, with a specific focus on anonymous auditing to reinforce privacy and security. We propose a novel framework that combines the decentralized nature of blockchain with advanced security protocols tailored for IoT contexts. Central to our approach is the implementation of anonymization techniques in auditing processes, ensuring user privacy while maintaining the integrity and transparency of blockchain transactions. We outline the architecture of blockchain in IoT environments, emphasizing the workflow and specific security mechanisms employed. Additionally, we introduce a security protocol that integrates privacy-enhancing tools and anonymous auditing methods, including the use of advanced cryptographic techniques for anonymity. This study also includes a comparative analysis of our proposed framework against existing models in the domain. Our work aims to provide a comprehensive blueprint for enhancing security and privacy in blockchain-based IoT environments, paving the way for more secure and private digital ecosystems.","sentences":["The integration of blockchain technology in Internet of Things (IoT) environments is a revolutionary step towards ensuring robust security and enhanced privacy.","This paper delves into the unique challenges and solutions associated with securing blockchain-based IoT systems, with a specific focus on anonymous auditing to reinforce privacy and security.","We propose a novel framework that combines the decentralized nature of blockchain with advanced security protocols tailored for IoT contexts.","Central to our approach is the implementation of anonymization techniques in auditing processes, ensuring user privacy while maintaining the integrity and transparency of blockchain transactions.","We outline the architecture of blockchain in IoT environments, emphasizing the workflow and specific security mechanisms employed.","Additionally, we introduce a security protocol that integrates privacy-enhancing tools and anonymous auditing methods, including the use of advanced cryptographic techniques for anonymity.","This study also includes a comparative analysis of our proposed framework against existing models in the domain.","Our work aims to provide a comprehensive blueprint for enhancing security and privacy in blockchain-based IoT environments, paving the way for more secure and private digital ecosystems."],"url":"http://arxiv.org/abs/2403.01356v1"}
{"created":"2024-03-03 00:56:05","title":"An Overview of Minimum Convex Cover and Maximum Hidden Set","abstract":"We give a review of results on the minimum convex cover and maximum hidden set problems. In addition, we give some new results. First we show that it is NP-hard to determine whether a polygon has the same convex cover number as its hidden set number. We then give some important examples in which these quantities don't always coincide. Finally, We present some consequences of insights from Browne, Kasthurirangan, Mitchell and Polishchuk [FOCS, 2023] on other classes of simple polygons.","sentences":["We give a review of results on the minimum convex cover and maximum hidden set problems.","In addition, we give some new results.","First we show that it is NP-hard to determine whether a polygon has the same convex cover number as its hidden set number.","We then give some important examples in which these quantities don't always coincide.","Finally, We present some consequences of insights from Browne, Kasthurirangan, Mitchell and Polishchuk [FOCS, 2023] on other classes of simple polygons."],"url":"http://arxiv.org/abs/2403.01354v1"}
{"created":"2024-03-03 00:14:12","title":"Improving Uncertainty Sampling with Bell Curve Weight Function","abstract":"Typically, a supervised learning model is trained using passive learning by randomly selecting unlabelled instances to annotate. This approach is effective for learning a model, but can be costly in cases where acquiring labelled instances is expensive. For example, it can be time-consuming to manually identify spam mails (labelled instances) from thousands of emails (unlabelled instances) flooding an inbox during initial data collection. Generally, we answer the above scenario with uncertainty sampling, an active learning method that improves the efficiency of supervised learning by using fewer labelled instances than passive learning. Given an unlabelled data pool, uncertainty sampling queries the labels of instances where the predicted probabilities, p, fall into the uncertainty region, i.e., $p \\approx 0.5$. The newly acquired labels are then added to the existing labelled data pool to learn a new model. Nonetheless, the performance of uncertainty sampling is susceptible to the area of unpredictable responses (AUR) and the nature of the dataset. It is difficult to determine whether to use passive learning or uncertainty sampling without prior knowledge of a new dataset. To address this issue, we propose bell curve sampling, which employs a bell curve weight function to acquire new labels. With the bell curve centred at p=0.5, bell curve sampling selects instances whose predicted values are in the uncertainty area most of the time without neglecting the rest. Simulation results show that, most of the time bell curve sampling outperforms uncertainty sampling and passive learning in datasets of different natures and with AUR.","sentences":["Typically, a supervised learning model is trained using passive learning by randomly selecting unlabelled instances to annotate.","This approach is effective for learning a model, but can be costly in cases where acquiring labelled instances is expensive.","For example, it can be time-consuming to manually identify spam mails (labelled instances) from thousands of emails (unlabelled instances) flooding an inbox during initial data collection.","Generally, we answer the above scenario with uncertainty sampling, an active learning method that improves the efficiency of supervised learning by using fewer labelled instances than passive learning.","Given an unlabelled data pool, uncertainty sampling queries the labels of instances where the predicted probabilities, p, fall into the uncertainty region, i.e., $p \\approx 0.5$. The newly acquired labels are then added to the existing labelled data pool to learn a new model.","Nonetheless, the performance of uncertainty sampling is susceptible to the area of unpredictable responses (AUR) and the nature of the dataset.","It is difficult to determine whether to use passive learning or uncertainty sampling without prior knowledge of a new dataset.","To address this issue, we propose bell curve sampling, which employs a bell curve weight function to acquire new labels.","With the bell curve centred at p=0.5, bell curve sampling selects instances whose predicted values are in the uncertainty area most of the time without neglecting the rest.","Simulation results show that, most of the time bell curve sampling outperforms uncertainty sampling and passive learning in datasets of different natures and with AUR."],"url":"http://arxiv.org/abs/2403.01352v1"}
{"created":"2024-03-03 00:13:26","title":"Efficient FIR filtering with Bit Layer Multiply Accumulator","abstract":"Bit Layer Multiplier Accumulator (BLMAC) is an efficient method to perform dot products without multiplications that exploits the bit level sparsity of the weights. A total of 1,980,000 low, high, band pass and band stop type I FIR filters were generated by systematically sweeping through the cut off frequencies and by varying the number of taps from 55 to 255. After their coefficients were quantized to 16 bits, applying the filter using a BLMAC required, on average, from ~123.3 to ~513.6 additions, depending on the number of taps. A BLMAC dot product machine, specialised for 127 taps FIR filters, was designed for AMD FPGAs. The design footprint is ~110 LUTs, including coefficient and sample storage and is able to apply the filter in ~232 clock cycles on average. This implies a filtering rate of 1.4-3.4 Msamples/s, depending on the FPGA family.","sentences":["Bit Layer Multiplier Accumulator (BLMAC) is an efficient method to perform dot products without multiplications that exploits the bit level sparsity of the weights.","A total of 1,980,000 low, high, band pass and band stop type I FIR filters were generated by systematically sweeping through the cut off frequencies and by varying the number of taps from 55 to 255.","After their coefficients were quantized to 16 bits, applying the filter using a BLMAC required, on average, from ~123.3 to ~513.6 additions, depending on the number of taps.","A BLMAC dot product machine, specialised for 127 taps FIR filters, was designed for AMD FPGAs.","The design footprint is ~110 LUTs, including coefficient and sample storage and is able to apply the filter in ~232 clock cycles on average.","This implies a filtering rate of 1.4-3.4 Msamples/s, depending on the FPGA family."],"url":"http://arxiv.org/abs/2403.01351v1"}
{"created":"2024-03-03 00:03:34","title":"OSM: Leveraging Model Checking for Observing Dynamic 1 behaviors in Aspect-Oriented Applications","abstract":"In the intricate domain of software systems verification, dynamically model checking multifaceted system characteristics remains paramount, yet challenging. This research proposes the advanced observe-based statistical model-checking (OSM) framework, devised to craft executable formal models directly from foundational system code. Leveraging model checking predicates, the framework melds seamlessly with aspect-oriented programming paradigms, yielding a potent method for the analytical verification of varied behavioral attributes. Exploiting the transformative capacity of OSM framework, primary system code undergoes a systematic metamorphosis into multifaceted analysis constructs. This not only simplifies the model verification process but also orchestrates feature interactions using an innovative observing join point abstraction mechanism. Within this framework, components encompassing parsing, formal verification, computational analytics, and rigorous validation are intrinsically interwoven. Marrying the principles of model checking with aspect-oriented (AO) modularization, OSM framework stands as a paragon, proficiently scrutinizing and affirming system specifications. This ensures the unyielding performance of electronic health record systems amidst shifting preconditions. OSM framework offers runtime verification of both object-oriented and AO deployments, positioning itself as an indispensable open-source resource, poised to automate the enhancement of system performance and scalability.","sentences":["In the intricate domain of software systems verification, dynamically model checking multifaceted system characteristics remains paramount, yet challenging.","This research proposes the advanced observe-based statistical model-checking (OSM) framework, devised to craft executable formal models directly from foundational system code.","Leveraging model checking predicates, the framework melds seamlessly with aspect-oriented programming paradigms, yielding a potent method for the analytical verification of varied behavioral attributes.","Exploiting the transformative capacity of OSM framework, primary system code undergoes a systematic metamorphosis into multifaceted analysis constructs.","This not only simplifies the model verification process but also orchestrates feature interactions using an innovative observing join point abstraction mechanism.","Within this framework, components encompassing parsing, formal verification, computational analytics, and rigorous validation are intrinsically interwoven.","Marrying the principles of model checking with aspect-oriented (AO) modularization, OSM framework stands as a paragon, proficiently scrutinizing and affirming system specifications.","This ensures the unyielding performance of electronic health record systems amidst shifting preconditions.","OSM framework offers runtime verification of both object-oriented and AO deployments, positioning itself as an indispensable open-source resource, poised to automate the enhancement of system performance and scalability."],"url":"http://arxiv.org/abs/2403.01349v1"}
{"created":"2024-03-03 00:01:29","title":"SANGRIA: Stacked Autoencoder Neural Networks with Gradient Boosting for Indoor Localization","abstract":"Indoor localization is a critical task in many embedded applications, such as asset tracking, emergency response, and realtime navigation. In this article, we propose a novel fingerprintingbased framework for indoor localization called SANGRIA that uses stacked autoencoder neural networks with gradient boosted trees. Our approach is designed to overcome the device heterogeneity challenge that can create uncertainty in wireless signal measurements across embedded devices used for localization. We compare SANGRIA to several state-of-the-art frameworks and demonstrate 42.96% lower average localization error across diverse indoor locales and heterogeneous devices.","sentences":["Indoor localization is a critical task in many embedded applications, such as asset tracking, emergency response, and realtime navigation.","In this article, we propose a novel fingerprintingbased framework for indoor localization called SANGRIA that uses stacked autoencoder neural networks with gradient boosted trees.","Our approach is designed to overcome the device heterogeneity challenge that can create uncertainty in wireless signal measurements across embedded devices used for localization.","We compare SANGRIA to several state-of-the-art frameworks and demonstrate 42.96% lower average localization error across diverse indoor locales and heterogeneous devices."],"url":"http://arxiv.org/abs/2403.01348v1"}
{"created":"2024-03-02 23:58:33","title":"The Repercussions of the COVID-19 Pandemic on Higher Education and its implications for Syrian Refugees Students (An Analytical Descriptive Study)","abstract":"This study aims to reveal the most important challenges and difficulties that refugee students faced in Jordanian universities (e.g., Yarmouk University, AL Al-Bayt, and the Private Zarqa University) due to the COVID-19 pandemic through measuring a different of indicators that are related, in addition, to identify some of the independent variables on e-educational challenges. In the study, the analytical description approach was used. The data collection tool is a questionnaire, which was distributed to a random sample of students electronically. Results show that the necessity to implement educational and psychological counseling programs and economic support programs to support the e-Learning costs. The study confirmed that refugees are the most affected students with the pandemic compared to the host community.   Keywords: Syrian refugees, COVID-19, e-learning","sentences":["This study aims to reveal the most important challenges and difficulties that refugee students faced in Jordanian universities (e.g., Yarmouk University, AL Al-Bayt, and the Private Zarqa University) due to the COVID-19 pandemic through measuring a different of indicators that are related, in addition, to identify some of the independent variables on e-educational challenges.","In the study, the analytical description approach was used.","The data collection tool is a questionnaire, which was distributed to a random sample of students electronically.","Results show that the necessity to implement educational and psychological counseling programs and economic support programs to support the e-Learning costs.","The study confirmed that refugees are the most affected students with the pandemic compared to the host community.   ","Keywords: Syrian refugees, COVID-19, e-learning"],"url":"http://arxiv.org/abs/2403.01347v1"}
{"created":"2024-03-02 23:53:24","title":"Improve Cost Efficiency of Active Learning over Noisy Dataset","abstract":"Active learning is a learning strategy whereby the machine learning algorithm actively identifies and labels data points to optimize its learning. This strategy is particularly effective in domains where an abundance of unlabeled data exists, but the cost of labeling these data points is prohibitively expensive. In this paper, we consider cases of binary classification, where acquiring a positive instance incurs a significantly higher cost compared to that of negative instances. For example, in the financial industry, such as in money-lending businesses, a defaulted loan constitutes a positive event leading to substantial financial loss. To address this issue, we propose a shifted normal distribution sampling function that samples from a wider range than typical uncertainty sampling. Our simulation underscores that our proposed sampling function limits both noisy and positive label selection, delivering between 20% and 32% improved cost efficiency over different test datasets.","sentences":["Active learning is a learning strategy whereby the machine learning algorithm actively identifies and labels data points to optimize its learning.","This strategy is particularly effective in domains where an abundance of unlabeled data exists, but the cost of labeling these data points is prohibitively expensive.","In this paper, we consider cases of binary classification, where acquiring a positive instance incurs a significantly higher cost compared to that of negative instances.","For example, in the financial industry, such as in money-lending businesses, a defaulted loan constitutes a positive event leading to substantial financial loss.","To address this issue, we propose a shifted normal distribution sampling function that samples from a wider range than typical uncertainty sampling.","Our simulation underscores that our proposed sampling function limits both noisy and positive label selection, delivering between 20% and 32% improved cost efficiency over different test datasets."],"url":"http://arxiv.org/abs/2403.01346v1"}
{"created":"2024-03-02 23:40:23","title":"ShapeBoost: Boosting Human Shape Estimation with Part-Based Parameterization and Clothing-Preserving Augmentation","abstract":"Accurate human shape recovery from a monocular RGB image is a challenging task because humans come in different shapes and sizes and wear different clothes. In this paper, we propose ShapeBoost, a new human shape recovery framework that achieves pixel-level alignment even for rare body shapes and high accuracy for people wearing different types of clothes. Unlike previous approaches that rely on the use of PCA-based shape coefficients, we adopt a new human shape parameterization that decomposes the human shape into bone lengths and the mean width of each part slice. This part-based parameterization technique achieves a balance between flexibility and validity using a semi-analytical shape reconstruction algorithm. Based on this new parameterization, a clothing-preserving data augmentation module is proposed to generate realistic images with diverse body shapes and accurate annotations. Experimental results show that our method outperforms other state-of-the-art methods in diverse body shape situations as well as in varied clothing situations.","sentences":["Accurate human shape recovery from a monocular RGB image is a challenging task because humans come in different shapes and sizes and wear different clothes.","In this paper, we propose ShapeBoost, a new human shape recovery framework that achieves pixel-level alignment even for rare body shapes and high accuracy for people wearing different types of clothes.","Unlike previous approaches that rely on the use of PCA-based shape coefficients, we adopt a new human shape parameterization that decomposes the human shape into bone lengths and the mean width of each part slice.","This part-based parameterization technique achieves a balance between flexibility and validity using a semi-analytical shape reconstruction algorithm.","Based on this new parameterization, a clothing-preserving data augmentation module is proposed to generate realistic images with diverse body shapes and accurate annotations.","Experimental results show that our method outperforms other state-of-the-art methods in diverse body shape situations as well as in varied clothing situations."],"url":"http://arxiv.org/abs/2403.01345v1"}
{"created":"2024-03-02 23:37:16","title":"Mitigating the Bias in the Model for Continual Test-Time Adaptation","abstract":"Continual Test-Time Adaptation (CTA) is a challenging task that aims to adapt a source pre-trained model to continually changing target domains. In the CTA setting, a model does not know when the target domain changes, thus facing a drastic change in the distribution of streaming inputs during the test-time. The key challenge is to keep adapting the model to the continually changing target domains in an online manner. We find that a model shows highly biased predictions as it constantly adapts to the chaining distribution of the target data. It predicts certain classes more often than other classes, making inaccurate over-confident predictions. This paper mitigates this issue to improve performance in the CTA scenario. To alleviate the bias issue, we make class-wise exponential moving average target prototypes with reliable target samples and exploit them to cluster the target features class-wisely. Moreover, we aim to align the target distributions to the source distribution by anchoring the target feature to its corresponding source prototype. With extensive experiments, our proposed method achieves noteworthy performance gain when applied on top of existing CTA methods without substantial adaptation time overhead.","sentences":["Continual Test-Time Adaptation (CTA) is a challenging task that aims to adapt a source pre-trained model to continually changing target domains.","In the CTA setting, a model does not know when the target domain changes, thus facing a drastic change in the distribution of streaming inputs during the test-time.","The key challenge is to keep adapting the model to the continually changing target domains in an online manner.","We find that a model shows highly biased predictions as it constantly adapts to the chaining distribution of the target data.","It predicts certain classes more often than other classes, making inaccurate over-confident predictions.","This paper mitigates this issue to improve performance in the CTA scenario.","To alleviate the bias issue, we make class-wise exponential moving average target prototypes with reliable target samples and exploit them to cluster the target features class-wisely.","Moreover, we aim to align the target distributions to the source distribution by anchoring the target feature to its corresponding source prototype.","With extensive experiments, our proposed method achieves noteworthy performance gain when applied on top of existing CTA methods without substantial adaptation time overhead."],"url":"http://arxiv.org/abs/2403.01344v1"}
{"created":"2024-03-02 23:33:44","title":"The Effectiveness of a Training Program Based on Health Education to Improve Health Empowerment Level among Refugees in Jordan","abstract":"Objectives: The study aimed to evaluate the effectiveness of a health education-based training program in enhancing the level of health empowerment among refugees in Jordan. Health empowerment is a key component to promote health as it enables individuals to control and manage their health outcomes and improve them. Refugees are a vulnerable population group with limited access to healthcare.   Methodology: The study sample consisted of 38 refugees in Irbid governorate, Jordan, who were conveniently selected in coordination with some organizations working in the field of asylum in the governorate. They were randomly divided into two groups: an experimental group (n = 19) that received the health education training program, and a control group (n = 19) that did not receive any health education training. The Health Empowerment Scale (HES), a validated tool, was used to collect data from both groups in the pre and post-tests, and a follow-up test was conducted for members of the experimental group only. Results: The results showed a statistically significant increase in the health empowerment scores for the experimental group that received the training program compared to the control group. The mean of the pre-test for the experimental group was (1.97 - 0.27), and for the control group, it was (1.84 - 0.21). The post-test mean for the experimental group became (3.88 - 0.13), while for the control group, it was (1.85 - 0.20). The follow-up test also demonstrated that the enhanced health empowerment levels were maintained in the experimental group, with no significant difference between the post-test and follow-up scores, indicating the effectiveness of the health education training program in enhancing health empowerment for refugees in Jordan.","sentences":["Objectives: The study aimed to evaluate the effectiveness of a health education-based training program in enhancing the level of health empowerment among refugees in Jordan.","Health empowerment is a key component to promote health as it enables individuals to control and manage their health outcomes and improve them.","Refugees are a vulnerable population group with limited access to healthcare.   ","Methodology:","The study sample consisted of 38 refugees in Irbid governorate, Jordan, who were conveniently selected in coordination with some organizations working in the field of asylum in the governorate.","They were randomly divided into two groups: an experimental group (n = 19) that received the health education training program, and a control group (n = 19) that did not receive any health education training.","The Health Empowerment Scale (HES), a validated tool, was used to collect data from both groups in the pre and post-tests, and a follow-up test was conducted for members of the experimental group only.","Results:","The results showed a statistically significant increase in the health empowerment scores for the experimental group that received the training program compared to the control group.","The mean of the pre-test for the experimental group was (1.97 - 0.27), and for the control group, it was (1.84 - 0.21).","The post-test mean for the experimental group became (3.88 - 0.13), while for the control group, it was (1.85 - 0.20).","The follow-up test also demonstrated that the enhanced health empowerment levels were maintained in the experimental group, with no significant difference between the post-test and follow-up scores, indicating the effectiveness of the health education training program in enhancing health empowerment for refugees in Jordan."],"url":"http://arxiv.org/abs/2403.01343v1"}
{"created":"2024-03-02 23:32:33","title":"LM4OPT: Unveiling the Potential of Large Language Models in Formulating Mathematical Optimization Problems","abstract":"In the rapidly evolving field of natural language processing, the translation of linguistic descriptions into mathematical formulation of optimization problems presents a formidable challenge, demanding intricate understanding and processing capabilities from Large Language Models (LLMs). This study compares prominent LLMs, including GPT-3.5, GPT-4, and Llama-2-7b, in zero-shot and one-shot settings for this task. Our findings show GPT-4's superior performance, particularly in the one-shot scenario. A central part of this research is the introduction of `LM4OPT,' a progressive fine-tuning framework for Llama-2-7b that utilizes noisy embeddings and specialized datasets. However, this research highlights a notable gap in the contextual understanding capabilities of smaller models such as Llama-2-7b compared to larger counterparts, especially in processing lengthy and complex input contexts. Our empirical investigation, utilizing the NL4Opt dataset, unveils that GPT-4 surpasses the baseline performance established by previous research, achieving an F1-score of 0.63, solely based on the problem description in natural language, and without relying on any additional named entity information. GPT-3.5 follows closely, both outperforming the fine-tuned Llama-2-7b. These findings not only benchmark the current capabilities of LLMs in a novel application area but also lay the groundwork for future improvements in mathematical formulation of optimization problems from natural language input.","sentences":["In the rapidly evolving field of natural language processing, the translation of linguistic descriptions into mathematical formulation of optimization problems presents a formidable challenge, demanding intricate understanding and processing capabilities from Large Language Models (LLMs).","This study compares prominent LLMs, including GPT-3.5, GPT-4, and Llama-2-7b, in zero-shot and one-shot settings for this task.","Our findings show GPT-4's superior performance, particularly in the one-shot scenario.","A central part of this research is the introduction of `LM4OPT,' a progressive fine-tuning framework for Llama-2-7b that utilizes noisy embeddings and specialized datasets.","However, this research highlights a notable gap in the contextual understanding capabilities of smaller models such as Llama-2-7b compared to larger counterparts, especially in processing lengthy and complex input contexts.","Our empirical investigation, utilizing the NL4Opt dataset, unveils that GPT-4 surpasses the baseline performance established by previous research, achieving an F1-score of 0.63, solely based on the problem description in natural language, and without relying on any additional named entity information.","GPT-3.5 follows closely, both outperforming the fine-tuned Llama-2-7b.","These findings not only benchmark the current capabilities of LLMs in a novel application area but also lay the groundwork for future improvements in mathematical formulation of optimization problems from natural language input."],"url":"http://arxiv.org/abs/2403.01342v1"}
{"created":"2024-03-02 23:19:10","title":"Uniform $\\mathcal{C}^k$ Approximation of $G$-Invariant and Antisymmetric Functions, Embedding Dimensions, and Polynomial Representations","abstract":"For any subgroup $G$ of the symmetric group $\\mathcal{S}_n$ on $n$ symbols, we present results for the uniform $\\mathcal{C}^k$ approximation of $G$-invariant functions by $G$-invariant polynomials. For the case of totally symmetric functions ($G = \\mathcal{S}_n$), we show that this gives rise to the sum-decomposition Deep Sets ansatz of Zaheer et al. (2018), where both the inner and outer functions can be chosen to be smooth, and moreover, the inner function can be chosen to be independent of the target function being approximated. In particular, we show that the embedding dimension required is independent of the regularity of the target function, the accuracy of the desired approximation, as well as $k$. Next, we show that a similar procedure allows us to obtain a uniform $\\mathcal{C}^k$ approximation of antisymmetric functions as a sum of $K$ terms, where each term is a product of a smooth totally symmetric function and a smooth antisymmetric homogeneous polynomial of degree at most $\\binom{n}{2}$. We also provide upper and lower bounds on $K$ and show that $K$ is independent of the regularity of the target function, the desired approximation accuracy, and $k$.","sentences":["For any subgroup $G$ of the symmetric group $\\mathcal{S}_n$ on $n$ symbols, we present results for the uniform $\\mathcal{C}^k$ approximation of $G$-invariant functions by $G$-invariant polynomials.","For the case of totally symmetric functions ($G = \\mathcal{S}_n$), we show that this gives rise to the sum-decomposition Deep Sets ansatz of Zaheer et al. (2018), where both the inner and outer functions can be chosen to be smooth, and moreover, the inner function can be chosen to be independent of the target function being approximated.","In particular, we show that the embedding dimension required is independent of the regularity of the target function, the accuracy of the desired approximation, as well as $k$. Next, we show that a similar procedure allows us to obtain a uniform $\\mathcal{C}^k$ approximation of antisymmetric functions as a sum of $K$ terms, where each term is a product of a smooth totally symmetric function and a smooth antisymmetric homogeneous polynomial of degree at most $\\binom{n}{2}$. We also provide upper and lower bounds on $K$ and show that $K$ is independent of the regularity of the target function, the desired approximation accuracy, and $k$."],"url":"http://arxiv.org/abs/2403.01339v1"}
{"created":"2024-03-02 22:53:06","title":"Making Hybrid Languages: A Recipe","abstract":"The dominant programming languages support only linear text to express ideas. Visual languages offer graphical representations for entire programs, when viewed with special tools. Hybrid languages, with support from existing tools, allow developers to express their ideas with a mix of textual and graphical syntax tailored to an application domain. This mix puts both kinds of syntax on equal footing and, importantly, the enriched language does not disrupt a programmer's typical workflow. This paper presents a recipe for equipping existing textual programming languages as well as accompanying IDEs with a mechanism for creating and using graphical interactive syntax. It also presents the first hybrid language and IDE created using the recipe.","sentences":["The dominant programming languages support only linear text to express ideas.","Visual languages offer graphical representations for entire programs, when viewed with special tools.","Hybrid languages, with support from existing tools, allow developers to express their ideas with a mix of textual and graphical syntax tailored to an application domain.","This mix puts both kinds of syntax on equal footing and, importantly, the enriched language does not disrupt a programmer's typical workflow.","This paper presents a recipe for equipping existing textual programming languages as well as accompanying IDEs with a mechanism for creating and using graphical interactive syntax.","It also presents the first hybrid language and IDE created using the recipe."],"url":"http://arxiv.org/abs/2403.01335v1"}
{"created":"2024-03-02 22:27:44","title":"Bespoke Non-Stationary Solvers for Fast Sampling of Diffusion and Flow Models","abstract":"This paper introduces Bespoke Non-Stationary (BNS) Solvers, a solver distillation approach to improve sample efficiency of Diffusion and Flow models. BNS solvers are based on a family of non-stationary solvers that provably subsumes existing numerical ODE solvers and consequently demonstrate considerable improvement in sample approximation (PSNR) over these baselines. Compared to model distillation, BNS solvers benefit from a tiny parameter space ($<$200 parameters), fast optimization (two orders of magnitude faster), maintain diversity of samples, and in contrast to previous solver distillation approaches nearly close the gap from standard distillation methods such as Progressive Distillation in the low-medium NFE regime. For example, BNS solver achieves 45 PSNR / 1.76 FID using 16 NFE in class-conditional ImageNet-64. We experimented with BNS solvers for conditional image generation, text-to-image generation, and text-2-audio generation showing significant improvement in sample approximation (PSNR) in all.","sentences":["This paper introduces Bespoke Non-Stationary (BNS) Solvers, a solver distillation approach to improve sample efficiency of Diffusion and Flow models.","BNS solvers are based on a family of non-stationary solvers that provably subsumes existing numerical ODE solvers and consequently demonstrate considerable improvement in sample approximation (PSNR) over these baselines.","Compared to model distillation, BNS solvers benefit from a tiny parameter space ($<$200 parameters), fast optimization (two orders of magnitude faster), maintain diversity of samples, and in contrast to previous solver distillation approaches nearly close the gap from standard distillation methods such as Progressive Distillation in the low-medium NFE regime.","For example, BNS solver achieves 45 PSNR / 1.76 FID using 16 NFE in class-conditional ImageNet-64.","We experimented with BNS solvers for conditional image generation, text-to-image generation, and text-2-audio generation showing significant improvement in sample approximation (PSNR) in all."],"url":"http://arxiv.org/abs/2403.01329v1"}
{"created":"2024-03-02 22:24:31","title":"Euclidean distance compression via deep random features","abstract":"Motivated by the problem of compressing point sets into as few bits as possible while maintaining information about approximate distances between points, we construct random nonlinear maps $\\varphi_\\ell$ that compress point sets in the following way. For a point set $S$, the map $\\varphi_\\ell:\\mathbb{R}^d \\to N^{-1/2}\\{-1,1\\}^N$ has the property that storing $\\varphi_\\ell(S)$ (a \\emph{sketch} of $S$) allows one to report pairwise squared distances between points in $S$ up to some multiplicative $(1\\pm \\epsilon)$ error with high probability as long as the minimum distance is not too small compared to $\\epsilon$. The maps $\\varphi_\\ell$ are the $\\ell$-fold composition of a certain type of random feature mapping. Moreover, we determine how large $N$ needs to be as a function of $\\epsilon$ and other parameters of the point set.   Compared to existing techniques, our maps offer several advantages. The standard method for compressing point sets by random mappings relies on the Johnson-Lindenstrauss lemma which implies that if a set of $n$ points is mapped by a Gaussian random matrix to $\\mathbb{R}^k$ with $k =\\Theta(\\epsilon^{-2}\\log n)$, then pairwise distances between points are preserved up to a multiplicative $(1\\pm \\epsilon)$ error with high probability. The main advantage of our maps $\\varphi_\\ell$ over random linear maps is that ours map point sets directly into the discrete cube $N^{-1/2}\\{-1,1\\}^N$ and so there is no additional step needed to convert the sketch to bits. For some range of parameters, our maps $\\varphi_\\ell$ produce sketches which require fewer bits of storage space.","sentences":["Motivated by the problem of compressing point sets into as few bits as possible while maintaining information about approximate distances between points, we construct random nonlinear maps $\\varphi_\\ell$ that compress point sets in the following way.","For a point set $S$, the map $\\varphi_\\ell:\\mathbb{R}^d \\to N^{-1/2}\\{-1,1\\}^N$ has the property that storing $\\varphi_\\ell(S)$ (a \\emph{sketch} of $S$) allows one to report pairwise squared distances between points in $S$ up to some multiplicative $(1\\pm \\epsilon)$ error with high probability as long as the minimum distance is not too small compared to $\\epsilon$. The maps $\\varphi_\\ell$ are the $\\ell$-fold composition of a certain type of random feature mapping.","Moreover, we determine how large $N$ needs to be as a function of $\\epsilon$ and other parameters of the point set.   ","Compared to existing techniques, our maps offer several advantages.","The standard method for compressing point sets by random mappings relies on the Johnson-Lindenstrauss lemma which implies that if a set of $n$ points is mapped by a Gaussian random matrix to $\\mathbb{R}^k$ with $k =\\Theta(\\epsilon^{-2}\\log n)$, then pairwise distances between points are preserved up to a multiplicative $(1\\pm \\epsilon)$ error with high probability.","The main advantage of our maps $\\varphi_\\ell$ over random linear maps is that ours map point sets directly into the discrete cube $N^{-1/2}\\{-1,1\\}^N$ and so there is no additional step needed to convert the sketch to bits.","For some range of parameters, our maps $\\varphi_\\ell$ produce sketches which require fewer bits of storage space."],"url":"http://arxiv.org/abs/2403.01327v1"}
{"created":"2024-03-02 22:16:47","title":"DNA Family: Boosting Weight-Sharing NAS with Block-Wise Supervisions","abstract":"Neural Architecture Search (NAS), aiming at automatically designing neural architectures by machines, has been considered a key step toward automatic machine learning. One notable NAS branch is the weight-sharing NAS, which significantly improves search efficiency and allows NAS algorithms to run on ordinary computers. Despite receiving high expectations, this category of methods suffers from low search effectiveness. By employing a generalization boundedness tool, we demonstrate that the devil behind this drawback is the untrustworthy architecture rating with the oversized search space of the possible architectures. Addressing this problem, we modularize a large search space into blocks with small search spaces and develop a family of models with the distilling neural architecture (DNA) techniques. These proposed models, namely a DNA family, are capable of resolving multiple dilemmas of the weight-sharing NAS, such as scalability, efficiency, and multi-modal compatibility. Our proposed DNA models can rate all architecture candidates, as opposed to previous works that can only access a sub- search space using heuristic algorithms. Moreover, under a certain computational complexity constraint, our method can seek architectures with different depths and widths. Extensive experimental evaluations show that our models achieve state-of-the-art top-1 accuracy of 78.9% and 83.6% on ImageNet for a mobile convolutional network and a small vision transformer, respectively. Additionally, we provide in-depth empirical analysis and insights into neural architecture ratings. Codes available: \\url{https://github.com/changlin31/DNA}.","sentences":["Neural Architecture Search (NAS), aiming at automatically designing neural architectures by machines, has been considered a key step toward automatic machine learning.","One notable NAS branch is the weight-sharing NAS, which significantly improves search efficiency and allows NAS algorithms to run on ordinary computers.","Despite receiving high expectations, this category of methods suffers from low search effectiveness.","By employing a generalization boundedness tool, we demonstrate that the devil behind this drawback is the untrustworthy architecture rating with the oversized search space of the possible architectures.","Addressing this problem, we modularize a large search space into blocks with small search spaces and develop a family of models with the distilling neural architecture (DNA) techniques.","These proposed models, namely a DNA family, are capable of resolving multiple dilemmas of the weight-sharing NAS, such as scalability, efficiency, and multi-modal compatibility.","Our proposed DNA models can rate all architecture candidates, as opposed to previous works that can only access a sub- search space using heuristic algorithms.","Moreover, under a certain computational complexity constraint, our method can seek architectures with different depths and widths.","Extensive experimental evaluations show that our models achieve state-of-the-art top-1 accuracy of 78.9% and 83.6% on ImageNet for a mobile convolutional network and a small vision transformer, respectively.","Additionally, we provide in-depth empirical analysis and insights into neural architecture ratings.","Codes available: \\url{https://github.com/changlin31/DNA}."],"url":"http://arxiv.org/abs/2403.01326v1"}
{"created":"2024-03-02 22:08:10","title":"NeRF-VPT: Learning Novel View Representations with Neural Radiance Fields via View Prompt Tuning","abstract":"Neural Radiance Fields (NeRF) have garnered remarkable success in novel view synthesis. Nonetheless, the task of generating high-quality images for novel views persists as a critical challenge. While the existing efforts have exhibited commendable progress, capturing intricate details, enhancing textures, and achieving superior Peak Signal-to-Noise Ratio (PSNR) metrics warrant further focused attention and advancement. In this work, we propose NeRF-VPT, an innovative method for novel view synthesis to address these challenges. Our proposed NeRF-VPT employs a cascading view prompt tuning paradigm, wherein RGB information gained from preceding rendering outcomes serves as instructive visual prompts for subsequent rendering stages, with the aspiration that the prior knowledge embedded in the prompts can facilitate the gradual enhancement of rendered image quality. NeRF-VPT only requires sampling RGB data from previous stage renderings as priors at each training stage, without relying on extra guidance or complex techniques. Thus, our NeRF-VPT is plug-and-play and can be readily integrated into existing methods. By conducting comparative analyses of our NeRF-VPT against several NeRF-based approaches on demanding real-scene benchmarks, such as Realistic Synthetic 360, Real Forward-Facing, Replica dataset, and a user-captured dataset, we substantiate that our NeRF-VPT significantly elevates baseline performance and proficiently generates more high-quality novel view images than all the compared state-of-the-art methods. Furthermore, the cascading learning of NeRF-VPT introduces adaptability to scenarios with sparse inputs, resulting in a significant enhancement of accuracy for sparse-view novel view synthesis. The source code and dataset are available at \\url{https://github.com/Freedomcls/NeRF-VPT}.","sentences":["Neural Radiance Fields (NeRF) have garnered remarkable success in novel view synthesis.","Nonetheless, the task of generating high-quality images for novel views persists as a critical challenge.","While the existing efforts have exhibited commendable progress, capturing intricate details, enhancing textures, and achieving superior Peak Signal-to-Noise Ratio (PSNR) metrics warrant further focused attention and advancement.","In this work, we propose NeRF-VPT, an innovative method for novel view synthesis to address these challenges.","Our proposed NeRF-VPT employs a cascading view prompt tuning paradigm, wherein RGB information gained from preceding rendering outcomes serves as instructive visual prompts for subsequent rendering stages, with the aspiration that the prior knowledge embedded in the prompts can facilitate the gradual enhancement of rendered image quality.","NeRF-VPT only requires sampling RGB data from previous stage renderings as priors at each training stage, without relying on extra guidance or complex techniques.","Thus, our NeRF-VPT is plug-and-play and can be readily integrated into existing methods.","By conducting comparative analyses of our NeRF-VPT against several NeRF-based approaches on demanding real-scene benchmarks, such as Realistic Synthetic 360, Real Forward-Facing, Replica dataset, and a user-captured dataset, we substantiate that our NeRF-VPT significantly elevates baseline performance and proficiently generates more high-quality novel view images than all the compared state-of-the-art methods.","Furthermore, the cascading learning of NeRF-VPT introduces adaptability to scenarios with sparse inputs, resulting in a significant enhancement of accuracy for sparse-view novel view synthesis.","The source code and dataset are available at \\url{https://github.com/Freedomcls/NeRF-VPT}."],"url":"http://arxiv.org/abs/2403.01325v1"}
{"created":"2024-03-02 22:01:14","title":"A non-cubic space-filling modular robot","abstract":"Space-filling building blocks of diverse shape permeate nature at all levels of organization, from atoms to honeycombs, and have proven useful in artificial systems, from molecular containers to clay bricks. But, despite the wide variety of space-filling polyhedra known to mathematics, only the cube has been explored in robotics. Thus, here we roboticize a non-cubic space-filling shape: the rhombic dodecahedron. This geometry offers an appealing alternative to cubes as it greatly simplifies rotational motion of one cell about the edge of another, and increases the number of neighbors each cell can communicate with and hold on to. To better understand the challenges and opportunities of these and other space-filling machines, we manufactured 48 rhombic dodecahedral cells and used them to build various superstructures. We report locomotive ability of some of the structures we built, and discuss the dis/advantages of the different designs we tested. We also introduce a strategy for genderless passive docking of cells that generalizes to any polyhedra with radially symmetrical faces. Future work will allow the cells to freely roll/rotate about one another so that they may realize the full potential of their unique shape.","sentences":["Space-filling building blocks of diverse shape permeate nature at all levels of organization, from atoms to honeycombs, and have proven useful in artificial systems, from molecular containers to clay bricks.","But, despite the wide variety of space-filling polyhedra known to mathematics, only the cube has been explored in robotics.","Thus, here we roboticize a non-cubic space-filling shape: the rhombic dodecahedron.","This geometry offers an appealing alternative to cubes as it greatly simplifies rotational motion of one cell about the edge of another, and increases the number of neighbors each cell can communicate with and hold on to.","To better understand the challenges and opportunities of these and other space-filling machines, we manufactured 48 rhombic dodecahedral cells and used them to build various superstructures.","We report locomotive ability of some of the structures we built, and discuss the dis/advantages of the different designs we tested.","We also introduce a strategy for genderless passive docking of cells that generalizes to any polyhedra with radially symmetrical faces.","Future work will allow the cells to freely roll/rotate about one another so that they may realize the full potential of their unique shape."],"url":"http://arxiv.org/abs/2403.01323v1"}
{"created":"2024-03-02 21:33:23","title":"Less is More: Hop-Wise Graph Attention for Scalable and Generalizable Learning on Circuits","abstract":"While graph neural networks (GNNs) have gained popularity for learning circuit representations in various electronic design automation (EDA) tasks, they face challenges in scalability when applied to large graphs and exhibit limited generalizability to new designs. These limitations make them less practical for addressing large-scale, complex circuit problems. In this work we propose HOGA, a novel attention-based model for learning circuit representations in a scalable and generalizable manner. HOGA first computes hop-wise features per node prior to model training. Subsequently, the hop-wise features are solely used to produce node representations through a gated self-attention module, which adaptively learns important features among different hops without involving the graph topology. As a result, HOGA is adaptive to various structures across different circuits and can be efficiently trained in a distributed manner. To demonstrate the efficacy of HOGA, we consider two representative EDA tasks: quality of results (QoR) prediction and functional reasoning. Our experimental results indicate that (1) HOGA reduces estimation error over conventional GNNs by 46.76% for predicting QoR after logic synthesis; (2) HOGA improves 10.0% reasoning accuracy over GNNs for identifying functional blocks on unseen gate-level netlists after complex technology mapping; (3) The training time for HOGA almost linearly decreases with an increase in computing resources.","sentences":["While graph neural networks (GNNs) have gained popularity for learning circuit representations in various electronic design automation (EDA) tasks, they face challenges in scalability when applied to large graphs and exhibit limited generalizability to new designs.","These limitations make them less practical for addressing large-scale, complex circuit problems.","In this work we propose HOGA, a novel attention-based model for learning circuit representations in a scalable and generalizable manner.","HOGA first computes hop-wise features per node prior to model training.","Subsequently, the hop-wise features are solely used to produce node representations through a gated self-attention module, which adaptively learns important features among different hops without involving the graph topology.","As a result, HOGA is adaptive to various structures across different circuits and can be efficiently trained in a distributed manner.","To demonstrate the efficacy of HOGA, we consider two representative EDA tasks: quality of results (QoR) prediction and functional reasoning.","Our experimental results indicate that (1) HOGA reduces estimation error over conventional GNNs by 46.76% for predicting QoR after logic synthesis; (2) HOGA improves 10.0% reasoning accuracy over GNNs for identifying functional blocks on unseen gate-level netlists after complex technology mapping; (3) The training time for HOGA almost linearly decreases with an increase in computing resources."],"url":"http://arxiv.org/abs/2403.01317v1"}
{"created":"2024-03-02 21:29:04","title":"TUMTraf V2X Cooperative Perception Dataset","abstract":"Cooperative perception offers several benefits for enhancing the capabilities of autonomous vehicles and improving road safety. Using roadside sensors in addition to onboard sensors increases reliability and extends the sensor range. External sensors offer higher situational awareness for automated vehicles and prevent occlusions. We propose CoopDet3D, a cooperative multi-modal fusion model, and TUMTraf-V2X, a perception dataset, for the cooperative 3D object detection and tracking task. Our dataset contains 2,000 labeled point clouds and 5,000 labeled images from five roadside and four onboard sensors. It includes 30k 3D boxes with track IDs and precise GPS and IMU data. We labeled eight categories and covered occlusion scenarios with challenging driving maneuvers, like traffic violations, near-miss events, overtaking, and U-turns. Through multiple experiments, we show that our CoopDet3D camera-LiDAR fusion model achieves an increase of +14.36 3D mAP compared to a vehicle camera-LiDAR fusion model. Finally, we make our dataset, model, labeling tool, and dev-kit publicly available on our website: https://tum-traffic-dataset.github.io/tumtraf-v2x.","sentences":["Cooperative perception offers several benefits for enhancing the capabilities of autonomous vehicles and improving road safety.","Using roadside sensors in addition to onboard sensors increases reliability and extends the sensor range.","External sensors offer higher situational awareness for automated vehicles and prevent occlusions.","We propose CoopDet3D, a cooperative multi-modal fusion model, and TUMTraf-V2X, a perception dataset, for the cooperative 3D object detection and tracking task.","Our dataset contains 2,000 labeled point clouds and 5,000 labeled images from five roadside and four onboard sensors.","It includes 30k 3D boxes with track IDs and precise GPS and IMU data.","We labeled eight categories and covered occlusion scenarios with challenging driving maneuvers, like traffic violations, near-miss events, overtaking, and U-turns.","Through multiple experiments, we show that our CoopDet3D camera-LiDAR fusion model achieves an increase of +14.36 3D mAP compared to a vehicle camera-LiDAR fusion model.","Finally, we make our dataset, model, labeling tool, and dev-kit publicly available on our website: https://tum-traffic-dataset.github.io/tumtraf-v2x."],"url":"http://arxiv.org/abs/2403.01316v1"}
{"created":"2024-03-02 21:22:46","title":"Near-optimal Per-Action Regret Bounds for Sleeping Bandits","abstract":"We derive near-optimal per-action regret bounds for sleeping bandits, in which both the sets of available arms and their losses in every round are chosen by an adversary. In a setting with $K$ total arms and at most $A$ available arms in each round over $T$ rounds, the best known upper bound is $O(K\\sqrt{TA\\ln{K}})$, obtained indirectly via minimizing internal sleeping regrets. Compared to the minimax $\\Omega(\\sqrt{TA})$ lower bound, this upper bound contains an extra multiplicative factor of $K\\ln{K}$. We address this gap by directly minimizing the per-action regret using generalized versions of EXP3, EXP3-IX and FTRL with Tsallis entropy, thereby obtaining near-optimal bounds of order $O(\\sqrt{TA\\ln{K}})$ and $O(\\sqrt{T\\sqrt{AK}})$. We extend our results to the setting of bandits with advice from sleeping experts, generalizing EXP4 along the way. This leads to new proofs for a number of existing adaptive and tracking regret bounds for standard non-sleeping bandits. Extending our results to the bandit version of experts that report their confidences leads to new bounds for the confidence regret that depends primarily on the sum of experts' confidences. We prove a lower bound, showing that for any minimax optimal algorithms, there exists an action whose regret is sublinear in $T$ but linear in the number of its active rounds.","sentences":["We derive near-optimal per-action regret bounds for sleeping bandits, in which both the sets of available arms and their losses in every round are chosen by an adversary.","In a setting with $K$ total arms and at most $A$ available arms in each round over $T$ rounds, the best known upper bound is $O(K\\sqrt{TA\\ln{K}})$, obtained indirectly via minimizing internal sleeping regrets.","Compared to the minimax $\\Omega(\\sqrt{TA})$ lower bound, this upper bound contains an extra multiplicative factor of $K\\ln{K}$. We address this gap by directly minimizing the per-action regret using generalized versions of EXP3, EXP3-IX and FTRL with Tsallis entropy, thereby obtaining near-optimal bounds of order $O(\\sqrt{TA\\ln{K}})$ and $O(\\sqrt{T\\sqrt{AK}})$. We extend our results to the setting of bandits with advice from sleeping experts, generalizing EXP4 along the way.","This leads to new proofs for a number of existing adaptive and tracking regret bounds for standard non-sleeping bandits.","Extending our results to the bandit version of experts that report their confidences leads to new bounds for the confidence regret that depends primarily on the sum of experts' confidences.","We prove a lower bound, showing that for any minimax optimal algorithms, there exists an action whose regret is sublinear in $T$ but linear in the number of its active rounds."],"url":"http://arxiv.org/abs/2403.01315v1"}
{"created":"2024-03-02 21:22:36","title":"Superflows: A New Tool for Forensic Network Flow Analysis","abstract":"Network security analysts gather data from diverse sources, from high-level summaries of network flow and traffic volumes to low-level details such as service logs from servers and the contents of individual packets. They validate and check this data against traffic patterns and historical indicators of compromise. Based on the results of this analysis, a decision is made to either automatically manage the traffic or report it to an analyst for further investigation. Unfortunately, due rapidly increasing traffic volumes, there are far more events to check than operational teams can handle for effective forensic analysis. However, just as packets are grouped into flows that share a commonality, we argue that a high-level construct for grouping network flows into a set a flows that share a hypothesis is needed to significantly improve the quality of operational network response by increasing Events Per Analysts Hour (EPAH).   In this paper, we propose a formalism for describing a superflow construct, which we characterize as an aggregation of one or more flows based on an analyst-specific hypothesis about traffic behavior. We demonstrate simple superflow constructions and representations, and perform a case study to explain how the formalism can be used to reduce the volume of data for forensic analysis.","sentences":["Network security analysts gather data from diverse sources, from high-level summaries of network flow and traffic volumes to low-level details such as service logs from servers and the contents of individual packets.","They validate and check this data against traffic patterns and historical indicators of compromise.","Based on the results of this analysis, a decision is made to either automatically manage the traffic or report it to an analyst for further investigation.","Unfortunately, due rapidly increasing traffic volumes, there are far more events to check than operational teams can handle for effective forensic analysis.","However, just as packets are grouped into flows that share a commonality, we argue that a high-level construct for grouping network flows into a set a flows that share a hypothesis is needed to significantly improve the quality of operational network response by increasing Events Per Analysts Hour (EPAH).   ","In this paper, we propose a formalism for describing a superflow construct, which we characterize as an aggregation of one or more flows based on an analyst-specific hypothesis about traffic behavior.","We demonstrate simple superflow constructions and representations, and perform a case study to explain how the formalism can be used to reduce the volume of data for forensic analysis."],"url":"http://arxiv.org/abs/2403.01314v1"}
{"created":"2024-03-02 21:01:01","title":"Image-Based Dietary Assessment: A Healthy Eating Plate Estimation System","abstract":"The nutritional quality of diets has significantly deteriorated over the past two to three decades, a decline often underestimated by the people. This deterioration, coupled with a hectic lifestyle, has contributed to escalating health concerns. Recognizing this issue, researchers at Harvard have advocated for a balanced nutritional plate model to promote health. Inspired by this research, our paper introduces an innovative Image-Based Dietary Assessment system aimed at evaluating the healthiness of meals through image analysis. Our system employs advanced image segmentation and classification techniques to analyze food items on a plate, assess their proportions, and calculate meal adherence to Harvard's healthy eating recommendations. This approach leverages machine learning and nutritional science to empower individuals with actionable insights for healthier eating choices. Our four-step framework involves segmenting the image, classifying the items, conducting a nutritional assessment based on the Harvard Healthy Eating Plate research, and offering tailored recommendations. The prototype system has shown promising results in promoting healthier eating habits by providing an accessible, evidence-based tool for dietary assessment.","sentences":["The nutritional quality of diets has significantly deteriorated over the past two to three decades, a decline often underestimated by the people.","This deterioration, coupled with a hectic lifestyle, has contributed to escalating health concerns.","Recognizing this issue, researchers at Harvard have advocated for a balanced nutritional plate model to promote health.","Inspired by this research, our paper introduces an innovative Image-Based Dietary Assessment system aimed at evaluating the healthiness of meals through image analysis.","Our system employs advanced image segmentation and classification techniques to analyze food items on a plate, assess their proportions, and calculate meal adherence to Harvard's healthy eating recommendations.","This approach leverages machine learning and nutritional science to empower individuals with actionable insights for healthier eating choices.","Our four-step framework involves segmenting the image, classifying the items, conducting a nutritional assessment based on the Harvard Healthy Eating Plate research, and offering tailored recommendations.","The prototype system has shown promising results in promoting healthier eating habits by providing an accessible, evidence-based tool for dietary assessment."],"url":"http://arxiv.org/abs/2403.01310v1"}
{"created":"2024-03-02 20:46:56","title":"VNLP: Turkish NLP Package","abstract":"In this work, we present VNLP: the first dedicated, complete, open-source, well-documented, lightweight, production-ready, state-of-the-art Natural Language Processing (NLP) package for the Turkish language. It contains a wide variety of tools, ranging from the simplest tasks, such as sentence splitting and text normalization, to the more advanced ones, such as text and token classification models. Its token classification models are based on \"Context Model\", a novel architecture that is both an encoder and an auto-regressive model. NLP tasks solved by VNLP models include but are not limited to Sentiment Analysis, Named Entity Recognition, Morphological Analysis \\& Disambiguation and Part-of-Speech Tagging. Moreover, it comes with pre-trained word embeddings and corresponding SentencePiece Unigram tokenizers. VNLP has an open-source GitHub repository, ReadtheDocs documentation, PyPi package for convenient installation, Python and command-line API and a demo page to test all the functionality. Consequently, our main contribution is a complete, compact, easy-to-install and easy-to-use NLP package for Turkish.","sentences":["In this work, we present VNLP: the first dedicated, complete, open-source, well-documented, lightweight, production-ready, state-of-the-art Natural Language Processing (NLP) package for the Turkish language.","It contains a wide variety of tools, ranging from the simplest tasks, such as sentence splitting and text normalization, to the more advanced ones, such as text and token classification models.","Its token classification models are based on \"Context Model\", a novel architecture that is both an encoder and an auto-regressive model.","NLP tasks solved by VNLP models include but are not limited to Sentiment Analysis, Named Entity Recognition, Morphological Analysis \\& Disambiguation and Part-of-Speech Tagging.","Moreover, it comes with pre-trained word embeddings and corresponding SentencePiece Unigram tokenizers.","VNLP has an open-source GitHub repository, ReadtheDocs documentation, PyPi package for convenient installation, Python and command-line API and a demo page to test all the functionality.","Consequently, our main contribution is a complete, compact, easy-to-install and easy-to-use NLP package for Turkish."],"url":"http://arxiv.org/abs/2403.01309v1"}
{"created":"2024-03-02 20:40:11","title":"VBART: The Turkish LLM","abstract":"We present VBART, the first Turkish sequence-to-sequence Large Language Models (LLMs) pre-trained on a large corpus from scratch. VBART are compact LLMs based on good ideas leveraged from BART and mBART models and come in two sizes, Large and XLarge. Fine-tuned VBART models surpass the prior state-of-the-art results in abstractive text summarization, title generation, text paraphrasing, question answering and question generation tasks. They allow fine-tuning for future text generation tasks and datasets, carving a new path for Turkish Natural Language Processing (NLP) research. Our work shows that having a pre-trained LLM for Turkish outperforms up to 3x multilingual models, improving existing results and providing efficient models for training and inference. Moreover, we show that our monolingual tokenizer is 7x more efficient than OpenAI's multilingual tokenizer. Last but not least, we introduce a method to enlarge an existing pre-trained LLM and question the relevancy of Chinchilla Scaling Law to sequence-to-sequence masked language models. Our fine-tuned models, tokenizer and cleaned web corpus of 135 GB are publicly available at huggingface.co/vngrs-ai.","sentences":["We present VBART, the first Turkish sequence-to-sequence Large Language Models (LLMs) pre-trained on a large corpus from scratch.","VBART are compact LLMs based on good ideas leveraged from BART and mBART models and come in two sizes, Large and XLarge.","Fine-tuned VBART models surpass the prior state-of-the-art results in abstractive text summarization, title generation, text paraphrasing, question answering and question generation tasks.","They allow fine-tuning for future text generation tasks and datasets, carving a new path for Turkish Natural Language Processing (NLP) research.","Our work shows that having a pre-trained LLM for Turkish outperforms up to 3x multilingual models, improving existing results and providing efficient models for training and inference.","Moreover, we show that our monolingual tokenizer is 7x more efficient than OpenAI's multilingual tokenizer.","Last but not least, we introduce a method to enlarge an existing pre-trained LLM and question the relevancy of Chinchilla Scaling Law to sequence-to-sequence masked language models.","Our fine-tuned models, tokenizer and cleaned web corpus of 135 GB are publicly available at huggingface.co/vngrs-ai."],"url":"http://arxiv.org/abs/2403.01308v1"}
{"created":"2024-03-02 20:36:10","title":"ICC: Quantifying Image Caption Concreteness for Multimodal Dataset Curation","abstract":"Web-scale training on paired text-image data is becoming increasingly central to multimodal learning, but is challenged by the highly noisy nature of datasets in the wild. Standard data filtering approaches succeed in removing mismatched text-image pairs, but permit semantically related but highly abstract or subjective text. These approaches lack the fine-grained ability to isolate the most concrete samples that provide the strongest signal for learning in a noisy dataset. In this work, we propose a new metric, image caption concreteness, that evaluates caption text without an image reference to measure its concreteness and relevancy for use in multimodal learning. Our approach leverages strong foundation models for measuring visual-semantic information loss in multimodal representations. We demonstrate that this strongly correlates with human evaluation of concreteness in both single-word and sentence-level texts. Moreover, we show that curation using ICC complements existing approaches: It succeeds in selecting the highest quality samples from multimodal web-scale datasets to allow for efficient training in resource-constrained settings.","sentences":["Web-scale training on paired text-image data is becoming increasingly central to multimodal learning, but is challenged by the highly noisy nature of datasets in the wild.","Standard data filtering approaches succeed in removing mismatched text-image pairs, but permit semantically related but highly abstract or subjective text.","These approaches lack the fine-grained ability to isolate the most concrete samples that provide the strongest signal for learning in a noisy dataset.","In this work, we propose a new metric, image caption concreteness, that evaluates caption text without an image reference to measure its concreteness and relevancy for use in multimodal learning.","Our approach leverages strong foundation models for measuring visual-semantic information loss in multimodal representations.","We demonstrate that this strongly correlates with human evaluation of concreteness in both single-word and sentence-level texts.","Moreover, we show that curation using ICC complements existing approaches: It succeeds in selecting the highest quality samples from multimodal web-scale datasets to allow for efficient training in resource-constrained settings."],"url":"http://arxiv.org/abs/2403.01306v1"}
{"created":"2024-03-02 20:25:50","title":"Improving the Validity of Automatically Generated Feedback via Reinforcement Learning","abstract":"Automatically generating feedback via large language models (LLMs) in intelligent tutoring systems and online learning platforms has the potential to improve the learning outcomes of many students. However, both feedback generation and evaluation are challenging: feedback content has to be valid especially in subjects like math, which requires models to understand the problem, the solution, and where the student's error lies. Feedback also has to be pedagogically valid to reflect effective tutoring strategies, such as explaining possible misconceptions and encouraging the student, among other desirable features. In this work, we address both problems of automatically generating and evaluating feedback while considering both correctness and alignment. First, we propose a rubric for evaluating math feedback and show that GPT-4 is able to effectively use it to annotate human-written and LLM-generated feedback. Second, we propose a framework for feedback generation that optimizes both correctness and alignment using reinforcement learning (RL). Specifically, we use GPT-4's annotations to create preferences over feedback pairs in an augmented dataset for training via direct preference optimization (DPO). We show that our methods significantly increase the correctness and alignment of generated feedback with Llama 2, an open-source LLM, qualitatively analyze our generation and evaluation systems using case studies, and outline several areas for future work.","sentences":["Automatically generating feedback via large language models (LLMs) in intelligent tutoring systems and online learning platforms has the potential to improve the learning outcomes of many students.","However, both feedback generation and evaluation are challenging: feedback content has to be valid especially in subjects like math, which requires models to understand the problem, the solution, and where the student's error lies.","Feedback also has to be pedagogically valid to reflect effective tutoring strategies, such as explaining possible misconceptions and encouraging the student, among other desirable features.","In this work, we address both problems of automatically generating and evaluating feedback while considering both correctness and alignment.","First, we propose a rubric for evaluating math feedback and show that GPT-4 is able to effectively use it to annotate human-written and LLM-generated feedback.","Second, we propose a framework for feedback generation that optimizes both correctness and alignment using reinforcement learning (RL).","Specifically, we use GPT-4's annotations to create preferences over feedback pairs in an augmented dataset for training via direct preference optimization (DPO).","We show that our methods significantly increase the correctness and alignment of generated feedback with Llama 2, an open-source LLM, qualitatively analyze our generation and evaluation systems using case studies, and outline several areas for future work."],"url":"http://arxiv.org/abs/2403.01304v1"}
