{"created":"2024-02-21 18:59:13","title":"Coercing LLMs to do and reveal (almost) anything","abstract":"It has recently been shown that adversarial attacks on large language models (LLMs) can \"jailbreak\" the model into making harmful statements. In this work, we argue that the spectrum of adversarial attacks on LLMs is much larger than merely jailbreaking. We provide a broad overview of possible attack surfaces and attack goals. Based on a series of concrete examples, we discuss, categorize and systematize attacks that coerce varied unintended behaviors, such as misdirection, model control, denial-of-service, or data extraction.   We analyze these attacks in controlled experiments, and find that many of them stem from the practice of pre-training LLMs with coding capabilities, as well as the continued existence of strange \"glitch\" tokens in common LLM vocabularies that should be removed for security reasons.","sentences":["It has recently been shown that adversarial attacks on large language models (LLMs) can \"jailbreak\" the model into making harmful statements.","In this work, we argue that the spectrum of adversarial attacks on LLMs is much larger than merely jailbreaking.","We provide a broad overview of possible attack surfaces and attack goals.","Based on a series of concrete examples, we discuss, categorize and systematize attacks that coerce varied unintended behaviors, such as misdirection, model control, denial-of-service, or data extraction.   ","We analyze these attacks in controlled experiments, and find that many of them stem from the practice of pre-training LLMs with coding capabilities, as well as the continued existence of strange \"glitch\" tokens in common LLM vocabularies that should be removed for security reasons."],"url":"http://arxiv.org/abs/2402.14020v1"}
{"created":"2024-02-21 18:56:03","title":"D-Flow: Differentiating through Flows for Controlled Generation","abstract":"Taming the generation outcome of state of the art Diffusion and Flow-Matching (FM) models without having to re-train a task-specific model unlocks a powerful tool for solving inverse problems, conditional generation, and controlled generation in general. In this work we introduce D-Flow, a simple framework for controlling the generation process by differentiating through the flow, optimizing for the source (noise) point. We motivate this framework by our key observation stating that for Diffusion/FM models trained with Gaussian probability paths, differentiating through the generation process projects gradient on the data manifold, implicitly injecting the prior into the optimization process. We validate our framework on linear and non-linear controlled generation problems including: image and audio inverse problems and conditional molecule generation reaching state of the art performance across all.","sentences":["Taming the generation outcome of state of the art Diffusion and Flow-Matching (FM) models without having to re-train a task-specific model unlocks a powerful tool for solving inverse problems, conditional generation, and controlled generation in general.","In this work we introduce D-Flow, a simple framework for controlling the generation process by differentiating through the flow, optimizing for the source (noise) point.","We motivate this framework by our key observation stating that for Diffusion/FM models trained with Gaussian probability paths, differentiating through the generation process projects gradient on the data manifold, implicitly injecting the prior into the optimization process.","We validate our framework on linear and non-linear controlled generation problems including: image and audio inverse problems and conditional molecule generation reaching state of the art performance across all."],"url":"http://arxiv.org/abs/2402.14017v1"}
{"created":"2024-02-21 18:55:20","title":"Is LLM-as-a-Judge Robust? Investigating Universal Adversarial Attacks on Zero-shot LLM Assessment","abstract":"Large Language Models (LLMs) are powerful zero-shot assessors and are increasingly used in real-world situations such as for written exams or benchmarking systems. Despite this, no existing work has analyzed the vulnerability of judge-LLMs against adversaries attempting to manipulate outputs. This work presents the first study on the adversarial robustness of assessment LLMs, where we search for short universal phrases that when appended to texts can deceive LLMs to provide high assessment scores. Experiments on SummEval and TopicalChat demonstrate that both LLM-scoring and pairwise LLM-comparative assessment are vulnerable to simple concatenation attacks, where in particular LLM-scoring is very susceptible and can yield maximum assessment scores irrespective of the input text quality. Interestingly, such attacks are transferable and phrases learned on smaller open-source LLMs can be applied to larger closed-source models, such as GPT3.5. This highlights the pervasive nature of the adversarial vulnerabilities across different judge-LLM sizes, families and methods. Our findings raise significant concerns on the reliability of LLMs-as-a-judge methods, and underscore the importance of addressing vulnerabilities in LLM assessment methods before deployment in high-stakes real-world scenarios.","sentences":["Large Language Models (LLMs) are powerful zero-shot assessors and are increasingly used in real-world situations such as for written exams or benchmarking systems.","Despite this, no existing work has analyzed the vulnerability of judge-LLMs against adversaries attempting to manipulate outputs.","This work presents the first study on the adversarial robustness of assessment LLMs, where we search for short universal phrases that when appended to texts can deceive LLMs to provide high assessment scores.","Experiments on SummEval and TopicalChat demonstrate that both LLM-scoring and pairwise LLM-comparative assessment are vulnerable to simple concatenation attacks, where in particular LLM-scoring is very susceptible and can yield maximum assessment scores irrespective of the input text quality.","Interestingly, such attacks are transferable and phrases learned on smaller open-source LLMs can be applied to larger closed-source models, such as GPT3.5.","This highlights the pervasive nature of the adversarial vulnerabilities across different judge-LLM sizes, families and methods.","Our findings raise significant concerns on the reliability of LLMs-as-a-judge methods, and underscore the importance of addressing vulnerabilities in LLM assessment methods before deployment in high-stakes real-world scenarios."],"url":"http://arxiv.org/abs/2402.14016v1"}
{"created":"2024-02-21 18:54:37","title":"Corrective Machine Unlearning","abstract":"Machine Learning models increasingly face data integrity challenges due to the use of large-scale training datasets drawn from the internet. We study what model developers can do if they detect that some data was manipulated or incorrect. Such manipulated data can cause adverse effects like vulnerability to backdoored samples, systematic biases, and in general, reduced accuracy on certain input domains. Often, all manipulated training samples are not known, and only a small, representative subset of the affected data is flagged.   We formalize \"Corrective Machine Unlearning\" as the problem of mitigating the impact of data affected by unknown manipulations on a trained model, possibly knowing only a subset of impacted samples. We demonstrate that the problem of corrective unlearning has significantly different requirements from traditional privacy-oriented unlearning. We find most existing unlearning methods, including the gold-standard retraining-from-scratch, require most of the manipulated data to be identified for effective corrective unlearning. However, one approach, SSD, achieves limited success in unlearning adverse effects with just a small portion of the manipulated samples, showing the tractability of this setting. We hope our work spurs research towards developing better methods for corrective unlearning and offers practitioners a new strategy to handle data integrity challenges arising from web-scale training.","sentences":["Machine Learning models increasingly face data integrity challenges due to the use of large-scale training datasets drawn from the internet.","We study what model developers can do if they detect that some data was manipulated or incorrect.","Such manipulated data can cause adverse effects like vulnerability to backdoored samples, systematic biases, and in general, reduced accuracy on certain input domains.","Often, all manipulated training samples are not known, and only a small, representative subset of the affected data is flagged.   ","We formalize \"Corrective Machine Unlearning\" as the problem of mitigating the impact of data affected by unknown manipulations on a trained model, possibly knowing only a subset of impacted samples.","We demonstrate that the problem of corrective unlearning has significantly different requirements from traditional privacy-oriented unlearning.","We find most existing unlearning methods, including the gold-standard retraining-from-scratch, require most of the manipulated data to be identified for effective corrective unlearning.","However, one approach, SSD, achieves limited success in unlearning adverse effects with just a small portion of the manipulated samples, showing the tractability of this setting.","We hope our work spurs research towards developing better methods for corrective unlearning and offers practitioners a new strategy to handle data integrity challenges arising from web-scale training."],"url":"http://arxiv.org/abs/2402.14015v1"}
{"created":"2024-02-21 18:52:20","title":"Misalignment, Learning, and Ranking: Harnessing Users Limited Attention","abstract":"In digital health and EdTech, recommendation systems face a significant challenge: users often choose impulsively, in ways that conflict with the platform's long-term payoffs. This misalignment makes it difficult to effectively learn to rank items, as it may hinder exploration of items with greater long-term payoffs. Our paper tackles this issue by utilizing users' limited attention spans. We propose a model where a platform presents items with unknown payoffs to the platform in a ranked list to $T$ users over time. Each user selects an item by first considering a prefix window of these ranked items and then picking the highest preferred item in that window (and the platform observes its payoff for this item). We study the design of online bandit algorithms that obtain vanishing regret against hindsight optimal benchmarks.   We first consider adversarial window sizes and stochastic iid payoffs. We design an active-elimination-based algorithm that achieves an optimal instance-dependent regret bound of $O(\\log(T))$, by showing matching regret upper and lower bounds. The key idea is using the combinatorial structure of the problem to either obtain a large payoff from each item or to explore by getting a sample from that item. This method systematically narrows down the item choices to enhance learning efficiency and payoff.   Second, we consider adversarial payoffs and stochastic iid window sizes. We start from the full-information problem of finding the permutation that maximizes the expected payoff. By a novel combinatorial argument, we characterize the polytope of admissible item selection probabilities by a permutation and show it has a polynomial-size representation. Using this representation, we show how standard algorithms for adversarial online linear optimization in the space of admissible probabilities can be used to obtain a polynomial-time algorithm with $O(\\sqrt{T})$ regret.","sentences":["In digital health and EdTech, recommendation systems face a significant challenge: users often choose impulsively, in ways that conflict with the platform's long-term payoffs.","This misalignment makes it difficult to effectively learn to rank items, as it may hinder exploration of items with greater long-term payoffs.","Our paper tackles this issue by utilizing users' limited attention spans.","We propose a model where a platform presents items with unknown payoffs to the platform in a ranked list to $T$ users over time.","Each user selects an item by first considering a prefix window of these ranked items and then picking the highest preferred item in that window (and the platform observes its payoff for this item).","We study the design of online bandit algorithms that obtain vanishing regret against hindsight optimal benchmarks.   ","We first consider adversarial window sizes and stochastic iid payoffs.","We design an active-elimination-based algorithm that achieves an optimal instance-dependent regret bound of $O(\\log(T))$, by showing matching regret upper and lower bounds.","The key idea is using the combinatorial structure of the problem to either obtain a large payoff from each item or to explore by getting a sample from that item.","This method systematically narrows down the item choices to enhance learning efficiency and payoff.   ","Second, we consider adversarial payoffs and stochastic iid window sizes.","We start from the full-information problem of finding the permutation that maximizes the expected payoff.","By a novel combinatorial argument, we characterize the polytope of admissible item selection probabilities by a permutation and show it has a polynomial-size representation.","Using this representation, we show how standard algorithms for adversarial online linear optimization in the space of admissible probabilities can be used to obtain a polynomial-time algorithm with $O(\\sqrt{T})$ regret."],"url":"http://arxiv.org/abs/2402.14013v1"}
{"created":"2024-02-21 18:51:42","title":"Chasing Convex Functions with Long-term Constraints","abstract":"We introduce and study a family of online metric problems with long-term constraints. In these problems, an online player makes decisions $\\mathbf{x}_t$ in a metric space $(X,d)$ to simultaneously minimize their hitting cost $f_t(\\mathbf{x}_t)$ and switching cost as determined by the metric. Over the time horizon $T$, the player must satisfy a long-term demand constraint $\\sum_{t} c(\\mathbf{x}_t) \\geq 1$, where $c(\\mathbf{x}_t)$ denotes the fraction of demand satisfied at time $t$. Such problems can find a wide array of applications to online resource allocation in sustainable energy and computing systems. We devise optimal competitive and learning-augmented algorithms for specific instantiations of these problems, and further show that our proposed algorithms perform well in numerical experiments.","sentences":["We introduce and study a family of online metric problems with long-term constraints.","In these problems, an online player makes decisions $\\mathbf{x}_t$ in a metric space $(X,d)$ to simultaneously minimize their hitting cost $f_t(\\mathbf{x}_t)$ and switching cost as determined by the metric.","Over the time horizon $T$, the player must satisfy a long-term demand constraint $\\sum_{t} c(\\mathbf{x}_t) \\geq 1$, where $c(\\mathbf{x}_t)$ denotes the fraction of demand satisfied at time $t$. Such problems can find a wide array of applications to online resource allocation in sustainable energy and computing systems.","We devise optimal competitive and learning-augmented algorithms for specific instantiations of these problems, and further show that our proposed algorithms perform well in numerical experiments."],"url":"http://arxiv.org/abs/2402.14012v1"}
{"created":"2024-02-21 18:50:12","title":"Geometry-Informed Neural Networks","abstract":"We introduce the concept of geometry-informed neural networks (GINNs), which encompass (i) learning under geometric constraints, (ii) neural fields as a suitable representation, and (iii) generating diverse solutions to under-determined systems often encountered in geometric tasks. Notably, the GINN formulation does not require training data, and as such can be considered generative modeling driven purely by constraints. We add an explicit diversity loss to mitigate mode collapse. We consider several constraints, in particular, the connectedness of components which we convert to a differentiable loss through Morse theory. Experimentally, we demonstrate the efficacy of the GINN learning paradigm across a range of two and three-dimensional scenarios with increasing levels of complexity.","sentences":["We introduce the concept of geometry-informed neural networks (GINNs), which encompass (i) learning under geometric constraints, (ii) neural fields as a suitable representation, and (iii) generating diverse solutions to under-determined systems often encountered in geometric tasks.","Notably, the GINN formulation does not require training data, and as such can be considered generative modeling driven purely by constraints.","We add an explicit diversity loss to mitigate mode collapse.","We consider several constraints, in particular, the connectedness of components which we convert to a differentiable loss through Morse theory.","Experimentally, we demonstrate the efficacy of the GINN learning paradigm across a range of two and three-dimensional scenarios with increasing levels of complexity."],"url":"http://arxiv.org/abs/2402.14009v1"}
{"created":"2024-02-21 18:49:26","title":"OlympiadBench: A Challenging Benchmark for Promoting AGI with Olympiad-Level Bilingual Multimodal Scientific Problems","abstract":"Recent advancements have seen Large Language Models (LLMs) and Large Multimodal Models (LMMs) surpassing general human capabilities in various tasks, approaching the proficiency level of human experts across multiple domains. With traditional benchmarks becoming less challenging for these models, new rigorous challenges are essential to gauge their advanced abilities. In this work, we present OlympiadBench, an Olympiad-level bilingual multimodal scientific benchmark, featuring 8,952 problems from Olympiad-level mathematics and physics competitions, including the Chinese college entrance exam. Each problem is detailed with expert-level annotations for step-by-step reasoning. Evaluating top-tier models on OlympiadBench, we implement a comprehensive assessment methodology to accurately evaluate model responses. Notably, the best-performing model, GPT-4V, attains an average score of 17.23% on OlympiadBench, with a mere 11.28% in physics, highlighting the benchmark rigor and the intricacy of physical reasoning. Our analysis orienting GPT-4V points out prevalent issues with hallucinations, knowledge omissions, and logical fallacies. We hope that our challenging benchmark can serve as a valuable resource for helping future AGI research endeavors.","sentences":["Recent advancements have seen Large Language Models (LLMs) and Large Multimodal Models (LMMs) surpassing general human capabilities in various tasks, approaching the proficiency level of human experts across multiple domains.","With traditional benchmarks becoming less challenging for these models, new rigorous challenges are essential to gauge their advanced abilities.","In this work, we present OlympiadBench, an Olympiad-level bilingual multimodal scientific benchmark, featuring 8,952 problems from Olympiad-level mathematics and physics competitions, including the Chinese college entrance exam.","Each problem is detailed with expert-level annotations for step-by-step reasoning.","Evaluating top-tier models on OlympiadBench, we implement a comprehensive assessment methodology to accurately evaluate model responses.","Notably, the best-performing model, GPT-4V, attains an average score of 17.23% on OlympiadBench, with a mere 11.28% in physics, highlighting the benchmark rigor and the intricacy of physical reasoning.","Our analysis orienting GPT-4V points out prevalent issues with hallucinations, knowledge omissions, and logical fallacies.","We hope that our challenging benchmark can serve as a valuable resource for helping future AGI research endeavors."],"url":"http://arxiv.org/abs/2402.14008v1"}
{"created":"2024-02-21 18:48:38","title":"Can Watermarks Survive Translation? On the Cross-lingual Consistency of Text Watermark for Large Language Models","abstract":"Text watermarking technology aims to tag and identify content produced by large language models (LLMs) to prevent misuse. In this study, we introduce the concept of ''cross-lingual consistency'' in text watermarking, which assesses the ability of text watermarks to maintain their effectiveness after being translated into other languages. Preliminary empirical results from two LLMs and three watermarking methods reveal that current text watermarking technologies lack consistency when texts are translated into various languages. Based on this observation, we propose a Cross-lingual Watermark Removal Attack (CWRA) to bypass watermarking by first obtaining a response from an LLM in a pivot language, which is then translated into the target language. CWRA can effectively remove watermarks by reducing the Area Under the Curve (AUC) from 0.95 to 0.67 without performance loss. Furthermore, we analyze two key factors that contribute to the cross-lingual consistency in text watermarking and propose a defense method that increases the AUC from 0.67 to 0.88 under CWRA.","sentences":["Text watermarking technology aims to tag and identify content produced by large language models (LLMs) to prevent misuse.","In this study, we introduce the concept of ''cross-lingual consistency'' in text watermarking, which assesses the ability of text watermarks to maintain their effectiveness after being translated into other languages.","Preliminary empirical results from two LLMs and three watermarking methods reveal that current text watermarking technologies lack consistency when texts are translated into various languages.","Based on this observation, we propose a Cross-lingual Watermark Removal Attack (CWRA) to bypass watermarking by first obtaining a response from an LLM in a pivot language, which is then translated into the target language.","CWRA can effectively remove watermarks by reducing the Area Under the Curve (AUC) from 0.95 to 0.67 without performance loss.","Furthermore, we analyze two key factors that contribute to the cross-lingual consistency in text watermarking and propose a defense method that increases the AUC from 0.67 to 0.88 under CWRA."],"url":"http://arxiv.org/abs/2402.14007v1"}
{"created":"2024-02-21 18:44:38","title":"Information Elicitation in Agency Games","abstract":"Rapid progress in scalable, commoditized tools for data collection and data processing has made it possible for firms and policymakers to employ ever more complex metrics as guides for decision-making. These developments have highlighted a prevailing challenge -- deciding *which* metrics to compute. In particular, a firm's ability to compute a wider range of existing metrics does not address the problem of *unknown unknowns*, which reflects informational limitations on the part of the firm. To guide the choice of metrics in the face of this informational problem, we turn to the evaluated agents themselves, who may have more information than a principal about how to measure outcomes effectively. We model this interaction as a simple agency game, where we ask: *When does an agent have an incentive to reveal the observability of a cost-correlated variable to the principal?* There are two effects: better information reduces the agent's information rents but also makes some projects go forward that otherwise would fail. We show that the agent prefers to reveal information that exposes a strong enough differentiation between high and low costs. Expanding the agent's action space to include the ability to *garble* their information, we show that the agent often prefers to garble over full revelation. Still, giving the agent the ability to garble can lead to higher total welfare. Our model has analogies with price discrimination, and we leverage some of these synergies to analyze total welfare.","sentences":["Rapid progress in scalable, commoditized tools for data collection and data processing has made it possible for firms and policymakers to employ ever more complex metrics as guides for decision-making.","These developments have highlighted a prevailing challenge -- deciding *which* metrics to compute.","In particular, a firm's ability to compute a wider range of existing metrics does not address the problem of *unknown unknowns*, which reflects informational limitations on the part of the firm.","To guide the choice of metrics in the face of this informational problem, we turn to the evaluated agents themselves, who may have more information than a principal about how to measure outcomes effectively.","We model this interaction as a simple agency game, where we ask: *When does an agent have an incentive to reveal the observability of a cost-correlated variable to the principal?","*","There are two effects: better information reduces the agent's information rents but also makes some projects go forward that otherwise would fail.","We show that the agent prefers to reveal information that exposes a strong enough differentiation between high and low costs.","Expanding the agent's action space to include the ability to *garble* their information, we show that the agent often prefers to garble over full revelation.","Still, giving the agent the ability to garble can lead to higher total welfare.","Our model has analogies with price discrimination, and we leverage some of these synergies to analyze total welfare."],"url":"http://arxiv.org/abs/2402.14005v1"}
{"created":"2024-02-21 18:40:24","title":"Hallucinations or Attention Misdirection? The Path to Strategic Value Extraction in Business Using Large Language Models","abstract":"Large Language Models with transformer architecture have revolutionized the domain of text generation, setting unprecedented benchmarks. Despite their impressive capabilities, LLMs have been criticized for generating outcomes that deviate from factual accuracy or display logical inconsistencies, phenomena commonly referred to as hallucinations. This term, however, has often been misapplied to any results deviating from the instructor's expectations, which this paper defines as attention misdirection rather than true hallucinations. Understanding the distinction between hallucinations and attention misdirection becomes increasingly relevant in business contexts, where the ramifications of such errors can significantly impact the value extraction from these inherently pre-trained models. This paper highlights the best practices of the PGI, Persona, Grouping, and Intelligence, method, a strategic framework that achieved a remarkable error rate of only 3,15 percent across 4,000 responses generated by GPT in response to a real business challenge. It emphasizes that by equipping experimentation with knowledge, businesses can unlock opportunities for innovation through the use of these natively pre-trained models. This reinforces the notion that strategic application grounded in a skilled team can maximize the benefits of emergent technologies such as the LLMs.","sentences":["Large Language Models with transformer architecture have revolutionized the domain of text generation, setting unprecedented benchmarks.","Despite their impressive capabilities, LLMs have been criticized for generating outcomes that deviate from factual accuracy or display logical inconsistencies, phenomena commonly referred to as hallucinations.","This term, however, has often been misapplied to any results deviating from the instructor's expectations, which this paper defines as attention misdirection rather than true hallucinations.","Understanding the distinction between hallucinations and attention misdirection becomes increasingly relevant in business contexts, where the ramifications of such errors can significantly impact the value extraction from these inherently pre-trained models.","This paper highlights the best practices of the PGI, Persona, Grouping, and Intelligence, method, a strategic framework that achieved a remarkable error rate of only 3,15 percent across 4,000 responses generated by GPT in response to a real business challenge.","It emphasizes that by equipping experimentation with knowledge, businesses can unlock opportunities for innovation through the use of these natively pre-trained models.","This reinforces the notion that strategic application grounded in a skilled team can maximize the benefits of emergent technologies such as the LLMs."],"url":"http://arxiv.org/abs/2402.14002v1"}
{"created":"2024-02-21 18:36:26","title":"Real-time 3D-aware Portrait Editing from a Single Image","abstract":"This work presents 3DPE, a practical tool that can efficiently edit a face image following given prompts, like reference images or text descriptions, in the 3D-aware manner. To this end, a lightweight module is distilled from a 3D portrait generator and a text-to-image model, which provide prior knowledge of face geometry and open-vocabulary editing capability, respectively. Such a design brings two compelling advantages over existing approaches. First, our system achieves real-time editing with a feedforward network (i.e., ~0.04s per image), over 100x faster than the second competitor. Second, thanks to the powerful priors, our module could focus on the learning of editing-related variations, such that it manages to handle various types of editing simultaneously in the training phase and further supports fast adaptation to user-specified novel types of editing during inference (e.g., with ~5min fine-tuning per case). The code, the model, and the interface will be made publicly available to facilitate future research.","sentences":["This work presents 3DPE, a practical tool that can efficiently edit a face image following given prompts, like reference images or text descriptions, in the 3D-aware manner.","To this end, a lightweight module is distilled from a 3D portrait generator and a text-to-image model, which provide prior knowledge of face geometry and open-vocabulary editing capability, respectively.","Such a design brings two compelling advantages over existing approaches.","First, our system achieves real-time editing with a feedforward network (i.e., ~0.04s per image), over 100x faster than the second competitor.","Second, thanks to the powerful priors, our module could focus on the learning of editing-related variations, such that it manages to handle various types of editing simultaneously in the training phase and further supports fast adaptation to user-specified novel types of editing during inference (e.g., with ~5min fine-tuning per case).","The code, the model, and the interface will be made publicly available to facilitate future research."],"url":"http://arxiv.org/abs/2402.14000v1"}
{"created":"2024-02-21 18:24:07","title":"Meditating in Live Stream: An Autoethnographic and Interview Study to Investigate Motivations, Interactions and Challenges","abstract":"Mindfulness practice has many mental and physical well-being benefits. With the increased popularity of live stream technologies and the impact of COVID-19, many people have turned to live stream tools to participate in online meditation sessions. To better understand the practices, challenges, and opportunities in live-stream meditation, we conducted a three-month autoethnographic study, during which two researchers participated in live-stream meditation sessions as the audience. Then we conducted a follow-up semi-structured interview study with 10 experienced live meditation teachers who use different live-stream tools. We found that live meditation, although having a weaker social presence than in-person meditation, facilitates attendees in establishing a practice routine and connecting with other meditators. Teachers use live streams to deliver the meditation practice to the world which also enhances their practice and brand building. We identified the challenges of using live-stream tools for meditation from the perspectives of both audiences and teachers, and provided design recommendations to better utilize live meditation as a resource for mental wellbeing.","sentences":["Mindfulness practice has many mental and physical well-being benefits.","With the increased popularity of live stream technologies and the impact of COVID-19, many people have turned to live stream tools to participate in online meditation sessions.","To better understand the practices, challenges, and opportunities in live-stream meditation, we conducted a three-month autoethnographic study, during which two researchers participated in live-stream meditation sessions as the audience.","Then we conducted a follow-up semi-structured interview study with 10 experienced live meditation teachers who use different live-stream tools.","We found that live meditation, although having a weaker social presence than in-person meditation, facilitates attendees in establishing a practice routine and connecting with other meditators.","Teachers use live streams to deliver the meditation practice to the world which also enhances their practice and brand building.","We identified the challenges of using live-stream tools for meditation from the perspectives of both audiences and teachers, and provided design recommendations to better utilize live meditation as a resource for mental wellbeing."],"url":"http://arxiv.org/abs/2402.13992v1"}
{"created":"2024-02-21 18:23:16","title":"Analysing The Impact of Sequence Composition on Language Model Pre-Training","abstract":"Most language model pre-training frameworks concatenate multiple documents into fixed-length sequences and use causal masking to compute the likelihood of each token given its context; this strategy is widely adopted due to its simplicity and efficiency. However, to this day, the influence of the pre-training sequence composition strategy on the generalisation properties of the model remains under-explored. In this work, we find that applying causal masking can lead to the inclusion of distracting information from previous documents during pre-training, which negatively impacts the performance of the models on language modelling and downstream tasks. In intra-document causal masking, the likelihood of each token is only conditioned on the previous tokens in the same document, eliminating potential distracting information from previous documents and significantly improving performance. Furthermore, we find that concatenating related documents can reduce some potential distractions during pre-training, and our proposed efficient retrieval-based sequence construction method, BM25Chunk, can improve in-context learning (+11.6\\%), knowledge memorisation (+9.8\\%), and context utilisation (+7.2\\%) abilities of language models without sacrificing efficiency.","sentences":["Most language model pre-training frameworks concatenate multiple documents into fixed-length sequences and use causal masking to compute the likelihood of each token given its context; this strategy is widely adopted due to its simplicity and efficiency.","However, to this day, the influence of the pre-training sequence composition strategy on the generalisation properties of the model remains under-explored.","In this work, we find that applying causal masking can lead to the inclusion of distracting information from previous documents during pre-training, which negatively impacts the performance of the models on language modelling and downstream tasks.","In intra-document causal masking, the likelihood of each token is only conditioned on the previous tokens in the same document, eliminating potential distracting information from previous documents and significantly improving performance.","Furthermore, we find that concatenating related documents can reduce some potential distractions during pre-training, and our proposed efficient retrieval-based sequence construction method, BM25Chunk, can improve in-context learning (+11.6\\%), knowledge memorisation (+9.8\\%), and context utilisation (+7.2\\%) abilities of language models without sacrificing efficiency."],"url":"http://arxiv.org/abs/2402.13991v1"}
{"created":"2024-02-21 18:19:20","title":"FedADMM-InSa: An Inexact and Self-Adaptive ADMM for Federated Learning","abstract":"Federated learning (FL) is a promising framework for learning from distributed data while maintaining privacy. The development of efficient FL algorithms encounters various challenges, including heterogeneous data and systems, limited communication capacities, and constrained local computational resources. Recently developed FedADMM methods show great resilience to both data and system heterogeneity. However, they still suffer from performance deterioration if the hyperparameters are not carefully tuned. To address this issue, we propose an inexact and self-adaptive FedADMM algorithm, termed FedADMM-InSa. First, we design an inexactness criterion for the clients' local updates to eliminate the need for empirically setting the local training accuracy. This inexactness criterion can be assessed by each client independently based on its unique condition, thereby reducing the local computational cost and mitigating the undesirable straggle effect. The convergence of the resulting inexact ADMM is proved under the assumption of strongly convex loss functions. Additionally, we present a self-adaptive scheme that dynamically adjusts each client's penalty parameter, enhancing algorithm robustness by mitigating the need for empirical penalty parameter choices for each client. Extensive numerical experiments on both synthetic and real-world datasets are conducted. As validated by some numerical tests, our proposed algorithm can reduce the clients' local computational load significantly and also accelerate the learning process compared to the vanilla FedADMM.","sentences":["Federated learning (FL) is a promising framework for learning from distributed data while maintaining privacy.","The development of efficient FL algorithms encounters various challenges, including heterogeneous data and systems, limited communication capacities, and constrained local computational resources.","Recently developed FedADMM methods show great resilience to both data and system heterogeneity.","However, they still suffer from performance deterioration if the hyperparameters are not carefully tuned.","To address this issue, we propose an inexact and self-adaptive FedADMM algorithm, termed FedADMM-InSa.","First, we design an inexactness criterion for the clients' local updates to eliminate the need for empirically setting the local training accuracy.","This inexactness criterion can be assessed by each client independently based on its unique condition, thereby reducing the local computational cost and mitigating the undesirable straggle effect.","The convergence of the resulting inexact ADMM is proved under the assumption of strongly convex loss functions.","Additionally, we present a self-adaptive scheme that dynamically adjusts each client's penalty parameter, enhancing algorithm robustness by mitigating the need for empirical penalty parameter choices for each client.","Extensive numerical experiments on both synthetic and real-world datasets are conducted.","As validated by some numerical tests, our proposed algorithm can reduce the clients' local computational load significantly and also accelerate the learning process compared to the vanilla FedADMM."],"url":"http://arxiv.org/abs/2402.13989v1"}
{"created":"2024-02-21 18:16:48","title":"A Simple and Yet Fairly Effective Defense for Graph Neural Networks","abstract":"Graph Neural Networks (GNNs) have emerged as the dominant approach for machine learning on graph-structured data. However, concerns have arisen regarding the vulnerability of GNNs to small adversarial perturbations. Existing defense methods against such perturbations suffer from high time complexity and can negatively impact the model's performance on clean graphs. To address these challenges, this paper introduces NoisyGNNs, a novel defense method that incorporates noise into the underlying model's architecture. We establish a theoretical connection between noise injection and the enhancement of GNN robustness, highlighting the effectiveness of our approach. We further conduct extensive empirical evaluations on the node classification task to validate our theoretical findings, focusing on two popular GNNs: the GCN and GIN. The results demonstrate that NoisyGNN achieves superior or comparable defense performance to existing methods while minimizing added time complexity. The NoisyGNN approach is model-agnostic, allowing it to be integrated with different GNN architectures. Successful combinations of our NoisyGNN approach with existing defense techniques demonstrate even further improved adversarial defense results. Our code is publicly available at: https://github.com/Sennadir/NoisyGNN.","sentences":["Graph Neural Networks (GNNs) have emerged as the dominant approach for machine learning on graph-structured data.","However, concerns have arisen regarding the vulnerability of GNNs to small adversarial perturbations.","Existing defense methods against such perturbations suffer from high time complexity and can negatively impact the model's performance on clean graphs.","To address these challenges, this paper introduces NoisyGNNs, a novel defense method that incorporates noise into the underlying model's architecture.","We establish a theoretical connection between noise injection and the enhancement of GNN robustness, highlighting the effectiveness of our approach.","We further conduct extensive empirical evaluations on the node classification task to validate our theoretical findings, focusing on two popular GNNs: the GCN and GIN.","The results demonstrate that NoisyGNN achieves superior or comparable defense performance to existing methods while minimizing added time complexity.","The NoisyGNN approach is model-agnostic, allowing it to be integrated with different GNN architectures.","Successful combinations of our NoisyGNN approach with existing defense techniques demonstrate even further improved adversarial defense results.","Our code is publicly available at: https://github.com/Sennadir/NoisyGNN."],"url":"http://arxiv.org/abs/2402.13987v1"}
{"created":"2024-02-21 18:14:24","title":"On Distributed Computation of the Minimum Triangle Edge Transversal","abstract":"The distance of a graph from being triangle-free is a fundamental graph parameter, counting the number of edges that need to be removed from a graph in order for it to become triangle-free. Its corresponding computational problem is the classic minimum triangle edge transversal problem, and its normalized value is the baseline for triangle-freeness testing algorithms. While triangle-freeness testing has been successfully studied in the distributed setting, computing the distance itself in a distributed setting is unknown, to the best of our knowledge, despite being well-studied in the centralized setting.   This work addresses the computation of the minimum triangle edge transversal in distributed networks. We show with a simple warm-up construction that this is a global task, requiring $\\Omega(D)$ rounds even in the $\\mathsf{LOCAL}$ model with unbounded messages, where $D$ is the diameter of the network. However, we show that approximating this value can be done much faster. A $(1+\\epsilon)$-approximation can be obtained in $\\text{poly}\\log{n}$ rounds, where $n$ is the size of the network graph. Moreover, faster approximations can be obtained, at the cost of increasing the approximation factor to roughly 3, by a reduction to the minimum hypergraph vertex cover problem. With a time overhead of the maximum degree $\\Delta$, this can also be applied to the $\\mathsf{CONGEST}$ model, in which messages are bounded.   Our key technical contribution is proving that computing an exact solution is ``as hard as it gets'' in $\\mathsf{CONGEST}$, requiring a near-quadratic number of rounds. Because this problem is an edge selection problem, as opposed to previous lower bounds that were for node selection problems, major challenges arise in constructing the lower bound, requiring us to develop novel ingredients.","sentences":["The distance of a graph from being triangle-free is a fundamental graph parameter, counting the number of edges that need to be removed from a graph in order for it to become triangle-free.","Its corresponding computational problem is the classic minimum triangle edge transversal problem, and its normalized value is the baseline for triangle-freeness testing algorithms.","While triangle-freeness testing has been successfully studied in the distributed setting, computing the distance itself in a distributed setting is unknown, to the best of our knowledge, despite being well-studied in the centralized setting.   ","This work addresses the computation of the minimum triangle edge transversal in distributed networks.","We show with a simple warm-up construction that this is a global task, requiring $\\Omega(D)$ rounds even in the $\\mathsf{LOCAL}$ model with unbounded messages, where $D$ is the diameter of the network.","However, we show that approximating this value can be done much faster.","A $(1+\\epsilon)$-approximation can be obtained in $\\text{poly}\\log{n}$ rounds, where $n$ is the size of the network graph.","Moreover, faster approximations can be obtained, at the cost of increasing the approximation factor to roughly 3, by a reduction to the minimum hypergraph vertex cover problem.","With a time overhead of the maximum degree $\\Delta$, this can also be applied to the $\\mathsf{CONGEST}$ model, in which messages are bounded.   ","Our key technical contribution is proving that computing an exact solution is ``as hard as it gets'' in $\\mathsf{CONGEST}$, requiring a near-quadratic number of rounds.","Because this problem is an edge selection problem, as opposed to previous lower bounds that were for node selection problems, major challenges arise in constructing the lower bound, requiring us to develop novel ingredients."],"url":"http://arxiv.org/abs/2402.13985v1"}
{"created":"2024-02-21 18:12:07","title":"Stability-Aware Training of Neural Network Interatomic Potentials with Differentiable Boltzmann Estimators","abstract":"Neural network interatomic potentials (NNIPs) are an attractive alternative to ab-initio methods for molecular dynamics (MD) simulations. However, they can produce unstable simulations which sample unphysical states, limiting their usefulness for modeling phenomena occurring over longer timescales. To address these challenges, we present Stability-Aware Boltzmann Estimator (StABlE) Training, a multi-modal training procedure which combines conventional supervised training from quantum-mechanical energies and forces with reference system observables, to produce stable and accurate NNIPs. StABlE Training iteratively runs MD simulations to seek out unstable regions, and corrects the instabilities via supervision with a reference observable. The training procedure is enabled by the Boltzmann Estimator, which allows efficient computation of gradients required to train neural networks to system observables, and can detect both global and local instabilities. We demonstrate our methodology across organic molecules, tetrapeptides, and condensed phase systems, along with using three modern NNIP architectures. In all three cases, StABlE-trained models achieve significant improvements in simulation stability and recovery of structural and dynamic observables. In some cases, StABlE-trained models outperform conventional models trained on datasets 50 times larger. As a general framework applicable across NNIP architectures and systems, StABlE Training is a powerful tool for training stable and accurate NNIPs, particularly in the absence of large reference datasets.","sentences":["Neural network interatomic potentials (NNIPs) are an attractive alternative to ab-initio methods for molecular dynamics (MD) simulations.","However, they can produce unstable simulations which sample unphysical states, limiting their usefulness for modeling phenomena occurring over longer timescales.","To address these challenges, we present Stability-Aware Boltzmann Estimator (StABlE) Training, a multi-modal training procedure which combines conventional supervised training from quantum-mechanical energies and forces with reference system observables, to produce stable and accurate NNIPs.","StABlE Training iteratively runs MD simulations to seek out unstable regions, and corrects the instabilities via supervision with a reference observable.","The training procedure is enabled by the Boltzmann Estimator, which allows efficient computation of gradients required to train neural networks to system observables, and can detect both global and local instabilities.","We demonstrate our methodology across organic molecules, tetrapeptides, and condensed phase systems, along with using three modern NNIP architectures.","In all three cases, StABlE-trained models achieve significant improvements in simulation stability and recovery of structural and dynamic observables.","In some cases, StABlE-trained models outperform conventional models trained on datasets 50 times larger.","As a general framework applicable across NNIP architectures and systems, StABlE Training is a powerful tool for training stable and accurate NNIPs, particularly in the absence of large reference datasets."],"url":"http://arxiv.org/abs/2402.13984v1"}
{"created":"2024-02-21 18:09:04","title":"The Importance of Architecture Choice in Deep Learning for Climate Applications","abstract":"Machine Learning has become a pervasive tool in climate science applications. However, current models fail to address nonstationarity induced by anthropogenic alterations in greenhouse emissions and do not routinely quantify the uncertainty of proposed projections. In this paper, we model the Atlantic Meridional Overturning Circulation (AMOC) which is of major importance to climate in Europe and the US East Coast by transporting warm water to these regions, and has the potential for abrupt collapse. We can generate arbitrarily extreme climate scenarios through arbitrary time scales which we then predict using neural networks. Our analysis shows that the AMOC is predictable using neural networks under a diverse set of climate scenarios. Further experiments reveal that MLPs and Deep Ensembles can learn the physics of the AMOC instead of imitating its progression through autocorrelation. With quantified uncertainty, an intriguing pattern of \"spikes\" before critical points of collapse in the AMOC casts doubt on previous analyses that predicted an AMOC collapse within this century. Our results show that Bayesian Neural Networks perform poorly compared to more dense architectures and care should be taken when applying neural networks to nonstationary scenarios such as climate projections. Further, our results highlight that big NN models might have difficulty in modeling global Earth System dynamics accurately and be successfully applied in nonstationary climate scenarios due to the physics being challenging for neural networks to capture.","sentences":["Machine Learning has become a pervasive tool in climate science applications.","However, current models fail to address nonstationarity induced by anthropogenic alterations in greenhouse emissions and do not routinely quantify the uncertainty of proposed projections.","In this paper, we model the Atlantic Meridional Overturning Circulation (AMOC) which is of major importance to climate in Europe and the US East Coast by transporting warm water to these regions, and has the potential for abrupt collapse.","We can generate arbitrarily extreme climate scenarios through arbitrary time scales which we then predict using neural networks.","Our analysis shows that the AMOC is predictable using neural networks under a diverse set of climate scenarios.","Further experiments reveal that MLPs and Deep Ensembles can learn the physics of the AMOC instead of imitating its progression through autocorrelation.","With quantified uncertainty, an intriguing pattern of \"spikes\" before critical points of collapse in the AMOC casts doubt on previous analyses that predicted an AMOC collapse within this century.","Our results show that Bayesian Neural Networks perform poorly compared to more dense architectures and care should be taken when applying neural networks to nonstationary scenarios such as climate projections.","Further, our results highlight that big NN models might have difficulty in modeling global Earth System dynamics accurately and be successfully applied in nonstationary climate scenarios due to the physics being challenging for neural networks to capture."],"url":"http://arxiv.org/abs/2402.13979v1"}
{"created":"2024-02-21 17:58:10","title":"Linear-Time Graph Neural Networks for Scalable Recommendations","abstract":"In an era of information explosion, recommender systems are vital tools to deliver personalized recommendations for users. The key of recommender systems is to forecast users' future behaviors based on previous user-item interactions. Due to their strong expressive power of capturing high-order connectivities in user-item interaction data, recent years have witnessed a rising interest in leveraging Graph Neural Networks (GNNs) to boost the prediction performance of recommender systems. Nonetheless, classic Matrix Factorization (MF) and Deep Neural Network (DNN) approaches still play an important role in real-world large-scale recommender systems due to their scalability advantages. Despite the existence of GNN-acceleration solutions, it remains an open question whether GNN-based recommender systems can scale as efficiently as classic MF and DNN methods. In this paper, we propose a Linear-Time Graph Neural Network (LTGNN) to scale up GNN-based recommender systems to achieve comparable scalability as classic MF approaches while maintaining GNNs' powerful expressiveness for superior prediction accuracy. Extensive experiments and ablation studies are presented to validate the effectiveness and scalability of the proposed algorithm. Our implementation based on PyTorch is available.","sentences":["In an era of information explosion, recommender systems are vital tools to deliver personalized recommendations for users.","The key of recommender systems is to forecast users' future behaviors based on previous user-item interactions.","Due to their strong expressive power of capturing high-order connectivities in user-item interaction data, recent years have witnessed a rising interest in leveraging Graph Neural Networks (GNNs) to boost the prediction performance of recommender systems.","Nonetheless, classic Matrix Factorization (MF) and Deep Neural Network (DNN) approaches still play an important role in real-world large-scale recommender systems due to their scalability advantages.","Despite the existence of GNN-acceleration solutions, it remains an open question whether GNN-based recommender systems can scale as efficiently as classic MF and DNN methods.","In this paper, we propose a Linear-Time Graph Neural Network (LTGNN) to scale up GNN-based recommender systems to achieve comparable scalability as classic MF approaches while maintaining GNNs' powerful expressiveness for superior prediction accuracy.","Extensive experiments and ablation studies are presented to validate the effectiveness and scalability of the proposed algorithm.","Our implementation based on PyTorch is available."],"url":"http://arxiv.org/abs/2402.13973v1"}
{"created":"2024-02-21 17:49:53","title":"Cybersecurity as a Service","abstract":"With the increasing sophistication and sheer number of cyberattacks, more and more companies come to the conclusion that they have to strengthen their cybersecurity posture. At the same time, well-educated Information technology (IT) security personnel are scarce. Cybersecurity as a service (CSaaS) is one possible solution to tackle this problem by outsourcing security functions to managed security service providers (MSSP). This chapter gives an overview of common CSaaS functions and their providers. Moreover, it provides guidance especially for small- and medium-sized businesses, for asking the appropriate questions when it comes to the selection of a specific MSSP.","sentences":["With the increasing sophistication and sheer number of cyberattacks, more and more companies come to the conclusion that they have to strengthen their cybersecurity posture.","At the same time, well-educated Information technology (IT) security personnel are scarce.","Cybersecurity as a service (CSaaS) is one possible solution to tackle this problem by outsourcing security functions to managed security service providers (MSSP).","This chapter gives an overview of common CSaaS functions and their providers.","Moreover, it provides guidance especially for small- and medium-sized businesses, for asking the appropriate questions when it comes to the selection of a specific MSSP."],"url":"http://arxiv.org/abs/2402.13965v1"}
{"created":"2024-02-21 17:47:20","title":"Towards Building Multilingual Language Model for Medicine","abstract":"In this paper, we aim to develop an open-source, multilingual language model for medicine, that the benefits a wider, linguistically diverse audience from different regions. In general, we present the contribution from the following aspects: first, for multilingual medical-specific adaptation, we construct a new multilingual medical corpus, that contains approximately 25.5B tokens encompassing 6 main languages, termed as MMedC, that enables auto-regressive training for existing general LLMs. second, to monitor the development of multilingual LLMs in medicine, we propose a new multilingual medical multi-choice question-answering benchmark with rationale, termed as MMedBench; third, we have assessed a number of popular, opensource large language models (LLMs) on our benchmark, along with those further auto-regressive trained on MMedC, as a result, our final model, termed as MMedLM 2, with only 7B parameters, achieves superior performance compared to all other open-source models, even rivaling GPT-4 on MMedBench. We will make the resources publicly available, including code, model weights, and datasets.","sentences":["In this paper, we aim to develop an open-source, multilingual language model for medicine, that the benefits a wider, linguistically diverse audience from different regions.","In general, we present the contribution from the following aspects: first, for multilingual medical-specific adaptation, we construct a new multilingual medical corpus, that contains approximately 25.5B tokens encompassing 6 main languages, termed as MMedC, that enables auto-regressive training for existing general LLMs.","second, to monitor the development of multilingual LLMs in medicine, we propose a new multilingual medical multi-choice question-answering benchmark with rationale, termed as MMedBench; third, we have assessed a number of popular, opensource large language models (LLMs) on our benchmark, along with those further auto-regressive trained on MMedC, as a result, our final model, termed as MMedLM 2, with only 7B parameters, achieves superior performance compared to all other open-source models, even rivaling GPT-4 on MMedBench.","We will make the resources publicly available, including code, model weights, and datasets."],"url":"http://arxiv.org/abs/2402.13963v1"}
{"created":"2024-02-21 17:41:17","title":"Retention Induced Biases in a Recommendation System with Heterogeneous Users","abstract":"I examine a conceptual model of a recommendation system (RS) with user inflow and churn dynamics. When inflow and churn balance out, the user distribution reaches a steady state. Changing the recommendation algorithm alters the steady state and creates a transition period. During this period, the RS behaves differently from its new steady state. In particular, A/B experiment metrics obtained in transition periods are biased indicators of the RS's long term performance. Scholars and practitioners, however, often conduct A/B tests shortly after introducing new algorithms to validate their effectiveness. This A/B experiment paradigm, widely regarded as the gold standard for assessing RS improvements, may consequently yield false conclusions. I also briefly discuss the data bias caused by the user retention dynamics.","sentences":["I examine a conceptual model of a recommendation system (RS) with user inflow and churn dynamics.","When inflow and churn balance out, the user distribution reaches a steady state.","Changing the recommendation algorithm alters the steady state and creates a transition period.","During this period, the RS behaves differently from its new steady state.","In particular, A/B experiment metrics obtained in transition periods are biased indicators of the RS's long term performance.","Scholars and practitioners, however, often conduct A/B tests shortly after introducing new algorithms to validate their effectiveness.","This A/B experiment paradigm, widely regarded as the gold standard for assessing RS improvements, may consequently yield false conclusions.","I also briefly discuss the data bias caused by the user retention dynamics."],"url":"http://arxiv.org/abs/2402.13959v1"}
{"created":"2024-02-21 17:37:30","title":"Advancing Audio Fingerprinting Accuracy Addressing Background Noise and Distortion Challenges","abstract":"Audio fingerprinting, exemplified by pioneers like Shazam, has transformed digital audio recognition. However, existing systems struggle with accuracy in challenging conditions, limiting broad applicability. This research proposes an AI and ML integrated audio fingerprinting algorithm to enhance accuracy. Built on the Dejavu Project's foundations, the study emphasizes real-world scenario simulations with diverse background noises and distortions. Signal processing, central to Dejavu's model, includes the Fast Fourier Transform, spectrograms, and peak extraction. The \"constellation\" concept and fingerprint hashing enable unique song identification. Performance evaluation attests to 100% accuracy within a 5-second audio input, with a system showcasing predictable matching speed for efficiency. Storage analysis highlights the critical space-speed trade-off for practical implementation. This research advances audio fingerprinting's adaptability, addressing challenges in varied environments and applications.","sentences":["Audio fingerprinting, exemplified by pioneers like Shazam, has transformed digital audio recognition.","However, existing systems struggle with accuracy in challenging conditions, limiting broad applicability.","This research proposes an AI and ML integrated audio fingerprinting algorithm to enhance accuracy.","Built on the Dejavu Project's foundations, the study emphasizes real-world scenario simulations with diverse background noises and distortions.","Signal processing, central to Dejavu's model, includes the Fast Fourier Transform, spectrograms, and peak extraction.","The \"constellation\" concept and fingerprint hashing enable unique song identification.","Performance evaluation attests to 100% accuracy within a 5-second audio input, with a system showcasing predictable matching speed for efficiency.","Storage analysis highlights the critical space-speed trade-off for practical implementation.","This research advances audio fingerprinting's adaptability, addressing challenges in varied environments and applications."],"url":"http://arxiv.org/abs/2402.13957v1"}
{"created":"2024-02-21 17:36:07","title":"Can You Learn Semantics Through Next-Word Prediction? The Case of Entailment","abstract":"Do LMs infer the semantics of text from co-occurrence patterns in their training data? Merrill et al. (2022) argue that, in theory, probabilities predicted by an optimal LM encode semantic information about entailment relations, but it is unclear whether neural LMs trained on corpora learn entailment in this way because of strong idealizing assumptions made by Merrill et al. In this work, we investigate whether their theory can be used to decode entailment judgments from neural LMs. We find that a test similar to theirs can decode entailment relations between natural sentences, well above random chance, though not perfectly, across many datasets and LMs. This suggests LMs implicitly model aspects of semantics to predict semantic effects on sentence co-occurrence patterns. However, we find the test that predicts entailment in practice works in the opposite direction to the theoretical test. We thus revisit the assumptions underlying the original test, finding its derivation did not adequately account for redundancy in human-written text. We argue that correctly accounting for redundancy related to explanations might derive the observed flipped test and, more generally, improve linguistic theories of human speakers.","sentences":["Do LMs infer the semantics of text from co-occurrence patterns in their training data?","Merrill et al. (2022) argue that, in theory, probabilities predicted by an optimal LM encode semantic information about entailment relations, but it is unclear whether neural LMs trained on corpora learn entailment in this way because of strong idealizing assumptions made by Merrill et al.","In this work, we investigate whether their theory can be used to decode entailment judgments from neural LMs.","We find that a test similar to theirs can decode entailment relations between natural sentences, well above random chance, though not perfectly, across many datasets and LMs.","This suggests LMs implicitly model aspects of semantics to predict semantic effects on sentence co-occurrence patterns.","However, we find the test that predicts entailment in practice works in the opposite direction to the theoretical test.","We thus revisit the assumptions underlying the original test, finding its derivation did not adequately account for redundancy in human-written text.","We argue that correctly accounting for redundancy related to explanations might derive the observed flipped test and, more generally, improve linguistic theories of human speakers."],"url":"http://arxiv.org/abs/2402.13956v1"}
{"created":"2024-02-21 17:35:51","title":"BEE-NET: A deep neural network to identify in-the-wild Bodily Expression of Emotions","abstract":"In this study, we investigate how environmental factors, specifically the scenes and objects involved, can affect the expression of emotions through body language. To this end, we introduce a novel multi-stream deep convolutional neural network named BEE-NET. We also propose a new late fusion strategy that incorporates meta-information on places and objects as prior knowledge in the learning process. Our proposed probabilistic pooling model leverages this information to generate a joint probability distribution of both available and anticipated non-available contextual information in latent space. Importantly, our fusion strategy is differentiable, allowing for end-to-end training and capturing of hidden associations among data points without requiring further post-processing or regularisation. To evaluate our deep model, we use the Body Language Database (BoLD), which is currently the largest available database for the Automatic Identification of the in-the-wild Bodily Expression of Emotions (AIBEE). Our experimental results demonstrate that our proposed approach surpasses the current state-of-the-art in AIBEE by a margin of 2.07%, achieving an Emotional Recognition Score of 66.33%.","sentences":["In this study, we investigate how environmental factors, specifically the scenes and objects involved, can affect the expression of emotions through body language.","To this end, we introduce a novel multi-stream deep convolutional neural network named BEE-NET.","We also propose a new late fusion strategy that incorporates meta-information on places and objects as prior knowledge in the learning process.","Our proposed probabilistic pooling model leverages this information to generate a joint probability distribution of both available and anticipated non-available contextual information in latent space.","Importantly, our fusion strategy is differentiable, allowing for end-to-end training and capturing of hidden associations among data points without requiring further post-processing or regularisation.","To evaluate our deep model, we use the Body Language Database (BoLD), which is currently the largest available database for the Automatic Identification of the in-the-wild Bodily Expression of Emotions (AIBEE).","Our experimental results demonstrate that our proposed approach surpasses the current state-of-the-art in AIBEE by a margin of 2.07%, achieving an Emotional Recognition Score of 66.33%."],"url":"http://arxiv.org/abs/2402.13955v1"}
{"created":"2024-02-21 17:33:13","title":"Measuring Social Biases in Masked Language Models by Proxy of Prediction Quality","abstract":"Social and political scientists often aim to discover and measure distinct biases from text data representations (embeddings). Innovative transformer-based language models produce contextually-aware token embeddings and have achieved state-of-the-art performance for a variety of natural language tasks, but have been shown to encode unwanted biases for downstream applications. In this paper, we evaluate the social biases encoded by transformers trained with the masked language modeling objective using proposed proxy functions within an iterative masking experiment to measure the quality of transformer models' predictions, and assess the preference of MLMs towards disadvantaged and advantaged groups. We compare bias estimations with those produced by other evaluation methods using two benchmark datasets, finding relatively high religious and disability biases across considered MLMs and low gender bias in one dataset relative to the other. Our measures outperform others in their agreement with human annotators. We extend on previous work by evaluating social biases introduced after re-training an MLM under the masked language modeling objective (w.r.t. the model's pre-trained base), and find that proposed measures produce more accurate estimations of relative preference for biased sentences between transformers than others based on our methods.","sentences":["Social and political scientists often aim to discover and measure distinct biases from text data representations (embeddings).","Innovative transformer-based language models produce contextually-aware token embeddings and have achieved state-of-the-art performance for a variety of natural language tasks, but have been shown to encode unwanted biases for downstream applications.","In this paper, we evaluate the social biases encoded by transformers trained with the masked language modeling objective using proposed proxy functions within an iterative masking experiment to measure the quality of transformer models' predictions, and assess the preference of MLMs towards disadvantaged and advantaged groups.","We compare bias estimations with those produced by other evaluation methods using two benchmark datasets, finding relatively high religious and disability biases across considered MLMs and low gender bias in one dataset relative to the other.","Our measures outperform others in their agreement with human annotators.","We extend on previous work by evaluating social biases introduced after re-training an MLM under the masked language modeling objective (w.r.t.","the model's pre-trained base), and find that proposed measures produce more accurate estimations of relative preference for biased sentences between transformers than others based on our methods."],"url":"http://arxiv.org/abs/2402.13954v1"}
{"created":"2024-02-21 17:29:34","title":"Aaronson-Ambainis Conjecture Is True For Random Restrictions","abstract":"In an attempt to show that the acceptance probability of a quantum query algorithm making $q$ queries can be well-approximated almost everywhere by a classical decision tree of depth $\\leq \\text{poly}(q)$, Aaronson and Ambainis proposed the following conjecture: let $f: \\{ \\pm 1\\}^n \\rightarrow [0,1]$ be a degree $d$ polynomial with variance $\\geq \\epsilon$. Then, there exists a coordinate of $f$ with influence $\\geq \\text{poly} (\\epsilon, 1/d)$.   We show that for any polynomial $f: \\{ \\pm 1\\}^n \\rightarrow [0,1]$ of degree $d$ $(d \\geq 2)$ and variance $\\text{Var}[f] \\geq 1/d$, if $\\rho$ denotes a random restriction with survival probability $\\dfrac{\\log(d)}{C_1 d}$, $$ \\text{Pr} \\left[f_{\\rho} \\text{ has a coordinate with influence} \\geq \\dfrac{\\text{Var}[f]^2 }{d^{C_2}} \\right] \\geq \\dfrac{\\text{Var}[f] \\log(d)}{50C_1 d}$$ where $C_1, C_2>0$ are universal constants. Thus, Aaronson-Ambainis conjecture is true for a non-negligible fraction of random restrictions of the given polynomial assuming its variance is not too low.","sentences":["In an attempt to show that the acceptance probability of a quantum query algorithm making $q$ queries can be well-approximated almost everywhere by a classical decision tree of depth $\\leq \\text{poly}(q)$, Aaronson and Ambainis proposed the following conjecture: let $f: \\{ \\pm 1\\}^n \\rightarrow","[0,1]$ be a degree $d$ polynomial with variance $\\geq \\epsilon$. Then, there exists a coordinate of $f$ with influence $\\geq \\text{poly} (\\epsilon, 1/d)$.   We show that for any polynomial $f: \\{ \\pm 1\\}^n \\rightarrow","[0,1]$ of degree $d$ $(d \\geq 2)$ and variance $\\text{Var}[f] \\geq 1/d$, if $\\rho$ denotes a random restriction with survival probability $\\dfrac{\\log(d)}{C_1 d}$, $$ \\text{Pr} \\left[f_{\\rho} \\text{ has a coordinate with influence} \\geq \\dfrac{\\text{Var}[f]^2 }{d^{C_2}} \\right] \\geq \\dfrac{\\text{Var}[f] \\log(d)}{50C_1 d}$$ where $C_1, C_2>0$ are universal constants.","Thus, Aaronson-Ambainis conjecture is true for a non-negligible fraction of random restrictions of the given polynomial assuming its variance is not too low."],"url":"http://arxiv.org/abs/2402.13952v1"}
{"created":"2024-02-21 17:23:59","title":"Making Reasoning Matter: Measuring and Improving Faithfulness of Chain-of-Thought Reasoning","abstract":"Large language models (LLMs) have been shown to perform better when asked to reason step-by-step before answering a question. However, it is unclear to what degree the model's final answer is faithful to the stated reasoning steps. In this paper, we perform a causal mediation analysis on twelve LLMs to examine how intermediate reasoning steps generated by the LLM influence the final outcome and find that LLMs do not reliably use their intermediate reasoning steps when generating an answer. To address this issue, we introduce FRODO, a framework to tailor small-sized LMs to generate correct reasoning steps and robustly reason over these steps. FRODO consists of an inference module that learns to generate correct reasoning steps using an implicit causal reward function and a reasoning module that learns to faithfully reason over these intermediate inferences using a counterfactual and causal preference objective. Our experiments show that FRODO significantly outperforms four competitive baselines. Furthermore, FRODO improves the robustness and generalization ability of the reasoning LM, yielding higher performance on out-of-distribution test sets. Finally, we find that FRODO's rationales are more faithful to its final answer predictions than standard supervised fine-tuning.","sentences":["Large language models (LLMs) have been shown to perform better when asked to reason step-by-step before answering a question.","However, it is unclear to what degree the model's final answer is faithful to the stated reasoning steps.","In this paper, we perform a causal mediation analysis on twelve LLMs to examine how intermediate reasoning steps generated by the LLM influence the final outcome and find that LLMs do not reliably use their intermediate reasoning steps when generating an answer.","To address this issue, we introduce FRODO, a framework to tailor small-sized LMs to generate correct reasoning steps and robustly reason over these steps.","FRODO consists of an inference module that learns to generate correct reasoning steps using an implicit causal reward function and a reasoning module that learns to faithfully reason over these intermediate inferences using a counterfactual and causal preference objective.","Our experiments show that FRODO significantly outperforms four competitive baselines.","Furthermore, FRODO improves the robustness and generalization ability of the reasoning LM, yielding higher performance on out-of-distribution test sets.","Finally, we find that FRODO's rationales are more faithful to its final answer predictions than standard supervised fine-tuning."],"url":"http://arxiv.org/abs/2402.13950v1"}
{"created":"2024-02-21 17:23:43","title":"Generating Realistic Arm Movements in Reinforcement Learning: A Quantitative Comparison of Reward Terms and Task Requirements","abstract":"The mimicking of human-like arm movement characteristics involves the consideration of three factors during control policy synthesis: (a) chosen task requirements, (b) inclusion of noise during movement execution and (c) chosen optimality principles. Previous studies showed that when considering these factors (a-c) individually, it is possible to synthesize arm movements that either kinematically match the experimental data or reproduce the stereotypical triphasic muscle activation pattern. However, to date no quantitative comparison has been made on how realistic the arm movement generated by each factor is; as well as whether a partial or total combination of all factors results in arm movements with human-like kinematic characteristics and a triphasic muscle pattern. To investigate this, we used reinforcement learning to learn a control policy for a musculoskeletal arm model, aiming to discern which combination of factors (a-c) results in realistic arm movements according to four frequently reported stereotypical characteristics. Our findings indicate that incorporating velocity and acceleration requirements into the reaching task, employing reward terms that encourage minimization of mechanical work, hand jerk, and control effort, along with the inclusion of noise during movement, leads to the emergence of realistic human arm movements in reinforcement learning. We expect that the gained insights will help in the future to better predict desired arm movements and corrective forces in wearable assistive devices.","sentences":["The mimicking of human-like arm movement characteristics involves the consideration of three factors during control policy synthesis: (a) chosen task requirements, (b) inclusion of noise during movement execution and (c) chosen optimality principles.","Previous studies showed that when considering these factors (a-c) individually, it is possible to synthesize arm movements that either kinematically match the experimental data or reproduce the stereotypical triphasic muscle activation pattern.","However, to date no quantitative comparison has been made on how realistic the arm movement generated by each factor is; as well as whether a partial or total combination of all factors results in arm movements with human-like kinematic characteristics and a triphasic muscle pattern.","To investigate this, we used reinforcement learning to learn a control policy for a musculoskeletal arm model, aiming to discern which combination of factors (a-c) results in realistic arm movements according to four frequently reported stereotypical characteristics.","Our findings indicate that incorporating velocity and acceleration requirements into the reaching task, employing reward terms that encourage minimization of mechanical work, hand jerk, and control effort, along with the inclusion of noise during movement, leads to the emergence of realistic human arm movements in reinforcement learning.","We expect that the gained insights will help in the future to better predict desired arm movements and corrective forces in wearable assistive devices."],"url":"http://arxiv.org/abs/2402.13949v1"}
{"created":"2024-02-21 17:23:29","title":"Improved Syndrome-based Neural Decoder for Linear Block Codes","abstract":"In this work, we investigate the problem of neural-based error correction decoding, and more specifically, the new so-called syndrome-based decoding technique introduced to tackle scalability in the training phase for larger code sizes. We improve on previous works in terms of allowing full decoding of the message rather than codewords, allowing thus the application to non-systematic codes, and proving that the single-message training property is still viable. The suggested system is implemented and tested on polar codes of sizes (64,32) and (128,64), and a BCH of size (63,51), leading to a significant improvement in both Bit Error Rate (BER) and Frame Error Rate (FER), with gains between 0.3dB and 1dB for the implemented codes in the high Signal-to-Noise Ratio (SNR) regime.","sentences":["In this work, we investigate the problem of neural-based error correction decoding, and more specifically, the new so-called syndrome-based decoding technique introduced to tackle scalability in the training phase for larger code sizes.","We improve on previous works in terms of allowing full decoding of the message rather than codewords, allowing thus the application to non-systematic codes, and proving that the single-message training property is still viable.","The suggested system is implemented and tested on polar codes of sizes (64,32) and (128,64), and a BCH of size (63,51), leading to a significant improvement in both Bit Error Rate (BER) and Frame Error Rate (FER), with gains between 0.3dB and 1dB for the implemented codes in the high Signal-to-Noise Ratio (SNR) regime."],"url":"http://arxiv.org/abs/2402.13948v1"}
{"created":"2024-02-21 17:18:25","title":"AttackGNN: Red-Teaming GNNs in Hardware Security Using Reinforcement Learning","abstract":"Machine learning has shown great promise in addressing several critical hardware security problems. In particular, researchers have developed novel graph neural network (GNN)-based techniques for detecting intellectual property (IP) piracy, detecting hardware Trojans (HTs), and reverse engineering circuits, to name a few. These techniques have demonstrated outstanding accuracy and have received much attention in the community. However, since these techniques are used for security applications, it is imperative to evaluate them thoroughly and ensure they are robust and do not compromise the security of integrated circuits.   In this work, we propose AttackGNN, the first red-team attack on GNN-based techniques in hardware security. To this end, we devise a novel reinforcement learning (RL) agent that generates adversarial examples, i.e., circuits, against the GNN-based techniques. We overcome three challenges related to effectiveness, scalability, and generality to devise a potent RL agent. We target five GNN-based techniques for four crucial classes of problems in hardware security: IP piracy, detecting/localizing HTs, reverse engineering, and hardware obfuscation. Through our approach, we craft circuits that fool all GNNs considered in this work. For instance, to evade IP piracy detection, we generate adversarial pirated circuits that fool the GNN-based defense into classifying our crafted circuits as not pirated. For attacking HT localization GNN, our attack generates HT-infested circuits that fool the defense on all tested circuits. We obtain a similar 100% success rate against GNNs for all classes of problems.","sentences":["Machine learning has shown great promise in addressing several critical hardware security problems.","In particular, researchers have developed novel graph neural network (GNN)-based techniques for detecting intellectual property (IP) piracy, detecting hardware Trojans (HTs), and reverse engineering circuits, to name a few.","These techniques have demonstrated outstanding accuracy and have received much attention in the community.","However, since these techniques are used for security applications, it is imperative to evaluate them thoroughly and ensure they are robust and do not compromise the security of integrated circuits.   ","In this work, we propose AttackGNN, the first red-team attack on GNN-based techniques in hardware security.","To this end, we devise a novel reinforcement learning (RL) agent that generates adversarial examples, i.e., circuits, against the GNN-based techniques.","We overcome three challenges related to effectiveness, scalability, and generality to devise a potent RL agent.","We target five GNN-based techniques for four crucial classes of problems in hardware security: IP piracy, detecting/localizing HTs, reverse engineering, and hardware obfuscation.","Through our approach, we craft circuits that fool all GNNs considered in this work.","For instance, to evade IP piracy detection, we generate adversarial pirated circuits that fool the GNN-based defense into classifying our crafted circuits as not pirated.","For attacking HT localization GNN, our attack generates HT-infested circuits that fool the defense on all tested circuits.","We obtain a similar 100% success rate against GNNs for all classes of problems."],"url":"http://arxiv.org/abs/2402.13946v1"}
{"created":"2024-02-21 17:07:09","title":"What is the focus of XAI in UI design? Prioritizing UI design principles for enhancing XAI user experience","abstract":"With the widespread application of artificial intelligence(AI), the explainable AI (XAI) field has undergone a notable resurgence. In this background, the importance of user experience in XAI has become increasingly prominent. Simultaneously, the user interface (UI) serves as a crucial link between XAI and users. However, despite the existence of UI design principles for XAI, there is a lack of prioritization based on their significance. This will lead practitioners to have a vague understanding of different design principles, making it difficult to allocate design space reasonably and emphasize design focal points. This paper aims to prioritize four design principles, providing clear guidance for UI design in XAI. Initially, we conducted a lightweight summary to derive five user experience standards for non-expert users in XAI. Subsequently, we developed four corresponding webpage prototypes for the four design principles. Nineteen participants then interacted with these prototypes, providing ratings based on five user experience standards, and We calculated the weights of the design principles. Our findings indicate that, for non-expert users, \"sensitivity\" is the optimal UI design principle (weight = 0.3296), followed by \"flexibility\" (weight = 0.3014). Finally, we engage in further discussion and summarization of our research results, and present future works and limitations.","sentences":["With the widespread application of artificial intelligence(AI), the explainable AI (XAI) field has undergone a notable resurgence.","In this background, the importance of user experience in XAI has become increasingly prominent.","Simultaneously, the user interface (UI) serves as a crucial link between XAI and users.","However, despite the existence of UI design principles for XAI, there is a lack of prioritization based on their significance.","This will lead practitioners to have a vague understanding of different design principles, making it difficult to allocate design space reasonably and emphasize design focal points.","This paper aims to prioritize four design principles, providing clear guidance for UI design in XAI.","Initially, we conducted a lightweight summary to derive five user experience standards for non-expert users in XAI.","Subsequently, we developed four corresponding webpage prototypes for the four design principles.","Nineteen participants then interacted with these prototypes, providing ratings based on five user experience standards, and We calculated the weights of the design principles.","Our findings indicate that, for non-expert users, \"sensitivity\" is the optimal UI design principle (weight = 0.3296), followed by \"flexibility\" (weight = 0.3014).","Finally, we engage in further discussion and summarization of our research results, and present future works and limitations."],"url":"http://arxiv.org/abs/2402.13939v1"}
{"created":"2024-02-21 17:07:04","title":"A $(5/3+\u03b5)$-Approximation for Tricolored Non-crossing Euclidean TSP","abstract":"In the Tricolored Euclidean Traveling Salesperson problem, we are given~$k=3$ sets of points in the plane and are looking for disjoint tours, each covering one of the sets. Arora (1998) famously gave a PTAS based on ``patching'' for the case $k=1$ and, recently, Dross et al.~(2023) generalized this result to~$k=2$. Our contribution is a $(5/3+\\epsilon)$-approximation algorithm for~$k=3$ that further generalizes Arora's approach. It is believed that patching is generally no longer possible for more than two tours. We circumvent this issue by either applying a conditional patching scheme for three tours or using an alternative approach based on a weighted solution for $k=2$.","sentences":["In the Tricolored Euclidean Traveling Salesperson problem, we are given~$k=3$ sets of points in the plane and are looking for disjoint tours, each covering one of the sets.","Arora (1998) famously gave a PTAS based on ``patching'' for the case $k=1$ and, recently, Dross et al.~(2023) generalized this result to~$k=2$. Our contribution is a $(5/3+\\epsilon)$-approximation algorithm for~$k=3$ that further generalizes Arora's approach.","It is believed that patching is generally no longer possible for more than two tours.","We circumvent this issue by either applying a conditional patching scheme for three tours or using an alternative approach based on a weighted solution for $k=2$."],"url":"http://arxiv.org/abs/2402.13938v1"}
{"created":"2024-02-21 17:05:06","title":"Distinctive Image Captioning: Leveraging Ground Truth Captions in CLIP Guided Reinforcement Learning","abstract":"Training image captioning models using teacher forcing results in very generic samples, whereas more distinctive captions can be very useful in retrieval applications or to produce alternative texts describing images for accessibility. Reinforcement Learning (RL) allows to use cross-modal retrieval similarity score between the generated caption and the input image as reward to guide the training, leading to more distinctive captions. Recent studies show that pre-trained cross-modal retrieval models can be used to provide this reward, completely eliminating the need for reference captions. However, we argue in this paper that Ground Truth (GT) captions can still be useful in this RL framework. We propose a new image captioning model training strategy that makes use of GT captions in different ways. Firstly, they can be used to train a simple MLP discriminator that serves as a regularization to prevent reward hacking and ensures the fluency of generated captions, resulting in a textual GAN setup extended for multimodal inputs. Secondly, they can serve as additional trajectories in the RL strategy, resulting in a teacher forcing loss weighted by the similarity of the GT to the image. This objective acts as an additional learning signal grounded to the distribution of the GT captions. Thirdly, they can serve as strong baselines when added to the pool of captions used to compute the proposed contrastive reward to reduce the variance of gradient estimate. Experiments on MS-COCO demonstrate the interest of the proposed training strategy to produce highly distinctive captions while maintaining high writing quality.","sentences":["Training image captioning models using teacher forcing results in very generic samples, whereas more distinctive captions can be very useful in retrieval applications or to produce alternative texts describing images for accessibility.","Reinforcement Learning (RL) allows to use cross-modal retrieval similarity score between the generated caption and the input image as reward to guide the training, leading to more distinctive captions.","Recent studies show that pre-trained cross-modal retrieval models can be used to provide this reward, completely eliminating the need for reference captions.","However, we argue in this paper that Ground Truth (GT) captions can still be useful in this RL framework.","We propose a new image captioning model training strategy that makes use of GT captions in different ways.","Firstly, they can be used to train a simple MLP discriminator that serves as a regularization to prevent reward hacking and ensures the fluency of generated captions, resulting in a textual GAN setup extended for multimodal inputs.","Secondly, they can serve as additional trajectories in the RL strategy, resulting in a teacher forcing loss weighted by the similarity of the GT to the image.","This objective acts as an additional learning signal grounded to the distribution of the GT captions.","Thirdly, they can serve as strong baselines when added to the pool of captions used to compute the proposed contrastive reward to reduce the variance of gradient estimate.","Experiments on MS-COCO demonstrate the interest of the proposed training strategy to produce highly distinctive captions while maintaining high writing quality."],"url":"http://arxiv.org/abs/2402.13936v1"}
{"created":"2024-02-21 17:00:56","title":"Do Efficient Transformers Really Save Computation?","abstract":"As transformer-based language models are trained on increasingly large datasets and with vast numbers of parameters, finding more efficient alternatives to the standard Transformer has become very valuable. While many efficient Transformers and Transformer alternatives have been proposed, none provide theoretical guarantees that they are a suitable replacement for the standard Transformer. This makes it challenging to identify when to use a specific model and what directions to prioritize for further investigation. In this paper, we aim to understand the capabilities and limitations of efficient Transformers, specifically the Sparse Transformer and the Linear Transformer. We focus on their reasoning capability as exhibited by Chain-of-Thought (CoT) prompts and follow previous works to model them as Dynamic Programming (DP) problems. Our results show that while these models are expressive enough to solve general DP tasks, contrary to expectations, they require a model size that scales with the problem size. Nonetheless, we identify a class of DP problems for which these models can be more efficient than the standard Transformer. We confirm our theoretical results through experiments on representative DP tasks, adding to the understanding of efficient Transformers' practical strengths and weaknesses.","sentences":["As transformer-based language models are trained on increasingly large datasets and with vast numbers of parameters, finding more efficient alternatives to the standard Transformer has become very valuable.","While many efficient Transformers and Transformer alternatives have been proposed, none provide theoretical guarantees that they are a suitable replacement for the standard Transformer.","This makes it challenging to identify when to use a specific model and what directions to prioritize for further investigation.","In this paper, we aim to understand the capabilities and limitations of efficient Transformers, specifically the Sparse Transformer and the Linear Transformer.","We focus on their reasoning capability as exhibited by Chain-of-Thought (CoT) prompts and follow previous works to model them as Dynamic Programming (DP) problems.","Our results show that while these models are expressive enough to solve general DP tasks, contrary to expectations, they require a model size that scales with the problem size.","Nonetheless, we identify a class of DP problems for which these models can be more efficient than the standard Transformer.","We confirm our theoretical results through experiments on representative DP tasks, adding to the understanding of efficient Transformers' practical strengths and weaknesses."],"url":"http://arxiv.org/abs/2402.13934v1"}
{"created":"2024-02-21 16:59:53","title":"Tumor segmentation on whole slide images: training or prompting?","abstract":"Tumor segmentation stands as a pivotal task in cancer diagnosis. Given the immense dimensions of whole slide images (WSI) in histology, deep learning approaches for WSI classification mainly operate at patch-wise or superpixel-wise level. However, these solutions often struggle to capture global WSI information and cannot directly generate the binary mask. Downsampling the WSI and performing semantic segmentation is another possible approach. While this method offers computational efficiency, it necessitates a large amount of annotated data since resolution reduction may lead to information loss. Visual prompting is a novel paradigm that allows the model to perform new tasks by making subtle modifications to the input space, rather than adapting the model itself. Such approach has demonstrated promising results on many computer vision tasks. In this paper, we show the efficacy of visual prompting in the context of tumor segmentation for three distinct organs. In comparison to classical methods trained for this specific task, our findings reveal that, with appropriate prompt examples, visual prompting can achieve comparable or better performance without extensive fine-tuning.","sentences":["Tumor segmentation stands as a pivotal task in cancer diagnosis.","Given the immense dimensions of whole slide images (WSI) in histology, deep learning approaches for WSI classification mainly operate at patch-wise or superpixel-wise level.","However, these solutions often struggle to capture global WSI information and cannot directly generate the binary mask.","Downsampling the WSI and performing semantic segmentation is another possible approach.","While this method offers computational efficiency, it necessitates a large amount of annotated data since resolution reduction may lead to information loss.","Visual prompting is a novel paradigm that allows the model to perform new tasks by making subtle modifications to the input space, rather than adapting the model itself.","Such approach has demonstrated promising results on many computer vision tasks.","In this paper, we show the efficacy of visual prompting in the context of tumor segmentation for three distinct organs.","In comparison to classical methods trained for this specific task, our findings reveal that, with appropriate prompt examples, visual prompting can achieve comparable or better performance without extensive fine-tuning."],"url":"http://arxiv.org/abs/2402.13932v1"}
{"created":"2024-02-21 16:52:26","title":"Enhancing Reinforcement Learning Agents with Local Guides","abstract":"This paper addresses the problem of integrating local guide policies into a Reinforcement Learning agent. For this, we show how to adapt existing algorithms to this setting before introducing a novel algorithm based on a noisy policy-switching procedure. This approach builds on a proper Approximate Policy Evaluation (APE) scheme to provide a perturbation that carefully leads the local guides towards better actions. We evaluated our method on a set of classical Reinforcement Learning problems, including safety-critical systems where the agent cannot enter some areas at the risk of triggering catastrophic consequences. In all the proposed environments, our agent proved to be efficient at leveraging those policies to improve the performance of any APE-based Reinforcement Learning algorithm, especially in its first learning stages.","sentences":["This paper addresses the problem of integrating local guide policies into a Reinforcement Learning agent.","For this, we show how to adapt existing algorithms to this setting before introducing a novel algorithm based on a noisy policy-switching procedure.","This approach builds on a proper Approximate Policy Evaluation (APE) scheme to provide a perturbation that carefully leads the local guides towards better actions.","We evaluated our method on a set of classical Reinforcement Learning problems, including safety-critical systems where the agent cannot enter some areas at the risk of triggering catastrophic consequences.","In all the proposed environments, our agent proved to be efficient at leveraging those policies to improve the performance of any APE-based Reinforcement Learning algorithm, especially in its first learning stages."],"url":"http://arxiv.org/abs/2402.13930v1"}
{"created":"2024-02-21 16:51:05","title":"SDXL-Lightning: Progressive Adversarial Diffusion Distillation","abstract":"We propose a diffusion distillation method that achieves new state-of-the-art in one-step/few-step 1024px text-to-image generation based on SDXL. Our method combines progressive and adversarial distillation to achieve a balance between quality and mode coverage. In this paper, we discuss the theoretical analysis, discriminator design, model formulation, and training techniques. We open-source our distilled SDXL-Lightning models both as LoRA and full UNet weights.","sentences":["We propose a diffusion distillation method that achieves new state-of-the-art in one-step/few-step 1024px text-to-image generation based on SDXL.","Our method combines progressive and adversarial distillation to achieve a balance between quality and mode coverage.","In this paper, we discuss the theoretical analysis, discriminator design, model formulation, and training techniques.","We open-source our distilled SDXL-Lightning models both as LoRA and full UNet weights."],"url":"http://arxiv.org/abs/2402.13929v1"}
{"created":"2024-02-21 16:48:07","title":"The Delusional Hedge Algorithm as a Model of Human Learning from Diverse Opinions","abstract":"Whereas cognitive models of learning often assume direct experience with both the features of an event and with a true label or outcome, much of everyday learning arises from hearing the opinions of others, without direct access to either the experience or the ground truth outcome. We consider how people can learn which opinions to trust in such scenarios by extending the hedge algorithm: a classic solution for learning from diverse information sources. We first introduce a semi-supervised variant we call the delusional hedge capable of learning from both supervised and unsupervised experiences. In two experiments, we examine the alignment between human judgments and predictions from the standard hedge, the delusional hedge, and a heuristic baseline model. Results indicate that humans effectively incorporate both labeled and unlabeled information in a manner consistent with the delusional hedge algorithm -- suggesting that human learners not only gauge the accuracy of information sources but also their consistency with other reliable sources. The findings advance our understanding of human learning from diverse opinions, with implications for the development of algorithms that better capture how people learn to weigh conflicting information sources.","sentences":["Whereas cognitive models of learning often assume direct experience with both the features of an event and with a true label or outcome, much of everyday learning arises from hearing the opinions of others, without direct access to either the experience or the ground truth outcome.","We consider how people can learn which opinions to trust in such scenarios by extending the hedge algorithm: a classic solution for learning from diverse information sources.","We first introduce a semi-supervised variant we call the delusional hedge capable of learning from both supervised and unsupervised experiences.","In two experiments, we examine the alignment between human judgments and predictions from the standard hedge, the delusional hedge, and a heuristic baseline model.","Results indicate that humans effectively incorporate both labeled and unlabeled information in a manner consistent with the delusional hedge algorithm -- suggesting that human learners not only gauge the accuracy of information sources but also their consistency with other reliable sources.","The findings advance our understanding of human learning from diverse opinions, with implications for the development of algorithms that better capture how people learn to weigh conflicting information sources."],"url":"http://arxiv.org/abs/2402.13927v1"}
{"created":"2024-02-21 16:46:36","title":"Large Language Models are Vulnerable to Bait-and-Switch Attacks for Generating Harmful Content","abstract":"The risks derived from large language models (LLMs) generating deceptive and damaging content have been the subject of considerable research, but even safe generations can lead to problematic downstream impacts. In our study, we shift the focus to how even safe text coming from LLMs can be easily turned into potentially dangerous content through Bait-and-Switch attacks. In such attacks, the user first prompts LLMs with safe questions and then employs a simple find-and-replace post-hoc technique to manipulate the outputs into harmful narratives. The alarming efficacy of this approach in generating toxic content highlights a significant challenge in developing reliable safety guardrails for LLMs. In particular, we stress that focusing on the safety of the verbatim LLM outputs is insufficient and that we also need to consider post-hoc transformations.","sentences":["The risks derived from large language models (LLMs) generating deceptive and damaging content have been the subject of considerable research, but even safe generations can lead to problematic downstream impacts.","In our study, we shift the focus to how even safe text coming from LLMs can be easily turned into potentially dangerous content through Bait-and-Switch attacks.","In such attacks, the user first prompts LLMs with safe questions and then employs a simple find-and-replace post-hoc technique to manipulate the outputs into harmful narratives.","The alarming efficacy of this approach in generating toxic content highlights a significant challenge in developing reliable safety guardrails for LLMs.","In particular, we stress that focusing on the safety of the verbatim LLM outputs is insufficient and that we also need to consider post-hoc transformations."],"url":"http://arxiv.org/abs/2402.13926v1"}
{"created":"2024-02-21 16:45:02","title":"UMAT4COMSOL: An Abaqus user material (UMAT) subroutine wrapper for COMSOL","abstract":"We present a wrapper that allows Abaqus user material subroutines (UMATs) to be used as an External Material library in the software COMSOL Multiphysics. The wrapper, written in C language, transforms COMSOL's external material subroutine inputs and outputs into Fortran-coded Abaqus UMAT inputs and outputs, by means of a consistent variable transformation. This significantly facilitates conducting coupled, multi-physics studies employing the advanced material models that the solid mechanics community has developed over the past decades. We exemplify the potential of our new framework, UMAT4COMSOL, by conducting numerical experiments in the areas of elastoplasticity, hyperelasticity and crystal plasticity. The source code, detailed documentation and example tutorials are made freely available to download at www.empaneda.com/codes.","sentences":["We present a wrapper that allows Abaqus user material subroutines (UMATs) to be used as an External Material library in the software COMSOL Multiphysics.","The wrapper, written in C language, transforms COMSOL's external material subroutine inputs and outputs into Fortran-coded Abaqus UMAT inputs and outputs, by means of a consistent variable transformation.","This significantly facilitates conducting coupled, multi-physics studies employing the advanced material models that the solid mechanics community has developed over the past decades.","We exemplify the potential of our new framework, UMAT4COMSOL, by conducting numerical experiments in the areas of elastoplasticity, hyperelasticity and crystal plasticity.","The source code, detailed documentation and example tutorials are made freely available to download at www.empaneda.com/codes."],"url":"http://arxiv.org/abs/2402.13925v1"}
{"created":"2024-02-21 16:40:31","title":"Improved Lower Bound on the Number of Pseudoline Arrangements","abstract":"We show that for large enough $n$, the number of non-isomorphic pseudoline arrangements of order $n$ is greater than $2^{c\\cdot n^2}$ for some constant $c > 0.2604$, improving the previous best bound of $c>0.2083$ by Dumitrescu and Mandal (2020). Arrangements of pseudolines (and in particular arrangements of lines) are important objects appearing in many forms in discrete and computational geometry. They have strong ties for example with oriented matroids, sorting networks and point configurations. Let $B_n$ be the number of non-isomorphic pseudoline arrangements of order $n$ and let $b_n := \\log_2(B_n)$. The problem of estimating $b_n$ dates back to Knuth, who conjectured that $b_n \\leq 0.5n^2 + o(n^2)$ and derived the first bounds $n^2/6-O(n) \\leq b_n \\leq 0.7924(n^2+n)$. Both the upper and the lower bound have been improved a couple of times since. For the upper bound, it was first improved to $b_n < 0.6988n^2$ (Felsner, 1997), then $b_n < 0.6571 n^2$ by Felsner and Valtr (2011), for large enough $n$. In the same paper, Felsner and Valtr improved the constant in the lower bound to $c> 0.1887$, which was subsequently improved by Dumitrescu and Mandal to $c>0.2083$. Our new bound is based on a construction which starts with one of the constructions of Dumitrescu and Mandal and breaks it into constant sized pieces. We then use software to compute the contribution of each piece to the overall number of pseudoline arrangements. This method adds a lot of flexibility to the construction and thus offers many avenues for future tweaks and improvements which could lead to further tightening of the lower bound.","sentences":["We show that for large enough $n$, the number of non-isomorphic pseudoline arrangements of order $n$ is greater than $2^{c\\cdot n^2}$ for some constant $c > 0.2604$, improving the previous best bound of $c>0.2083$ by Dumitrescu and Mandal (2020).","Arrangements of pseudolines (and in particular arrangements of lines) are important objects appearing in many forms in discrete and computational geometry.","They have strong ties for example with oriented matroids, sorting networks and point configurations.","Let $B_n$ be the number of non-isomorphic pseudoline arrangements of order $n$ and let $b_n := \\log_2(B_n)$. The problem of estimating $b_n$ dates back to Knuth, who conjectured that $b_n","\\leq 0.5n^2","+ o(n^2)$ and derived the first bounds $n^2/6-O(n) \\leq b_n","\\leq 0.7924(n^2+n)$. Both the upper and the lower bound have been improved a couple of times since.","For the upper bound, it was first improved to $b_n <","0.6988n^2$ (Felsner, 1997), then $b_n < 0.6571 n^2$ by Felsner and Valtr (2011), for large enough $n$. In the same paper, Felsner and Valtr improved the constant in the lower bound to $c> 0.1887$, which was subsequently improved by Dumitrescu and Mandal to $c>0.2083$. Our new bound is based on a construction which starts with one of the constructions of Dumitrescu and Mandal and breaks it into constant sized pieces.","We then use software to compute the contribution of each piece to the overall number of pseudoline arrangements.","This method adds a lot of flexibility to the construction and thus offers many avenues for future tweaks and improvements which could lead to further tightening of the lower bound."],"url":"http://arxiv.org/abs/2402.13923v1"}
{"created":"2024-02-21 16:40:13","title":"Robust recovery for stochastic block models, simplified and generalized","abstract":"We study the problem of $\\textit{robust community recovery}$: efficiently recovering communities in sparse stochastic block models in the presence of adversarial corruptions. In the absence of adversarial corruptions, there are efficient algorithms when the $\\textit{signal-to-noise ratio}$ exceeds the $\\textit{Kesten--Stigum (KS) threshold}$, widely believed to be the computational threshold for this problem. The question we study is: does the computational threshold for robust community recovery also lie at the KS threshold? We answer this question affirmatively, providing an algorithm for robust community recovery for arbitrary stochastic block models on any constant number of communities, generalizing the work of Ding, d'Orsi, Nasser & Steurer on an efficient algorithm above the KS threshold in the case of $2$-community block models.   There are three main ingredients to our work:   (i) The Bethe Hessian of the graph is defined as $H_G(t) \\triangleq (D_G-I)t^2 - A_Gt + I$ where $D_G$ is the diagonal matrix of degrees and $A_G$ is the adjacency matrix. Empirical work suggested that the Bethe Hessian for the stochastic block model has outlier eigenvectors corresponding to the communities right above the Kesten-Stigum threshold. We formally confirm the existence of outlier eigenvalues for the Bethe Hessian, by explicitly constructing outlier eigenvectors from the community vectors.   (ii) We develop an algorithm for a variant of robust PCA on sparse matrices. Specifically, an algorithm to partially recover top eigenspaces from adversarially corrupted sparse matrices under mild delocalization constraints.   (iii) A rounding algorithm to turn vector assignments of vertices into a community assignment, inspired by the algorithm of Charikar \\& Wirth \\cite{CW04} for $2$XOR.","sentences":["We study the problem of $\\textit{robust community recovery}$: efficiently recovering communities in sparse stochastic block models in the presence of adversarial corruptions.","In the absence of adversarial corruptions, there are efficient algorithms when the $\\textit{signal-to-noise ratio}$ exceeds the $\\textit{Kesten--Stigum (KS) threshold}$, widely believed to be the computational threshold for this problem.","The question we study is: does the computational threshold for robust community recovery also lie at the KS threshold?","We answer this question affirmatively, providing an algorithm for robust community recovery for arbitrary stochastic block models on any constant number of communities, generalizing the work of Ding, d'Orsi, Nasser & Steurer on an efficient algorithm above the KS threshold in the case of $2$-community block models.   ","There are three main ingredients to our work:   (i) The Bethe Hessian of the graph is defined as $H_G(t)","\\triangleq (D_G-I)t^2 - A_Gt","+ I$ where $D_G$ is the diagonal matrix of degrees and $A_G$ is the adjacency matrix.","Empirical work suggested that the Bethe Hessian for the stochastic block model has outlier eigenvectors corresponding to the communities right above the Kesten-Stigum threshold.","We formally confirm the existence of outlier eigenvalues for the Bethe Hessian, by explicitly constructing outlier eigenvectors from the community vectors.   ","(ii) We develop an algorithm for a variant of robust PCA on sparse matrices.","Specifically, an algorithm to partially recover top eigenspaces from adversarially corrupted sparse matrices under mild delocalization constraints.   ","(iii) A rounding algorithm to turn vector assignments of vertices into a community assignment, inspired by the algorithm of Charikar \\& Wirth \\cite{CW04} for $2$XOR."],"url":"http://arxiv.org/abs/2402.13921v1"}
{"created":"2024-02-21 16:39:28","title":"Practical algorithms for Hierarchical overlap graphs","abstract":"Genome assembly is a prominent problem studied in bioinformatics, which computes the source string using a set of its overlapping substrings. Classically, genome assembly uses assembly graphs built using this set of substrings to compute the source string efficiently, having a tradeoff between scalability and avoiding information loss. The scalable de Bruijn graphs come at the price of losing crucial overlap information. The complete overlap information is stored in overlap graphs using quadratic space. Hierarchical overlap graphs [IPL20] (HOG) overcome these limitations, avoiding information loss despite using linear space. After a series of suboptimal improvements, Khan and Park et al. simultaneously presented two optimal algorithms [CPM2021], where only the former was seemingly practical.   We empirically analyze all the practical algorithms for computing HOG, where the optimal algorithm [CPM2021] outperforms the previous algorithms as expected, though at the expense of extra memory. However, it uses non-intuitive approach and non-trivial data structures. We present arguably the most intuitive algorithm, using only elementary arrays, which is also optimal. Our algorithm empirically proves even better for both time and memory over all the algorithms, highlighting its significance in both theory and practice.   We further explore the applications of hierarchical overlap graphs to solve various forms of suffix-prefix queries on a set of strings. Loukides et al. [CPM2023] recently presented state-of-the-art algorithms for these queries. However, these algorithms require complex black-box data structures and are seemingly impractical. Our algorithms, despite failing to match the state-of-the-art algorithms theoretically, answer different queries ranging from 0.01-100 milliseconds for a data set having around a billion characters.","sentences":["Genome assembly is a prominent problem studied in bioinformatics, which computes the source string using a set of its overlapping substrings.","Classically, genome assembly uses assembly graphs built using this set of substrings to compute the source string efficiently, having a tradeoff between scalability and avoiding information loss.","The scalable de Bruijn graphs come at the price of losing crucial overlap information.","The complete overlap information is stored in overlap graphs using quadratic space.","Hierarchical overlap graphs [IPL20] (HOG) overcome these limitations, avoiding information loss despite using linear space.","After a series of suboptimal improvements, Khan and Park et al. simultaneously presented two optimal algorithms","[CPM2021], where only the former was seemingly practical.   ","We empirically analyze all the practical algorithms for computing HOG, where the optimal algorithm","[CPM2021] outperforms the previous algorithms as expected, though at the expense of extra memory.","However, it uses non-intuitive approach and non-trivial data structures.","We present arguably the most intuitive algorithm, using only elementary arrays, which is also optimal.","Our algorithm empirically proves even better for both time and memory over all the algorithms, highlighting its significance in both theory and practice.   ","We further explore the applications of hierarchical overlap graphs to solve various forms of suffix-prefix queries on a set of strings.","Loukides et al.","[CPM2023] recently presented state-of-the-art algorithms for these queries.","However, these algorithms require complex black-box data structures and are seemingly impractical.","Our algorithms, despite failing to match the state-of-the-art algorithms theoretically, answer different queries ranging from 0.01-100 milliseconds for a data set having around a billion characters."],"url":"http://arxiv.org/abs/2402.13920v1"}
{"created":"2024-02-21 16:33:22","title":"SYNFAC-EDIT: Synthetic Imitation Edit Feedback for Factual Alignment in Clinical Summarization","abstract":"Large Language Models (LLMs) such as GPT and Llama have demonstrated significant achievements in summarization tasks but struggle with factual inaccuracies, a critical issue in clinical NLP applications where errors could lead to serious consequences. To counter the high costs and limited availability of expert-annotated data for factual alignment, this study introduces an innovative pipeline that utilizes GPT-3.5 and GPT-4 to generate high-quality feedback aimed at enhancing factual consistency in clinical note summarization. Our research primarily focuses on edit feedback, mirroring the practical scenario in which medical professionals refine AI system outputs without the need for additional annotations. Despite GPT's proven expertise in various clinical NLP tasks, such as the Medical Licensing Examination, there is scant research on its capacity to deliver expert-level edit feedback for improving weaker LMs or LLMs generation quality. This work leverages GPT's advanced capabilities in clinical NLP to offer expert-level edit feedback. Through the use of two distinct alignment algorithms (DPO and SALT) based on GPT edit feedback, our goal is to reduce hallucinations and align closely with medical facts, endeavoring to narrow the divide between AI-generated content and factual accuracy. This highlights the substantial potential of GPT edits in enhancing the alignment of clinical factuality.","sentences":["Large Language Models (LLMs) such as GPT and Llama have demonstrated significant achievements in summarization tasks but struggle with factual inaccuracies, a critical issue in clinical NLP applications where errors could lead to serious consequences.","To counter the high costs and limited availability of expert-annotated data for factual alignment, this study introduces an innovative pipeline that utilizes GPT-3.5 and GPT-4 to generate high-quality feedback aimed at enhancing factual consistency in clinical note summarization.","Our research primarily focuses on edit feedback, mirroring the practical scenario in which medical professionals refine AI system outputs without the need for additional annotations.","Despite GPT's proven expertise in various clinical NLP tasks, such as the Medical Licensing Examination, there is scant research on its capacity to deliver expert-level edit feedback for improving weaker LMs or LLMs generation quality.","This work leverages GPT's advanced capabilities in clinical NLP to offer expert-level edit feedback.","Through the use of two distinct alignment algorithms (DPO and SALT) based on GPT edit feedback, our goal is to reduce hallucinations and align closely with medical facts, endeavoring to narrow the divide between AI-generated content and factual accuracy.","This highlights the substantial potential of GPT edits in enhancing the alignment of clinical factuality."],"url":"http://arxiv.org/abs/2402.13919v1"}
{"created":"2024-02-21 16:32:43","title":"BenchCloudVision: A Benchmark Analysis of Deep Learning Approaches for Cloud Detection and Segmentation in Remote Sensing Imagery","abstract":"Satellites equipped with optical sensors capture high-resolution imagery, providing valuable insights into various environmental phenomena. In recent years, there has been a surge of research focused on addressing some challenges in remote sensing, ranging from water detection in diverse landscapes to the segmentation of mountainous and terrains. Ongoing investigations goals to enhance the precision and efficiency of satellite imagery analysis. Especially, there is a growing emphasis on developing methodologies for accurate water body detection, snow and clouds, important for environmental monitoring, resource management, and disaster response. Within this context, this paper focus on the cloud segmentation from remote sensing imagery. Accurate remote sensing data analysis can be challenging due to the presence of clouds in optical sensor-based applications. The quality of resulting products such as applications and research is directly impacted by cloud detection, which plays a key role in the remote sensing data processing pipeline. This paper examines seven cutting-edge semantic segmentation and detection algorithms applied to clouds identification, conducting a benchmark analysis to evaluate their architectural approaches and identify the most performing ones. To increase the model's adaptability, critical elements including the type of imagery and the amount of spectral bands used during training are analyzed. Additionally, this research tries to produce machine learning algorithms that can perform cloud segmentation using only a few spectral bands, including RGB and RGBN-IR combinations. The model's flexibility for a variety of applications and user scenarios is assessed by using imagery from Sentinel-2 and Landsat-8 as datasets. This benchmark can be reproduced using the material from this github link: \\url{https://github.com/toelt-llc/cloud\\_segmentation\\_comparative}.","sentences":["Satellites equipped with optical sensors capture high-resolution imagery, providing valuable insights into various environmental phenomena.","In recent years, there has been a surge of research focused on addressing some challenges in remote sensing, ranging from water detection in diverse landscapes to the segmentation of mountainous and terrains.","Ongoing investigations goals to enhance the precision and efficiency of satellite imagery analysis.","Especially, there is a growing emphasis on developing methodologies for accurate water body detection, snow and clouds, important for environmental monitoring, resource management, and disaster response.","Within this context, this paper focus on the cloud segmentation from remote sensing imagery.","Accurate remote sensing data analysis can be challenging due to the presence of clouds in optical sensor-based applications.","The quality of resulting products such as applications and research is directly impacted by cloud detection, which plays a key role in the remote sensing data processing pipeline.","This paper examines seven cutting-edge semantic segmentation and detection algorithms applied to clouds identification, conducting a benchmark analysis to evaluate their architectural approaches and identify the most performing ones.","To increase the model's adaptability, critical elements including the type of imagery and the amount of spectral bands used during training are analyzed.","Additionally, this research tries to produce machine learning algorithms that can perform cloud segmentation using only a few spectral bands, including RGB and RGBN-IR combinations.","The model's flexibility for a variety of applications and user scenarios is assessed by using imagery from Sentinel-2 and Landsat-8 as datasets.","This benchmark can be reproduced using the material from this github link: \\url{https://github.com/toelt-llc/cloud\\_segmentation\\_comparative}."],"url":"http://arxiv.org/abs/2402.13918v1"}
{"created":"2024-02-21 16:32:38","title":"What Linguistic Features and Languages are Important in LLM Translation?","abstract":"Large Language Models (LLMs) demonstrate strong capability across multiple tasks, including machine translation. Our study focuses on evaluating Llama2's machine translation capabilities and exploring how translation depends on languages in its training data. Our experiments show that the 7B Llama2 model yields above 10 BLEU score for all languages it has seen, but not always for languages it has not seen. Most gains for those unseen languages are observed the most with the model scale compared to using chat versions or adding shot count. Furthermore, our linguistic distance analysis reveals that syntactic similarity is not always the primary linguistic factor in determining translation quality. Interestingly, we discovered that under specific circumstances, some languages, despite having significantly less training data than English, exhibit strong correlations comparable to English. Our discoveries here give new perspectives for the current landscape of LLMs, raising the possibility that LLMs centered around languages other than English may offer a more effective foundation for a multilingual model.","sentences":["Large Language Models (LLMs) demonstrate strong capability across multiple tasks, including machine translation.","Our study focuses on evaluating Llama2's machine translation capabilities and exploring how translation depends on languages in its training data.","Our experiments show that the 7B Llama2 model yields above 10 BLEU score for all languages it has seen, but not always for languages it has not seen.","Most gains for those unseen languages are observed the most with the model scale compared to using chat versions or adding shot count.","Furthermore, our linguistic distance analysis reveals that syntactic similarity is not always the primary linguistic factor in determining translation quality.","Interestingly, we discovered that under specific circumstances, some languages, despite having significantly less training data than English, exhibit strong correlations comparable to English.","Our discoveries here give new perspectives for the current landscape of LLMs, raising the possibility that LLMs centered around languages other than English may offer a more effective foundation for a multilingual model."],"url":"http://arxiv.org/abs/2402.13917v1"}
{"created":"2024-02-21 16:31:45","title":"Bias correction of wind power forecasts with SCADA data and continuous learning","abstract":"Wind energy plays a critical role in the transition towards renewable energy sources. However, the uncertainty and variability of wind can impede its full potential and the necessary growth of wind power capacity. To mitigate these challenges, wind power forecasting methods are employed for applications in power management, energy trading, or maintenance scheduling. In this work, we present, evaluate, and compare four machine learning-based wind power forecasting models. Our models correct and improve 48-hour forecasts extracted from a numerical weather prediction (NWP) model. The models are evaluated on datasets from a wind park comprising 65 wind turbines. The best improvement in forecasting error and mean bias was achieved by a convolutional neural network, reducing the average NRMSE down to 22%, coupled with a significant reduction in mean bias, compared to a NRMSE of 35% from the strongly biased baseline model using uncorrected NWP forecasts. Our findings further indicate that changes to neural network architectures play a minor role in affecting the forecasting performance, and that future research should rather investigate changes in the model pipeline. Moreover, we introduce a continuous learning strategy, which is shown to achieve the highest forecasting performance improvements when new data is made available.","sentences":["Wind energy plays a critical role in the transition towards renewable energy sources.","However, the uncertainty and variability of wind can impede its full potential and the necessary growth of wind power capacity.","To mitigate these challenges, wind power forecasting methods are employed for applications in power management, energy trading, or maintenance scheduling.","In this work, we present, evaluate, and compare four machine learning-based wind power forecasting models.","Our models correct and improve 48-hour forecasts extracted from a numerical weather prediction (NWP) model.","The models are evaluated on datasets from a wind park comprising 65 wind turbines.","The best improvement in forecasting error and mean bias was achieved by a convolutional neural network, reducing the average NRMSE down to 22%, coupled with a significant reduction in mean bias, compared to a NRMSE of 35% from the strongly biased baseline model using uncorrected NWP forecasts.","Our findings further indicate that changes to neural network architectures play a minor role in affecting the forecasting performance, and that future research should rather investigate changes in the model pipeline.","Moreover, we introduce a continuous learning strategy, which is shown to achieve the highest forecasting performance improvements when new data is made available."],"url":"http://arxiv.org/abs/2402.13916v1"}
{"created":"2024-02-21 16:31:07","title":"A Combined Learning and Optimization Framework to Transfer Human Whole-body Loco-manipulation Skills to Mobile Manipulators","abstract":"Humans' ability to smoothly switch between locomotion and manipulation is a remarkable feature of sensorimotor coordination. Leaning and replication of such human-like strategies can lead to the development of more sophisticated robots capable of performing complex whole-body tasks in real-world environments. To this end, this paper proposes a combined learning and optimization framework for transferring human's loco-manipulation soft-switching skills to mobile manipulators. The methodology departs from data collection of human demonstrations for a locomotion-integrated manipulation task through a vision system. Next, the wrist and pelvis motions are mapped to mobile manipulators' End-Effector (EE) and mobile base. A kernelized movement primitive algorithm learns the wrist and pelvis trajectories and generalizes to new desired points according to task requirements. Next, the reference trajectories are sent to a hierarchical quadratic programming controller, where the EE and the mobile base reference trajectories are provided as the first and second priority tasks, generating the feasible and optimal joint level commands. A locomotion-integrated pick-and-place task is executed to validate the proposed approach. After a human demonstrates the task, a mobile manipulator executes the task with the same and new settings, grasping a bottle at non-zero velocity. The results showed that the proposed approach successfully transfers the human loco-manipulation skills to mobile manipulators, even with different geometry.","sentences":["Humans' ability to smoothly switch between locomotion and manipulation is a remarkable feature of sensorimotor coordination.","Leaning and replication of such human-like strategies can lead to the development of more sophisticated robots capable of performing complex whole-body tasks in real-world environments.","To this end, this paper proposes a combined learning and optimization framework for transferring human's loco-manipulation soft-switching skills to mobile manipulators.","The methodology departs from data collection of human demonstrations for a locomotion-integrated manipulation task through a vision system.","Next, the wrist and pelvis motions are mapped to mobile manipulators' End-Effector (EE) and mobile base.","A kernelized movement primitive algorithm learns the wrist and pelvis trajectories and generalizes to new desired points according to task requirements.","Next, the reference trajectories are sent to a hierarchical quadratic programming controller, where the EE and the mobile base reference trajectories are provided as the first and second priority tasks, generating the feasible and optimal joint level commands.","A locomotion-integrated pick-and-place task is executed to validate the proposed approach.","After a human demonstrates the task, a mobile manipulator executes the task with the same and new settings, grasping a bottle at non-zero velocity.","The results showed that the proposed approach successfully transfers the human loco-manipulation skills to mobile manipulators, even with different geometry."],"url":"http://arxiv.org/abs/2402.13915v1"}
{"created":"2024-02-21 16:30:24","title":"Explain to Question not to Justify","abstract":"Explainable Artificial Intelligence (XAI) is a young but very promising field of research. Unfortunately, the progress in this field is currently slowed down by divergent and incompatible goals. In this paper, we separate various threads tangled within the area of XAI into two complementary cultures of human/value-oriented explanations (BLUE XAI) and model/validation-oriented explanations (RED XAI). We also argue that the area of RED XAI is currently under-explored and hides great opportunities and potential for important research necessary to ensure the safety of AI systems. We conclude this paper by presenting promising challenges in this area.","sentences":["Explainable Artificial Intelligence (XAI) is a young but very promising field of research.","Unfortunately, the progress in this field is currently slowed down by divergent and incompatible goals.","In this paper, we separate various threads tangled within the area of XAI into two complementary cultures of human/value-oriented explanations (BLUE XAI) and model/validation-oriented explanations (RED XAI).","We also argue that the area of RED XAI is currently under-explored and hides great opportunities and potential for important research necessary to ensure the safety of AI systems.","We conclude this paper by presenting promising challenges in this area."],"url":"http://arxiv.org/abs/2402.13914v1"}
{"created":"2024-02-21 16:26:59","title":"Replication Study: Enhancing Hydrological Modeling with Physics-Guided Machine Learning","abstract":"Current hydrological modeling methods combine data-driven Machine Learning (ML) algorithms and traditional physics-based models to address their respective limitations incorrect parameter estimates from rigid physics-based models and the neglect of physical process constraints by ML algorithms. Despite the accuracy of ML in outcome prediction, the integration of scientific knowledge is crucial for reliable predictions. This study introduces a Physics Informed Machine Learning (PIML) model, which merges the process understanding of conceptual hydrological models with the predictive efficiency of ML algorithms. Applied to the Anandapur sub-catchment, the PIML model demonstrates superior performance in forecasting monthly streamflow and actual evapotranspiration over both standalone conceptual models and ML algorithms, ensuring physical consistency of the outputs. This study replicates the methodologies of Bhasme, P., Vagadiya, J., & Bhatia, U. (2022) from their pivotal work on Physics Informed Machine Learning for hydrological processes, utilizing their shared code and datasets to further explore the predictive capabilities in hydrological modeling.","sentences":["Current hydrological modeling methods combine data-driven Machine Learning (ML) algorithms and traditional physics-based models to address their respective limitations incorrect parameter estimates from rigid physics-based models and the neglect of physical process constraints by ML algorithms.","Despite the accuracy of ML in outcome prediction, the integration of scientific knowledge is crucial for reliable predictions.","This study introduces a Physics Informed Machine Learning (PIML) model, which merges the process understanding of conceptual hydrological models with the predictive efficiency of ML algorithms.","Applied to the Anandapur sub-catchment, the PIML model demonstrates superior performance in forecasting monthly streamflow and actual evapotranspiration over both standalone conceptual models and ML algorithms, ensuring physical consistency of the outputs.","This study replicates the methodologies of Bhasme, P., Vagadiya, J., & Bhatia, U. (2022) from their pivotal work on Physics Informed Machine Learning for hydrological processes, utilizing their shared code and datasets to further explore the predictive capabilities in hydrological modeling."],"url":"http://arxiv.org/abs/2402.13911v1"}
{"created":"2024-02-21 16:22:21","title":"Leveraging Collection-Wide Similarities for Unsupervised Document Structure Extraction","abstract":"Document collections of various domains, e.g., legal, medical, or financial, often share some underlying collection-wide structure, which captures information that can aid both human users and structure-aware models. We propose to identify the typical structure of document within a collection, which requires to capture recurring topics across the collection, while abstracting over arbitrary header paraphrases, and ground each topic to respective document locations. These requirements pose several challenges: headers that mark recurring topics frequently differ in phrasing, certain section headers are unique to individual documents and do not reflect the typical structure, and the order of topics can vary between documents. Subsequently, we develop an unsupervised graph-based method which leverages both inter- and intra-document similarities, to extract the underlying collection-wide structure. Our evaluations on three diverse domains in both English and Hebrew indicate that our method extracts meaningful collection-wide structure, and we hope that future work will leverage our method for multi-document applications and structure-aware models.","sentences":["Document collections of various domains, e.g., legal, medical, or financial, often share some underlying collection-wide structure, which captures information that can aid both human users and structure-aware models.","We propose to identify the typical structure of document within a collection, which requires to capture recurring topics across the collection, while abstracting over arbitrary header paraphrases, and ground each topic to respective document locations.","These requirements pose several challenges: headers that mark recurring topics frequently differ in phrasing, certain section headers are unique to individual documents and do not reflect the typical structure, and the order of topics can vary between documents.","Subsequently, we develop an unsupervised graph-based method which leverages both inter- and intra-document similarities, to extract the underlying collection-wide structure.","Our evaluations on three diverse domains in both English and Hebrew indicate that our method extracts meaningful collection-wide structure, and we hope that future work will leverage our method for multi-document applications and structure-aware models."],"url":"http://arxiv.org/abs/2402.13906v1"}
{"created":"2024-02-21 16:15:20","title":"Calibrating Large Language Models with Sample Consistency","abstract":"Accurately gauging the confidence level of Large Language Models' (LLMs) predictions is pivotal for their reliable application. However, LLMs are often uncalibrated inherently and elude conventional calibration techniques due to their proprietary nature and massive scale. In this work, we explore the potential of deriving confidence from the distribution of multiple randomly sampled model generations, via three measures of consistency. We perform an extensive evaluation across various open and closed-source models on nine reasoning datasets. Results show that consistency-based calibration methods outperform existing post-hoc approaches. Meanwhile, we find that factors such as intermediate explanations, model scaling, and larger sample sizes enhance calibration, while instruction-tuning makes calibration more difficult. Moreover, confidence scores obtained from consistency have the potential to enhance model performance. Finally, we offer practical guidance on choosing suitable consistency metrics for calibration, tailored to the characteristics of various LMs.","sentences":["Accurately gauging the confidence level of Large Language Models' (LLMs) predictions is pivotal for their reliable application.","However, LLMs are often uncalibrated inherently and elude conventional calibration techniques due to their proprietary nature and massive scale.","In this work, we explore the potential of deriving confidence from the distribution of multiple randomly sampled model generations, via three measures of consistency.","We perform an extensive evaluation across various open and closed-source models on nine reasoning datasets.","Results show that consistency-based calibration methods outperform existing post-hoc approaches.","Meanwhile, we find that factors such as intermediate explanations, model scaling, and larger sample sizes enhance calibration, while instruction-tuning makes calibration more difficult.","Moreover, confidence scores obtained from consistency have the potential to enhance model performance.","Finally, we offer practical guidance on choosing suitable consistency metrics for calibration, tailored to the characteristics of various LMs."],"url":"http://arxiv.org/abs/2402.13904v1"}
{"created":"2024-02-21 16:13:49","title":"Dealing with unbounded gradients in stochastic saddle-point optimization","abstract":"We study the performance of stochastic first-order methods for finding saddle points of convex-concave functions. A notorious challenge faced by such methods is that the gradients can grow arbitrarily large during optimization, which may result in instability and divergence. In this paper, we propose a simple and effective regularization technique that stabilizes the iterates and yields meaningful performance guarantees even if the domain and the gradient noise scales linearly with the size of the iterates (and is thus potentially unbounded). Besides providing a set of general results, we also apply our algorithm to a specific problem in reinforcement learning, where it leads to performance guarantees for finding near-optimal policies in an average-reward MDP without prior knowledge of the bias span.","sentences":["We study the performance of stochastic first-order methods for finding saddle points of convex-concave functions.","A notorious challenge faced by such methods is that the gradients can grow arbitrarily large during optimization, which may result in instability and divergence.","In this paper, we propose a simple and effective regularization technique that stabilizes the iterates and yields meaningful performance guarantees even if the domain and the gradient noise scales linearly with the size of the iterates (and is thus potentially unbounded).","Besides providing a set of general results, we also apply our algorithm to a specific problem in reinforcement learning, where it leads to performance guarantees for finding near-optimal policies in an average-reward MDP without prior knowledge of the bias span."],"url":"http://arxiv.org/abs/2402.13903v1"}
{"created":"2024-02-21 16:11:47","title":"Non-asymptotic Convergence of Discrete-time Diffusion Models: New Approach and Improved Rate","abstract":"The denoising diffusion model emerges recently as a powerful generative technique that converts noise into data. Theoretical convergence guarantee has been mainly studied for continuous-time diffusion models, and has been obtained for discrete-time diffusion models only for distributions with bounded support in the literature. In this paper, we establish the convergence guarantee for substantially larger classes of distributions under discrete-time diffusion models and further improve the convergence rate for distributions with bounded support. In particular, we first establish the convergence rates for both smooth and general (possibly non-smooth) distributions having finite second moment. We then specialize our results to a number of interesting classes of distributions with explicit parameter dependencies, including distributions with Lipschitz scores, Gaussian mixture distributions, and distributions with bounded support. We further propose a novel accelerated sampler and show that it improves the convergence rates of the corresponding regular sampler by orders of magnitude with respect to all system parameters. For distributions with bounded support, our result improves the dimensional dependence of the previous convergence rate by orders of magnitude. Our study features a novel analysis technique that constructs tilting factor representation of the convergence error and exploits Tweedie's formula for handling Taylor expansion power terms.","sentences":["The denoising diffusion model emerges recently as a powerful generative technique that converts noise into data.","Theoretical convergence guarantee has been mainly studied for continuous-time diffusion models, and has been obtained for discrete-time diffusion models only for distributions with bounded support in the literature.","In this paper, we establish the convergence guarantee for substantially larger classes of distributions under discrete-time diffusion models and further improve the convergence rate for distributions with bounded support.","In particular, we first establish the convergence rates for both smooth and general (possibly non-smooth) distributions having finite second moment.","We then specialize our results to a number of interesting classes of distributions with explicit parameter dependencies, including distributions with Lipschitz scores, Gaussian mixture distributions, and distributions with bounded support.","We further propose a novel accelerated sampler and show that it improves the convergence rates of the corresponding regular sampler by orders of magnitude with respect to all system parameters.","For distributions with bounded support, our result improves the dimensional dependence of the previous convergence rate by orders of magnitude.","Our study features a novel analysis technique that constructs tilting factor representation of the convergence error and exploits Tweedie's formula for handling Taylor expansion power terms."],"url":"http://arxiv.org/abs/2402.13901v1"}
{"created":"2024-02-21 16:09:25","title":"Science Checker Reloaded: A Bidirectional Paradigm for Transparency and Logical Reasoning","abstract":"Information retrieval is a rapidly evolving field. However it still faces significant limitations in the scientific and industrial vast amounts of information, such as semantic divergence and vocabulary gaps in sparse retrieval, low precision and lack of interpretability in semantic search, or hallucination and outdated information in generative models. In this paper, we introduce a two-block approach to tackle these hurdles for long documents. The first block enhances language understanding in sparse retrieval by query expansion to retrieve relevant documents. The second block deepens the result by providing comprehensive and informative answers to the complex question using only the information spread in the long document, enabling bidirectional engagement. At various stages of the pipeline, intermediate results are presented to users to facilitate understanding of the system's reasoning. We believe this bidirectional approach brings significant advancements in terms of transparency, logical thinking, and comprehensive understanding in the field of scientific information retrieval.","sentences":["Information retrieval is a rapidly evolving field.","However it still faces significant limitations in the scientific and industrial vast amounts of information, such as semantic divergence and vocabulary gaps in sparse retrieval, low precision and lack of interpretability in semantic search, or hallucination and outdated information in generative models.","In this paper, we introduce a two-block approach to tackle these hurdles for long documents.","The first block enhances language understanding in sparse retrieval by query expansion to retrieve relevant documents.","The second block deepens the result by providing comprehensive and informative answers to the complex question using only the information spread in the long document, enabling bidirectional engagement.","At various stages of the pipeline, intermediate results are presented to users to facilitate understanding of the system's reasoning.","We believe this bidirectional approach brings significant advancements in terms of transparency, logical thinking, and comprehensive understanding in the field of scientific information retrieval."],"url":"http://arxiv.org/abs/2402.13897v1"}
{"created":"2024-02-21 16:02:14","title":"Overcoming Saturation in Density Ratio Estimation by Iterated Regularization","abstract":"Estimating the ratio of two probability densities from finitely many samples, is a central task in machine learning and statistics. In this work, we show that a large class of kernel methods for density ratio estimation suffers from error saturation, which prevents algorithms from achieving fast error convergence rates on highly regular learning problems. To resolve saturation, we introduce iterated regularization in density ratio estimation to achieve fast error rates. Our methods outperform its non-iteratively regularized versions on benchmarks for density ratio estimation as well as on large-scale evaluations for importance-weighted ensembling of deep unsupervised domain adaptation models.","sentences":["Estimating the ratio of two probability densities from finitely many samples, is a central task in machine learning and statistics.","In this work, we show that a large class of kernel methods for density ratio estimation suffers from error saturation, which prevents algorithms from achieving fast error convergence rates on highly regular learning problems.","To resolve saturation, we introduce iterated regularization in density ratio estimation to achieve fast error rates.","Our methods outperform its non-iteratively regularized versions on benchmarks for density ratio estimation as well as on large-scale evaluations for importance-weighted ensembling of deep unsupervised domain adaptation models."],"url":"http://arxiv.org/abs/2402.13891v1"}
{"created":"2024-02-21 15:58:37","title":"Beyond Probabilities: Unveiling the Misalignment in Evaluating Large Language Models","abstract":"Large Language Models (LLMs) have demonstrated remarkable capabilities across various applications, fundamentally reshaping the landscape of natural language processing (NLP) research. However, recent evaluation frameworks often rely on the output probabilities of LLMs for predictions, primarily due to computational constraints, diverging from real-world LLM usage scenarios. While widely employed, the efficacy of these probability-based evaluation strategies remains an open research question. This study aims to scrutinize the validity of such probability-based evaluation methods within the context of using LLMs for Multiple Choice Questions (MCQs), highlighting their inherent limitations. Our empirical investigation reveals that the prevalent probability-based evaluation method inadequately aligns with generation-based prediction. Furthermore, current evaluation frameworks typically assess LLMs through predictive tasks based on output probabilities rather than directly generating responses, owing to computational limitations. We illustrate that these probability-based approaches do not effectively correspond with generative predictions. The outcomes of our study can enhance the understanding of LLM evaluation methodologies and provide insights for future research in this domain.","sentences":["Large Language Models (LLMs) have demonstrated remarkable capabilities across various applications, fundamentally reshaping the landscape of natural language processing (NLP) research.","However, recent evaluation frameworks often rely on the output probabilities of LLMs for predictions, primarily due to computational constraints, diverging from real-world LLM usage scenarios.","While widely employed, the efficacy of these probability-based evaluation strategies remains an open research question.","This study aims to scrutinize the validity of such probability-based evaluation methods within the context of using LLMs for Multiple Choice Questions (MCQs), highlighting their inherent limitations.","Our empirical investigation reveals that the prevalent probability-based evaluation method inadequately aligns with generation-based prediction.","Furthermore, current evaluation frameworks typically assess LLMs through predictive tasks based on output probabilities rather than directly generating responses, owing to computational limitations.","We illustrate that these probability-based approaches do not effectively correspond with generative predictions.","The outcomes of our study can enhance the understanding of LLM evaluation methodologies and provide insights for future research in this domain."],"url":"http://arxiv.org/abs/2402.13887v1"}
{"created":"2024-02-21 15:35:59","title":"Scene Prior Filtering for Depth Map Super-Resolution","abstract":"Multi-modal fusion is vital to the success of super-resolution of depth images. However, commonly used fusion strategies, such as addition and concatenation, fall short of effectively bridging the modal gap. As a result, guided image filtering methods have been introduced to mitigate this issue. Nevertheless, it is observed that their filter kernels usually encounter significant texture interference and edge inaccuracy. To tackle these two challenges, we introduce a Scene Prior Filtering network, SPFNet, which utilizes the priors surface normal and semantic map from large-scale models. Specifically, we design an All-in-one Prior Propagation that computes the similarity between multi-modal scene priors, \\textit{i.e.}, RGB, normal, semantic, and depth, to reduce the texture interference. In addition, we present a One-to-one Prior Embedding that continuously embeds each single-modal prior into depth using Mutual Guided Filtering, further alleviating the texture interference while enhancing edges. Our SPFNet has been extensively evaluated on both real and synthetic datasets, achieving state-of-the-art performance.","sentences":["Multi-modal fusion is vital to the success of super-resolution of depth images.","However, commonly used fusion strategies, such as addition and concatenation, fall short of effectively bridging the modal gap.","As a result, guided image filtering methods have been introduced to mitigate this issue.","Nevertheless, it is observed that their filter kernels usually encounter significant texture interference and edge inaccuracy.","To tackle these two challenges, we introduce a Scene Prior Filtering network, SPFNet, which utilizes the priors surface normal and semantic map from large-scale models.","Specifically, we design an All-in-one Prior Propagation that computes the similarity between multi-modal scene priors, \\textit{i.e.}, RGB, normal, semantic, and depth, to reduce the texture interference.","In addition, we present a One-to-one Prior Embedding that continuously embeds each single-modal prior into depth using Mutual Guided Filtering, further alleviating the texture interference while enhancing edges.","Our SPFNet has been extensively evaluated on both real and synthetic datasets, achieving state-of-the-art performance."],"url":"http://arxiv.org/abs/2402.13876v1"}
{"created":"2024-02-21 15:35:04","title":"$\\texttt{Se}^2$: $\\textit{Se}$quential Example $\\textit{Se}$lection for In-Context Learning","abstract":"The remarkable capability of large language models (LLMs) for in-context learning (ICL) needs to be activated by demonstration examples. Prior work has extensively explored the selection of examples for ICL, predominantly following the \"select then organize\" paradigm, such approaches often neglect the internal relationships between examples and exist an inconsistency between the training and inference. In this paper, we formulate the problem as a $\\textit{se}$quential $\\textit{se}$lection problem and introduce $\\texttt{Se}^2$, a sequential-aware method that leverages the LLM's feedback on varying context, aiding in capturing inter-relationships and sequential information among examples, significantly enriching the contextuality and relevance of ICL prompts. Meanwhile, we utilize beam search to seek and construct example sequences, enhancing both quality and diversity. Extensive experiments across 23 NLP tasks from 8 distinct categories illustrate that $\\texttt{Se}^2$ markedly surpasses competitive baselines and achieves 42% relative improvement over random selection. Further in-depth analysis show the effectiveness of proposed strategies, highlighting $\\texttt{Se}^2$'s exceptional stability and adaptability across various scenarios. Our code will be released to facilitate future research.","sentences":["The remarkable capability of large language models (LLMs) for in-context learning (ICL) needs to be activated by demonstration examples.","Prior work has extensively explored the selection of examples for ICL, predominantly following the \"select then organize\" paradigm, such approaches often neglect the internal relationships between examples and exist an inconsistency between the training and inference.","In this paper, we formulate the problem as a $\\textit{se}$quential $\\textit{se}$lection problem and introduce $\\texttt{Se}^2$, a sequential-aware method that leverages the LLM's feedback on varying context, aiding in capturing inter-relationships and sequential information among examples, significantly enriching the contextuality and relevance of ICL prompts.","Meanwhile, we utilize beam search to seek and construct example sequences, enhancing both quality and diversity.","Extensive experiments across 23 NLP tasks from 8 distinct categories illustrate that $\\texttt{Se}^2$ markedly surpasses competitive baselines and achieves 42% relative improvement over random selection.","Further in-depth analysis show the effectiveness of proposed strategies, highlighting $\\texttt{Se}^2$'s exceptional stability and adaptability across various scenarios.","Our code will be released to facilitate future research."],"url":"http://arxiv.org/abs/2402.13874v1"}
{"created":"2024-02-21 15:23:21","title":"Generative Probabilistic Time Series Forecasting and Applications in Grid Operations","abstract":"Generative probabilistic forecasting produces future time series samples according to the conditional probability distribution given past time series observations. Such techniques are essential in risk-based decision-making and planning under uncertainty with broad applications in grid operations, including electricity price forecasting, risk-based economic dispatch, and stochastic optimizations. Inspired by Wiener and Kallianpur's innovation representation, we propose a weak innovation autoencoder architecture and a learning algorithm to extract independent and identically distributed innovation sequences from nonparametric stationary time series. We show that the weak innovation sequence is Bayesian sufficient, which makes the proposed weak innovation autoencoder a canonical architecture for generative probabilistic forecasting. The proposed technique is applied to forecasting highly volatile real-time electricity prices, demonstrating superior performance across multiple forecasting measures over leading probabilistic and point forecasting techniques.","sentences":["Generative probabilistic forecasting produces future time series samples according to the conditional probability distribution given past time series observations.","Such techniques are essential in risk-based decision-making and planning under uncertainty with broad applications in grid operations, including electricity price forecasting, risk-based economic dispatch, and stochastic optimizations.","Inspired by Wiener and Kallianpur's innovation representation, we propose a weak innovation autoencoder architecture and a learning algorithm to extract independent and identically distributed innovation sequences from nonparametric stationary time series.","We show that the weak innovation sequence is Bayesian sufficient, which makes the proposed weak innovation autoencoder a canonical architecture for generative probabilistic forecasting.","The proposed technique is applied to forecasting highly volatile real-time electricity prices, demonstrating superior performance across multiple forecasting measures over leading probabilistic and point forecasting techniques."],"url":"http://arxiv.org/abs/2402.13870v1"}
{"created":"2024-02-21 15:23:21","title":"An Explainable Transformer-based Model for Phishing Email Detection: A Large Language Model Approach","abstract":"Phishing email is a serious cyber threat that tries to deceive users by sending false emails with the intention of stealing confidential information or causing financial harm. Attackers, often posing as trustworthy entities, exploit technological advancements and sophistication to make detection and prevention of phishing more challenging. Despite extensive academic research, phishing detection remains an ongoing and formidable challenge in the cybersecurity landscape. Large Language Models (LLMs) and Masked Language Models (MLMs) possess immense potential to offer innovative solutions to address long-standing challenges. In this research paper, we present an optimized, fine-tuned transformer-based DistilBERT model designed for the detection of phishing emails. In the detection process, we work with a phishing email dataset and utilize the preprocessing techniques to clean and solve the imbalance class issues. Through our experiments, we found that our model effectively achieves high accuracy, demonstrating its capability to perform well. Finally, we demonstrate our fine-tuned model using Explainable-AI (XAI) techniques such as Local Interpretable Model-Agnostic Explanations (LIME) and Transformer Interpret to explain how our model makes predictions in the context of text classification for phishing emails.","sentences":["Phishing email is a serious cyber threat that tries to deceive users by sending false emails with the intention of stealing confidential information or causing financial harm.","Attackers, often posing as trustworthy entities, exploit technological advancements and sophistication to make detection and prevention of phishing more challenging.","Despite extensive academic research, phishing detection remains an ongoing and formidable challenge in the cybersecurity landscape.","Large Language Models (LLMs) and Masked Language Models (MLMs) possess immense potential to offer innovative solutions to address long-standing challenges.","In this research paper, we present an optimized, fine-tuned transformer-based DistilBERT model designed for the detection of phishing emails.","In the detection process, we work with a phishing email dataset and utilize the preprocessing techniques to clean and solve the imbalance class issues.","Through our experiments, we found that our model effectively achieves high accuracy, demonstrating its capability to perform well.","Finally, we demonstrate our fine-tuned model using Explainable-AI (XAI) techniques such as Local Interpretable Model-Agnostic Explanations (LIME) and Transformer Interpret to explain how our model makes predictions in the context of text classification for phishing emails."],"url":"http://arxiv.org/abs/2402.13871v1"}
{"created":"2024-02-21 15:20:58","title":"A Uniformly Random Solution to Algorithmic Redistricting","abstract":"The process of drawing electoral district boundaries is known as political redistricting. Within this context, gerrymandering is the practice of drawing these boundaries such that they unfairly favor a particular political party, often leading to unequal representation and skewed electoral outcomes. One of the few ways to detect gerrymandering is by algorithmically sampling redistricting plans. Previous methods mainly focus on sampling from some neighborhood of ``realistic' districting plans, rather than a uniform sample of the entire space. We present a deterministic subexponential time algorithm to uniformly sample from the space of all possible $ k $-partitions of a bounded degree planar graph, and with this construct a sample of the entire space of redistricting plans. We also give a way to restrict this sample space to plans that match certain compactness and population constraints at the cost of added complexity. The algorithm runs in $ 2^{O(\\sqrt{n}\\log n)} $ time, although we only give a heuristic implementation. Our method generalizes an algorithm to count self-avoiding walks on a square to count paths that split general planar graphs into $ k $ regions, and uses this to sample from the space of all $ k $-partitions of a planar graph.","sentences":["The process of drawing electoral district boundaries is known as political redistricting.","Within this context, gerrymandering is the practice of drawing these boundaries such that they unfairly favor a particular political party, often leading to unequal representation and skewed electoral outcomes.","One of the few ways to detect gerrymandering is by algorithmically sampling redistricting plans.","Previous methods mainly focus on sampling from some neighborhood of ``realistic' districting plans, rather than a uniform sample of the entire space.","We present a deterministic subexponential time algorithm to uniformly sample from the space of all possible $ k $-partitions of a bounded degree planar graph, and with this construct a sample of the entire space of redistricting plans.","We also give a way to restrict this sample space to plans that match certain compactness and population constraints at the cost of added complexity.","The algorithm runs in $ 2^{O(\\sqrt{n}\\log n)} $ time, although we only give a heuristic implementation.","Our method generalizes an algorithm to count self-avoiding walks on a square to count paths that split general planar graphs into $ k $ regions, and uses this to sample from the space of all $ k $-partitions of a planar graph."],"url":"http://arxiv.org/abs/2402.13868v1"}
{"created":"2024-02-21 15:14:20","title":"Kuaiji: the First Chinese Accounting Large Language Model","abstract":"Large Language Models (LLMs) like ChatGPT and GPT-4 have demonstrated impressive proficiency in comprehending and generating natural language. However, they encounter difficulties when tasked with adapting to specialized domains such as accounting. To address this challenge, we introduce Kuaiji, a tailored Accounting Large Language Model. Kuaiji is meticulously fine-tuned using the Baichuan framework, which encompasses continuous pre-training and supervised fine-tuning processes. Supported by CAtAcctQA, a dataset containing large genuine accountant-client dialogues, Kuaiji exhibits exceptional accuracy and response speed. Our contributions encompass the creation of the first Chinese accounting dataset, the establishment of Kuaiji as a leading open-source Chinese accounting LLM, and the validation of its efficacy through real-world accounting scenarios.","sentences":["Large Language Models (LLMs) like ChatGPT and GPT-4 have demonstrated impressive proficiency in comprehending and generating natural language.","However, they encounter difficulties when tasked with adapting to specialized domains such as accounting.","To address this challenge, we introduce Kuaiji, a tailored Accounting Large Language Model.","Kuaiji is meticulously fine-tuned using the Baichuan framework, which encompasses continuous pre-training and supervised fine-tuning processes.","Supported by CAtAcctQA, a dataset containing large genuine accountant-client dialogues, Kuaiji exhibits exceptional accuracy and response speed.","Our contributions encompass the creation of the first Chinese accounting dataset, the establishment of Kuaiji as a leading open-source Chinese accounting LLM, and the validation of its efficacy through real-world accounting scenarios."],"url":"http://arxiv.org/abs/2402.13866v1"}
{"created":"2024-02-21 15:10:20","title":"Improving Efficiency of Iso-Surface Extraction on Implicit Neural Representations Using Uncertainty Propagation","abstract":"Implicit Neural representations (INRs) are widely used for scientific data reduction and visualization by modeling the function that maps a spatial location to a data value. Without any prior knowledge about the spatial distribution of values, we are forced to sample densely from INRs to perform visualization tasks like iso-surface extraction which can be very computationally expensive. Recently, range analysis has shown promising results in improving the efficiency of geometric queries, such as ray casting and hierarchical mesh extraction, on INRs for 3D geometries by using arithmetic rules to bound the output range of the network within a spatial region. However, the analysis bounds are often too conservative for complex scientific data. In this paper, we present an improved technique for range analysis by revisiting the arithmetic rules and analyzing the probability distribution of the network output within a spatial region. We model this distribution efficiently as a Gaussian distribution by applying the central limit theorem. Excluding low probability values, we are able to tighten the output bounds, resulting in a more accurate estimation of the value range, and hence more accurate identification of iso-surface cells and more efficient iso-surface extraction on INRs. Our approach demonstrates superior performance in terms of the iso-surface extraction time on four datasets compared to the original range analysis method and can also be generalized to other geometric query tasks.","sentences":["Implicit Neural representations (INRs) are widely used for scientific data reduction and visualization by modeling the function that maps a spatial location to a data value.","Without any prior knowledge about the spatial distribution of values, we are forced to sample densely from INRs to perform visualization tasks like iso-surface extraction which can be very computationally expensive.","Recently, range analysis has shown promising results in improving the efficiency of geometric queries, such as ray casting and hierarchical mesh extraction, on INRs for 3D geometries by using arithmetic rules to bound the output range of the network within a spatial region.","However, the analysis bounds are often too conservative for complex scientific data.","In this paper, we present an improved technique for range analysis by revisiting the arithmetic rules and analyzing the probability distribution of the network output within a spatial region.","We model this distribution efficiently as a Gaussian distribution by applying the central limit theorem.","Excluding low probability values, we are able to tighten the output bounds, resulting in a more accurate estimation of the value range, and hence more accurate identification of iso-surface cells and more efficient iso-surface extraction on INRs.","Our approach demonstrates superior performance in terms of the iso-surface extraction time on four datasets compared to the original range analysis method and can also be generalized to other geometric query tasks."],"url":"http://arxiv.org/abs/2402.13861v1"}
{"created":"2024-02-21 15:09:51","title":"Diversity-Aware $k$-Maximum Inner Product Search Revisited","abstract":"The $k$-Maximum Inner Product Search ($k$MIPS) serves as a foundational component in recommender systems and various data mining tasks. However, while most existing $k$MIPS approaches prioritize the efficient retrieval of highly relevant items for users, they often neglect an equally pivotal facet of search results: \\emph{diversity}. To bridge this gap, we revisit and refine the diversity-aware $k$MIPS (D$k$MIPS) problem by incorporating two well-known diversity objectives -- minimizing the average and maximum pairwise item similarities within the results -- into the original relevance objective. This enhancement, inspired by Maximal Marginal Relevance (MMR), offers users a controllable trade-off between relevance and diversity. We introduce \\textsc{Greedy} and \\textsc{DualGreedy}, two linear scan-based algorithms tailored for D$k$MIPS. They both achieve data-dependent approximations and, when aiming to minimize the average pairwise similarity, \\textsc{DualGreedy} attains an approximation ratio of $1/4$ with an additive term for regularization. To further improve query efficiency, we integrate a lightweight Ball-Cone Tree (BC-Tree) index with the two algorithms. Finally, comprehensive experiments on ten real-world data sets demonstrate the efficacy of our proposed methods, showcasing their capability to efficiently deliver diverse and relevant search results to users.","sentences":["The $k$-Maximum Inner Product Search ($k$MIPS) serves as a foundational component in recommender systems and various data mining tasks.","However, while most existing $k$MIPS approaches prioritize the efficient retrieval of highly relevant items for users, they often neglect an equally pivotal facet of search results: \\emph{diversity}.","To bridge this gap, we revisit and refine the diversity-aware $k$MIPS (D$k$MIPS) problem by incorporating two well-known diversity objectives -- minimizing the average and maximum pairwise item similarities within the results -- into the original relevance objective.","This enhancement, inspired by Maximal Marginal Relevance (MMR), offers users a controllable trade-off between relevance and diversity.","We introduce \\textsc{Greedy} and \\textsc{DualGreedy}, two linear scan-based algorithms tailored for D$k$MIPS.","They both achieve data-dependent approximations and, when aiming to minimize the average pairwise similarity, \\textsc{DualGreedy} attains an approximation ratio of $1/4$ with an additive term for regularization.","To further improve query efficiency, we integrate a lightweight Ball-Cone Tree (BC-Tree) index with the two algorithms.","Finally, comprehensive experiments on ten real-world data sets demonstrate the efficacy of our proposed methods, showcasing their capability to efficiently deliver diverse and relevant search results to users."],"url":"http://arxiv.org/abs/2402.13858v1"}
{"created":"2024-02-21 15:06:51","title":"Replicable Learning of Large-Margin Halfspaces","abstract":"We provide efficient replicable algorithms for the problem of learning large-margin halfspaces. Our results improve upon the algorithms provided by Impagliazzo, Lei, Pitassi, and Sorrell [STOC, 2022]. We design the first dimension-independent replicable algorithms for this task which runs in polynomial time, is proper, and has strictly improved sample complexity compared to the one achieved by Impagliazzo et al. [2022] with respect to all the relevant parameters. Moreover, our first algorithm has sample complexity that is optimal with respect to the accuracy parameter $\\epsilon$. We also design an SGD-based replicable algorithm that, in some parameters' regimes, achieves better sample and time complexity than our first algorithm.   Departing from the requirement of polynomial time algorithms, using the DP-to-Replicability reduction of Bun, Gaboardi, Hopkins, Impagliazzo, Lei, Pitassi, Sorrell, and Sivakumar [STOC, 2023], we show how to obtain a replicable algorithm for large-margin halfspaces with improved sample complexity with respect to the margin parameter $\\tau$, but running time doubly exponential in $1/\\tau^2$ and worse sample complexity dependence on $\\epsilon$ than one of our previous algorithms. We then design an improved algorithm with better sample complexity than all three of our previous algorithms and running time exponential in $1/\\tau^{2}$.","sentences":["We provide efficient replicable algorithms for the problem of learning large-margin halfspaces.","Our results improve upon the algorithms provided by Impagliazzo, Lei, Pitassi, and Sorrell","[STOC, 2022].","We design the first dimension-independent replicable algorithms for this task which runs in polynomial time, is proper, and has strictly improved sample complexity compared to the one achieved by Impagliazzo et al.","[2022] with respect to all the relevant parameters.","Moreover, our first algorithm has sample complexity that is optimal with respect to the accuracy parameter $\\epsilon$. We also design an SGD-based replicable algorithm that, in some parameters' regimes, achieves better sample and time complexity than our first algorithm.   ","Departing from the requirement of polynomial time algorithms, using the DP-to-Replicability reduction of Bun, Gaboardi, Hopkins, Impagliazzo, Lei, Pitassi, Sorrell, and","Sivakumar","[STOC, 2023], we show how to obtain a replicable algorithm for large-margin halfspaces with improved sample complexity with respect to the margin parameter $\\tau$, but running time doubly exponential in $1/\\tau^2$ and worse sample complexity dependence on $\\epsilon$ than one of our previous algorithms.","We then design an improved algorithm with better sample complexity than all three of our previous algorithms and running time exponential in $1/\\tau^{2}$."],"url":"http://arxiv.org/abs/2402.13857v1"}
{"created":"2024-02-21 14:59:49","title":"What we can learn from TikTok through its Research API","abstract":"TikTok is a social media platform that has gained immense popularity over the last few years, particularly among younger demographics, due to the viral trends and challenges shared worldwide. The recent release of a free Research API opens doors to collect data on posted videos, associated comments, and user activities. Our study focuses on evaluating the reliability of results returned by the Research API, by collecting and analyzing a random sample of TikTok videos posted in a span of 6 years. Our preliminary results are instrumental for future research that aims to study the platform, highlighting caveats on the geographical distribution of videos and on the global prevalence of viral hashtags.","sentences":["TikTok is a social media platform that has gained immense popularity over the last few years, particularly among younger demographics, due to the viral trends and challenges shared worldwide.","The recent release of a free Research API opens doors to collect data on posted videos, associated comments, and user activities.","Our study focuses on evaluating the reliability of results returned by the Research API, by collecting and analyzing a random sample of TikTok videos posted in a span of 6 years.","Our preliminary results are instrumental for future research that aims to study the platform, highlighting caveats on the geographical distribution of videos and on the global prevalence of viral hashtags."],"url":"http://arxiv.org/abs/2402.13855v1"}
{"created":"2024-02-21 14:59:46","title":"RealDex: Towards Human-like Grasping for Robotic Dexterous Hand","abstract":"In this paper, we introduce RealDex, a pioneering dataset capturing authentic dexterous hand grasping motions infused with human behavioral patterns, enriched by multi-view and multimodal visual data. Utilizing a teleoperation system, we seamlessly synchronize human-robot hand poses in real time. This collection of human-like motions is crucial for training dexterous hands to mimic human movements more naturally and precisely. RealDex holds immense promise in advancing humanoid robot for automated perception, cognition, and manipulation in real-world scenarios. Moreover, we introduce a cutting-edge dexterous grasping motion generation framework, which aligns with human experience and enhances real-world applicability through effectively utilizing Multimodal Large Language Models. Extensive experiments have demonstrated the superior performance of our method on RealDex and other open datasets. The complete dataset and code will be made available upon the publication of this work.","sentences":["In this paper, we introduce RealDex, a pioneering dataset capturing authentic dexterous hand grasping motions infused with human behavioral patterns, enriched by multi-view and multimodal visual data.","Utilizing a teleoperation system, we seamlessly synchronize human-robot hand poses in real time.","This collection of human-like motions is crucial for training dexterous hands to mimic human movements more naturally and precisely.","RealDex holds immense promise in advancing humanoid robot for automated perception, cognition, and manipulation in real-world scenarios.","Moreover, we introduce a cutting-edge dexterous grasping motion generation framework, which aligns with human experience and enhances real-world applicability through effectively utilizing Multimodal Large Language Models.","Extensive experiments have demonstrated the superior performance of our method on RealDex and other open datasets.","The complete dataset and code will be made available upon the publication of this work."],"url":"http://arxiv.org/abs/2402.13853v1"}
{"created":"2024-02-21 14:56:36","title":"Neural Control System for Continuous Glucose Monitoring and Maintenance","abstract":"Precise glucose level management is pivotal for individuals with diabetes, averting severe complications. In this work, we introduce a novel neural control system for continuous glucose monitoring and maintenance, utilizing differential predictive control. Our system, guided by a sophisticated neural policy and differentiable modeling, dynamically adjusts insulin delivery in real-time, enhancing glucose optimization. This end-to-end approach maximizes efficiency, ensuring personalized care and improved health outcomes, as affirmed by empirical findings.","sentences":["Precise glucose level management is pivotal for individuals with diabetes, averting severe complications.","In this work, we introduce a novel neural control system for continuous glucose monitoring and maintenance, utilizing differential predictive control.","Our system, guided by a sophisticated neural policy and differentiable modeling, dynamically adjusts insulin delivery in real-time, enhancing glucose optimization.","This end-to-end approach maximizes efficiency, ensuring personalized care and improved health outcomes, as affirmed by empirical findings."],"url":"http://arxiv.org/abs/2402.13852v1"}
{"created":"2024-02-21 14:54:30","title":"VL-Trojan: Multimodal Instruction Backdoor Attacks against Autoregressive Visual Language Models","abstract":"Autoregressive Visual Language Models (VLMs) showcase impressive few-shot learning capabilities in a multimodal context. Recently, multimodal instruction tuning has been proposed to further enhance instruction-following abilities. However, we uncover the potential threat posed by backdoor attacks on autoregressive VLMs during instruction tuning. Adversaries can implant a backdoor by injecting poisoned samples with triggers embedded in instructions or images, enabling malicious manipulation of the victim model's predictions with predefined triggers. Nevertheless, the frozen visual encoder in autoregressive VLMs imposes constraints on the learning of conventional image triggers. Additionally, adversaries may encounter restrictions in accessing the parameters and architectures of the victim model. To address these challenges, we propose a multimodal instruction backdoor attack, namely VL-Trojan. Our approach facilitates image trigger learning through an isolating and clustering strategy and enhance black-box-attack efficacy via an iterative character-level text trigger generation method. Our attack successfully induces target outputs during inference, significantly surpassing baselines (+62.52\\%) in ASR. Moreover, it demonstrates robustness across various model scales and few-shot in-context reasoning scenarios.","sentences":["Autoregressive Visual Language Models (VLMs) showcase impressive few-shot learning capabilities in a multimodal context.","Recently, multimodal instruction tuning has been proposed to further enhance instruction-following abilities.","However, we uncover the potential threat posed by backdoor attacks on autoregressive VLMs during instruction tuning.","Adversaries can implant a backdoor by injecting poisoned samples with triggers embedded in instructions or images, enabling malicious manipulation of the victim model's predictions with predefined triggers.","Nevertheless, the frozen visual encoder in autoregressive VLMs imposes constraints on the learning of conventional image triggers.","Additionally, adversaries may encounter restrictions in accessing the parameters and architectures of the victim model.","To address these challenges, we propose a multimodal instruction backdoor attack, namely VL-Trojan.","Our approach facilitates image trigger learning through an isolating and clustering strategy and enhance black-box-attack efficacy via an iterative character-level text trigger generation method.","Our attack successfully induces target outputs during inference, significantly surpassing baselines (+62.52\\%) in ASR.","Moreover, it demonstrates robustness across various model scales and few-shot in-context reasoning scenarios."],"url":"http://arxiv.org/abs/2402.13851v1"}
{"created":"2024-02-21 14:50:24","title":"Zero-BEV: Zero-shot Projection of Any First-Person Modality to BEV Maps","abstract":"Bird's-eye view (BEV) maps are an important geometrically structured representation widely used in robotics, in particular self-driving vehicles and terrestrial robots. Existing algorithms either require depth information for the geometric projection, which is not always reliably available, or are trained end-to-end in a fully supervised way to map visual first-person observations to BEV representation, and are therefore restricted to the output modality they have been trained for. In contrast, we propose a new model capable of performing zero-shot projections of any modality available in a first person view to the corresponding BEV map. This is achieved by disentangling the geometric inverse perspective projection from the modality transformation, eg. RGB to occupancy. The method is general and we showcase experiments projecting to BEV three different modalities: semantic segmentation, motion vectors and object bounding boxes detected in first person. We experimentally show that the model outperforms competing methods, in particular the widely used baseline resorting to monocular depth estimation.","sentences":["Bird's-eye view (BEV) maps are an important geometrically structured representation widely used in robotics, in particular self-driving vehicles and terrestrial robots.","Existing algorithms either require depth information for the geometric projection, which is not always reliably available, or are trained end-to-end in a fully supervised way to map visual first-person observations to BEV representation, and are therefore restricted to the output modality they have been trained for.","In contrast, we propose a new model capable of performing zero-shot projections of any modality available in a first person view to the corresponding BEV map.","This is achieved by disentangling the geometric inverse perspective projection from the modality transformation, eg.","RGB to occupancy.","The method is general and we showcase experiments projecting to BEV three different modalities: semantic segmentation, motion vectors and object bounding boxes detected in first person.","We experimentally show that the model outperforms competing methods, in particular the widely used baseline resorting to monocular depth estimation."],"url":"http://arxiv.org/abs/2402.13848v1"}
{"created":"2024-02-21 14:44:00","title":"Large Language Models are Advanced Anonymizers","abstract":"Recent work in privacy research on large language models has shown that they achieve near human-level performance at inferring personal data from real-world online texts. With consistently increasing model capabilities, existing text anonymization methods are currently lacking behind regulatory requirements and adversarial threats. This raises the question of how individuals can effectively protect their personal data in sharing online texts. In this work, we take two steps to answer this question: We first present a new setting for evaluating anonymizations in the face of adversarial LLMs inferences, allowing for a natural measurement of anonymization performance while remedying some of the shortcomings of previous metrics. We then present our LLM-based adversarial anonymization framework leveraging the strong inferential capabilities of LLMs to inform our anonymization procedure. In our experimental evaluation, we show on real-world and synthetic online texts how adversarial anonymization outperforms current industry-grade anonymizers both in terms of the resulting utility and privacy.","sentences":["Recent work in privacy research on large language models has shown that they achieve near human-level performance at inferring personal data from real-world online texts.","With consistently increasing model capabilities, existing text anonymization methods are currently lacking behind regulatory requirements and adversarial threats.","This raises the question of how individuals can effectively protect their personal data in sharing online texts.","In this work, we take two steps to answer this question: We first present a new setting for evaluating anonymizations in the face of adversarial LLMs inferences, allowing for a natural measurement of anonymization performance while remedying some of the shortcomings of previous metrics.","We then present our LLM-based adversarial anonymization framework leveraging the strong inferential capabilities of LLMs to inform our anonymization procedure.","In our experimental evaluation, we show on real-world and synthetic online texts how adversarial anonymization outperforms current industry-grade anonymizers both in terms of the resulting utility and privacy."],"url":"http://arxiv.org/abs/2402.13846v1"}
{"created":"2024-02-21 14:43:34","title":"Multi-Agent Online Graph Exploration on Cycles and Tadpole Graphs","abstract":"We study the problem of multi-agent online graph exploration, in which a team of k agents has to explore a given graph, starting and ending on the same node. The graph is initially unknown. Whenever a node is visited by an agent, its neighborhood and adjacent edges are revealed. The agents share a global view of the explored parts of the graph. The cost of the exploration has to be minimized, where cost either describes the time needed for the entire exploration (time model), or the length of the longest path traversed by any agent (energy model). We investigate graph exploration on cycles and tadpole graphs for 2-4 agents, providing optimal results on the competitive ratio in the energy model (1-competitive with two agents on cycles and three agents on tadpole graphs), and for tadpole graphs in the time model (1.5-competitive with four agents). We also show competitive upper bounds of 2 for the exploration of tadpole graphs with three agents, and 2.5 for the exploration of tadpole graphs with two agents in the time model.","sentences":["We study the problem of multi-agent online graph exploration, in which a team of k agents has to explore a given graph, starting and ending on the same node.","The graph is initially unknown.","Whenever a node is visited by an agent, its neighborhood and adjacent edges are revealed.","The agents share a global view of the explored parts of the graph.","The cost of the exploration has to be minimized, where cost either describes the time needed for the entire exploration (time model), or the length of the longest path traversed by any agent (energy model).","We investigate graph exploration on cycles and tadpole graphs for 2-4 agents, providing optimal results on the competitive ratio in the energy model (1-competitive with two agents on cycles and three agents on tadpole graphs), and for tadpole graphs in the time model (1.5-competitive with four agents).","We also show competitive upper bounds of 2 for the exploration of tadpole graphs with three agents, and 2.5 for the exploration of tadpole graphs with two agents in the time model."],"url":"http://arxiv.org/abs/2402.13845v1"}
{"created":"2024-02-21 14:38:37","title":"Equilibria, Efficiency, and Inequality in Network Formation for Hiring and Opportunity","abstract":"Professional networks -- the social networks among people in a given line of work -- can serve as a conduit for job prospects and other opportunities. Here we propose a model for the formation of such networks and the transfer of opportunities within them. In our theoretical model, individuals strategically connect with others to maximize the probability that they receive opportunities from them. We explore how professional networks balance connectivity, where connections facilitate opportunity transfers to those who did not get them from outside sources, and congestion, where some individuals receive too many opportunities from their connections and waste some of them.   We show that strategic individuals are over-connected at equilibrium relative to a social optimum, leading to a price of anarchy for which we derive nearly tight asymptotic bounds. We also show that, at equilibrium, individuals form connections to those who provide similar benefit to them as they provide to others. Thus, our model provides a microfoundation in professional networking contexts for the fundamental sociological principle of homophily, that \"similarity breeds connection,\" which in our setting is realized as a form of status homophily based on alignment in individual benefit. We further explore how, even if individuals are a priori equally likely to receive opportunities from outside sources, equilibria can be unequal, and we provide nearly tight bounds on how unequal they can be. Finally, we explore the ability for online platforms to intervene to improve social welfare and show that natural heuristics may result in adverse effects at equilibrium. Our simple model allows for a surprisingly rich analysis of coordination problems in professional networks and suggests many directions for further exploration.","sentences":["Professional networks -- the social networks among people in a given line of work -- can serve as a conduit for job prospects and other opportunities.","Here we propose a model for the formation of such networks and the transfer of opportunities within them.","In our theoretical model, individuals strategically connect with others to maximize the probability that they receive opportunities from them.","We explore how professional networks balance connectivity, where connections facilitate opportunity transfers to those who did not get them from outside sources, and congestion, where some individuals receive too many opportunities from their connections and waste some of them.   ","We show that strategic individuals are over-connected at equilibrium relative to a social optimum, leading to a price of anarchy for which we derive nearly tight asymptotic bounds.","We also show that, at equilibrium, individuals form connections to those who provide similar benefit to them as they provide to others.","Thus, our model provides a microfoundation in professional networking contexts for the fundamental sociological principle of homophily, that \"similarity breeds connection,\" which in our setting is realized as a form of status homophily based on alignment in individual benefit.","We further explore how, even if individuals are a priori equally likely to receive opportunities from outside sources, equilibria can be unequal, and we provide nearly tight bounds on how unequal they can be.","Finally, we explore the ability for online platforms to intervene to improve social welfare and show that natural heuristics may result in adverse effects at equilibrium.","Our simple model allows for a surprisingly rich analysis of coordination problems in professional networks and suggests many directions for further exploration."],"url":"http://arxiv.org/abs/2402.13841v1"}
{"created":"2024-02-21 14:38:02","title":"LLM4SBR: A Lightweight and Effective Framework for Integrating Large Language Models in Session-based Recommendation","abstract":"Traditional session-based recommendation (SBR) utilizes session behavior sequences from anonymous users for recommendation. Although this strategy is highly efficient, it sacrifices the inherent semantic information of the items, making it difficult for the model to understand the true intent of the session and resulting in a lack of interpretability in the recommended results. Recently, large language models (LLMs) have flourished across various domains, offering a glimpse of hope in addressing the aforementioned challenges. Inspired by the impact of LLMs, research exploring the integration of LLMs with the Recommender system (RS) has surged like mushrooms after rain. However, constrained by high time and space costs, as well as the brief and anonymous nature of session data, the first LLM recommendation framework suitable for industrial deployment has yet to emerge in the field of SBR. To address the aforementioned challenges, we have proposed the LLM Integration Framework for SBR (LLM4SBR). Serving as a lightweight and plug-and-play framework, LLM4SBR adopts a two-step strategy. Firstly, we transform session data into a bimodal form of text and behavior. In the first step, leveraging the inferential capabilities of LLMs, we conduct inference on session text data from different perspectives and design the component for auxiliary enhancement. In the second step, the SBR model is trained on behavior data, aligning and averaging two modal session representations from different perspectives. Finally, we fuse session representations from different perspectives and modalities as the ultimate session representation for recommendation. We conducted experiments on two real-world datasets, and the results demonstrate that LLM4SBR significantly improves the performance of traditional SBR models and is highly lightweight and efficient, making it suitable for industrial deployment.","sentences":["Traditional session-based recommendation (SBR) utilizes session behavior sequences from anonymous users for recommendation.","Although this strategy is highly efficient, it sacrifices the inherent semantic information of the items, making it difficult for the model to understand the true intent of the session and resulting in a lack of interpretability in the recommended results.","Recently, large language models (LLMs) have flourished across various domains, offering a glimpse of hope in addressing the aforementioned challenges.","Inspired by the impact of LLMs, research exploring the integration of LLMs with the Recommender system (RS) has surged like mushrooms after rain.","However, constrained by high time and space costs, as well as the brief and anonymous nature of session data, the first LLM recommendation framework suitable for industrial deployment has yet to emerge in the field of SBR.","To address the aforementioned challenges, we have proposed the LLM Integration Framework for SBR (LLM4SBR).","Serving as a lightweight and plug-and-play framework, LLM4SBR adopts a two-step strategy.","Firstly, we transform session data into a bimodal form of text and behavior.","In the first step, leveraging the inferential capabilities of LLMs, we conduct inference on session text data from different perspectives and design the component for auxiliary enhancement.","In the second step, the SBR model is trained on behavior data, aligning and averaging two modal session representations from different perspectives.","Finally, we fuse session representations from different perspectives and modalities as the ultimate session representation for recommendation.","We conducted experiments on two real-world datasets, and the results demonstrate that LLM4SBR significantly improves the performance of traditional SBR models and is highly lightweight and efficient, making it suitable for industrial deployment."],"url":"http://arxiv.org/abs/2402.13840v1"}
{"created":"2024-02-21 14:29:27","title":"Design of a Miniature Underwater Vehicle and Data Collection System for Indoor Experimentation","abstract":"This paper describes the design of a miniature uncrewed underwater vehicle (MiniUUV) and related instrumentation for indoor experimentation. The MiniUUV was developed using 3D printed components and low-cost, off-the-shelf electronics. The vehicle uses a propeller differential propulsion drive and a peristaltic pump with a syringe for buoyancy control. A water tank with an overhead camera system was constructed to allow for convenient indoor data collection in a controlled environment. Several tests were conducted to demonstrate the capabilities of the MiniUUV and data collection system, including buoyancy pump actuation tests and straight line, circular, and zig-zag motion tests on the surface. During each planar motion test an AprilTag was attached to the MiniUUV and an overhead camera system obtained video recordings that were processed offline to estimate vehicle position, surge velocity, sway velocity, yaw angle, and yaw rate.","sentences":["This paper describes the design of a miniature uncrewed underwater vehicle (MiniUUV) and related instrumentation for indoor experimentation.","The MiniUUV was developed using 3D printed components and low-cost, off-the-shelf electronics.","The vehicle uses a propeller differential propulsion drive and a peristaltic pump with a syringe for buoyancy control.","A water tank with an overhead camera system was constructed to allow for convenient indoor data collection in a controlled environment.","Several tests were conducted to demonstrate the capabilities of the MiniUUV and data collection system, including buoyancy pump actuation tests and straight line, circular, and zig-zag motion tests on the surface.","During each planar motion test an AprilTag was attached to the MiniUUV and an overhead camera system obtained video recordings that were processed offline to estimate vehicle position, surge velocity, sway velocity, yaw angle, and yaw rate."],"url":"http://arxiv.org/abs/2402.13837v1"}
{"created":"2024-02-21 14:22:20","title":"MLXP: A framework for conducting replicable Machine Learning eXperiments in Python","abstract":"Replicability in machine learning (ML) research is increasingly concerning due to the utilization of complex non-deterministic algorithms and the dependence on numerous hyper-parameter choices, such as model architecture and training datasets. Ensuring reproducible and replicable results is crucial for advancing the field, yet often requires significant technical effort to conduct systematic and well-organized experiments that yield robust conclusions. Several tools have been developed to facilitate experiment management and enhance reproducibility; however, they often introduce complexity that hinders adoption within the research community, despite being well-handled in industrial settings. To address the challenge of low adoption, we propose MLXP, an open-source, simple, and lightweight experiment management tool based on Python, available at https://github.com/inria-thoth/mlxp . MLXP streamlines the experimental process with minimal practitioner overhead while ensuring a high level of reproducibility.","sentences":["Replicability in machine learning (ML) research is increasingly concerning due to the utilization of complex non-deterministic algorithms and the dependence on numerous hyper-parameter choices, such as model architecture and training datasets.","Ensuring reproducible and replicable results is crucial for advancing the field, yet often requires significant technical effort to conduct systematic and well-organized experiments that yield robust conclusions.","Several tools have been developed to facilitate experiment management and enhance reproducibility; however, they often introduce complexity that hinders adoption within the research community, despite being well-handled in industrial settings.","To address the challenge of low adoption, we propose MLXP, an open-source, simple, and lightweight experiment management tool based on Python, available at https://github.com/inria-thoth/mlxp .","MLXP streamlines the experimental process with minimal practitioner overhead while ensuring a high level of reproducibility."],"url":"http://arxiv.org/abs/2402.13831v1"}
{"created":"2024-02-21 14:17:45","title":"Origami: (un)folding the abstraction of recursion schemes for program synthesis","abstract":"Program synthesis with Genetic Programming searches for a correct program that satisfies the input specification, which is usually provided as input-output examples. One particular challenge is how to effectively handle loops and recursion avoiding programs that never terminate. A helpful abstraction that can alleviate this problem is the employment of Recursion Schemes that generalize the combination of data production and consumption. Recursion Schemes are very powerful as they allow the construction of programs that can summarize data, create sequences, and perform advanced calculations. The main advantage of writing a program using Recursion Schemes is that the programs are composed of well defined templates with only a few parts that need to be synthesized. In this paper we make an initial study of the benefits of using program synthesis with fold and unfold templates, and outline some preliminary experimental results. To highlight the advantages and disadvantages of this approach, we manually solved the entire GPSB benchmark using recursion schemes, highlighting the parts that should be evolved compared to alternative implementations. We noticed that, once the choice of which recursion scheme is made, the synthesis process can be simplified as each of the missing parts of the template are reduced to simpler functions, which are further constrained by their own input and output types.","sentences":["Program synthesis with Genetic Programming searches for a correct program that satisfies the input specification, which is usually provided as input-output examples.","One particular challenge is how to effectively handle loops and recursion avoiding programs that never terminate.","A helpful abstraction that can alleviate this problem is the employment of Recursion Schemes that generalize the combination of data production and consumption.","Recursion Schemes are very powerful as they allow the construction of programs that can summarize data, create sequences, and perform advanced calculations.","The main advantage of writing a program using Recursion Schemes is that the programs are composed of well defined templates with only a few parts that need to be synthesized.","In this paper we make an initial study of the benefits of using program synthesis with fold and unfold templates, and outline some preliminary experimental results.","To highlight the advantages and disadvantages of this approach, we manually solved the entire GPSB benchmark using recursion schemes, highlighting the parts that should be evolved compared to alternative implementations.","We noticed that, once the choice of which recursion scheme is made, the synthesis process can be simplified as each of the missing parts of the template are reduced to simpler functions, which are further constrained by their own input and output types."],"url":"http://arxiv.org/abs/2402.13828v1"}
{"created":"2024-02-21 14:16:49","title":"Identifying Unnecessary 3D Gaussians using Clustering for Fast Rendering of 3D Gaussian Splatting","abstract":"3D Gaussian splatting (3D-GS) is a new rendering approach that outperforms the neural radiance field (NeRF) in terms of both speed and image quality. 3D-GS represents 3D scenes by utilizing millions of 3D Gaussians and projects these Gaussians onto the 2D image plane for rendering. However, during the rendering process, a substantial number of unnecessary 3D Gaussians exist for the current view direction, resulting in significant computation costs associated with their identification. In this paper, we propose a computational reduction technique that quickly identifies unnecessary 3D Gaussians in real-time for rendering the current view without compromising image quality. This is accomplished through the offline clustering of 3D Gaussians that are close in distance, followed by the projection of these clusters onto a 2D image plane during runtime. Additionally, we analyze the bottleneck associated with the proposed technique when executed on GPUs and propose an efficient hardware architecture that seamlessly supports the proposed scheme. For the Mip-NeRF360 dataset, the proposed technique excludes 63% of 3D Gaussians on average before the 2D image projection, which reduces the overall rendering computation by almost 38.3% without sacrificing peak-signal-to-noise-ratio (PSNR). The proposed accelerator also achieves a speedup of 10.7x compared to a GPU.","sentences":["3D Gaussian splatting (3D-GS) is a new rendering approach that outperforms the neural radiance field (NeRF) in terms of both speed and image quality.","3D-GS represents 3D scenes by utilizing millions of 3D Gaussians and projects these Gaussians onto the 2D image plane for rendering.","However, during the rendering process, a substantial number of unnecessary 3D Gaussians exist for the current view direction, resulting in significant computation costs associated with their identification.","In this paper, we propose a computational reduction technique that quickly identifies unnecessary 3D Gaussians in real-time for rendering the current view without compromising image quality.","This is accomplished through the offline clustering of 3D Gaussians that are close in distance, followed by the projection of these clusters onto a 2D image plane during runtime.","Additionally, we analyze the bottleneck associated with the proposed technique when executed on GPUs and propose an efficient hardware architecture that seamlessly supports the proposed scheme.","For the Mip-NeRF360 dataset, the proposed technique excludes 63% of 3D Gaussians on average before the 2D image projection, which reduces the overall rendering computation by almost 38.3% without sacrificing peak-signal-to-noise-ratio (PSNR).","The proposed accelerator also achieves a speedup of 10.7x compared to a GPU."],"url":"http://arxiv.org/abs/2402.13827v1"}
{"created":"2024-02-21 14:04:04","title":"Multi-Agent Contract Design beyond Binary Actions","abstract":"We study hidden-action principal-agent problems with multiple agents. Unlike previous work, we consider a general setting in which each agent has an arbitrary number of actions, and the joint action induces outcomes according to an arbitrary distribution. We study two classes of mechanisms: a class of deterministic mechanisms that is the natural extension of single-agent contracts, in which the agents play a Nash equilibrium of the game induced by the contract, and a class of randomized mechanisms that is inspired by single-agent randomized contracts and correlated equilibria.","sentences":["We study hidden-action principal-agent problems with multiple agents.","Unlike previous work, we consider a general setting in which each agent has an arbitrary number of actions, and the joint action induces outcomes according to an arbitrary distribution.","We study two classes of mechanisms: a class of deterministic mechanisms that is the natural extension of single-agent contracts, in which the agents play a Nash equilibrium of the game induced by the contract, and a class of randomized mechanisms that is inspired by single-agent randomized contracts and correlated equilibria."],"url":"http://arxiv.org/abs/2402.13824v1"}
{"created":"2024-02-21 14:00:52","title":"Using Large Language Models for Natural Language Processing Tasks in Requirements Engineering: A Systematic Guideline","abstract":"To use Large Language Models (LLMs) in a targeted way for NLP problems in RE, we require both (1) basic knowledge about the inner workings of LLMs and (2) a guideline on how to select and systematically utilize or repurpose LLMs for NLP4RE tasks. This chapter establishes the required knowledge and introduces the fundamentals of LLMs in the first part. In the second part, we present a detailed guideline for students, researchers, and practitioners on using LLMs for their purposes.","sentences":["To use Large Language Models (LLMs) in a targeted way for NLP problems in RE, we require both (1) basic knowledge about the inner workings of LLMs and (2) a guideline on how to select and systematically utilize or repurpose LLMs for NLP4RE tasks.","This chapter establishes the required knowledge and introduces the fundamentals of LLMs in the first part.","In the second part, we present a detailed guideline for students, researchers, and practitioners on using LLMs for their purposes."],"url":"http://arxiv.org/abs/2402.13823v1"}
{"created":"2024-02-21 13:59:55","title":"MSTAR: Multi-Scale Backbone Architecture Search for Timeseries Classification","abstract":"Most of the previous approaches to Time Series Classification (TSC) highlight the significance of receptive fields and frequencies while overlooking the time resolution. Hence, unavoidably suffered from scalability issues as they integrated an extensive range of receptive fields into classification models. Other methods, while having a better adaptation for large datasets, require manual design and yet not being able to reach the optimal architecture due to the uniqueness of each dataset. We overcome these challenges by proposing a novel multi-scale search space and a framework for Neural architecture search (NAS), which addresses both the problem of frequency and time resolution, discovering the suitable scale for a specific dataset. We further show that our model can serve as a backbone to employ a powerful Transformer module with both untrained and pre-trained weights. Our search space reaches the state-of-the-art performance on four datasets on four different domains while introducing more than ten highly fine-tuned models for each data.","sentences":["Most of the previous approaches to Time Series Classification (TSC) highlight the significance of receptive fields and frequencies while overlooking the time resolution.","Hence, unavoidably suffered from scalability issues as they integrated an extensive range of receptive fields into classification models.","Other methods, while having a better adaptation for large datasets, require manual design and yet not being able to reach the optimal architecture due to the uniqueness of each dataset.","We overcome these challenges by proposing a novel multi-scale search space and a framework for Neural architecture search (NAS), which addresses both the problem of frequency and time resolution, discovering the suitable scale for a specific dataset.","We further show that our model can serve as a backbone to employ a powerful Transformer module with both untrained and pre-trained weights.","Our search space reaches the state-of-the-art performance on four datasets on four different domains while introducing more than ten highly fine-tuned models for each data."],"url":"http://arxiv.org/abs/2402.13822v1"}
{"created":"2024-02-21 13:59:47","title":"Performance Improvement Bounds for Lipschitz Configurable Markov Decision Processes","abstract":"Configurable Markov Decision Processes (Conf-MDPs) have recently been introduced as an extension of the traditional Markov Decision Processes (MDPs) to model the real-world scenarios in which there is the possibility to intervene in the environment in order to configure some of its parameters. In this paper, we focus on a particular subclass of Conf-MDP that satisfies regularity conditions, namely Lipschitz continuity. We start by providing a bound on the Wasserstein distance between $\\gamma$-discounted stationary distributions induced by changing policy and configuration. This result generalizes the already existing bounds both for Conf-MDPs and traditional MDPs. Then, we derive a novel performance improvement lower bound.","sentences":["Configurable Markov Decision Processes (Conf-MDPs) have recently been introduced as an extension of the traditional Markov Decision Processes (MDPs) to model the real-world scenarios in which there is the possibility to intervene in the environment in order to configure some of its parameters.","In this paper, we focus on a particular subclass of Conf-MDP that satisfies regularity conditions, namely Lipschitz continuity.","We start by providing a bound on the Wasserstein distance between $\\gamma$-discounted stationary distributions induced by changing policy and configuration.","This result generalizes the already existing bounds both for Conf-MDPs and traditional MDPs.","Then, we derive a novel performance improvement lower bound."],"url":"http://arxiv.org/abs/2402.13821v1"}
{"created":"2024-02-21 13:59:21","title":"FLD: Fourier Latent Dynamics for Structured Motion Representation and Learning","abstract":"Motion trajectories offer reliable references for physics-based motion learning but suffer from sparsity, particularly in regions that lack sufficient data coverage. To address this challenge, we introduce a self-supervised, structured representation and generation method that extracts spatial-temporal relationships in periodic or quasi-periodic motions. The motion dynamics in a continuously parameterized latent space enable our method to enhance the interpolation and generalization capabilities of motion learning algorithms. The motion learning controller, informed by the motion parameterization, operates online tracking of a wide range of motions, including targets unseen during training. With a fallback mechanism, the controller dynamically adapts its tracking strategy and automatically resorts to safe action execution when a potentially risky target is proposed. By leveraging the identified spatial-temporal structure, our work opens new possibilities for future advancements in general motion representation and learning algorithms.","sentences":["Motion trajectories offer reliable references for physics-based motion learning but suffer from sparsity, particularly in regions that lack sufficient data coverage.","To address this challenge, we introduce a self-supervised, structured representation and generation method that extracts spatial-temporal relationships in periodic or quasi-periodic motions.","The motion dynamics in a continuously parameterized latent space enable our method to enhance the interpolation and generalization capabilities of motion learning algorithms.","The motion learning controller, informed by the motion parameterization, operates online tracking of a wide range of motions, including targets unseen during training.","With a fallback mechanism, the controller dynamically adapts its tracking strategy and automatically resorts to safe action execution when a potentially risky target is proposed.","By leveraging the identified spatial-temporal structure, our work opens new possibilities for future advancements in general motion representation and learning algorithms."],"url":"http://arxiv.org/abs/2402.13820v1"}
{"created":"2024-02-21 13:57:36","title":"Beyond Hate Speech: NLP's Challenges and Opportunities in Uncovering Dehumanizing Language","abstract":"Dehumanization, characterized as a subtle yet harmful manifestation of hate speech, involves denying individuals of their human qualities and often results in violence against marginalized groups. Despite significant progress in Natural Language Processing across various domains, its application in detecting dehumanizing language is limited, largely due to the scarcity of publicly available annotated data for this domain. This paper evaluates the performance of cutting-edge NLP models, including GPT-4, GPT-3.5, and LLAMA-2, in identifying dehumanizing language. Our findings reveal that while these models demonstrate potential, achieving a 70\\% accuracy rate in distinguishing dehumanizing language from broader hate speech, they also display biases. They are over-sensitive in classifying other forms of hate speech as dehumanization for a specific subset of target groups, while more frequently failing to identify clear cases of dehumanization for other target groups. Moreover, leveraging one of the best-performing models, we automatically annotated a larger dataset for training more accessible models. However, our findings indicate that these models currently do not meet the high-quality data generation threshold necessary for this task.","sentences":["Dehumanization, characterized as a subtle yet harmful manifestation of hate speech, involves denying individuals of their human qualities and often results in violence against marginalized groups.","Despite significant progress in Natural Language Processing across various domains, its application in detecting dehumanizing language is limited, largely due to the scarcity of publicly available annotated data for this domain.","This paper evaluates the performance of cutting-edge NLP models, including GPT-4, GPT-3.5, and LLAMA-2, in identifying dehumanizing language.","Our findings reveal that while these models demonstrate potential, achieving a 70\\% accuracy rate in distinguishing dehumanizing language from broader hate speech, they also display biases.","They are over-sensitive in classifying other forms of hate speech as dehumanization for a specific subset of target groups, while more frequently failing to identify clear cases of dehumanization for other target groups.","Moreover, leveraging one of the best-performing models, we automatically annotated a larger dataset for training more accessible models.","However, our findings indicate that these models currently do not meet the high-quality data generation threshold necessary for this task."],"url":"http://arxiv.org/abs/2402.13818v1"}
