{"created":"2024-02-27 18:59:27","title":"Factors that Affect Personalization of Robots for Older Adults","abstract":"We introduce a taxonomy of important factors to consider when designing interactions with an assistive robot in a senior living facility. These factors are derived from our reflection on two field studies and are grouped into the following high-level categories: primary user (residents), care partners, robot, facility and external circumstances. We outline how multiple factors in these categories impact different aspects of personalization, such as adjusting interactions based on the unique needs of a resident or modifying alerts about the robot's status for different care partners. This preliminary taxonomy serves as a framework for considering how to deploy personalized assistive robots in the complex caregiving ecosystem.","sentences":["We introduce a taxonomy of important factors to consider when designing interactions with an assistive robot in a senior living facility.","These factors are derived from our reflection on two field studies and are grouped into the following high-level categories: primary user (residents), care partners, robot, facility and external circumstances.","We outline how multiple factors in these categories impact different aspects of personalization, such as adjusting interactions based on the unique needs of a resident or modifying alerts about the robot's status for different care partners.","This preliminary taxonomy serves as a framework for considering how to deploy personalized assistive robots in the complex caregiving ecosystem."],"url":"http://arxiv.org/abs/2402.17769v1"}
{"created":"2024-02-27 18:59:18","title":"Diffusion Meets DAgger: Supercharging Eye-in-hand Imitation Learning","abstract":"A common failure mode for policies trained with imitation is compounding execution errors at test time. When the learned policy encounters states that were not present in the expert demonstrations, the policy fails, leading to degenerate behavior. The Dataset Aggregation, or DAgger approach to this problem simply collects more data to cover these failure states. However, in practice, this is often prohibitively expensive. In this work, we propose Diffusion Meets DAgger (DMD), a method to reap the benefits of DAgger without the cost for eye-in-hand imitation learning problems. Instead of collecting new samples to cover out-of-distribution states, DMD uses recent advances in diffusion models to create these samples with diffusion models. This leads to robust performance from few demonstrations. In experiments conducted for non-prehensile pushing on a Franka Research 3, we show that DMD can achieve a success rate of 80% with as few as 8 expert demonstrations, where naive behavior cloning reaches only 20%. DMD also outperform competing NeRF-based augmentation schemes by 50%.","sentences":["A common failure mode for policies trained with imitation is compounding execution errors at test time.","When the learned policy encounters states that were not present in the expert demonstrations, the policy fails, leading to degenerate behavior.","The Dataset Aggregation, or DAgger approach to this problem simply collects more data to cover these failure states.","However, in practice, this is often prohibitively expensive.","In this work, we propose Diffusion Meets DAgger (DMD), a method to reap the benefits of DAgger without the cost for eye-in-hand imitation learning problems.","Instead of collecting new samples to cover out-of-distribution states, DMD uses recent advances in diffusion models to create these samples with diffusion models.","This leads to robust performance from few demonstrations.","In experiments conducted for non-prehensile pushing on a Franka Research 3, we show that DMD can achieve a success rate of 80% with as few as 8 expert demonstrations, where naive behavior cloning reaches only 20%.","DMD also outperform competing NeRF-based augmentation schemes by 50%."],"url":"http://arxiv.org/abs/2402.17768v1"}
{"created":"2024-02-27 18:58:54","title":"Opening Cabinets and Drawers in the Real World using a Commodity Mobile Manipulator","abstract":"Pulling open cabinets and drawers presents many difficult technical challenges in perception (inferring articulation parameters for objects from onboard sensors), planning (producing motion plans that conform to tight task constraints), and control (making and maintaining contact while applying forces on the environment). In this work, we build an end-to-end system that enables a commodity mobile manipulator (Stretch RE2) to pull open cabinets and drawers in diverse previously unseen real world environments. We conduct 4 days of real world testing of this system spanning 31 different objects from across 13 different real world environments. Our system achieves a success rate of 61% on opening novel cabinets and drawers in unseen environments zero-shot. An analysis of the failure modes suggests that errors in perception are the most significant challenge for our system. We will open source code and models for others to replicate and build upon our system.","sentences":["Pulling open cabinets and drawers presents many difficult technical challenges in perception (inferring articulation parameters for objects from onboard sensors), planning (producing motion plans that conform to tight task constraints), and control (making and maintaining contact while applying forces on the environment).","In this work, we build an end-to-end system that enables a commodity mobile manipulator (Stretch RE2) to pull open cabinets and drawers in diverse previously unseen real world environments.","We conduct 4 days of real world testing of this system spanning 31 different objects from across 13 different real world environments.","Our system achieves a success rate of 61% on opening novel cabinets and drawers in unseen environments zero-shot.","An analysis of the failure modes suggests that errors in perception are the most significant challenge for our system.","We will open source code and models for others to replicate and build upon our system."],"url":"http://arxiv.org/abs/2402.17767v1"}
{"created":"2024-02-27 18:57:12","title":"ShapeLLM: Universal 3D Object Understanding for Embodied Interaction","abstract":"This paper presents ShapeLLM, the first 3D Multimodal Large Language Model (LLM) designed for embodied interaction, exploring a universal 3D object understanding with 3D point clouds and languages. ShapeLLM is built upon an improved 3D encoder by extending ReCon to ReCon++ that benefits from multi-view image distillation for enhanced geometry understanding. By utilizing ReCon++ as the 3D point cloud input encoder for LLMs, ShapeLLM is trained on constructed instruction-following data and tested on our newly human-curated evaluation benchmark, 3D MM-Vet. ReCon++ and ShapeLLM achieve state-of-the-art performance in 3D geometry understanding and language-unified 3D interaction tasks, such as embodied visual grounding.","sentences":["This paper presents ShapeLLM, the first 3D Multimodal Large Language Model (LLM) designed for embodied interaction, exploring a universal 3D object understanding with 3D point clouds and languages.","ShapeLLM is built upon an improved 3D encoder by extending ReCon to ReCon++ that benefits from multi-view image distillation for enhanced geometry understanding.","By utilizing ReCon++ as the 3D point cloud input encoder for LLMs, ShapeLLM is trained on constructed instruction-following data and tested on our newly human-curated evaluation benchmark, 3D MM-Vet. ReCon++ and ShapeLLM achieve state-of-the-art performance in 3D geometry understanding and language-unified 3D interaction tasks, such as embodied visual grounding."],"url":"http://arxiv.org/abs/2402.17766v1"}
{"created":"2024-02-27 18:56:19","title":"The Era of 1-bit LLMs: All Large Language Models are in 1.58 Bits","abstract":"Recent research, such as BitNet, is paving the way for a new era of 1-bit Large Language Models (LLMs). In this work, we introduce a 1-bit LLM variant, namely BitNet b1.58, in which every single parameter (or weight) of the LLM is ternary {-1, 0, 1}. It matches the full-precision (i.e., FP16 or BF16) Transformer LLM with the same model size and training tokens in terms of both perplexity and end-task performance, while being significantly more cost-effective in terms of latency, memory, throughput, and energy consumption. More profoundly, the 1.58-bit LLM defines a new scaling law and recipe for training new generations of LLMs that are both high-performance and cost-effective. Furthermore, it enables a new computation paradigm and opens the door for designing specific hardware optimized for 1-bit LLMs.","sentences":["Recent research, such as BitNet, is paving the way for a new era of 1-bit Large Language Models (LLMs).","In this work, we introduce a 1-bit LLM variant, namely BitNet b1.58, in which every single parameter (or weight) of the LLM is ternary {-1, 0, 1}.","It matches the full-precision (i.e., FP16 or BF16) Transformer LLM with the same model size and training tokens in terms of both perplexity and end-task performance, while being significantly more cost-effective in terms of latency, memory, throughput, and energy consumption.","More profoundly, the 1.58-bit LLM defines a new scaling law and recipe for training new generations of LLMs that are both high-performance and cost-effective.","Furthermore, it enables a new computation paradigm and opens the door for designing specific hardware optimized for 1-bit LLMs."],"url":"http://arxiv.org/abs/2402.17764v1"}
{"created":"2024-02-27 18:55:50","title":"Reducing Unnecessary Alerts in Pedestrian Protection Systems Based on P2V Communications","abstract":"There are different proposals in the literature on how to protect pedestrians using warning systems to alert drivers of their presence. They can be based on onboard perception systems or wireless communications. The evaluation of these systems has been focused on testing their ability to detect pedestrians. A problem that has received much less attention is the possibility of generating too many alerts in the warning systems. In this paper, we propose and analyze four different algorithms to take the decision on generating alerts in a warning system that is based on direct wireless communications between vehicles and pedestrians. With the algorithms, we explore different strategies to reduce unnecessary alerts. The feasibility of the implementation of the algorithms was evaluated with a deployment using real equipment, and tests were carried out to verify their behavior in real scenarios. The ability of each algorithm to reduce unnecessary alerts was evaluated with realistic simulations in an urban scenario, using a traffic simulator with vehicular and pedestrian flows. The results show the importance of tackling the problem of driver overload in warning systems, and that it is not straightforward to predict the load of alerts generated by an algorithm in a large-scale deployment, in which there are multiple interactions between vehicles and pedestrians.","sentences":["There are different proposals in the literature on how to protect pedestrians using warning systems to alert drivers of their presence.","They can be based on onboard perception systems or wireless communications.","The evaluation of these systems has been focused on testing their ability to detect pedestrians.","A problem that has received much less attention is the possibility of generating too many alerts in the warning systems.","In this paper, we propose and analyze four different algorithms to take the decision on generating alerts in a warning system that is based on direct wireless communications between vehicles and pedestrians.","With the algorithms, we explore different strategies to reduce unnecessary alerts.","The feasibility of the implementation of the algorithms was evaluated with a deployment using real equipment, and tests were carried out to verify their behavior in real scenarios.","The ability of each algorithm to reduce unnecessary alerts was evaluated with realistic simulations in an urban scenario, using a traffic simulator with vehicular and pedestrian flows.","The results show the importance of tackling the problem of driver overload in warning systems, and that it is not straightforward to predict the load of alerts generated by an algorithm in a large-scale deployment, in which there are multiple interactions between vehicles and pedestrians."],"url":"http://arxiv.org/abs/2402.17763v1"}
{"created":"2024-02-27 18:55:17","title":"Massive Activations in Large Language Models","abstract":"We observe an empirical phenomenon in Large Language Models (LLMs) -- very few activations exhibit significantly larger values than others (e.g., 100,000 times larger). We call them massive activations. First, we demonstrate the widespread existence of massive activations across various LLMs and characterize their locations. Second, we find their values largely stay constant regardless of the input, and they function as indispensable bias terms in LLMs. Third, these massive activations lead to the concentration of attention probabilities to their corresponding tokens, and further, implicit bias terms in the self-attention output. Last, we also study massive activations in Vision Transformers.","sentences":["We observe an empirical phenomenon in Large Language Models (LLMs) -- very few activations exhibit significantly larger values than others (e.g., 100,000 times larger).","We call them massive activations.","First, we demonstrate the widespread existence of massive activations across various LLMs and characterize their locations.","Second, we find their values largely stay constant regardless of the input, and they function as indispensable bias terms in LLMs.","Third, these massive activations lead to the concentration of attention probabilities to their corresponding tokens, and further, implicit bias terms in the self-attention output.","Last, we also study massive activations in Vision Transformers."],"url":"http://arxiv.org/abs/2402.17762v1"}
{"created":"2024-02-27 18:52:19","title":"Towards Optimal Learning of Language Models","abstract":"This work studies the general principles of improving the learning of language models (LMs), which aims at reducing the necessary training steps for achieving superior performance. Specifically, we present a theory for the optimal learning of LMs. We first propose an objective that optimizes LM learning by maximizing the data compression ratio in an \"LM-training-as-lossless-compression\" view. Then, we derive a theorem, named Learning Law, to reveal the properties of the dynamics in the optimal learning process under our objective. The theorem is then validated by experiments on a linear classification and a real-world language modeling task. Finally, we empirically verify that the optimal learning of LMs essentially stems from the improvement of the coefficients in the scaling law of LMs, indicating great promise and significance for designing practical learning acceleration methods. Our code can be found at https://aka.ms/LearningLaw.","sentences":["This work studies the general principles of improving the learning of language models (LMs), which aims at reducing the necessary training steps for achieving superior performance.","Specifically, we present a theory for the optimal learning of LMs.","We first propose an objective that optimizes LM learning by maximizing the data compression ratio in an \"LM-training-as-lossless-compression\" view.","Then, we derive a theorem, named Learning Law, to reveal the properties of the dynamics in the optimal learning process under our objective.","The theorem is then validated by experiments on a linear classification and a real-world language modeling task.","Finally, we empirically verify that the optimal learning of LMs essentially stems from the improvement of the coefficients in the scaling law of LMs, indicating great promise and significance for designing practical learning acceleration methods.","Our code can be found at https://aka.ms/LearningLaw."],"url":"http://arxiv.org/abs/2402.17759v1"}
{"created":"2024-02-27 18:51:52","title":"ADL4D: Towards A Contextually Rich Dataset for 4D Activities of Daily Living","abstract":"Hand-Object Interactions (HOIs) are conditioned on spatial and temporal contexts like surrounding objects, pre- vious actions, and future intents (for example, grasping and handover actions vary greatly based on objects proximity and trajectory obstruction). However, existing datasets for 4D HOI (3D HOI over time) are limited to one subject inter- acting with one object only. This restricts the generalization of learning-based HOI methods trained on those datasets. We introduce ADL4D, a dataset of up to two subjects inter- acting with different sets of objects performing Activities of Daily Living (ADL) like breakfast or lunch preparation ac- tivities. The transition between multiple objects to complete a certain task over time introduces a unique context lacking in existing datasets. Our dataset consists of 75 sequences with a total of 1.1M RGB-D frames, hand and object poses, and per-hand fine-grained action annotations. We develop an automatic system for multi-view multi-hand 3D pose an- notation capable of tracking hand poses over time. We inte- grate and test it against publicly available datasets. Finally, we evaluate our dataset on the tasks of Hand Mesh Recov- ery (HMR) and Hand Action Segmentation (HAS).","sentences":["Hand-Object Interactions (HOIs) are conditioned on spatial and temporal contexts like surrounding objects, pre- vious actions, and future intents (for example, grasping and handover actions vary greatly based on objects proximity and trajectory obstruction).","However, existing datasets for 4D HOI (3D HOI over time) are limited to one subject inter- acting with one object only.","This restricts the generalization of learning-based HOI methods trained on those datasets.","We introduce ADL4D, a dataset of up to two subjects inter- acting with different sets of objects performing Activities of Daily Living (ADL) like breakfast or lunch preparation ac- tivities.","The transition between multiple objects to complete a certain task over time introduces a unique context lacking in existing datasets.","Our dataset consists of 75 sequences with a total of 1.1M RGB-D frames, hand and object poses, and per-hand fine-grained action annotations.","We develop an automatic system for multi-view multi-hand 3D pose an- notation capable of tracking hand poses over time.","We inte- grate and test it against publicly available datasets.","Finally, we evaluate our dataset on the tasks of Hand Mesh Recov- ery (HMR) and Hand Action Segmentation (HAS)."],"url":"http://arxiv.org/abs/2402.17758v1"}
{"created":"2024-02-27 18:48:07","title":"Robustly Learning Single-Index Models via Alignment Sharpness","abstract":"We study the problem of learning Single-Index Models under the $L_2^2$ loss in the agnostic model. We give an efficient learning algorithm, achieving a constant factor approximation to the optimal loss, that succeeds under a range of distributions (including log-concave distributions) and a broad class of monotone and Lipschitz link functions. This is the first efficient constant factor approximate agnostic learner, even for Gaussian data and for any nontrivial class of link functions. Prior work for the case of unknown link function either works in the realizable setting or does not attain constant factor approximation. The main technical ingredient enabling our algorithm and analysis is a novel notion of a local error bound in optimization that we term alignment sharpness and that may be of broader interest.","sentences":["We study the problem of learning Single-Index Models under the $L_2^2$ loss in the agnostic model.","We give an efficient learning algorithm, achieving a constant factor approximation to the optimal loss, that succeeds under a range of distributions (including log-concave distributions) and a broad class of monotone and Lipschitz link functions.","This is the first efficient constant factor approximate agnostic learner, even for Gaussian data and for any nontrivial class of link functions.","Prior work for the case of unknown link function either works in the realizable setting or does not attain constant factor approximation.","The main technical ingredient enabling our algorithm and analysis is a novel notion of a local error bound in optimization that we term alignment sharpness and that may be of broader interest."],"url":"http://arxiv.org/abs/2402.17756v1"}
{"created":"2024-02-27 18:45:31","title":"Increasing the Diversity of Investment Portfolio with Integration of Gamified Components in the FinTech Applications Lifecycle","abstract":"Gamification has the potential to make significant contributions to financial product delivery, Fintech services, and inclusive growth. The integration of gamification into FinTech applications has shown a positive correlation with the social impact theory. Utilizing gamification in a sustainable and effective manner can be crucial for long-term prospects in the FinTech industry. Therefore, it is essential to develop efficient and modern financial software that improves the customer experience. The current literature aims to contribute to this area by highlighting the relationship between interrelated theories and the key factors to consider when designing a gamified element. This study aims to explore the effects of gamification on altering user intention and its significant influence on customer value propositions.","sentences":["Gamification has the potential to make significant contributions to financial product delivery, Fintech services, and inclusive growth.","The integration of gamification into FinTech applications has shown a positive correlation with the social impact theory.","Utilizing gamification in a sustainable and effective manner can be crucial for long-term prospects in the FinTech industry.","Therefore, it is essential to develop efficient and modern financial software that improves the customer experience.","The current literature aims to contribute to this area by highlighting the relationship between interrelated theories and the key factors to consider when designing a gamified element.","This study aims to explore the effects of gamification on altering user intention and its significant influence on customer value propositions."],"url":"http://arxiv.org/abs/2402.17754v1"}
{"created":"2024-02-27 18:42:31","title":"Evaluating Very Long-Term Conversational Memory of LLM Agents","abstract":"Existing works on long-term open-domain dialogues focus on evaluating model responses within contexts spanning no more than five chat sessions. Despite advancements in long-context large language models (LLMs) and retrieval augmented generation (RAG) techniques, their efficacy in very long-term dialogues remains unexplored. To address this research gap, we introduce a machine-human pipeline to generate high-quality, very long-term dialogues by leveraging LLM-based agent architectures and grounding their dialogues on personas and temporal event graphs. Moreover, we equip each agent with the capability of sharing and reacting to images. The generated conversations are verified and edited by human annotators for long-range consistency and grounding to the event graphs. Using this pipeline, we collect LoCoMo, a dataset of very long-term conversations, each encompassing 300 turns and 9K tokens on avg., over up to 35 sessions. Based on LoCoMo, we present a comprehensive evaluation benchmark to measure long-term memory in models, encompassing question answering, event summarization, and multi-modal dialogue generation tasks. Our experimental results indicate that LLMs exhibit challenges in understanding lengthy conversations and comprehending long-range temporal and causal dynamics within dialogues. Employing strategies like long-context LLMs or RAG can offer improvements but these models still substantially lag behind human performance.","sentences":["Existing works on long-term open-domain dialogues focus on evaluating model responses within contexts spanning no more than five chat sessions.","Despite advancements in long-context large language models (LLMs) and retrieval augmented generation (RAG) techniques, their efficacy in very long-term dialogues remains unexplored.","To address this research gap, we introduce a machine-human pipeline to generate high-quality, very long-term dialogues by leveraging LLM-based agent architectures and grounding their dialogues on personas and temporal event graphs.","Moreover, we equip each agent with the capability of sharing and reacting to images.","The generated conversations are verified and edited by human annotators for long-range consistency and grounding to the event graphs.","Using this pipeline, we collect LoCoMo, a dataset of very long-term conversations, each encompassing 300 turns and 9K tokens on avg., over up to 35 sessions.","Based on LoCoMo, we present a comprehensive evaluation benchmark to measure long-term memory in models, encompassing question answering, event summarization, and multi-modal dialogue generation tasks.","Our experimental results indicate that LLMs exhibit challenges in understanding lengthy conversations and comprehending long-range temporal and causal dynamics within dialogues.","Employing strategies like long-context LLMs or RAG can offer improvements but these models still substantially lag behind human performance."],"url":"http://arxiv.org/abs/2402.17753v1"}
{"created":"2024-02-27 18:38:05","title":"An Eye Gaze Heatmap Analysis of Uncertainty Head-Up Display Designs for Conditional Automated Driving","abstract":"This paper reports results from a high-fidelity driving simulator study (N=215) about a head-up display (HUD) that conveys a conditional automated vehicle's dynamic \"uncertainty\" about the current situation while fallback drivers watch entertaining videos. We compared (between-group) three design interventions: display (a bar visualisation of uncertainty close to the video), interruption (interrupting the video during uncertain situations), and combination (a combination of both), against a baseline (video-only). We visualised eye-tracking data to conduct a heatmap analysis of the four groups' gaze behaviour over time. We found interruptions initiated a phase during which participants interleaved their attention between monitoring and entertainment. This improved monitoring behaviour was more pronounced in combination compared to interruption, suggesting pre-warning interruptions have positive effects. The same addition had negative effects without interruptions (comparing baseline & display). Intermittent interruptions may have safety benefits over placing additional peripheral displays without compromising usability.","sentences":["This paper reports results from a high-fidelity driving simulator study (N=215) about a head-up display (HUD) that conveys a conditional automated vehicle's dynamic \"uncertainty\" about the current situation while fallback drivers watch entertaining videos.","We compared (between-group) three design interventions: display (a bar visualisation of uncertainty close to the video), interruption (interrupting the video during uncertain situations), and combination (a combination of both), against a baseline (video-only).","We visualised eye-tracking data to conduct a heatmap analysis of the four groups' gaze behaviour over time.","We found interruptions initiated a phase during which participants interleaved their attention between monitoring and entertainment.","This improved monitoring behaviour was more pronounced in combination compared to interruption, suggesting pre-warning interruptions have positive effects.","The same addition had negative effects without interruptions (comparing baseline & display).","Intermittent interruptions may have safety benefits over placing additional peripheral displays without compromising usability."],"url":"http://arxiv.org/abs/2402.17751v1"}
{"created":"2024-02-27 18:34:15","title":"Exploring the Market Dynamics of Liquid Staking Derivatives (LSDs)","abstract":"Staking has emerged as a crucial concept following Ethereum's transition to Proof-of-Stake consensus. The introduction of Liquid Staking Derivatives (LSDs) has effectively addressed the illiquidity issue associated with solo staking, gaining significant market attention. This paper analyzes the LSD market dynamics from the perspectives of both liquidity takers (LTs) and liquidity providers (LPs). We first quantify the price discrepancy between the LSD primary and secondary markets. Then we investigate and empirically measure how LTs can leverage such discrepancy to exploit arbitrage opportunities, unveiling the potential barriers to LSD arbitrages. In addition, we evaluate the financial profit and losses experienced by LPs who supply LSDs for liquidity provision. Our findings reveal that 66% LSD liquidity provision positions yield an Annual Percentage Rate (APR) lower than simply holding the corresponding LSDs.","sentences":["Staking has emerged as a crucial concept following Ethereum's transition to Proof-of-Stake consensus.","The introduction of Liquid Staking Derivatives (LSDs) has effectively addressed the illiquidity issue associated with solo staking, gaining significant market attention.","This paper analyzes the LSD market dynamics from the perspectives of both liquidity takers (LTs) and liquidity providers (LPs).","We first quantify the price discrepancy between the LSD primary and secondary markets.","Then we investigate and empirically measure how LTs can leverage such discrepancy to exploit arbitrage opportunities, unveiling the potential barriers to LSD arbitrages.","In addition, we evaluate the financial profit and losses experienced by LPs who supply LSDs for liquidity provision.","Our findings reveal that 66% LSD liquidity provision positions yield an Annual Percentage Rate (APR) lower than simply holding the corresponding LSDs."],"url":"http://arxiv.org/abs/2402.17748v1"}
{"created":"2024-02-27 18:32:11","title":"When Your AI Deceives You: Challenges with Partial Observability of Human Evaluators in Reward Learning","abstract":"Past analyses of reinforcement learning from human feedback (RLHF) assume that the human fully observes the environment. What happens when human feedback is based only on partial observations? We formally define two failure cases: deception and overjustification. Modeling the human as Boltzmann-rational w.r.t. a belief over trajectories, we prove conditions under which RLHF is guaranteed to result in policies that deceptively inflate their performance, overjustify their behavior to make an impression, or both. To help address these issues, we mathematically characterize how partial observability of the environment translates into (lack of) ambiguity in the learned return function. In some cases, accounting for partial observability makes it theoretically possible to recover the return function and thus the optimal policy, while in other cases, there is irreducible ambiguity. We caution against blindly applying RLHF in partially observable settings and propose research directions to help tackle these challenges.","sentences":["Past analyses of reinforcement learning from human feedback (RLHF) assume that the human fully observes the environment.","What happens when human feedback is based only on partial observations?","We formally define two failure cases: deception and overjustification.","Modeling the human as Boltzmann-rational w.r.t.","a belief over trajectories, we prove conditions under which RLHF is guaranteed to result in policies that deceptively inflate their performance, overjustify their behavior to make an impression, or both.","To help address these issues, we mathematically characterize how partial observability of the environment translates into (lack of) ambiguity in the learned return function.","In some cases, accounting for partial observability makes it theoretically possible to recover the return function and thus the optimal policy, while in other cases, there is irreducible ambiguity.","We caution against blindly applying RLHF in partially observable settings and propose research directions to help tackle these challenges."],"url":"http://arxiv.org/abs/2402.17747v1"}
{"created":"2024-02-27 18:25:16","title":"Analyzing Regional Organization of the Human Hippocampus in 3D-PLI Using Contrastive Learning and Geometric Unfolding","abstract":"Understanding the cortical organization of the human brain requires interpretable descriptors for distinct structural and functional imaging data. 3D polarized light imaging (3D-PLI) is an imaging modality for visualizing fiber architecture in postmortem brains with high resolution that also captures the presence of cell bodies, for example, to identify hippocampal subfields. The rich texture in 3D-PLI images, however, makes this modality particularly difficult to analyze and best practices for characterizing architectonic patterns still need to be established. In this work, we demonstrate a novel method to analyze the regional organization of the human hippocampus in 3D-PLI by combining recent advances in unfolding methods with deep texture features obtained using a self-supervised contrastive learning approach. We identify clusters in the representations that correspond well with classical descriptions of hippocampal subfields, lending validity to the developed methodology.","sentences":["Understanding the cortical organization of the human brain requires interpretable descriptors for distinct structural and functional imaging data.","3D polarized light imaging (3D-PLI) is an imaging modality for visualizing fiber architecture in postmortem brains with high resolution that also captures the presence of cell bodies, for example, to identify hippocampal subfields.","The rich texture in 3D-PLI images, however, makes this modality particularly difficult to analyze and best practices for characterizing architectonic patterns still need to be established.","In this work, we demonstrate a novel method to analyze the regional organization of the human hippocampus in 3D-PLI by combining recent advances in unfolding methods with deep texture features obtained using a self-supervised contrastive learning approach.","We identify clusters in the representations that correspond well with classical descriptions of hippocampal subfields, lending validity to the developed methodology."],"url":"http://arxiv.org/abs/2402.17744v1"}
{"created":"2024-02-27 18:22:48","title":"Rose: Efficient and Extensible Autodiff on the Web","abstract":"Automatic differentiation (AD) has become the backbone for a new wave of optimization-driven domains such as computer graphics and machine learning over the past decade. However, existing AD systems face limitations, either lacking support for in-browser development or failing to harness more recent, compilerbased approaches to achieve both expressiveness and size-preserving differentiation. This work introduces Rose, a portable, extensible AD library that runs on the web. Rose allows users to write opaque functions with custom derivatives and supports dynamic construction of AD functions. We integrated Rose into two differentiable simulations and a diagram authoring tool to demonstrate the utility of Rose's design. Finally, we show that Rose is 173x as fast as TensorFlow.js in compiling and running a benchmark suite of optimized diagrams.","sentences":["Automatic differentiation (AD) has become the backbone for a new wave of optimization-driven domains such as computer graphics and machine learning over the past decade.","However, existing AD systems face limitations, either lacking support for in-browser development or failing to harness more recent, compilerbased approaches to achieve both expressiveness and size-preserving differentiation.","This work introduces Rose, a portable, extensible AD library that runs on the web.","Rose allows users to write opaque functions with custom derivatives and supports dynamic construction of AD functions.","We integrated Rose into two differentiable simulations and a diagram authoring tool to demonstrate the utility of Rose's design.","Finally, we show that Rose is 173x as fast as TensorFlow.js in compiling and running a benchmark suite of optimized diagrams."],"url":"http://arxiv.org/abs/2402.17743v1"}
{"created":"2024-02-27 18:18:23","title":"reBandit: Random Effects based Online RL algorithm for Reducing Cannabis Use","abstract":"The escalating prevalence of cannabis use, and associated cannabis-use disorder (CUD), poses a significant public health challenge globally. With a notably wide treatment gap, especially among emerging adults (EAs; ages 18-25), addressing cannabis use and CUD remains a pivotal objective within the 2030 United Nations Agenda for Sustainable Development Goals (SDG). In this work, we develop an online reinforcement learning (RL) algorithm called reBandit which will be utilized in a mobile health study to deliver personalized mobile health interventions aimed at reducing cannabis use among EAs. reBandit utilizes random effects and informative Bayesian priors to learn quickly and efficiently in noisy mobile health environments. Moreover, reBandit employs Empirical Bayes and optimization techniques to autonomously update its hyper-parameters online. To evaluate the performance of our algorithm, we construct a simulation testbed using data from a prior study, and compare against commonly used algorithms in mobile health studies. We show that reBandit performs equally well or better than all the baseline algorithms, and the performance gap widens as population heterogeneity increases in the simulation environment, proving its adeptness to adapt to diverse population of study participants.","sentences":["The escalating prevalence of cannabis use, and associated cannabis-use disorder (CUD), poses a significant public health challenge globally.","With a notably wide treatment gap, especially among emerging adults (EAs; ages 18-25), addressing cannabis use and CUD remains a pivotal objective within the 2030 United Nations Agenda for Sustainable Development Goals (SDG).","In this work, we develop an online reinforcement learning (RL) algorithm called reBandit which will be utilized in a mobile health study to deliver personalized mobile health interventions aimed at reducing cannabis use among EAs.","reBandit utilizes random effects and informative Bayesian priors to learn quickly and efficiently in noisy mobile health environments.","Moreover, reBandit employs Empirical Bayes and optimization techniques to autonomously update its hyper-parameters online.","To evaluate the performance of our algorithm, we construct a simulation testbed using data from a prior study, and compare against commonly used algorithms in mobile health studies.","We show that reBandit performs equally well or better than all the baseline algorithms, and the performance gap widens as population heterogeneity increases in the simulation environment, proving its adeptness to adapt to diverse population of study participants."],"url":"http://arxiv.org/abs/2402.17739v1"}
{"created":"2024-02-27 18:12:58","title":"Learning-Based Algorithms for Graph Searching Problems","abstract":"We consider the problem of graph searching with prediction recently introduced by Banerjee et al. (2022). In this problem, an agent, starting at some vertex $r$ has to traverse a (potentially unknown) graph $G$ to find a hidden goal node $g$ while minimizing the total distance travelled. We study a setting in which at any node $v$, the agent receives a noisy estimate of the distance from $v$ to $g$. We design algorithms for this search task on unknown graphs. We establish the first formal guarantees on unknown weighted graphs and provide lower bounds showing that the algorithms we propose have optimal or nearly-optimal dependence on the prediction error. Further, we perform numerical experiments demonstrating that in addition to being robust to adversarial error, our algorithms perform well in typical instances in which the error is stochastic. Finally, we provide alternative simpler performance bounds on the algorithms of Banerjee et al. (2022) for the case of searching on a known graph, and establish new lower bounds for this setting.","sentences":["We consider the problem of graph searching with prediction recently introduced by Banerjee et al. (2022).","In this problem, an agent, starting at some vertex $r$ has to traverse a (potentially unknown) graph $G$ to find a hidden goal node $g$ while minimizing the total distance travelled.","We study a setting in which at any node $v$, the agent receives a noisy estimate of the distance from $v$ to $g$. We design algorithms for this search task on unknown graphs.","We establish the first formal guarantees on unknown weighted graphs and provide lower bounds showing that the algorithms we propose have optimal or nearly-optimal dependence on the prediction error.","Further, we perform numerical experiments demonstrating that in addition to being robust to adversarial error, our algorithms perform well in typical instances in which the error is stochastic.","Finally, we provide alternative simpler performance bounds on the algorithms of Banerjee et al. (2022) for the case of searching on a known graph, and establish new lower bounds for this setting."],"url":"http://arxiv.org/abs/2402.17736v1"}
{"created":"2024-02-27 18:09:36","title":"Tower: An Open Multilingual Large Language Model for Translation-Related Tasks","abstract":"While general-purpose large language models (LLMs) demonstrate proficiency on multiple tasks within the domain of translation, approaches based on open LLMs are competitive only when specializing on a single task. In this paper, we propose a recipe for tailoring LLMs to multiple tasks present in translation workflows. We perform continued pretraining on a multilingual mixture of monolingual and parallel data, creating TowerBase, followed by finetuning on instructions relevant for translation processes, creating TowerInstruct. Our final model surpasses open alternatives on several tasks relevant to translation workflows and is competitive with general-purpose closed LLMs. To facilitate future research, we release the Tower models, our specialization dataset, an evaluation framework for LLMs focusing on the translation ecosystem, and a collection of model generations, including ours, on our benchmark.","sentences":["While general-purpose large language models (LLMs) demonstrate proficiency on multiple tasks within the domain of translation, approaches based on open LLMs are competitive only when specializing on a single task.","In this paper, we propose a recipe for tailoring LLMs to multiple tasks present in translation workflows.","We perform continued pretraining on a multilingual mixture of monolingual and parallel data, creating TowerBase, followed by finetuning on instructions relevant for translation processes, creating TowerInstruct.","Our final model surpasses open alternatives on several tasks relevant to translation workflows and is competitive with general-purpose closed LLMs.","To facilitate future research, we release the Tower models, our specialization dataset, an evaluation framework for LLMs focusing on the translation ecosystem, and a collection of model generations, including ours, on our benchmark."],"url":"http://arxiv.org/abs/2402.17733v1"}
{"created":"2024-02-27 18:04:59","title":"Markovletics: Methods and A Novel Application for Learning Continuous-Time Markov Chain Mixtures","abstract":"Sequential data naturally arises from user engagement on digital platforms like social media, music streaming services, and web navigation, encapsulating evolving user preferences and behaviors through continuous information streams. A notable unresolved query in stochastic processes is learning mixtures of continuous-time Markov chains (CTMCs). While there is progress in learning mixtures of discrete-time Markov chains with recovery guarantees [GKV16,ST23,KTT2023], the continuous scenario uncovers unique unexplored challenges. The intrigue in CTMC mixtures stems from their potential to model intricate continuous-time stochastic processes prevalent in various fields including social media, finance, and biology.   In this study, we introduce a novel framework for exploring CTMCs, emphasizing the influence of observed trails' length and mixture parameters on problem regimes, which demands specific algorithms. Through thorough experimentation, we examine the impact of discretizing continuous-time trails on the learnability of the continuous-time mixture, given that these processes are often observed via discrete, resource-demanding observations. Our comparative analysis with leading methods explores sample complexity and the trade-off between the number of trails and their lengths, offering crucial insights for method selection in different problem instances. We apply our algorithms on an extensive collection of Lastfm's user-generated trails spanning three years, demonstrating the capability of our algorithms to differentiate diverse user preferences. We pioneer the use of CTMC mixtures on a basketball passing dataset to unveil intricate offensive tactics of NBA teams. This underscores the pragmatic utility and versatility of our proposed framework. All results presented in this study are replicable, and we provide the implementations to facilitate reproducibility.","sentences":["Sequential data naturally arises from user engagement on digital platforms like social media, music streaming services, and web navigation, encapsulating evolving user preferences and behaviors through continuous information streams.","A notable unresolved query in stochastic processes is learning mixtures of continuous-time Markov chains (CTMCs).","While there is progress in learning mixtures of discrete-time Markov chains with recovery guarantees [GKV16,ST23,KTT2023], the continuous scenario uncovers unique unexplored challenges.","The intrigue in CTMC mixtures stems from their potential to model intricate continuous-time stochastic processes prevalent in various fields including social media, finance, and biology.   ","In this study, we introduce a novel framework for exploring CTMCs, emphasizing the influence of observed trails' length and mixture parameters on problem regimes, which demands specific algorithms.","Through thorough experimentation, we examine the impact of discretizing continuous-time trails on the learnability of the continuous-time mixture, given that these processes are often observed via discrete, resource-demanding observations.","Our comparative analysis with leading methods explores sample complexity and the trade-off between the number of trails and their lengths, offering crucial insights for method selection in different problem instances.","We apply our algorithms on an extensive collection of Lastfm's user-generated trails spanning three years, demonstrating the capability of our algorithms to differentiate diverse user preferences.","We pioneer the use of CTMC mixtures on a basketball passing dataset to unveil intricate offensive tactics of NBA teams.","This underscores the pragmatic utility and versatility of our proposed framework.","All results presented in this study are replicable, and we provide the implementations to facilitate reproducibility."],"url":"http://arxiv.org/abs/2402.17730v1"}
{"created":"2024-02-27 18:01:59","title":"Towards Fairness-Aware Adversarial Learning","abstract":"Although adversarial training (AT) has proven effective in enhancing the model's robustness, the recently revealed issue of fairness in robustness has not been well addressed, i.e. the robust accuracy varies significantly among different categories. In this paper, instead of uniformly evaluating the model's average class performance, we delve into the issue of robust fairness, by considering the worst-case distribution across various classes. We propose a novel learning paradigm, named Fairness-Aware Adversarial Learning (FAAL). As a generalization of conventional AT, we re-define the problem of adversarial training as a min-max-max framework, to ensure both robustness and fairness of the trained model. Specifically, by taking advantage of distributional robust optimization, our method aims to find the worst distribution among different categories, and the solution is guaranteed to obtain the upper bound performance with high probability. In particular, FAAL can fine-tune an unfair robust model to be fair within only two epochs, without compromising the overall clean and robust accuracies. Extensive experiments on various image datasets validate the superior performance and efficiency of the proposed FAAL compared to other state-of-the-art methods.","sentences":["Although adversarial training (AT) has proven effective in enhancing the model's robustness, the recently revealed issue of fairness in robustness has not been well addressed, i.e. the robust accuracy varies significantly among different categories.","In this paper, instead of uniformly evaluating the model's average class performance, we delve into the issue of robust fairness, by considering the worst-case distribution across various classes.","We propose a novel learning paradigm, named Fairness-Aware Adversarial Learning (FAAL).","As a generalization of conventional AT, we re-define the problem of adversarial training as a min-max-max framework, to ensure both robustness and fairness of the trained model.","Specifically, by taking advantage of distributional robust optimization, our method aims to find the worst distribution among different categories, and the solution is guaranteed to obtain the upper bound performance with high probability.","In particular, FAAL can fine-tune an unfair robust model to be fair within only two epochs, without compromising the overall clean and robust accuracies.","Extensive experiments on various image datasets validate the superior performance and efficiency of the proposed FAAL compared to other state-of-the-art methods."],"url":"http://arxiv.org/abs/2402.17729v1"}
{"created":"2024-02-27 17:58:09","title":"VRP-SAM: SAM with Visual Reference Prompt","abstract":"In this paper, we propose a novel Visual Reference Prompt (VRP) encoder that empowers the Segment Anything Model (SAM) to utilize annotated reference images as prompts for segmentation, creating the VRP-SAM model. In essence, VRP-SAM can utilize annotated reference images to comprehend specific objects and perform segmentation of specific objects in target image. It is note that the VRP encoder can support a variety of annotation formats for reference images, including \\textbf{point}, \\textbf{box}, \\textbf{scribble}, and \\textbf{mask}. VRP-SAM achieves a breakthrough within the SAM framework by extending its versatility and applicability while preserving SAM's inherent strengths, thus enhancing user-friendliness. To enhance the generalization ability of VRP-SAM, the VRP encoder adopts a meta-learning strategy. To validate the effectiveness of VRP-SAM, we conducted extensive empirical studies on the Pascal and COCO datasets. Remarkably, VRP-SAM achieved state-of-the-art performance in visual reference segmentation with minimal learnable parameters. Furthermore, VRP-SAM demonstrates strong generalization capabilities, allowing it to perform segmentation of unseen objects and enabling cross-domain segmentation.","sentences":["In this paper, we propose a novel Visual Reference Prompt (VRP) encoder that empowers the Segment Anything Model (SAM) to utilize annotated reference images as prompts for segmentation, creating the VRP-SAM model.","In essence, VRP-SAM can utilize annotated reference images to comprehend specific objects and perform segmentation of specific objects in target image.","It is note that the VRP encoder can support a variety of annotation formats for reference images, including \\textbf{point}, \\textbf{box}, \\textbf{scribble}, and \\textbf{mask}.","VRP-SAM achieves a breakthrough within the SAM framework by extending its versatility and applicability while preserving SAM's inherent strengths, thus enhancing user-friendliness.","To enhance the generalization ability of VRP-SAM, the VRP encoder adopts a meta-learning strategy.","To validate the effectiveness of VRP-SAM, we conducted extensive empirical studies on the Pascal and COCO datasets.","Remarkably, VRP-SAM achieved state-of-the-art performance in visual reference segmentation with minimal learnable parameters.","Furthermore, VRP-SAM demonstrates strong generalization capabilities, allowing it to perform segmentation of unseen objects and enabling cross-domain segmentation."],"url":"http://arxiv.org/abs/2402.17726v1"}
{"created":"2024-02-27 17:57:04","title":"Seeing and Hearing: Open-domain Visual-Audio Generation with Diffusion Latent Aligners","abstract":"Video and audio content creation serves as the core technique for the movie industry and professional users. Recently, existing diffusion-based methods tackle video and audio generation separately, which hinders the technique transfer from academia to industry. In this work, we aim at filling the gap, with a carefully designed optimization-based framework for cross-visual-audio and joint-visual-audio generation. We observe the powerful generation ability of off-the-shelf video or audio generation models. Thus, instead of training the giant models from scratch, we propose to bridge the existing strong models with a shared latent representation space. Specifically, we propose a multimodality latent aligner with the pre-trained ImageBind model. Our latent aligner shares a similar core as the classifier guidance that guides the diffusion denoising process during inference time. Through carefully designed optimization strategy and loss functions, we show the superior performance of our method on joint video-audio generation, visual-steered audio generation, and audio-steered visual generation tasks. The project website can be found at https://yzxing87.github.io/Seeing-and-Hearing/","sentences":["Video and audio content creation serves as the core technique for the movie industry and professional users.","Recently, existing diffusion-based methods tackle video and audio generation separately, which hinders the technique transfer from academia to industry.","In this work, we aim at filling the gap, with a carefully designed optimization-based framework for cross-visual-audio and joint-visual-audio generation.","We observe the powerful generation ability of off-the-shelf video or audio generation models.","Thus, instead of training the giant models from scratch, we propose to bridge the existing strong models with a shared latent representation space.","Specifically, we propose a multimodality latent aligner with the pre-trained ImageBind model.","Our latent aligner shares a similar core as the classifier guidance that guides the diffusion denoising process during inference time.","Through carefully designed optimization strategy and loss functions, we show the superior performance of our method on joint video-audio generation, visual-steered audio generation, and audio-steered visual generation tasks.","The project website can be found at https://yzxing87.github.io/Seeing-and-Hearing/"],"url":"http://arxiv.org/abs/2402.17723v1"}
{"created":"2024-02-27 17:56:10","title":"Content-Centric Prototyping of Generative AI Applications: Emerging Approaches and Challenges in Collaborative Software Teams","abstract":"Generative AI models are increasingly powering software applications, offering the capability to produce expressive content across varied contexts. However, unlike previous iterations of human-AI design, the emerging design process for generative capabilities primarily hinges on prompt engineering strategies. Given this fundamental shift in approach, our work aims to understand how collaborative software teams set up and apply design guidelines and values, iteratively prototype prompts, and evaluate prompts to achieve desired outcomes. We conducted design studies with 39 industry professionals, including designers, software engineers, and product managers. Our findings reveal a content-centric prototyping approach in which teams begin with the content they want to generate, then identify specific attributes, constraints, and values, and explore methods to give users the ability to influence and interact with those attributes. Based on associated challenges, such as the lack of model interpretability and overfitting the design to examples, we outline considerations for generative AI prototyping.","sentences":["Generative AI models are increasingly powering software applications, offering the capability to produce expressive content across varied contexts.","However, unlike previous iterations of human-AI design, the emerging design process for generative capabilities primarily hinges on prompt engineering strategies.","Given this fundamental shift in approach, our work aims to understand how collaborative software teams set up and apply design guidelines and values, iteratively prototype prompts, and evaluate prompts to achieve desired outcomes.","We conducted design studies with 39 industry professionals, including designers, software engineers, and product managers.","Our findings reveal a content-centric prototyping approach in which teams begin with the content they want to generate, then identify specific attributes, constraints, and values, and explore methods to give users the ability to influence and interact with those attributes.","Based on associated challenges, such as the lack of model interpretability and overfitting the design to examples, we outline considerations for generative AI prototyping."],"url":"http://arxiv.org/abs/2402.17721v1"}
{"created":"2024-02-27 17:55:33","title":"The SMART approach to instance-optimal online learning","abstract":"We devise an online learning algorithm -- titled Switching via Monotone Adapted Regret Traces (SMART) -- that adapts to the data and achieves regret that is instance optimal, i.e., simultaneously competitive on every input sequence compared to the performance of the follow-the-leader (FTL) policy and the worst case guarantee of any other input policy. We show that the regret of the SMART policy on any input sequence is within a multiplicative factor $e/(e-1) \\approx 1.58$ of the smaller of: 1) the regret obtained by FTL on the sequence, and 2) the upper bound on regret guaranteed by the given worst-case policy. This implies a strictly stronger guarantee than typical `best-of-both-worlds' bounds as the guarantee holds for every input sequence regardless of how it is generated. SMART is simple to implement as it begins by playing FTL and switches at most once during the time horizon to the worst-case algorithm. Our approach and results follow from an operational reduction of instance optimal online learning to competitive analysis for the ski-rental problem. We complement our competitive ratio upper bounds with a fundamental lower bound showing that over all input sequences, no algorithm can get better than a $1.43$-fraction of the minimum regret achieved by FTL and the minimax-optimal policy. We also present a modification of SMART that combines FTL with a ``small-loss\" algorithm to achieve instance optimality between the regret of FTL and the small loss regret bound.","sentences":["We devise an online learning algorithm -- titled Switching via Monotone Adapted Regret Traces (SMART) -- that adapts to the data and achieves regret that is instance optimal, i.e., simultaneously competitive on every input sequence compared to the performance of the follow-the-leader (FTL) policy and the worst case guarantee of any other input policy.","We show that the regret of the SMART policy on any input sequence is within a multiplicative factor $e/(e-1) \\approx 1.58$ of the smaller of: 1) the regret obtained by FTL on the sequence, and 2) the upper bound on regret guaranteed by the given worst-case policy.","This implies a strictly stronger guarantee than typical `best-of-both-worlds' bounds as the guarantee holds for every input sequence regardless of how it is generated.","SMART is simple to implement as it begins by playing FTL and switches at most once during the time horizon to the worst-case algorithm.","Our approach and results follow from an operational reduction of instance optimal online learning to competitive analysis for the ski-rental problem.","We complement our competitive ratio upper bounds with a fundamental lower bound showing that over all input sequences, no algorithm can get better than a $1.43$-fraction of the minimum regret achieved by FTL and the minimax-optimal policy.","We also present a modification of SMART that combines FTL with a ``small-loss\" algorithm to achieve instance optimality between the regret of FTL and the small loss regret bound."],"url":"http://arxiv.org/abs/2402.17720v1"}
{"created":"2024-02-27 17:53:13","title":"Towards a Digital Twin Framework in Additive Manufacturing: Machine Learning and Bayesian Optimization for Time Series Process Optimization","abstract":"Laser-directed-energy deposition (DED) offers advantages in additive manufacturing (AM) for creating intricate geometries and material grading. Yet, challenges like material inconsistency and part variability remain, mainly due to its layer-wise fabrication. A key issue is heat accumulation during DED, which affects the material microstructure and properties. While closed-loop control methods for heat management are common in DED research, few integrate real-time monitoring, physics-based modeling, and control in a unified framework. Our work presents a digital twin (DT) framework for real-time predictive control of DED process parameters to meet specific design objectives. We develop a surrogate model using Long Short-Term Memory (LSTM)-based machine learning with Bayesian Inference to predict temperatures in DED parts. This model predicts future temperature states in real time. We also introduce Bayesian Optimization (BO) for Time Series Process Optimization (BOTSPO), based on traditional BO but featuring a unique time series process profile generator with reduced dimensions. BOTSPO dynamically optimizes processes, identifying optimal laser power profiles to attain desired mechanical properties. The established process trajectory guides online optimizations, aiming to enhance performance. This paper outlines the digital twin framework's components, promoting its integration into a comprehensive system for AM.","sentences":["Laser-directed-energy deposition (DED) offers advantages in additive manufacturing (AM) for creating intricate geometries and material grading.","Yet, challenges like material inconsistency and part variability remain, mainly due to its layer-wise fabrication.","A key issue is heat accumulation during DED, which affects the material microstructure and properties.","While closed-loop control methods for heat management are common in DED research, few integrate real-time monitoring, physics-based modeling, and control in a unified framework.","Our work presents a digital twin (DT) framework for real-time predictive control of DED process parameters to meet specific design objectives.","We develop a surrogate model using Long Short-Term Memory (LSTM)-based machine learning with Bayesian Inference to predict temperatures in DED parts.","This model predicts future temperature states in real time.","We also introduce Bayesian Optimization (BO) for Time Series Process Optimization (BOTSPO), based on traditional BO but featuring a unique time series process profile generator with reduced dimensions.","BOTSPO dynamically optimizes processes, identifying optimal laser power profiles to attain desired mechanical properties.","The established process trajectory guides online optimizations, aiming to enhance performance.","This paper outlines the digital twin framework's components, promoting its integration into a comprehensive system for AM."],"url":"http://arxiv.org/abs/2402.17718v1"}
{"created":"2024-02-27 17:52:33","title":"AmbigNLG: Addressing Task Ambiguity in Instruction for NLG","abstract":"In this study, we introduce AmbigNLG, a new task designed to tackle the challenge of task ambiguity in instructions for Natural Language Generation (NLG) tasks. Despite the impressive capabilities of Large Language Models (LLMs) in understanding and executing a wide range of tasks through natural language interaction, their performance is significantly hindered by the ambiguity present in real-world instructions. To address this, AmbigNLG seeks to identify and mitigate such ambiguities, aiming to refine instructions to match user expectations better. We introduce a dataset, AmbigSNI-NLG, consisting of 2,500 instances, and develop an ambiguity taxonomy for categorizing and annotating instruction ambiguities. Our approach demonstrates substantial improvements in text generation quality, highlighting the critical role of clear and specific instructions in enhancing LLM performance in NLG tasks.","sentences":["In this study, we introduce AmbigNLG, a new task designed to tackle the challenge of task ambiguity in instructions for Natural Language Generation (NLG) tasks.","Despite the impressive capabilities of Large Language Models (LLMs) in understanding and executing a wide range of tasks through natural language interaction, their performance is significantly hindered by the ambiguity present in real-world instructions.","To address this, AmbigNLG seeks to identify and mitigate such ambiguities, aiming to refine instructions to match user expectations better.","We introduce a dataset, AmbigSNI-NLG, consisting of 2,500 instances, and develop an ambiguity taxonomy for categorizing and annotating instruction ambiguities.","Our approach demonstrates substantial improvements in text generation quality, highlighting the critical role of clear and specific instructions in enhancing LLM performance in NLG tasks."],"url":"http://arxiv.org/abs/2402.17717v1"}
{"created":"2024-02-27 17:49:52","title":"On Central Primitives for Quantum Cryptography with Classical Communication","abstract":"Recent work has introduced the \"Quantum-Computation Classical-Communication\" (QCCC) (Chung et. al.) setting for cryptography. There has been some evidence that One Way Puzzles (OWPuzz) are the natural central cryptographic primitive for this setting (Khurana and Tomer). For a primitive to be considered central it should have several characteristics. It should be well behaved (which for this paper we will think of as having amplification, combiners, and universal constructions); it should be implied by a wide variety of other primitives; and it should be equivalent to some class of useful primitives. We present combiners, correctness and security amplification, and a universal construction for OWPuzz. Our proof of security amplification uses a new and cleaner version construction of EFI from OWPuzz (in comparison to the result of Khurana and Tomer) that generalizes to weak OWPuzz and is the most technically involved section of the paper. It was previously known that OWPuzz are implied by other primitives of interest including commitments, symmetric key encryption, one way state generators (OWSG), and therefore pseudorandom states (PRS). However we are able to rule out OWPuzz's equivalence to many of these primitives by showing a black box separation between general OWPuzz and a restricted class of OWPuzz (those with efficient verification, which we call EV-OWPuzz). We then show that EV-OWPuzz are also implied by most of these primitives, which separates them from OWPuzz as well. This separation also separates extending PRS from highly compressing PRS answering an open question of Ananth et. al.","sentences":["Recent work has introduced the \"Quantum-Computation Classical-Communication\" (QCCC) (Chung et.","al.)","setting for cryptography.","There has been some evidence that One Way Puzzles (OWPuzz) are the natural central cryptographic primitive for this setting (Khurana and Tomer).","For a primitive to be considered central it should have several characteristics.","It should be well behaved (which for this paper we will think of as having amplification, combiners, and universal constructions); it should be implied by a wide variety of other primitives; and it should be equivalent to some class of useful primitives.","We present combiners, correctness and security amplification, and a universal construction for OWPuzz.","Our proof of security amplification uses a new and cleaner version construction of EFI from OWPuzz (in comparison to the result of Khurana and Tomer) that generalizes to weak OWPuzz and is the most technically involved section of the paper.","It was previously known that OWPuzz are implied by other primitives of interest including commitments, symmetric key encryption, one way state generators (OWSG), and therefore pseudorandom states (PRS).","However we are able to rule out OWPuzz's equivalence to many of these primitives by showing a black box separation between general OWPuzz and a restricted class of OWPuzz (those with efficient verification, which we call EV-OWPuzz).","We then show that EV-OWPuzz are also implied by most of these primitives, which separates them from OWPuzz as well.","This separation also separates extending PRS from highly compressing PRS answering an open question of Ananth et.","al."],"url":"http://arxiv.org/abs/2402.17715v1"}
{"created":"2024-02-27 17:43:51","title":"Understanding Neural Network Binarization with Forward and Backward Proximal Quantizers","abstract":"In neural network binarization, BinaryConnect (BC) and its variants are considered the standard. These methods apply the sign function in their forward pass and their respective gradients are backpropagated to update the weights. However, the derivative of the sign function is zero whenever defined, which consequently freezes training. Therefore, implementations of BC (e.g., BNN) usually replace the derivative of sign in the backward computation with identity or other approximate gradient alternatives. Although such practice works well empirically, it is largely a heuristic or ''training trick.'' We aim at shedding some light on these training tricks from the optimization perspective. Building from existing theory on ProxConnect (PC, a generalization of BC), we (1) equip PC with different forward-backward quantizers and obtain ProxConnect++ (PC++) that includes existing binarization techniques as special cases; (2) derive a principled way to synthesize forward-backward quantizers with automatic theoretical guarantees; (3) illustrate our theory by proposing an enhanced binarization algorithm BNN++; (4) conduct image classification experiments on CNNs and vision transformers, and empirically verify that BNN++ generally achieves competitive results on binarizing these models.","sentences":["In neural network binarization, BinaryConnect (BC) and its variants are considered the standard.","These methods apply the sign function in their forward pass and their respective gradients are backpropagated to update the weights.","However, the derivative of the sign function is zero whenever defined, which consequently freezes training.","Therefore, implementations of BC (e.g., BNN) usually replace the derivative of sign in the backward computation with identity or other approximate gradient alternatives.","Although such practice works well empirically, it is largely a heuristic or ''training trick.''","We aim at shedding some light on these training tricks from the optimization perspective.","Building from existing theory on ProxConnect (PC, a generalization of BC), we (1) equip PC with different forward-backward quantizers and obtain ProxConnect++ (PC++) that includes existing binarization techniques as special cases; (2) derive a principled way to synthesize forward-backward quantizers with automatic theoretical guarantees; (3) illustrate our theory by proposing an enhanced binarization algorithm BNN++; (4) conduct image classification experiments on CNNs and vision transformers, and empirically verify that BNN++ generally achieves competitive results on binarizing these models."],"url":"http://arxiv.org/abs/2402.17710v1"}
{"created":"2024-02-27 17:41:58","title":"Case-Based or Rule-Based: How Do Transformers Do the Math?","abstract":"Despite the impressive performance in a variety of complex tasks, modern large language models (LLMs) still have trouble dealing with some math problems that are simple and intuitive for humans, such as addition. While we can easily learn basic rules of addition and apply them to new problems of any length, LLMs struggle to do the same. Instead, they may rely on similar \"cases\" seen in the training corpus for help. We define these two different reasoning mechanisms as \"rule-based reasoning\" and \"case-based reasoning\". Since rule-based reasoning is essential for acquiring the systematic generalization ability, we aim to explore exactly whether transformers use rule-based or case-based reasoning for math problems. Through carefully designed intervention experiments on five math tasks, we confirm that transformers are performing case-based reasoning, no matter whether scratchpad is used, which aligns with the previous observations that transformers use subgraph matching/shortcut learning to reason. To mitigate such problems, we propose a Rule-Following Fine-Tuning (RFFT) technique to teach transformers to perform rule-based reasoning. Specifically, we provide explicit rules in the input and then instruct transformers to recite and follow the rules step by step. Through RFFT, we successfully enable LLMs fine-tuned on 1-5 digit addition to generalize to up to 12-digit addition with over 95% accuracy, which is over 40% higher than scratchpad. The significant improvement demonstrates that teaching LLMs to explicitly use rules helps them learn rule-based reasoning and generalize better in length.","sentences":["Despite the impressive performance in a variety of complex tasks, modern large language models (LLMs) still have trouble dealing with some math problems that are simple and intuitive for humans, such as addition.","While we can easily learn basic rules of addition and apply them to new problems of any length, LLMs struggle to do the same.","Instead, they may rely on similar \"cases\" seen in the training corpus for help.","We define these two different reasoning mechanisms as \"rule-based reasoning\" and \"case-based reasoning\".","Since rule-based reasoning is essential for acquiring the systematic generalization ability, we aim to explore exactly whether transformers use rule-based or case-based reasoning for math problems.","Through carefully designed intervention experiments on five math tasks, we confirm that transformers are performing case-based reasoning, no matter whether scratchpad is used, which aligns with the previous observations that transformers use subgraph matching/shortcut learning to reason.","To mitigate such problems, we propose a Rule-Following Fine-Tuning (RFFT) technique to teach transformers to perform rule-based reasoning.","Specifically, we provide explicit rules in the input and then instruct transformers to recite and follow the rules step by step.","Through RFFT, we successfully enable LLMs fine-tuned on 1-5 digit addition to generalize to up to 12-digit addition with over 95% accuracy, which is over 40% higher than scratchpad.","The significant improvement demonstrates that teaching LLMs to explicitly use rules helps them learn rule-based reasoning and generalize better in length."],"url":"http://arxiv.org/abs/2402.17709v1"}
{"created":"2024-02-27 17:36:01","title":"Adaptive quantization with mixed-precision based on low-cost proxy","abstract":"It is critical to deploy complicated neural network models on hardware with limited resources. This paper proposes a novel model quantization method, named the Low-Cost Proxy-Based Adaptive Mixed-Precision Model Quantization (LCPAQ), which contains three key modules. The hardware-aware module is designed by considering the hardware limitations, while an adaptive mixed-precision quantization module is developed to evaluate the quantization sensitivity by using the Hessian matrix and Pareto frontier techniques. Integer linear programming is used to fine-tune the quantization across different layers. Then the low-cost proxy neural architecture search module efficiently explores the ideal quantization hyperparameters. Experiments on the ImageNet demonstrate that the proposed LCPAQ achieves comparable or superior quantization accuracy to existing mixed-precision models. Notably, LCPAQ achieves 1/200 of the search time compared with existing methods, which provides a shortcut in practical quantization use for resource-limited devices.","sentences":["It is critical to deploy complicated neural network models on hardware with limited resources.","This paper proposes a novel model quantization method, named the Low-Cost Proxy-Based Adaptive Mixed-Precision Model Quantization (LCPAQ), which contains three key modules.","The hardware-aware module is designed by considering the hardware limitations, while an adaptive mixed-precision quantization module is developed to evaluate the quantization sensitivity by using the Hessian matrix and Pareto frontier techniques.","Integer linear programming is used to fine-tune the quantization across different layers.","Then the low-cost proxy neural architecture search module efficiently explores the ideal quantization hyperparameters.","Experiments on the ImageNet demonstrate that the proposed LCPAQ achieves comparable or superior quantization accuracy to existing mixed-precision models.","Notably, LCPAQ achieves 1/200 of the search time compared with existing methods, which provides a shortcut in practical quantization use for resource-limited devices."],"url":"http://arxiv.org/abs/2402.17706v1"}
{"created":"2024-02-27 17:33:23","title":"Federated Learning for Estimating Heterogeneous Treatment Effects","abstract":"Machine learning methods for estimating heterogeneous treatment effects (HTE) facilitate large-scale personalized decision-making across various domains such as healthcare, policy making, education, and more. Current machine learning approaches for HTE require access to substantial amounts of data per treatment, and the high costs associated with interventions makes centrally collecting so much data for each intervention a formidable challenge. To overcome this obstacle, in this work, we propose a novel framework for collaborative learning of HTE estimators across institutions via Federated Learning. We show that even under a diversity of interventions and subject populations across clients, one can jointly learn a common feature representation, while concurrently and privately learning the specific predictive functions for outcomes under distinct interventions across institutions. Our framework and the associated algorithm are based on this insight, and leverage tabular transformers to map multiple input data to feature representations which are then used for outcome prediction via multi-task learning. We also propose a novel way of federated training of personalised transformers that can work with heterogeneous input feature spaces. Experimental results on real-world clinical trial data demonstrate the effectiveness of our method.","sentences":["Machine learning methods for estimating heterogeneous treatment effects (HTE) facilitate large-scale personalized decision-making across various domains such as healthcare, policy making, education, and more.","Current machine learning approaches for HTE require access to substantial amounts of data per treatment, and the high costs associated with interventions makes centrally collecting so much data for each intervention a formidable challenge.","To overcome this obstacle, in this work, we propose a novel framework for collaborative learning of HTE estimators across institutions via Federated Learning.","We show that even under a diversity of interventions and subject populations across clients, one can jointly learn a common feature representation, while concurrently and privately learning the specific predictive functions for outcomes under distinct interventions across institutions.","Our framework and the associated algorithm are based on this insight, and leverage tabular transformers to map multiple input data to feature representations which are then used for outcome prediction via multi-task learning.","We also propose a novel way of federated training of personalised transformers that can work with heterogeneous input feature spaces.","Experimental results on real-world clinical trial data demonstrate the effectiveness of our method."],"url":"http://arxiv.org/abs/2402.17705v1"}
{"created":"2024-02-27 17:25:37","title":"RAVEL: Evaluating Interpretability Methods on Disentangling Language Model Representations","abstract":"Individual neurons participate in the representation of multiple high-level concepts. To what extent can different interpretability methods successfully disentangle these roles? To help address this question, we introduce RAVEL (Resolving Attribute-Value Entanglements in Language Models), a dataset that enables tightly controlled, quantitative comparisons between a variety of existing interpretability methods. We use the resulting conceptual framework to define the new method of Multi-task Distributed Alignment Search (MDAS), which allows us to find distributed representations satisfying multiple causal criteria. With Llama2-7B as the target language model, MDAS achieves state-of-the-art results on RAVEL, demonstrating the importance of going beyond neuron-level analyses to identify features distributed across activations. We release our benchmark at https://github.com/explanare/ravel.","sentences":["Individual neurons participate in the representation of multiple high-level concepts.","To what extent can different interpretability methods successfully disentangle these roles?","To help address this question, we introduce RAVEL (Resolving Attribute-Value Entanglements in Language Models), a dataset that enables tightly controlled, quantitative comparisons between a variety of existing interpretability methods.","We use the resulting conceptual framework to define the new method of Multi-task Distributed Alignment Search (MDAS), which allows us to find distributed representations satisfying multiple causal criteria.","With Llama2-7B as the target language model, MDAS achieves state-of-the-art results on RAVEL, demonstrating the importance of going beyond neuron-level analyses to identify features distributed across activations.","We release our benchmark at https://github.com/explanare/ravel."],"url":"http://arxiv.org/abs/2402.17700v1"}
{"created":"2024-02-27 17:23:40","title":"Gradient-based Discrete Sampling with Automatic Cyclical Scheduling","abstract":"Discrete distributions, particularly in high-dimensional deep models, are often highly multimodal due to inherent discontinuities. While gradient-based discrete sampling has proven effective, it is susceptible to becoming trapped in local modes due to the gradient information. To tackle this challenge, we propose an automatic cyclical scheduling, designed for efficient and accurate sampling in multimodal discrete distributions. Our method contains three key components: (1) a cyclical step size schedule where large steps discover new modes and small steps exploit each mode; (2) a cyclical balancing schedule, ensuring ``balanced\" proposals for given step sizes and high efficiency of the Markov chain; and (3) an automatic tuning scheme for adjusting the hyperparameters in the cyclical schedules, allowing adaptability across diverse datasets with minimal tuning. We prove the non-asymptotic convergence and inference guarantee for our method in general discrete distributions. Extensive experiments demonstrate the superiority of our method in sampling complex multimodal discrete distributions.","sentences":["Discrete distributions, particularly in high-dimensional deep models, are often highly multimodal due to inherent discontinuities.","While gradient-based discrete sampling has proven effective, it is susceptible to becoming trapped in local modes due to the gradient information.","To tackle this challenge, we propose an automatic cyclical scheduling, designed for efficient and accurate sampling in multimodal discrete distributions.","Our method contains three key components: (1) a cyclical step size schedule where large steps discover new modes and small steps exploit each mode; (2) a cyclical balancing schedule, ensuring ``balanced\" proposals for given step sizes and high efficiency of the Markov chain; and (3) an automatic tuning scheme for adjusting the hyperparameters in the cyclical schedules, allowing adaptability across diverse datasets with minimal tuning.","We prove the non-asymptotic convergence and inference guarantee for our method in general discrete distributions.","Extensive experiments demonstrate the superiority of our method in sampling complex multimodal discrete distributions."],"url":"http://arxiv.org/abs/2402.17699v1"}
{"created":"2024-02-27 17:11:35","title":"Geometric Deep Learning for Computer-Aided Design: A Survey","abstract":"Geometric Deep Learning techniques have become a transformative force in the field of Computer-Aided Design (CAD), and have the potential to revolutionize how designers and engineers approach and enhance the design process. By harnessing the power of machine learning-based methods, CAD designers can optimize their workflows, save time and effort while making better informed decisions, and create designs that are both innovative and practical. The ability to process the CAD designs represented by geometric data and to analyze their encoded features enables the identification of similarities among diverse CAD models, the proposition of alternative designs and enhancements, and even the generation of novel design alternatives. This survey offers a comprehensive overview of learning-based methods in computer-aided design across various categories, including similarity analysis and retrieval, 2D and 3D CAD model synthesis, and CAD generation from point clouds. Additionally, it provides a complete list of benchmark datasets and their characteristics, along with open-source codes that have propelled research in this domain. The final discussion delves into the challenges prevalent in this field, followed by potential future research directions in this rapidly evolving field.","sentences":["Geometric Deep Learning techniques have become a transformative force in the field of Computer-Aided Design (CAD), and have the potential to revolutionize how designers and engineers approach and enhance the design process.","By harnessing the power of machine learning-based methods, CAD designers can optimize their workflows, save time and effort while making better informed decisions, and create designs that are both innovative and practical.","The ability to process the CAD designs represented by geometric data and to analyze their encoded features enables the identification of similarities among diverse CAD models, the proposition of alternative designs and enhancements, and even the generation of novel design alternatives.","This survey offers a comprehensive overview of learning-based methods in computer-aided design across various categories, including similarity analysis and retrieval, 2D and 3D CAD model synthesis, and CAD generation from point clouds.","Additionally, it provides a complete list of benchmark datasets and their characteristics, along with open-source codes that have propelled research in this domain.","The final discussion delves into the challenges prevalent in this field, followed by potential future research directions in this rapidly evolving field."],"url":"http://arxiv.org/abs/2402.17695v1"}
{"created":"2024-02-27 17:07:18","title":"Autonomous Vehicles: Evolution of Artificial Intelligence and Learning Algorithms","abstract":"The advent of autonomous vehicles has heralded a transformative era in transportation, reshaping the landscape of mobility through cutting-edge technologies. Central to this evolu- tion is the integration of Artificial Intelligence (AI) and learning algorithms, propelling vehicles into realms of unprecedented autonomy. This paper provides a comprehensive exploration of the evolutionary trajectory of AI within autonomous vehicles, tracing the journey from foundational principles to the most recent advancements. Commencing with a current landscape overview, the paper delves into the fundamental role of AI in shaping the autonomous decision-making capabilities of vehicles. It elucidates the steps involved in the AI-powered development life cycle in vehicles, addressing ethical considerations and bias in AI-driven software development for autonomous vehicles. The study presents statis- tical insights into the usage and types of AI/learning algorithms over the years, showcasing the evolving research landscape within the automotive industry. Furthermore, the paper highlights the pivotal role of parameters in refining algorithms for both trucks and cars, facilitating vehicles to adapt, learn, and improve performance over time. It concludes by outlining different levels of autonomy, elucidating the nuanced usage of AI and learning algorithms, and automating key tasks at each level. Additionally, the document discusses the variation in software package sizes across different autonomy levels","sentences":["The advent of autonomous vehicles has heralded a transformative era in transportation, reshaping the landscape of mobility through cutting-edge technologies.","Central to this evolu- tion is the integration of Artificial Intelligence (AI) and learning algorithms, propelling vehicles into realms of unprecedented autonomy.","This paper provides a comprehensive exploration of the evolutionary trajectory of AI within autonomous vehicles, tracing the journey from foundational principles to the most recent advancements.","Commencing with a current landscape overview, the paper delves into the fundamental role of AI in shaping the autonomous decision-making capabilities of vehicles.","It elucidates the steps involved in the AI-powered development life cycle in vehicles, addressing ethical considerations and bias in AI-driven software development for autonomous vehicles.","The study presents statis- tical insights into the usage and types of AI/learning algorithms over the years, showcasing the evolving research landscape within the automotive industry.","Furthermore, the paper highlights the pivotal role of parameters in refining algorithms for both trucks and cars, facilitating vehicles to adapt, learn, and improve performance over time.","It concludes by outlining different levels of autonomy, elucidating the nuanced usage of AI and learning algorithms, and automating key tasks at each level.","Additionally, the document discusses the variation in software package sizes across different autonomy levels"],"url":"http://arxiv.org/abs/2402.17690v1"}
{"created":"2024-02-27 17:05:41","title":"QoS prediction in radio vehicular environments via prior user information","abstract":"Reliable wireless communications play an important role in the automotive industry as it helps to enhance current use cases and enable new ones such as connected autonomous driving, platooning, cooperative maneuvering, teleoperated driving, and smart navigation. These and other use cases often rely on specific quality of service (QoS) levels for communication. Recently, the area of predictive quality of service (QoS) has received a great deal of attention as a key enabler to forecast communication quality well enough in advance. However, predicting QoS in a reliable manner is a notoriously difficult task. In this paper, we evaluate ML tree-ensemble methods to predict QoS in the range of minutes with data collected from a cellular test network. We discuss radio environment characteristics and we showcase how these can be used to improve ML performance and further support the uptake of ML in commercial networks. Specifically, we use the correlations of the measurements coming from the radio environment by including information of prior vehicles to enhance the prediction of the target vehicles. Moreover, we are extending prior art by showing how longer prediction horizons can be supported.","sentences":["Reliable wireless communications play an important role in the automotive industry as it helps to enhance current use cases and enable new ones such as connected autonomous driving, platooning, cooperative maneuvering, teleoperated driving, and smart navigation.","These and other use cases often rely on specific quality of service (QoS) levels for communication.","Recently, the area of predictive quality of service (QoS) has received a great deal of attention as a key enabler to forecast communication quality well enough in advance.","However, predicting QoS in a reliable manner is a notoriously difficult task.","In this paper, we evaluate ML tree-ensemble methods to predict QoS in the range of minutes with data collected from a cellular test network.","We discuss radio environment characteristics and we showcase how these can be used to improve ML performance and further support the uptake of ML in commercial networks.","Specifically, we use the correlations of the measurements coming from the radio environment by including information of prior vehicles to enhance the prediction of the target vehicles.","Moreover, we are extending prior art by showing how longer prediction horizons can be supported."],"url":"http://arxiv.org/abs/2402.17689v1"}
{"created":"2024-02-27 16:56:30","title":"NextLevelBERT: Investigating Masked Language Modeling with Higher-Level Representations for Long Documents","abstract":"While (large) language models have significantly improved over the last years, they still struggle to sensibly process long sequences found, e.g., in books, due to the quadratic scaling of the underlying attention mechanism. To address this, we propose NextLevelBERT, a Masked Language Model operating not on tokens, but on higher-level semantic representations in the form of text embeddings. We pretrain NextLevelBERT to predict the vector representation of entire masked text chunks and evaluate the effectiveness of the resulting document vectors on three task types: 1) Semantic Textual Similarity via zero-shot document embeddings, 2) Long document classification, 3) Multiple-choice question answering. We find that next level Masked Language Modeling is an effective technique to tackle long-document use cases and can outperform much larger embedding models as long as the required level of detail is not too high. We make model and code available.","sentences":["While (large) language models have significantly improved over the last years, they still struggle to sensibly process long sequences found, e.g., in books, due to the quadratic scaling of the underlying attention mechanism.","To address this, we propose NextLevelBERT, a Masked Language Model operating not on tokens, but on higher-level semantic representations in the form of text embeddings.","We pretrain NextLevelBERT to predict the vector representation of entire masked text chunks and evaluate the effectiveness of the resulting document vectors on three task types: 1) Semantic Textual Similarity via zero-shot document embeddings, 2) Long document classification, 3) Multiple-choice question answering.","We find that next level Masked Language Modeling is an effective technique to tackle long-document use cases and can outperform much larger embedding models as long as the required level of detail is not too high.","We make model and code available."],"url":"http://arxiv.org/abs/2402.17682v1"}
{"created":"2024-02-27 16:54:08","title":"MCF-VC: Mitigate Catastrophic Forgetting in Class-Incremental Learning for Multimodal Video Captioning","abstract":"To address the problem of catastrophic forgetting due to the invisibility of old categories in sequential input, existing work based on relatively simple categorization tasks has made some progress. In contrast, video captioning is a more complex task in multimodal scenario, which has not been explored in the field of incremental learning. After identifying this stability-plasticity problem when analyzing video with sequential input, we originally propose a method to Mitigate Catastrophic Forgetting in class-incremental learning for multimodal Video Captioning (MCF-VC). As for effectively maintaining good performance on old tasks at the macro level, we design Fine-grained Sensitivity Selection (FgSS) based on the Mask of Linear's Parameters and Fisher Sensitivity to pick useful knowledge from old tasks. Further, in order to better constrain the knowledge characteristics of old and new tasks at the specific feature level, we have created the Two-stage Knowledge Distillation (TsKD), which is able to learn the new task well while weighing the old task. Specifically, we design two distillation losses, which constrain the cross modal semantic information of semantic attention feature map and the textual information of the final outputs respectively, so that the inter-model and intra-model stylized knowledge of the old class is retained while learning the new class. In order to illustrate the ability of our model to resist forgetting, we designed a metric CIDER_t to detect the stage forgetting rate. Our experiments on the public dataset MSR-VTT show that the proposed method significantly resists the forgetting of previous tasks without replaying old samples, and performs well on the new task.","sentences":["To address the problem of catastrophic forgetting due to the invisibility of old categories in sequential input, existing work based on relatively simple categorization tasks has made some progress.","In contrast, video captioning is a more complex task in multimodal scenario, which has not been explored in the field of incremental learning.","After identifying this stability-plasticity problem when analyzing video with sequential input, we originally propose a method to Mitigate Catastrophic Forgetting in class-incremental learning for multimodal Video Captioning (MCF-VC).","As for effectively maintaining good performance on old tasks at the macro level, we design Fine-grained Sensitivity Selection (FgSS) based on the Mask of Linear's Parameters and Fisher Sensitivity to pick useful knowledge from old tasks.","Further, in order to better constrain the knowledge characteristics of old and new tasks at the specific feature level, we have created the Two-stage Knowledge Distillation (TsKD), which is able to learn the new task well while weighing the old task.","Specifically, we design two distillation losses, which constrain the cross modal semantic information of semantic attention feature map and the textual information of the final outputs respectively, so that the inter-model and intra-model stylized knowledge of the old class is retained while learning the new class.","In order to illustrate the ability of our model to resist forgetting, we designed a metric CIDER_t to detect the stage forgetting rate.","Our experiments on the public dataset MSR-VTT show that the proposed method significantly resists the forgetting of previous tasks without replaying old samples, and performs well on the new task."],"url":"http://arxiv.org/abs/2402.17680v1"}
{"created":"2024-02-27 16:53:53","title":"The Emergence of Large Language Models in Static Analysis: A First Look through Micro-Benchmarks","abstract":"The application of Large Language Models (LLMs) in software engineering, particularly in static analysis tasks, represents a paradigm shift in the field. In this paper, we investigate the role that current LLMs can play in improving callgraph analysis and type inference for Python programs. Using the PyCG, HeaderGen, and TypeEvalPy micro-benchmarks, we evaluate 26 LLMs, including OpenAI's GPT series and open-source models such as LLaMA. Our study reveals that LLMs show promising results in type inference, demonstrating higher accuracy than traditional methods, yet they exhibit limitations in callgraph analysis. This contrast emphasizes the need for specialized fine-tuning of LLMs to better suit specific static analysis tasks. Our findings provide a foundation for further research towards integrating LLMs for static analysis tasks.","sentences":["The application of Large Language Models (LLMs) in software engineering, particularly in static analysis tasks, represents a paradigm shift in the field.","In this paper, we investigate the role that current LLMs can play in improving callgraph analysis and type inference for Python programs.","Using the PyCG, HeaderGen, and TypeEvalPy micro-benchmarks, we evaluate 26 LLMs, including OpenAI's GPT series and open-source models such as LLaMA.","Our study reveals that LLMs show promising results in type inference, demonstrating higher accuracy than traditional methods, yet they exhibit limitations in callgraph analysis.","This contrast emphasizes the need for specialized fine-tuning of LLMs to better suit specific static analysis tasks.","Our findings provide a foundation for further research towards integrating LLMs for static analysis tasks."],"url":"http://arxiv.org/abs/2402.17679v1"}
{"created":"2024-02-27 16:53:16","title":"CAD-SIGNet: CAD Language Inference from Point Clouds using Layer-wise Sketch Instance Guided Attention","abstract":"Reverse engineering in the realm of Computer-Aided Design (CAD) has been a longstanding aspiration, though not yet entirely realized. Its primary aim is to uncover the CAD process behind a physical object given its 3D scan. We propose CAD-SIGNet, an end-to-end trainable and auto-regressive architecture to recover the design history of a CAD model represented as a sequence of sketch-and-extrusion from an input point cloud. Our model learns visual-language representations by layer-wise cross-attention between point cloud and CAD language embedding. In particular, a new Sketch instance Guided Attention (SGA) module is proposed in order to reconstruct the fine-grained details of the sketches. Thanks to its auto-regressive nature, CAD-SIGNet not only reconstructs a unique full design history of the corresponding CAD model given an input point cloud but also provides multiple plausible design choices. This allows for an interactive reverse engineering scenario by providing designers with multiple next-step choices along with the design process. Extensive experiments on publicly available CAD datasets showcase the effectiveness of our approach against existing baseline models in two settings, namely, full design history recovery and conditional auto-completion from point clouds.","sentences":["Reverse engineering in the realm of Computer-Aided Design (CAD) has been a longstanding aspiration, though not yet entirely realized.","Its primary aim is to uncover the CAD process behind a physical object given its 3D scan.","We propose CAD-SIGNet, an end-to-end trainable and auto-regressive architecture to recover the design history of a CAD model represented as a sequence of sketch-and-extrusion from an input point cloud.","Our model learns visual-language representations by layer-wise cross-attention between point cloud and CAD language embedding.","In particular, a new Sketch instance Guided Attention (SGA) module is proposed in order to reconstruct the fine-grained details of the sketches.","Thanks to its auto-regressive nature, CAD-SIGNet not only reconstructs a unique full design history of the corresponding CAD model given an input point cloud but also provides multiple plausible design choices.","This allows for an interactive reverse engineering scenario by providing designers with multiple next-step choices along with the design process.","Extensive experiments on publicly available CAD datasets showcase the effectiveness of our approach against existing baseline models in two settings, namely, full design history recovery and conditional auto-completion from point clouds."],"url":"http://arxiv.org/abs/2402.17678v1"}
{"created":"2024-02-27 16:48:12","title":"Highly-Efficient Persistent FIFO Queues","abstract":"In this paper, we study the question whether techniques employed, in a conventional system, by state-of-the-art concurrent algorithms to avoid contended hot spots are still efficient for recoverable computing in settings with Non-Volatile Memory (NVM). We focus on concurrent FIFO queues that have two end-points, head and tail, which are highly contended.   We present a persistent FIFO queue implementation that performs a pair of persistence instructions per operation (enqueue or dequeue). The algorithm achieves to perform these instructions on variables of low contention by employing Fetch&Increment and using the state-of-the-art queue implementation by Afek and Morrison (PPoPP'13). These result in performance that is up to 2x faster than state-of-the-art persistent FIFO queue implementations.","sentences":["In this paper, we study the question whether techniques employed, in a conventional system, by state-of-the-art concurrent algorithms to avoid contended hot spots are still efficient for recoverable computing in settings with Non-Volatile Memory (NVM).","We focus on concurrent FIFO queues that have two end-points, head and tail, which are highly contended.   ","We present a persistent FIFO queue implementation that performs a pair of persistence instructions per operation (enqueue or dequeue).","The algorithm achieves to perform these instructions on variables of low contention by employing Fetch&Increment and using the state-of-the-art queue implementation by Afek and Morrison (PPoPP'13).","These result in performance that is up to 2x faster than state-of-the-art persistent FIFO queue implementations."],"url":"http://arxiv.org/abs/2402.17674v1"}
{"created":"2024-02-27 16:46:21","title":"SDF2Net: Shallow to Deep Feature Fusion Network for PolSAR Image Classification","abstract":"Polarimetric synthetic aperture radar (PolSAR) images encompass valuable information that can facilitate extensive land cover interpretation and generate diverse output products. Extracting meaningful features from PolSAR data poses challenges distinct from those encountered in optical imagery. Deep learning (DL) methods offer effective solutions for overcoming these challenges in PolSAR feature extraction. Convolutional neural networks (CNNs) play a crucial role in capturing PolSAR image characteristics by leveraging kernel capabilities to consider local information and the complex-valued nature of PolSAR data. In this study, a novel three-branch fusion of complex-valued CNN, named the Shallow to Deep Feature Fusion Network (SDF2Net), is proposed for PolSAR image classification. To validate the performance of the proposed method, classification results are compared against multiple state-of-the-art approaches using the airborne synthetic aperture radar (AIRSAR) datasets of Flevoland and San Francisco, as well as the ESAR Oberpfaffenhofen dataset. The results indicate that the proposed approach demonstrates improvements in overallaccuracy, with a 1.3% and 0.8% enhancement for the AIRSAR datasets and a 0.5% improvement for the ESAR dataset. Analyses conducted on the Flevoland data underscore the effectiveness of the SDF2Net model, revealing a promising overall accuracy of 96.01% even with only a 1% sampling ratio.","sentences":["Polarimetric synthetic aperture radar (PolSAR) images encompass valuable information that can facilitate extensive land cover interpretation and generate diverse output products.","Extracting meaningful features from PolSAR data poses challenges distinct from those encountered in optical imagery.","Deep learning (DL) methods offer effective solutions for overcoming these challenges in PolSAR feature extraction.","Convolutional neural networks (CNNs) play a crucial role in capturing PolSAR image characteristics by leveraging kernel capabilities to consider local information and the complex-valued nature of PolSAR data.","In this study, a novel three-branch fusion of complex-valued CNN, named the Shallow to Deep Feature Fusion Network (SDF2Net), is proposed for PolSAR image classification.","To validate the performance of the proposed method, classification results are compared against multiple state-of-the-art approaches using the airborne synthetic aperture radar (AIRSAR) datasets of Flevoland and San Francisco, as well as the ESAR Oberpfaffenhofen dataset.","The results indicate that the proposed approach demonstrates improvements in overallaccuracy, with a 1.3% and 0.8% enhancement for the AIRSAR datasets and a 0.5% improvement for the ESAR dataset.","Analyses conducted on the Flevoland data underscore the effectiveness of the SDF2Net model, revealing a promising overall accuracy of 96.01% even with only a 1% sampling ratio."],"url":"http://arxiv.org/abs/2402.17672v1"}
{"created":"2024-02-27 16:44:09","title":"Securing Reliability: A Brief Overview on Enhancing In-Context Learning for Foundation Models","abstract":"As foundation models (FMs) continue to shape the landscape of AI, the in-context learning (ICL) paradigm thrives but also encounters issues such as toxicity, hallucination, disparity, adversarial vulnerability, and inconsistency. Ensuring the reliability and responsibility of FMs is crucial for the sustainable development of the AI ecosystem. In this concise overview, we investigate recent advancements in enhancing the reliability and trustworthiness of FMs within ICL frameworks, focusing on four key methodologies, each with its corresponding subgoals. We sincerely hope this paper can provide valuable insights for researchers and practitioners endeavoring to build safe and dependable FMs and foster a stable and consistent ICL environment, thereby unlocking their vast potential.","sentences":["As foundation models (FMs) continue to shape the landscape of AI, the in-context learning (ICL) paradigm thrives but also encounters issues such as toxicity, hallucination, disparity, adversarial vulnerability, and inconsistency.","Ensuring the reliability and responsibility of FMs is crucial for the sustainable development of the AI ecosystem.","In this concise overview, we investigate recent advancements in enhancing the reliability and trustworthiness of FMs within ICL frameworks, focusing on four key methodologies, each with its corresponding subgoals.","We sincerely hope this paper can provide valuable insights for researchers and practitioners endeavoring to build safe and dependable FMs and foster a stable and consistent ICL environment, thereby unlocking their vast potential."],"url":"http://arxiv.org/abs/2402.17671v1"}
{"created":"2024-02-27 16:36:53","title":"Multi-Agent Deep Reinforcement Learning for Distributed Satellite Routing","abstract":"This paper introduces a Multi-Agent Deep Reinforcement Learning (MA-DRL) approach for routing in Low Earth Orbit Satellite Constellations (LSatCs). Each satellite is an independent decision-making agent with a partial knowledge of the environment, and supported by feedback received from the nearby agents. Building on our previous work that introduced a Q-routing solution, the contribution of this paper is to extend it to a deep learning framework able to quickly adapt to the network and traffic changes, and based on two phases: (1) An offline exploration learning phase that relies on a global Deep Neural Network (DNN) to learn the optimal paths at each possible position and congestion level; (2) An online exploitation phase with local, on-board, pre-trained DNNs. Results show that MA-DRL efficiently learns optimal routes offline that are then loaded for an efficient distributed routing online.","sentences":["This paper introduces a Multi-Agent Deep Reinforcement Learning (MA-DRL) approach for routing in Low Earth Orbit Satellite Constellations (LSatCs).","Each satellite is an independent decision-making agent with a partial knowledge of the environment, and supported by feedback received from the nearby agents.","Building on our previous work that introduced a Q-routing solution, the contribution of this paper is to extend it to a deep learning framework able to quickly adapt to the network and traffic changes, and based on two phases: (1) An offline exploration learning phase that relies on a global Deep Neural Network (DNN) to learn the optimal paths at each possible position and congestion level; (2) An online exploitation phase with local, on-board, pre-trained DNNs.","Results show that MA-DRL efficiently learns optimal routes offline that are then loaded for an efficient distributed routing online."],"url":"http://arxiv.org/abs/2402.17666v1"}
{"created":"2024-02-27 16:35:07","title":"Bayesian Differentiable Physics for Cloth Digitalization","abstract":"We propose a new method for cloth digitalization. Deviating from existing methods which learn from data captured under relatively casual settings, we propose to learn from data captured in strictly tested measuring protocols, and find plausible physical parameters of the cloths. However, such data is currently absent, so we first propose a new dataset with accurate cloth measurements. Further, the data size is considerably smaller than the ones in current deep learning, due to the nature of the data capture process. To learn from small data, we propose a new Bayesian differentiable cloth model to estimate the complex material heterogeneity of real cloths. It can provide highly accurate digitalization from very limited data samples. Through exhaustive evaluation and comparison, we show our method is accurate in cloth digitalization, efficient in learning from limited data samples, and general in capturing material variations. Code and data are available https://github.com/realcrane/Bayesian-Differentiable-Physics-for-Cloth-Digitalization","sentences":["We propose a new method for cloth digitalization.","Deviating from existing methods which learn from data captured under relatively casual settings, we propose to learn from data captured in strictly tested measuring protocols, and find plausible physical parameters of the cloths.","However, such data is currently absent, so we first propose a new dataset with accurate cloth measurements.","Further, the data size is considerably smaller than the ones in current deep learning, due to the nature of the data capture process.","To learn from small data, we propose a new Bayesian differentiable cloth model to estimate the complex material heterogeneity of real cloths.","It can provide highly accurate digitalization from very limited data samples.","Through exhaustive evaluation and comparison, we show our method is accurate in cloth digitalization, efficient in learning from limited data samples, and general in capturing material variations.","Code and data are available https://github.com/realcrane/Bayesian-Differentiable-Physics-for-Cloth-Digitalization"],"url":"http://arxiv.org/abs/2402.17664v1"}
{"created":"2024-02-27 16:27:06","title":"TorchMD-Net 2.0: Fast Neural Network Potentials for Molecular Simulations","abstract":"Achieving a balance between computational speed, prediction accuracy, and universal applicability in molecular simulations has been a persistent challenge. This paper presents substantial advancements in the TorchMD-Net software, a pivotal step forward in the shift from conventional force fields to neural network-based potentials. The evolution of TorchMD-Net into a more comprehensive and versatile framework is highlighted, incorporating cutting-edge architectures such as TensorNet. This transformation is achieved through a modular design approach, encouraging customized applications within the scientific community. The most notable enhancement is a significant improvement in computational efficiency, achieving a very remarkable acceleration in the computation of energy and forces for TensorNet models, with performance gains ranging from 2-fold to 10-fold over previous iterations. Other enhancements include highly optimized neighbor search algorithms that support periodic boundary conditions and the smooth integration with existing molecular dynamics frameworks. Additionally, the updated version introduces the capability to integrate physical priors, further enriching its application spectrum and utility in research. The software is available at https://github.com/torchmd/torchmd-net.","sentences":["Achieving a balance between computational speed, prediction accuracy, and universal applicability in molecular simulations has been a persistent challenge.","This paper presents substantial advancements in the TorchMD-Net software, a pivotal step forward in the shift from conventional force fields to neural network-based potentials.","The evolution of TorchMD-Net into a more comprehensive and versatile framework is highlighted, incorporating cutting-edge architectures such as TensorNet.","This transformation is achieved through a modular design approach, encouraging customized applications within the scientific community.","The most notable enhancement is a significant improvement in computational efficiency, achieving a very remarkable acceleration in the computation of energy and forces for TensorNet models, with performance gains ranging from 2-fold to 10-fold over previous iterations.","Other enhancements include highly optimized neighbor search algorithms that support periodic boundary conditions and the smooth integration with existing molecular dynamics frameworks.","Additionally, the updated version introduces the capability to integrate physical priors, further enriching its application spectrum and utility in research.","The software is available at https://github.com/torchmd/torchmd-net."],"url":"http://arxiv.org/abs/2402.17660v1"}
{"created":"2024-02-27 16:26:05","title":"SoK: Cryptocurrency Wallets -- A Security Review and Classification based on Authentication Factors","abstract":"In this work, we review existing cryptocurrency wallet solutions with regard to authentication methods and factors from the user's point of view. In particular, we distinguish between authentication factors that are verified against the blockchain and the ones verified locally (or against a centralized party). With this in mind, we define notions for $k-factor$ authentication against the blockchain and $k-factor$ authentication against the authentication factors. Based on these notions, we propose a classification of authentication schemes. We extend our classification to accommodate the threshold signatures and signing transactions by centralized parties (such as exchanges or co-signing services). Finally, we apply our classification to existing wallet solutions, which we compare based on various security and key-management features.","sentences":["In this work, we review existing cryptocurrency wallet solutions with regard to authentication methods and factors from the user's point of view.","In particular, we distinguish between authentication factors that are verified against the blockchain and the ones verified locally (or against a centralized party).","With this in mind, we define notions for $k-factor$ authentication against the blockchain and $k-factor$ authentication against the authentication factors.","Based on these notions, we propose a classification of authentication schemes.","We extend our classification to accommodate the threshold signatures and signing transactions by centralized parties (such as exchanges or co-signing services).","Finally, we apply our classification to existing wallet solutions, which we compare based on various security and key-management features."],"url":"http://arxiv.org/abs/2402.17659v1"}
{"created":"2024-02-27 16:24:28","title":"Confidence-Aware Multi-Field Model Calibration","abstract":"Accurately predicting the probabilities of user feedback, such as clicks and conversions, is critical for ad ranking and bidding. However, there often exist unwanted mismatches between predicted probabilities and true likelihoods due to the shift of data distributions and intrinsic model biases. Calibration aims to address this issue by post-processing model predictions, and field-aware calibration can adjust model output on different feature field values to satisfy fine-grained advertising demands. Unfortunately, the observed samples corresponding to certain field values can be too limited to make confident calibrations, which may yield bias amplification and online disturbance. In this paper, we propose a confidence-aware multi-field calibration method, which adaptively adjusts the calibration intensity based on the confidence levels derived from sample statistics. It also utilizes multiple feature fields for joint model calibration with awareness of their importance to mitigate the data sparsity effect of a single field. Extensive offline and online experiments show the superiority of our method in boosting advertising performance and reducing prediction deviations.","sentences":["Accurately predicting the probabilities of user feedback, such as clicks and conversions, is critical for ad ranking and bidding.","However, there often exist unwanted mismatches between predicted probabilities and true likelihoods due to the shift of data distributions and intrinsic model biases.","Calibration aims to address this issue by post-processing model predictions, and field-aware calibration can adjust model output on different feature field values to satisfy fine-grained advertising demands.","Unfortunately, the observed samples corresponding to certain field values can be too limited to make confident calibrations, which may yield bias amplification and online disturbance.","In this paper, we propose a confidence-aware multi-field calibration method, which adaptively adjusts the calibration intensity based on the confidence levels derived from sample statistics.","It also utilizes multiple feature fields for joint model calibration with awareness of their importance to mitigate the data sparsity effect of a single field.","Extensive offline and online experiments show the superiority of our method in boosting advertising performance and reducing prediction deviations."],"url":"http://arxiv.org/abs/2402.17655v1"}
{"created":"2024-02-27 16:23:11","title":"Mitigating Distributional Shift in Semantic Segmentation via Uncertainty Estimation from Unlabelled Data","abstract":"Knowing when a trained segmentation model is encountering data that is different to its training data is important. Understanding and mitigating the effects of this play an important part in their application from a performance and assurance perspective - this being a safety concern in applications such as autonomous vehicles (AVs). This work presents a segmentation network that can detect errors caused by challenging test domains without any additional annotation in a single forward pass. As annotation costs limit the diversity of labelled datasets, we use easy-to-obtain, uncurated and unlabelled data to learn to perform uncertainty estimation by selectively enforcing consistency over data augmentation. To this end, a novel segmentation benchmark based on the SAX Dataset is used, which includes labelled test data spanning three autonomous-driving domains, ranging in appearance from dense urban to off-road. The proposed method, named Gamma-SSL, consistently outperforms uncertainty estimation and Out-of-Distribution (OoD) techniques on this difficult benchmark - by up to 10.7% in area under the receiver operating characteristic (ROC) curve and 19.2% in area under the precision-recall (PR) curve in the most challenging of the three scenarios.","sentences":["Knowing when a trained segmentation model is encountering data that is different to its training data is important.","Understanding and mitigating the effects of this play an important part in their application from a performance and assurance perspective - this being a safety concern in applications such as autonomous vehicles (AVs).","This work presents a segmentation network that can detect errors caused by challenging test domains without any additional annotation in a single forward pass.","As annotation costs limit the diversity of labelled datasets, we use easy-to-obtain, uncurated and unlabelled data to learn to perform uncertainty estimation by selectively enforcing consistency over data augmentation.","To this end, a novel segmentation benchmark based on the SAX Dataset is used, which includes labelled test data spanning three autonomous-driving domains, ranging in appearance from dense urban to off-road.","The proposed method, named Gamma-SSL, consistently outperforms uncertainty estimation and Out-of-Distribution (OoD) techniques on this difficult benchmark - by up to 10.7% in area under the receiver operating characteristic (ROC) curve and 19.2% in area under the precision-recall (PR) curve in the most challenging of the three scenarios."],"url":"http://arxiv.org/abs/2402.17653v1"}
{"created":"2024-02-27 16:21:28","title":"Navigator: A Decentralized Scheduler for Latency-Sensitive ML Workflows","abstract":"We consider ML query processing in distributed systems where GPU-enabled workers coordinate to execute complex queries: a computing style often seen in applications that interact with users in support of image processing and natural language processing. In such systems, coscheduling of GPU memory management and task placement represents a promising opportunity. We propose Navigator, a novel framework that unifies these functions to reduce job latency while using resources efficiently, placing tasks where data dependencies will be satisfied, collocating tasks from the same job (when this will not overload the host or its GPU), and efficiently managing GPU memory. Comparison with other state of the art schedulers shows a significant reduction in completion times while requiring the same amount or even fewer resources. In one case, just half the servers were needed for processing the same workload.","sentences":["We consider ML query processing in distributed systems where GPU-enabled workers coordinate to execute complex queries: a computing style often seen in applications that interact with users in support of image processing and natural language processing.","In such systems, coscheduling of GPU memory management and task placement represents a promising opportunity.","We propose Navigator, a novel framework that unifies these functions to reduce job latency while using resources efficiently, placing tasks where data dependencies will be satisfied, collocating tasks from the same job (when this will not overload the host or its GPU), and efficiently managing GPU memory.","Comparison with other state of the art schedulers shows a significant reduction in completion times while requiring the same amount or even fewer resources.","In one case, just half the servers were needed for processing the same workload."],"url":"http://arxiv.org/abs/2402.17652v1"}
{"created":"2024-02-27 16:21:09","title":"CSI-Free Optimization of Reconfigurable Intelligent Surfaces with Interference by Using Multiport Network Theory","abstract":"Reconfigurable Intelligent Surfaces (RIS) will play a pivotal role in next-generation wireless systems. Despite efforts to minimize pilot overhead associated with channel estimation, the necessity of configuring the RIS multiple times before obtaining reliable Channel State Information (CSI) may significantly diminish their benefits. Therefore, we propose a CSI-free approach that explores the feasibility of optimizing the RIS for the uplink of a communication system in the presence of interfering users without relying on CSI estimation but leveraging solely some a priori statistical knowledge of the channel. In this context, we consider a multiport network model that accounts for several aspects overlooked by traditional RIS models used in Communication Theory, such as mutual coupling among scattering elements and the presence of structural scattering. The proposed approach targets the maximization of the average achievable rate and is shown to achieve performance that, in some cases, can be very close to the case where the RIS is optimized leveraging perfect CSI.","sentences":["Reconfigurable Intelligent Surfaces (RIS) will play a pivotal role in next-generation wireless systems.","Despite efforts to minimize pilot overhead associated with channel estimation, the necessity of configuring the RIS multiple times before obtaining reliable Channel State Information (CSI) may significantly diminish their benefits.","Therefore, we propose a CSI-free approach that explores the feasibility of optimizing the RIS for the uplink of a communication system in the presence of interfering users without relying on CSI estimation but leveraging solely some a priori statistical knowledge of the channel.","In this context, we consider a multiport network model that accounts for several aspects overlooked by traditional RIS models used in Communication Theory, such as mutual coupling among scattering elements and the presence of structural scattering.","The proposed approach targets the maximization of the average achievable rate and is shown to achieve performance that, in some cases, can be very close to the case where the RIS is optimized leveraging perfect CSI."],"url":"http://arxiv.org/abs/2402.17651v1"}
{"created":"2024-02-27 16:19:37","title":"Beyond prompt brittleness: Evaluating the reliability and consistency of political worldviews in LLMs","abstract":"Due to the widespread use of large language models (LLMs) in ubiquitous systems, we need to understand whether they embed a specific worldview and what these views reflect. Recent studies report that, prompted with political questionnaires, LLMs show left-liberal leanings. However, it is as yet unclear whether these leanings are reliable (robust to prompt variations) and whether the leaning is consistent across policies and political leaning. We propose a series of tests which assess the reliability and consistency of LLMs' stances on political statements based on a dataset of voting-advice questionnaires collected from seven EU countries and annotated for policy domains. We study LLMs ranging in size from 7B to 70B parameters and find that their reliability increases with parameter count. Larger models show overall stronger alignment with left-leaning parties but differ among policy programs: They evince a (left-wing) positive stance towards environment protection, social welfare but also (right-wing) law and order, with no consistent preferences in foreign policy, migration, and economy.","sentences":["Due to the widespread use of large language models (LLMs) in ubiquitous systems, we need to understand whether they embed a specific worldview and what these views reflect.","Recent studies report that, prompted with political questionnaires, LLMs show left-liberal leanings.","However, it is as yet unclear whether these leanings are reliable (robust to prompt variations) and whether the leaning is consistent across policies and political leaning.","We propose a series of tests which assess the reliability and consistency of LLMs' stances on political statements based on a dataset of voting-advice questionnaires collected from seven EU countries and annotated for policy domains.","We study LLMs ranging in size from 7B to 70B parameters and find that their reliability increases with parameter count.","Larger models show overall stronger alignment with left-leaning parties but differ among policy programs: They evince a (left-wing) positive stance towards environment protection, social welfare but also (right-wing) law and order, with no consistent preferences in foreign policy, migration, and economy."],"url":"http://arxiv.org/abs/2402.17649v1"}
{"created":"2024-02-27 16:15:28","title":"SongComposer: A Large Language Model for Lyric and Melody Composition in Song Generation","abstract":"We present SongComposer, an innovative LLM designed for song composition. It could understand and generate melodies and lyrics in symbolic song representations, by leveraging the capability of LLM. Existing music-related LLM treated the music as quantized audio signals, while such implicit encoding leads to inefficient encoding and poor flexibility. In contrast, we resort to symbolic song representation, the mature and efficient way humans designed for music, and enable LLM to explicitly compose songs like humans. In practice, we design a novel tuple design to format lyric and three note attributes (pitch, duration, and rest duration) in the melody, which guarantees the correct LLM understanding of musical symbols and realizes precise alignment between lyrics and melody. To impart basic music understanding to LLM, we carefully collected SongCompose-PT, a large-scale song pretraining dataset that includes lyrics, melodies, and paired lyrics-melodies in either Chinese or English. After adequate pre-training, 10K carefully crafted QA pairs are used to empower the LLM with the instruction-following capability and solve diverse tasks. With extensive experiments, SongComposer demonstrates superior performance in lyric-to-melody generation, melody-to-lyric generation, song continuation, and text-to-song creation, outperforming advanced LLMs like GPT-4.","sentences":["We present SongComposer, an innovative LLM designed for song composition.","It could understand and generate melodies and lyrics in symbolic song representations, by leveraging the capability of LLM.","Existing music-related LLM treated the music as quantized audio signals, while such implicit encoding leads to inefficient encoding and poor flexibility.","In contrast, we resort to symbolic song representation, the mature and efficient way humans designed for music, and enable LLM to explicitly compose songs like humans.","In practice, we design a novel tuple design to format lyric and three note attributes (pitch, duration, and rest duration) in the melody, which guarantees the correct LLM understanding of musical symbols and realizes precise alignment between lyrics and melody.","To impart basic music understanding to LLM, we carefully collected SongCompose-PT, a large-scale song pretraining dataset that includes lyrics, melodies, and paired lyrics-melodies in either Chinese or English.","After adequate pre-training, 10K carefully crafted QA pairs are used to empower the LLM with the instruction-following capability and solve diverse tasks.","With extensive experiments, SongComposer demonstrates superior performance in lyric-to-melody generation, melody-to-lyric generation, song continuation, and text-to-song creation, outperforming advanced LLMs like GPT-4."],"url":"http://arxiv.org/abs/2402.17645v1"}
{"created":"2024-02-27 16:15:03","title":"Are LLMs Capable of Data-based Statistical and Causal Reasoning? Benchmarking Advanced Quantitative Reasoning with Data","abstract":"Quantitative reasoning is a critical skill to analyze data, yet the assessment of such ability remains limited. To address this gap, we introduce the Quantitative Reasoning with Data (QRData) benchmark, aiming to evaluate Large Language Models' capability in statistical and causal reasoning with real-world data. The benchmark comprises a carefully constructed dataset of 411 questions accompanied by data sheets from textbooks, online learning materials, and academic papers. To compare models' quantitative reasoning abilities on data and text, we enrich the benchmark with an auxiliary set of 290 text-only questions, namely QRText. We evaluate natural language reasoning, program-based reasoning, and agent reasoning methods including Chain-of-Thought, Program-of-Thoughts, ReAct, and code interpreter assistants on diverse models. The strongest model GPT-4 achieves an accuracy of 58%, which has a large room for improvement. Among open-source models, Deepseek-coder-instruct, a code LLM pretrained on 2T tokens, gets the highest accuracy of 37%. Analysis reveals that models encounter difficulties in data analysis and causal reasoning, and struggle in using causal knowledge and provided data simultaneously. Code and data are in https://github.com/xxxiaol/QRData.","sentences":["Quantitative reasoning is a critical skill to analyze data, yet the assessment of such ability remains limited.","To address this gap, we introduce the Quantitative Reasoning with Data (QRData) benchmark, aiming to evaluate Large Language Models' capability in statistical and causal reasoning with real-world data.","The benchmark comprises a carefully constructed dataset of 411 questions accompanied by data sheets from textbooks, online learning materials, and academic papers.","To compare models' quantitative reasoning abilities on data and text, we enrich the benchmark with an auxiliary set of 290 text-only questions, namely QRText.","We evaluate natural language reasoning, program-based reasoning, and agent reasoning methods including Chain-of-Thought, Program-of-Thoughts, ReAct, and code interpreter assistants on diverse models.","The strongest model GPT-4 achieves an accuracy of 58%, which has a large room for improvement.","Among open-source models, Deepseek-coder-instruct, a code LLM pretrained on 2T tokens, gets the highest accuracy of 37%.","Analysis reveals that models encounter difficulties in data analysis and causal reasoning, and struggle in using causal knowledge and provided data simultaneously.","Code and data are in https://github.com/xxxiaol/QRData."],"url":"http://arxiv.org/abs/2402.17644v1"}
{"created":"2024-02-27 16:11:05","title":"Variational Learning is Effective for Large Deep Networks","abstract":"We give extensive empirical evidence against the common belief that variational learning is ineffective for large neural networks. We show that an optimizer called Improved Variational Online Newton (IVON) consistently matches or outperforms Adam for training large networks such as GPT-2 and ResNets from scratch. IVON's computational costs are nearly identical to Adam but its predictive uncertainty is better. We show several new use cases of IVON where we improve fine-tuning and model merging in Large Language Models, accurately predict generalization error, and faithfully estimate sensitivity to data. We find overwhelming evidence in support of effectiveness of variational learning.","sentences":["We give extensive empirical evidence against the common belief that variational learning is ineffective for large neural networks.","We show that an optimizer called Improved Variational Online Newton (IVON) consistently matches or outperforms Adam for training large networks such as GPT-2 and ResNets from scratch.","IVON's computational costs are nearly identical to Adam but its predictive uncertainty is better.","We show several new use cases of IVON where we improve fine-tuning and model merging in Large Language Models, accurately predict generalization error, and faithfully estimate sensitivity to data.","We find overwhelming evidence in support of effectiveness of variational learning."],"url":"http://arxiv.org/abs/2402.17641v1"}
{"created":"2024-02-27 15:59:37","title":"From Text Segmentation to Smart Chaptering: A Novel Benchmark for Structuring Video Transcriptions","abstract":"Text segmentation is a fundamental task in natural language processing, where documents are split into contiguous sections. However, prior research in this area has been constrained by limited datasets, which are either small in scale, synthesized, or only contain well-structured documents. In this paper, we address these limitations by introducing a novel benchmark YTSeg focusing on spoken content that is inherently more unstructured and both topically and structurally diverse. As part of this work, we introduce an efficient hierarchical segmentation model MiniSeg, that outperforms state-of-the-art baselines. Lastly, we expand the notion of text segmentation to a more practical \"smart chaptering\" task that involves the segmentation of unstructured content, the generation of meaningful segment titles, and a potential real-time application of the models.","sentences":["Text segmentation is a fundamental task in natural language processing, where documents are split into contiguous sections.","However, prior research in this area has been constrained by limited datasets, which are either small in scale, synthesized, or only contain well-structured documents.","In this paper, we address these limitations by introducing a novel benchmark YTSeg focusing on spoken content that is inherently more unstructured and both topically and structurally diverse.","As part of this work, we introduce an efficient hierarchical segmentation model MiniSeg, that outperforms state-of-the-art baselines.","Lastly, we expand the notion of text segmentation to a more practical \"smart chaptering\" task that involves the segmentation of unstructured content, the generation of meaningful segment titles, and a potential real-time application of the models."],"url":"http://arxiv.org/abs/2402.17633v1"}
{"created":"2024-02-27 15:58:32","title":"Securing OPEN-RAN Equipment Using Blockchain-Based Supply Chain Verification","abstract":"The disaggregated and multi-vendor nature of OPEN-RAN networks introduces new supply chain security risks, making equipment authenticity and integrity crucial challenges. Robust solutions are needed to mitigate vulnerabilities in manufacturing and integration. This paper puts forth a novel blockchain-based approach to secure OPEN-RAN equipment through its lifecycle. By combining firmware authentication codes, a permissioned blockchain ledger, and equipment node validators, we architect a tamper-resistant ecosystem to track provenance. The outlined design, while conceptual, establishes a foundation and roadmap for future realization. Through careful implementation planning, development of core components like firmware signed hashes and smart contracts, and rigorous performance evaluation, this paper can evolve from concept to practice. There is a vivid potential to make OPEN-RAN supply chains corner to corner secure, igniting further research and real-world deployment.","sentences":["The disaggregated and multi-vendor nature of OPEN-RAN networks introduces new supply chain security risks, making equipment authenticity and integrity crucial challenges.","Robust solutions are needed to mitigate vulnerabilities in manufacturing and integration.","This paper puts forth a novel blockchain-based approach to secure OPEN-RAN equipment through its lifecycle.","By combining firmware authentication codes, a permissioned blockchain ledger, and equipment node validators, we architect a tamper-resistant ecosystem to track provenance.","The outlined design, while conceptual, establishes a foundation and roadmap for future realization.","Through careful implementation planning, development of core components like firmware signed hashes and smart contracts, and rigorous performance evaluation, this paper can evolve from concept to practice.","There is a vivid potential to make OPEN-RAN supply chains corner to corner secure, igniting further research and real-world deployment."],"url":"http://arxiv.org/abs/2402.17632v1"}
{"created":"2024-02-27 15:57:31","title":"Deterministic Cache-Oblivious Funnelselect","abstract":"In the multiple-selection problem one is given an unsorted array $S$ of $N$ elements and an array of $q$ query ranks $r_1<\\cdots<r_q$, and the task is to return, in sorted order, the $q$ elements in $S$ of rank $r_1, \\ldots, r_q$, respectively. The asymptotic deterministic comparison complexity of the problem was settled by Dobkin and Munro [JACM 1981]. In the I/O model an optimal I/O complexity was achieved by Hu et al. [SPAA 2014]. Recently [ESA 2023], we presented a cache-oblivious algorithm with matching I/O complexity, named funnelselect, since it heavily borrows ideas from the cache-oblivious sorting algorithm funnelsort from the seminal paper by Frigo, Leiserson, Prokop and Ramachandran [FOCS 1999]. Funnelselect is inherently randomized as it relies on sampling for cheaply finding many good pivots. In this paper we present deterministic funnelselect, achieving the same optional I/O complexity cache-obliviously without randomization. Our new algorithm essentially replaces a single (in expectation) reversed-funnel computation using random pivots by a recursive algorithm using multiple reversed-funnel computations. To meet the I/O bound, this requires a carefully chosen subproblem size based on the entropy of the sequence of query ranks; deterministic funnelselect thus raises distinct technical challenges not met by randomized funnelselect. The resulting worst-case I/O bound is $O\\bigl(\\sum_{i=1}^{q+1} \\frac{\\Delta_i}{B} \\cdot \\log_{M/B} \\frac{N}{\\Delta_i} + \\frac{N}{B}\\bigr)$, where $B$ is the external memory block size, $M\\geq B^{1+\\epsilon}$ is the internal memory size, for some constant $\\epsilon>0$, and $\\Delta_i = r_{i} - r_{i-1}$ (assuming $r_0=0$ and $r_{q+1}=N + 1$).","sentences":["In the multiple-selection problem one is given an unsorted array $S$ of $N$ elements and an array of $q$ query ranks $r_1<\\cdots<r_q$, and the task is to return, in sorted order, the $q$ elements in $S$ of rank $r_1, \\ldots, r_q$, respectively.","The asymptotic deterministic comparison complexity of the problem was settled by Dobkin and Munro","[JACM 1981].","In the I/O model an optimal I/O complexity was achieved by Hu et al.","[SPAA 2014].","Recently [ESA 2023], we presented a cache-oblivious algorithm with matching I/O complexity, named funnelselect, since it heavily borrows ideas from the cache-oblivious sorting algorithm funnelsort from the seminal paper by Frigo, Leiserson, Prokop and Ramachandran","[FOCS 1999].","Funnelselect is inherently randomized as it relies on sampling for cheaply finding many good pivots.","In this paper we present deterministic funnelselect, achieving the same optional I/O complexity cache-obliviously without randomization.","Our new algorithm essentially replaces a single (in expectation) reversed-funnel computation using random pivots by a recursive algorithm using multiple reversed-funnel computations.","To meet the I/O bound, this requires a carefully chosen subproblem size based on the entropy of the sequence of query ranks; deterministic funnelselect thus raises distinct technical challenges not met by randomized funnelselect.","The resulting worst-case I/O bound is $O\\bigl(\\sum_{i=1}^{q+1} \\frac{\\Delta_i}{B} \\cdot \\log_{M/B} \\frac{N}{\\Delta_i} + \\frac{N}{B}\\bigr)$, where $B$ is the external memory block size, $M\\geq B^{1+\\epsilon}$ is the internal memory size, for some constant $\\epsilon>0$, and $\\Delta_i = r_{i} - r_{i-1}$ (assuming $r_0=0$ and $r_{q+1}=N + 1$)."],"url":"http://arxiv.org/abs/2402.17631v1"}
{"created":"2024-02-27 15:57:11","title":"Fine-Grained Natural Language Inference Based Faithfulness Evaluation for Diverse Summarisation Tasks","abstract":"We study existing approaches to leverage off-the-shelf Natural Language Inference (NLI) models for the evaluation of summary faithfulness and argue that these are sub-optimal due to the granularity level considered for premises and hypotheses. That is, the smaller content unit considered as hypothesis is a sentence and premises are made up of a fixed number of document sentences. We propose a novel approach, namely InFusE, that uses a variable premise size and simplifies summary sentences into shorter hypotheses. Departing from previous studies which focus on single short document summarisation, we analyse NLI based faithfulness evaluation for diverse summarisation tasks. We introduce DiverSumm, a new benchmark comprising long form summarisation (long documents and summaries) and diverse summarisation tasks (e.g., meeting and multi-document summarisation). In experiments, InFusE obtains superior performance across the different summarisation tasks. Our code and data are available at https://github.com/HJZnlp/infuse.","sentences":["We study existing approaches to leverage off-the-shelf Natural Language Inference (NLI) models for the evaluation of summary faithfulness and argue that these are sub-optimal due to the granularity level considered for premises and hypotheses.","That is, the smaller content unit considered as hypothesis is a sentence and premises are made up of a fixed number of document sentences.","We propose a novel approach, namely InFusE, that uses a variable premise size and simplifies summary sentences into shorter hypotheses.","Departing from previous studies which focus on single short document summarisation, we analyse NLI based faithfulness evaluation for diverse summarisation tasks.","We introduce DiverSumm, a new benchmark comprising long form summarisation (long documents and summaries) and diverse summarisation tasks (e.g., meeting and multi-document summarisation).","In experiments, InFusE obtains superior performance across the different summarisation tasks.","Our code and data are available at https://github.com/HJZnlp/infuse."],"url":"http://arxiv.org/abs/2402.17630v1"}
{"created":"2024-02-27 15:52:59","title":"CustomSketching: Sketch Concept Extraction for Sketch-based Image Synthesis and Editing","abstract":"Personalization techniques for large text-to-image (T2I) models allow users to incorporate new concepts from reference images. However, existing methods primarily rely on textual descriptions, leading to limited control over customized images and failing to support fine-grained and local editing (e.g., shape, pose, and details). In this paper, we identify sketches as an intuitive and versatile representation that can facilitate such control, e.g., contour lines capturing shape information and flow lines representing texture. This motivates us to explore a novel task of sketch concept extraction: given one or more sketch-image pairs, we aim to extract a special sketch concept that bridges the correspondence between the images and sketches, thus enabling sketch-based image synthesis and editing at a fine-grained level. To accomplish this, we introduce CustomSketching, a two-stage framework for extracting novel sketch concepts. Considering that an object can often be depicted by a contour for general shapes and additional strokes for internal details, we introduce a dual-sketch representation to reduce the inherent ambiguity in sketch depiction. We employ a shape loss and a regularization loss to balance fidelity and editability during optimization. Through extensive experiments, a user study, and several applications, we show our method is effective and superior to the adapted baselines.","sentences":["Personalization techniques for large text-to-image (T2I) models allow users to incorporate new concepts from reference images.","However, existing methods primarily rely on textual descriptions, leading to limited control over customized images and failing to support fine-grained and local editing (e.g., shape, pose, and details).","In this paper, we identify sketches as an intuitive and versatile representation that can facilitate such control, e.g., contour lines capturing shape information and flow lines representing texture.","This motivates us to explore a novel task of sketch concept extraction: given one or more sketch-image pairs, we aim to extract a special sketch concept that bridges the correspondence between the images and sketches, thus enabling sketch-based image synthesis and editing at a fine-grained level.","To accomplish this, we introduce CustomSketching, a two-stage framework for extracting novel sketch concepts.","Considering that an object can often be depicted by a contour for general shapes and additional strokes for internal details, we introduce a dual-sketch representation to reduce the inherent ambiguity in sketch depiction.","We employ a shape loss and a regularization loss to balance fidelity and editability during optimization.","Through extensive experiments, a user study, and several applications, we show our method is effective and superior to the adapted baselines."],"url":"http://arxiv.org/abs/2402.17624v1"}
{"created":"2024-02-27 15:49:54","title":"Masked Gamma-SSL: Learning Uncertainty Estimation via Masked Image Modeling","abstract":"This work proposes a semantic segmentation network that produces high-quality uncertainty estimates in a single forward pass. We exploit general representations from foundation models and unlabelled datasets through a Masked Image Modeling (MIM) approach, which is robust to augmentation hyper-parameters and simpler than previous techniques. For neural networks used in safety-critical applications, bias in the training data can lead to errors; therefore it is crucial to understand a network's limitations at run time and act accordingly. To this end, we test our proposed method on a number of test domains including the SAX Segmentation benchmark, which includes labelled test data from dense urban, rural and off-road driving domains. The proposed method consistently outperforms uncertainty estimation and Out-of-Distribution (OoD) techniques on this difficult benchmark.","sentences":["This work proposes a semantic segmentation network that produces high-quality uncertainty estimates in a single forward pass.","We exploit general representations from foundation models and unlabelled datasets through a Masked Image Modeling (MIM) approach, which is robust to augmentation hyper-parameters and simpler than previous techniques.","For neural networks used in safety-critical applications, bias in the training data can lead to errors; therefore it is crucial to understand a network's limitations at run time and act accordingly.","To this end, we test our proposed method on a number of test domains including the SAX Segmentation benchmark, which includes labelled test data from dense urban, rural and off-road driving domains.","The proposed method consistently outperforms uncertainty estimation and Out-of-Distribution (OoD) techniques on this difficult benchmark."],"url":"http://arxiv.org/abs/2402.17622v1"}
{"created":"2024-02-27 15:44:12","title":"A Multi-Agent Model for Opinion Evolution under Cognitive Biases","abstract":"We generalize the DeGroot model for opinion dynamics to better capture realistic social scenarios. We introduce a model where each agent has their own individual cognitive biases. Society is represented as a directed graph whose edges indicate how much agents influence one another. Biases are represented as the functions in the square region $[-1,1]^2$ and categorized into four sub-regions based on the potential reactions they may elicit in an agent during instances of opinion disagreement. Under the assumption that each bias of every agent is a continuous function within the region of receptive but resistant reactions ($\\mathbf{R}$), we show that the society converges to a consensus if the graph is strongly connected. Under the same assumption, we also establish that the entire society converges to a unanimous opinion if and only if the source components of the graph-namely, strongly connected components with no external influence-converge to that opinion. We illustrate that convergence is not guaranteed for strongly connected graphs when biases are either discontinuous functions in $\\mathbf{R}$ or not included in $\\mathbf{R}$. We showcase our model through a series of examples and simulations, offering insights into how opinions form in social networks under cognitive biases.","sentences":["We generalize the DeGroot model for opinion dynamics to better capture realistic social scenarios.","We introduce a model where each agent has their own individual cognitive biases.","Society is represented as a directed graph whose edges indicate how much agents influence one another.","Biases are represented as the functions in the square region $","[-1,1]^2$ and categorized into four sub-regions based on the potential reactions they may elicit in an agent during instances of opinion disagreement.","Under the assumption that each bias of every agent is a continuous function within the region of receptive but resistant reactions ($\\mathbf{R}$), we show that the society converges to a consensus if the graph is strongly connected.","Under the same assumption, we also establish that the entire society converges to a unanimous opinion if and only if the source components of the graph-namely, strongly connected components with no external influence-converge to that opinion.","We illustrate that convergence is not guaranteed for strongly connected graphs when biases are either discontinuous functions in $\\mathbf{R}$ or not included in $\\mathbf{R}$. We showcase our model through a series of examples and simulations, offering insights into how opinions form in social networks under cognitive biases."],"url":"http://arxiv.org/abs/2402.17615v1"}
{"created":"2024-02-27 15:43:53","title":"Adapt Before Comparison: A New Perspective on Cross-Domain Few-Shot Segmentation","abstract":"Few-shot segmentation performance declines substantially when facing images from a domain different than the training domain, effectively limiting real-world use cases. To alleviate this, recently cross-domain few-shot segmentation (CD-FSS) has emerged. Works that address this task mainly attempted to learn segmentation on a source domain in a manner that generalizes across domains. Surprisingly, we can outperform these approaches while eliminating the training stage and removing their main segmentation network. We show test-time task-adaption is the key for successful CD-FSS instead. Task-adaption is achieved by appending small networks to the feature pyramid of a conventionally classification-pretrained backbone. To avoid overfitting to the few labeled samples in supervised fine-tuning, consistency across augmented views of input images serves as guidance while learning the parameters of the attached layers. Despite our self-restriction not to use any images other than the few labeled samples at test time, we achieve new state-of-the-art performance in CD-FSS, evidencing the need to rethink approaches for the task.","sentences":["Few-shot segmentation performance declines substantially when facing images from a domain different than the training domain, effectively limiting real-world use cases.","To alleviate this, recently cross-domain few-shot segmentation (CD-FSS) has emerged.","Works that address this task mainly attempted to learn segmentation on a source domain in a manner that generalizes across domains.","Surprisingly, we can outperform these approaches while eliminating the training stage and removing their main segmentation network.","We show test-time task-adaption is the key for successful CD-FSS instead.","Task-adaption is achieved by appending small networks to the feature pyramid of a conventionally classification-pretrained backbone.","To avoid overfitting to the few labeled samples in supervised fine-tuning, consistency across augmented views of input images serves as guidance while learning the parameters of the attached layers.","Despite our self-restriction not to use any images other than the few labeled samples at test time, we achieve new state-of-the-art performance in CD-FSS, evidencing the need to rethink approaches for the task."],"url":"http://arxiv.org/abs/2402.17614v1"}
{"created":"2024-02-27 15:42:33","title":"Neural Automated Writing Evaluation with Corrective Feedback","abstract":"The utilization of technology in second language learning and teaching has become ubiquitous. For the assessment of writing specifically, automated writing evaluation (AWE) and grammatical error correction (GEC) have become immensely popular and effective methods for enhancing writing proficiency and delivering instant and individualized feedback to learners. By leveraging the power of natural language processing (NLP) and machine learning algorithms, AWE and GEC systems have been developed separately to provide language learners with automated corrective feedback and more accurate and unbiased scoring that would otherwise be subject to examiners. In this paper, we propose an integrated system for automated writing evaluation with corrective feedback as a means of bridging the gap between AWE and GEC results for second language learners. This system enables language learners to simulate the essay writing tests: a student writes and submits an essay, and the system returns the assessment of the writing along with suggested grammatical error corrections. Given that automated scoring and grammatical correction are more efficient and cost-effective than human grading, this integrated system would also alleviate the burden of manually correcting innumerable essays.","sentences":["The utilization of technology in second language learning and teaching has become ubiquitous.","For the assessment of writing specifically, automated writing evaluation (AWE) and grammatical error correction (GEC) have become immensely popular and effective methods for enhancing writing proficiency and delivering instant and individualized feedback to learners.","By leveraging the power of natural language processing (NLP) and machine learning algorithms, AWE and GEC systems have been developed separately to provide language learners with automated corrective feedback and more accurate and unbiased scoring that would otherwise be subject to examiners.","In this paper, we propose an integrated system for automated writing evaluation with corrective feedback as a means of bridging the gap between AWE and GEC results for second language learners.","This system enables language learners to simulate the essay writing tests: a student writes and submits an essay, and the system returns the assessment of the writing along with suggested grammatical error corrections.","Given that automated scoring and grammatical correction are more efficient and cost-effective than human grading, this integrated system would also alleviate the burden of manually correcting innumerable essays."],"url":"http://arxiv.org/abs/2402.17613v1"}
{"created":"2024-02-27 15:37:15","title":"A Large-scale Evaluation of Pretraining Paradigms for the Detection of Defects in Electroluminescence Solar Cell Images","abstract":"Pretraining has been shown to improve performance in many domains, including semantic segmentation, especially in domains with limited labelled data. In this work, we perform a large-scale evaluation and benchmarking of various pretraining methods for Solar Cell Defect Detection (SCDD) in electroluminescence images, a field with limited labelled datasets. We cover supervised training with semantic segmentation, semi-supervised learning, and two self-supervised techniques. We also experiment with both in-distribution and out-of-distribution (OOD) pretraining and observe how this affects downstream performance. The results suggest that supervised training on a large OOD dataset (COCO), self-supervised pretraining on a large OOD dataset (ImageNet), and semi-supervised pretraining (CCT) all yield statistically equivalent performance for mean Intersection over Union (mIoU). We achieve a new state-of-the-art for SCDD and demonstrate that certain pretraining schemes result in superior performance on underrepresented classes. Additionally, we provide a large-scale unlabelled EL image dataset of $22000$ images, and a $642$-image labelled semantic segmentation EL dataset, for further research in developing self- and semi-supervised training techniques in this domain.","sentences":["Pretraining has been shown to improve performance in many domains, including semantic segmentation, especially in domains with limited labelled data.","In this work, we perform a large-scale evaluation and benchmarking of various pretraining methods for Solar Cell Defect Detection (SCDD) in electroluminescence images, a field with limited labelled datasets.","We cover supervised training with semantic segmentation, semi-supervised learning, and two self-supervised techniques.","We also experiment with both in-distribution and out-of-distribution (OOD) pretraining and observe how this affects downstream performance.","The results suggest that supervised training on a large OOD dataset (COCO), self-supervised pretraining on a large OOD dataset (ImageNet), and semi-supervised pretraining (CCT) all yield statistically equivalent performance for mean Intersection over Union (mIoU).","We achieve a new state-of-the-art for SCDD and demonstrate that certain pretraining schemes result in superior performance on underrepresented classes.","Additionally, we provide a large-scale unlabelled EL image dataset of $22000$ images, and a $642$-image labelled semantic segmentation EL dataset, for further research in developing self- and semi-supervised training techniques in this domain."],"url":"http://arxiv.org/abs/2402.17611v1"}
{"created":"2024-02-27 15:34:15","title":"Linguistic Knowledge Can Enhance Encoder-Decoder Models (If You Let It)","abstract":"In this paper, we explore the impact of augmenting pre-trained Encoder-Decoder models, specifically T5, with linguistic knowledge for the prediction of a target task. In particular, we investigate whether fine-tuning a T5 model on an intermediate task that predicts structural linguistic properties of sentences modifies its performance in the target task of predicting sentence-level complexity. Our study encompasses diverse experiments conducted on Italian and English datasets, employing both monolingual and multilingual T5 models at various sizes. Results obtained for both languages and in cross-lingual configurations show that linguistically motivated intermediate fine-tuning has generally a positive impact on target task performance, especially when applied to smaller models and in scenarios with limited data availability.","sentences":["In this paper, we explore the impact of augmenting pre-trained Encoder-Decoder models, specifically T5, with linguistic knowledge for the prediction of a target task.","In particular, we investigate whether fine-tuning a T5 model on an intermediate task that predicts structural linguistic properties of sentences modifies its performance in the target task of predicting sentence-level complexity.","Our study encompasses diverse experiments conducted on Italian and English datasets, employing both monolingual and multilingual T5 models at various sizes.","Results obtained for both languages and in cross-lingual configurations show that linguistically motivated intermediate fine-tuning has generally a positive impact on target task performance, especially when applied to smaller models and in scenarios with limited data availability."],"url":"http://arxiv.org/abs/2402.17608v1"}
{"created":"2024-02-27 15:33:20","title":"Learning Topological Representations with Bidirectional Graph Attention Network for Solving Job Shop Scheduling Problem","abstract":"Existing learning-based methods for solving job shop scheduling problem (JSSP) usually use off-the-shelf GNN models tailored to undirected graphs and neglect the rich and meaningful topological structures of disjunctive graphs (DGs). This paper proposes the topology-aware bidirectional graph attention network (TBGAT), a novel GNN architecture based on the attention mechanism, to embed the DG for solving JSSP in a local search framework. Specifically, TBGAT embeds the DG from a forward and a backward view, respectively, where the messages are propagated by following the different topologies of the views and aggregated via graph attention. Then, we propose a novel operator based on the message-passing mechanism to calculate the forward and backward topological sorts of the DG, which are the features for characterizing the topological structures and exploited by our model. In addition, we theoretically and experimentally show that TBGAT has linear computational complexity to the number of jobs and machines, respectively, which strengthens the practical value of our method. Besides, extensive experiments on five synthetic datasets and seven classic benchmarks show that TBGAT achieves new SOTA results by outperforming a wide range of neural methods by a large margin.","sentences":["Existing learning-based methods for solving job shop scheduling problem (JSSP) usually use off-the-shelf GNN models tailored to undirected graphs and neglect the rich and meaningful topological structures of disjunctive graphs (DGs).","This paper proposes the topology-aware bidirectional graph attention network (TBGAT), a novel GNN architecture based on the attention mechanism, to embed the DG for solving JSSP in a local search framework.","Specifically, TBGAT embeds the DG from a forward and a backward view, respectively, where the messages are propagated by following the different topologies of the views and aggregated via graph attention.","Then, we propose a novel operator based on the message-passing mechanism to calculate the forward and backward topological sorts of the DG, which are the features for characterizing the topological structures and exploited by our model.","In addition, we theoretically and experimentally show that TBGAT has linear computational complexity to the number of jobs and machines, respectively, which strengthens the practical value of our method.","Besides, extensive experiments on five synthetic datasets and seven classic benchmarks show that TBGAT achieves new SOTA results by outperforming a wide range of neural methods by a large margin."],"url":"http://arxiv.org/abs/2402.17606v1"}
{"created":"2024-02-27 15:31:00","title":"Equivariant ideals of polynomials","abstract":"We study existence and computability of finite bases for ideals of polynomials over infinitely many variables. In our setting, variables come from a countable logical structure A, and embeddings from A to A act on polynomials by renaming variables. First, we give a sufficient and necessary condition for A to guarantee the following generalisation of Hilbert's Basis Theorem: every polynomial ideal which is equivariant, i.e. invariant under renaming of variables, is finitely generated. Second, we develop an extension of classical Buchberger's algorithm to compute a Gr\\\"obner basis of a given equivariant ideal. This implies decidability of the membership problem for equivariant ideals. Finally, we sketch upon various applications of these results to register automata, Petri nets with data, orbit-finitely generated vector spaces, and orbit-finite systems of linear equations.","sentences":["We study existence and computability of finite bases for ideals of polynomials over infinitely many variables.","In our setting, variables come from a countable logical structure A, and embeddings from A to A act on polynomials by renaming variables.","First, we give a sufficient and necessary condition for A to guarantee the following generalisation of Hilbert's Basis Theorem: every polynomial ideal which is equivariant, i.e. invariant under renaming of variables, is finitely generated.","Second, we develop an extension of classical Buchberger's algorithm to compute a Gr\\\"obner basis of a given equivariant ideal.","This implies decidability of the membership problem for equivariant ideals.","Finally, we sketch upon various applications of these results to register automata, Petri nets with data, orbit-finitely generated vector spaces, and orbit-finite systems of linear equations."],"url":"http://arxiv.org/abs/2402.17604v1"}
{"created":"2024-02-27 15:30:01","title":"Advancing sleep detection by modelling weak label sets: A novel weakly supervised learning approach","abstract":"Understanding sleep and activity patterns plays a crucial role in physical and mental health. This study introduces a novel approach for sleep detection using weakly supervised learning for scenarios where reliable ground truth labels are unavailable. The proposed method relies on a set of weak labels, derived from the predictions generated by conventional sleep detection algorithms. Introducing a novel approach, we suggest a novel generalised non-linear statistical model in which the number of weak sleep labels is modelled as outcome of a binomial distribution. The probability of sleep in the binomial distribution is linked to the outcomes of neural networks trained to detect sleep based on actigraphy. We show that maximizing the likelihood function of the model, is equivalent to minimizing the soft cross-entropy loss. Additionally, we explored the use of the Brier score as a loss function for weak labels. The efficacy of the suggested modelling framework was demonstrated using the Multi-Ethnic Study of Atherosclerosis dataset. A \\gls{lstm} trained on the soft cross-entropy outperformed conventional sleep detection algorithms, other neural network architectures and loss functions in accuracy and model calibration. This research not only advances sleep detection techniques in scenarios where ground truth data is scarce but also contributes to the broader field of weakly supervised learning by introducing innovative approach in modelling sets of weak labels.","sentences":["Understanding sleep and activity patterns plays a crucial role in physical and mental health.","This study introduces a novel approach for sleep detection using weakly supervised learning for scenarios where reliable ground truth labels are unavailable.","The proposed method relies on a set of weak labels, derived from the predictions generated by conventional sleep detection algorithms.","Introducing a novel approach, we suggest a novel generalised non-linear statistical model in which the number of weak sleep labels is modelled as outcome of a binomial distribution.","The probability of sleep in the binomial distribution is linked to the outcomes of neural networks trained to detect sleep based on actigraphy.","We show that maximizing the likelihood function of the model, is equivalent to minimizing the soft cross-entropy loss.","Additionally, we explored the use of the Brier score as a loss function for weak labels.","The efficacy of the suggested modelling framework was demonstrated using the Multi-Ethnic Study of Atherosclerosis dataset.","A \\gls{lstm} trained on the soft cross-entropy outperformed conventional sleep detection algorithms, other neural network architectures and loss functions in accuracy and model calibration.","This research not only advances sleep detection techniques in scenarios where ground truth data is scarce but also contributes to the broader field of weakly supervised learning by introducing innovative approach in modelling sets of weak labels."],"url":"http://arxiv.org/abs/2402.17601v1"}
{"created":"2024-02-27 15:28:46","title":"A duality for nonabelian group codes","abstract":"In 1962, Jesse MacWilliams published a set of formulas for linear and abelian group codes that among other applications, were incredibly valuable in the study of self-dual codes. Now called the MacWilliams Identities, her results relate the weight enumerator and complete weight enumerator of a code to those of its dual code. A similar set of MacWilliams identities has been proven to exist for many other types of codes. In 2013, Dougherty, Sol\\'{e}, and Kim published a list of fundamental open questions in coding theory. Among them, Open Question 4.3: \"Is there a duality and MacWilliams formula for codes over non-Abelian groups?\" In this paper, we propose a duality for nonabelian group codes in terms of the irreducible representations of the group. We show that there is a Greene's Theorem and MacWilliams Identities which hold for this notion of duality. When the group is abelian, our results are equivalent to existing formulas in the literature.","sentences":["In 1962, Jesse MacWilliams published a set of formulas for linear and abelian group codes that among other applications, were incredibly valuable in the study of self-dual codes.","Now called the MacWilliams Identities, her results relate the weight enumerator and complete weight enumerator of a code to those of its dual code.","A similar set of MacWilliams identities has been proven to exist for many other types of codes.","In 2013, Dougherty, Sol\\'{e}, and Kim published a list of fundamental open questions in coding theory.","Among them, Open Question 4.3: \"Is there a duality and MacWilliams formula for codes over non-Abelian groups?\"","In this paper, we propose a duality for nonabelian group codes in terms of the irreducible representations of the group.","We show that there is a Greene's Theorem and MacWilliams Identities which hold for this notion of duality.","When the group is abelian, our results are equivalent to existing formulas in the literature."],"url":"http://arxiv.org/abs/2402.17597v1"}
{"created":"2024-02-27 15:28:19","title":"Corridor MPC for Multi-Agent Inspection of Orbiting Structures","abstract":"In this work, we propose an extension of the previously introduced Corridor Model Predictive Control scheme for high-order and distributed systems, with an application for on-orbit inspection. To this end, we leverage high order control barrier function (HOCBF) constraints as a suitable control approach to maintain each agent in the formation within a safe corridor from its reference trajectory. The recursive feasibility of the designed MPC scheme is tested numerically, while suitable modifications of the classical HOCBF constraint definition are introduced such that safety is guaranteed both in sampled and continuous time. The designed controller is validated through computer simulation in a realistic inspection scenario of the International Space Station.","sentences":["In this work, we propose an extension of the previously introduced Corridor Model Predictive Control scheme for high-order and distributed systems, with an application for on-orbit inspection.","To this end, we leverage high order control barrier function (HOCBF) constraints as a suitable control approach to maintain each agent in the formation within a safe corridor from its reference trajectory.","The recursive feasibility of the designed MPC scheme is tested numerically, while suitable modifications of the classical HOCBF constraint definition are introduced such that safety is guaranteed both in sampled and continuous time.","The designed controller is validated through computer simulation in a realistic inspection scenario of the International Space Station."],"url":"http://arxiv.org/abs/2402.17596v1"}
{"created":"2024-02-27 15:28:01","title":"Implicit Regularization via Spectral Neural Networks and Non-linear Matrix Sensing","abstract":"The phenomenon of implicit regularization has attracted interest in recent years as a fundamental aspect of the remarkable generalizing ability of neural networks. In a nutshell, it entails that gradient descent dynamics in many neural nets, even without any explicit regularizer in the loss function, converges to the solution of a regularized learning problem. However, known results attempting to theoretically explain this phenomenon focus overwhelmingly on the setting of linear neural nets, and the simplicity of the linear structure is particularly crucial to existing arguments. In this paper, we explore this problem in the context of more realistic neural networks with a general class of non-linear activation functions, and rigorously demonstrate the implicit regularization phenomenon for such networks in the setting of matrix sensing problems, together with rigorous rate guarantees that ensure exponentially fast convergence of gradient descent.In this vein, we contribute a network architecture called Spectral Neural Networks (abbrv. SNN) that is particularly suitable for matrix learning problems. Conceptually, this entails coordinatizing the space of matrices by their singular values and singular vectors, as opposed to by their entries, a potentially fruitful perspective for matrix learning. We demonstrate that the SNN architecture is inherently much more amenable to theoretical analysis than vanilla neural nets and confirm its effectiveness in the context of matrix sensing, via both mathematical guarantees and empirical investigations. We believe that the SNN architecture has the potential to be of wide applicability in a broad class of matrix learning scenarios.","sentences":["The phenomenon of implicit regularization has attracted interest in recent years as a fundamental aspect of the remarkable generalizing ability of neural networks.","In a nutshell, it entails that gradient descent dynamics in many neural nets, even without any explicit regularizer in the loss function, converges to the solution of a regularized learning problem.","However, known results attempting to theoretically explain this phenomenon focus overwhelmingly on the setting of linear neural nets, and the simplicity of the linear structure is particularly crucial to existing arguments.","In this paper, we explore this problem in the context of more realistic neural networks with a general class of non-linear activation functions, and rigorously demonstrate the implicit regularization phenomenon for such networks in the setting of matrix sensing problems, together with rigorous rate guarantees that ensure exponentially fast convergence of gradient descent.","In this vein, we contribute a network architecture called Spectral Neural Networks (abbrv.","SNN) that is particularly suitable for matrix learning problems.","Conceptually, this entails coordinatizing the space of matrices by their singular values and singular vectors, as opposed to by their entries, a potentially fruitful perspective for matrix learning.","We demonstrate that the SNN architecture is inherently much more amenable to theoretical analysis than vanilla neural nets and confirm its effectiveness in the context of matrix sensing, via both mathematical guarantees and empirical investigations.","We believe that the SNN architecture has the potential to be of wide applicability in a broad class of matrix learning scenarios."],"url":"http://arxiv.org/abs/2402.17595v1"}
{"created":"2024-02-27 15:25:05","title":"Autonomous Shuttle Operation for Vulnerable Populations: Lessons and Experiences","abstract":"The increasing shortage of drivers poses a significant threat to vulnerable populations, particularly seniors and disabled individuals who heavily depend on public transportation for accessing healthcare services and social events. Autonomous Vehicles (AVs) emerge as a promising alternative, offering potential improvements in accessibility and independence for these groups. However, current designs and studies often overlook the unique needs and experiences of these populations, leading to potential accessibility barriers. This paper presents a detailed case study of an autonomous shuttle test specifically tailored for seniors and disabled individuals, conducted during the early stages of the COVID-19 pandemic. The service, which lasted 13 weeks, catered to approximately 1500 passengers in an urban setting, aiming to facilitate access to essential services. Drawing from the safety operator's experiences and direct observations, we identify critical user experience and safety challenges faced by vulnerable passengers. Based on our findings, we propose targeted initiatives to enhance the safety, accessibility, and user education of AV technology for seniors and disabled individuals. These include increasing educational opportunities to familiarize these groups with AV technology, designing AVs with a focus on diversity and inclusion, and improving training programs for AV operators to address the unique needs of vulnerable populations. Through these initiatives, we aim to bridge the gap in AV accessibility and ensure that these technologies benefit all members of society.","sentences":["The increasing shortage of drivers poses a significant threat to vulnerable populations, particularly seniors and disabled individuals who heavily depend on public transportation for accessing healthcare services and social events.","Autonomous Vehicles (AVs) emerge as a promising alternative, offering potential improvements in accessibility and independence for these groups.","However, current designs and studies often overlook the unique needs and experiences of these populations, leading to potential accessibility barriers.","This paper presents a detailed case study of an autonomous shuttle test specifically tailored for seniors and disabled individuals, conducted during the early stages of the COVID-19 pandemic.","The service, which lasted 13 weeks, catered to approximately 1500 passengers in an urban setting, aiming to facilitate access to essential services.","Drawing from the safety operator's experiences and direct observations, we identify critical user experience and safety challenges faced by vulnerable passengers.","Based on our findings, we propose targeted initiatives to enhance the safety, accessibility, and user education of AV technology for seniors and disabled individuals.","These include increasing educational opportunities to familiarize these groups with AV technology, designing AVs with a focus on diversity and inclusion, and improving training programs for AV operators to address the unique needs of vulnerable populations.","Through these initiatives, we aim to bridge the gap in AV accessibility and ensure that these technologies benefit all members of society."],"url":"http://arxiv.org/abs/2402.17593v1"}
{"created":"2024-02-27 15:22:20","title":"PLReMix: Combating Noisy Labels with Pseudo-Label Relaxed Contrastive Representation Learning","abstract":"Recently, the application of Contrastive Representation Learning (CRL) in learning with noisy labels (LNL) has shown promising advancements due to its remarkable ability to learn well-distributed representations for better distinguishing noisy labels. However, CRL is mainly used as a pre-training technique, leading to a complicated multi-stage training pipeline. We also observed that trivially combining CRL with supervised LNL methods decreases performance. Using different images from the same class as negative pairs in CRL creates optimization conflicts between CRL and the supervised loss. To address these two issues, we propose an end-to-end PLReMix framework that avoids the complicated pipeline by introducing a Pseudo-Label Relaxed (PLR) contrastive loss to alleviate the conflicts between losses. This PLR loss constructs a reliable negative set of each sample by filtering out its inappropriate negative pairs that overlap at the top k indices of prediction probabilities, leading to more compact semantic clusters than vanilla CRL. Furthermore, a two-dimensional Gaussian Mixture Model (GMM) is adopted to distinguish clean and noisy samples by leveraging semantic information and model outputs simultaneously, which is expanded on the previously widely used one-dimensional form. The PLR loss and a semi-supervised loss are simultaneously applied to train on the GMM divided clean and noisy samples. Experiments on multiple benchmark datasets demonstrate the effectiveness of the proposed method. Our proposed PLR loss is scalable, which can be easily integrated into other LNL methods and boost their performance. Codes will be available.","sentences":["Recently, the application of Contrastive Representation Learning (CRL) in learning with noisy labels (LNL) has shown promising advancements due to its remarkable ability to learn well-distributed representations for better distinguishing noisy labels.","However, CRL is mainly used as a pre-training technique, leading to a complicated multi-stage training pipeline.","We also observed that trivially combining CRL with supervised LNL methods decreases performance.","Using different images from the same class as negative pairs in CRL creates optimization conflicts between CRL and the supervised loss.","To address these two issues, we propose an end-to-end PLReMix framework that avoids the complicated pipeline by introducing a Pseudo-Label Relaxed (PLR) contrastive loss to alleviate the conflicts between losses.","This PLR loss constructs a reliable negative set of each sample by filtering out its inappropriate negative pairs that overlap at the top k indices of prediction probabilities, leading to more compact semantic clusters than vanilla CRL.","Furthermore, a two-dimensional Gaussian Mixture Model (GMM) is adopted to distinguish clean and noisy samples by leveraging semantic information and model outputs simultaneously, which is expanded on the previously widely used one-dimensional form.","The PLR loss and a semi-supervised loss are simultaneously applied to train on the GMM divided clean and noisy samples.","Experiments on multiple benchmark datasets demonstrate the effectiveness of the proposed method.","Our proposed PLR loss is scalable, which can be easily integrated into other LNL methods and boost their performance.","Codes will be available."],"url":"http://arxiv.org/abs/2402.17589v1"}
{"created":"2024-02-27 15:20:11","title":"Chronicles of CI/CD: A Deep Dive into its Usage Over Time","abstract":"DevOps is a combination of methodologies and tools that improves the software development, build, deployment, and monitoring processes by shortening its lifecycle and improving software quality. Part of this process is CI/CD, which embodies mostly the first parts, right up to the deployment. Despite the many benefits of DevOps and CI/CD, it still presents many challenges promoted by the tremendous proliferation of different tools, languages, and syntaxes, which makes the field quite challenging to learn and keep up to date. Software repositories contain data regarding various software practices, tools, and uses. This data can help gather multiple insights that inform technical and academic decision-making. GitHub is currently the most popular software hosting platform and provides a search API that lets users query its repositories. Our goal with this paper is to gain insights into the technologies developers use for CI/CD by analyzing GitHub repositories. Using a list of the state-of-the-art CI/CD technologies, we use the GitHub search API to find repositories using each of these technologies. We also use the API to extract various insights regarding those repositories. We then organize and analyze the data collected. From our analysis, we provide an overview of the use of CI/CD technologies in our days, but also what happened in the last 12 years. We also show developers use several technologies simultaneously in the same project and that the change between technologies is quite common. From these insights, we find several research paths, from how to support the use of multiple technologies, both in terms of techniques, but also in terms of human-computer interaction, to aiding developers in evolving their CI/CD pipelines, again considering the various dimensions of the problem.","sentences":["DevOps is a combination of methodologies and tools that improves the software development, build, deployment, and monitoring processes by shortening its lifecycle and improving software quality.","Part of this process is CI/CD, which embodies mostly the first parts, right up to the deployment.","Despite the many benefits of DevOps and CI/CD, it still presents many challenges promoted by the tremendous proliferation of different tools, languages, and syntaxes, which makes the field quite challenging to learn and keep up to date.","Software repositories contain data regarding various software practices, tools, and uses.","This data can help gather multiple insights that inform technical and academic decision-making.","GitHub is currently the most popular software hosting platform and provides a search API that lets users query its repositories.","Our goal with this paper is to gain insights into the technologies developers use for CI/CD by analyzing GitHub repositories.","Using a list of the state-of-the-art CI/CD technologies, we use the GitHub search API to find repositories using each of these technologies.","We also use the API to extract various insights regarding those repositories.","We then organize and analyze the data collected.","From our analysis, we provide an overview of the use of CI/CD technologies in our days, but also what happened in the last 12 years.","We also show developers use several technologies simultaneously in the same project and that the change between technologies is quite common.","From these insights, we find several research paths, from how to support the use of multiple technologies, both in terms of techniques, but also in terms of human-computer interaction, to aiding developers in evolving their CI/CD pipelines, again considering the various dimensions of the problem."],"url":"http://arxiv.org/abs/2402.17588v1"}
{"created":"2024-02-27 15:14:19","title":"FaultProfIT: Hierarchical Fault Profiling of Incident Tickets in Large-scale Cloud Systems","abstract":"Postmortem analysis is essential in the management of incidents within cloud systems, which provides valuable insights to improve system's reliability and robustness. At CloudA, fault pattern profiling is performed during the postmortem phase, which involves the classification of incidents' faults into unique categories, referred to as fault pattern. By aggregating and analyzing these fault patterns, engineers can discern common faults, vulnerable components and emerging fault trends. However, this process is currently conducted by manual labeling, which has inherent drawbacks. On the one hand, the sheer volume of incidents means only the most severe ones are analyzed, causing a skewed overview of fault patterns. On the other hand, the complexity of the task demands extensive domain knowledge, which leads to errors and inconsistencies. To address these limitations, we propose an automated approach, named FaultProfIT, for Fault pattern Profiling of Incident Tickets. It leverages hierarchy-guided contrastive learning to train a hierarchy-aware incident encoder and predicts fault patterns with enhanced incident representations. We evaluate FaultProfIT using the production incidents from CloudA. The results demonstrate that FaultProfIT outperforms state-of-the-art methods. Our ablation study and analysis also verify the effectiveness of hierarchy-guided contrastive learning. Additionally, we have deployed FaultProfIT at CloudA for six months. To date, FaultProfIT has analyzed 10,000+ incidents from 30+ cloud services, successfully revealing several fault trends that have informed system improvements.","sentences":["Postmortem analysis is essential in the management of incidents within cloud systems, which provides valuable insights to improve system's reliability and robustness.","At CloudA, fault pattern profiling is performed during the postmortem phase, which involves the classification of incidents' faults into unique categories, referred to as fault pattern.","By aggregating and analyzing these fault patterns, engineers can discern common faults, vulnerable components and emerging fault trends.","However, this process is currently conducted by manual labeling, which has inherent drawbacks.","On the one hand, the sheer volume of incidents means only the most severe ones are analyzed, causing a skewed overview of fault patterns.","On the other hand, the complexity of the task demands extensive domain knowledge, which leads to errors and inconsistencies.","To address these limitations, we propose an automated approach, named FaultProfIT, for Fault pattern Profiling of Incident Tickets.","It leverages hierarchy-guided contrastive learning to train a hierarchy-aware incident encoder and predicts fault patterns with enhanced incident representations.","We evaluate FaultProfIT using the production incidents from CloudA. The results demonstrate that FaultProfIT outperforms state-of-the-art methods.","Our ablation study and analysis also verify the effectiveness of hierarchy-guided contrastive learning.","Additionally, we have deployed FaultProfIT at CloudA for six months.","To date, FaultProfIT has analyzed 10,000+ incidents from 30+ cloud services, successfully revealing several fault trends that have informed system improvements."],"url":"http://arxiv.org/abs/2402.17583v1"}
{"created":"2024-02-27 15:13:05","title":"A highly efficient computational approach for part-scale microstructure predictions in Ti-6Al-4V additive manufacturing","abstract":"Fast and efficient simulations of metal additive manufacturing (AM) processes are highly relevant to exploring the full potential of this promising manufacturing technique. The microstructure composition plays an important role in characterizing the part quality and deriving mechanical properties. When complete parts are simulated, one often needs to resort to strong simplifications such as layer-wise heating due to the large number of simulated time steps compared to the small time step sizes. This article proposes a scan-resolved approach to the coupled thermo-microstructural problem. Building on a highly efficient thermal model, we discuss the implementation of a phenomenological microstructure model for the evolution of the three main constituents of Ti-6Al-4V: stable $\\alpha_s$-phase, martensite $\\alpha_m$-phase and $\\beta$-phase. The implementation is tailored to modern hardware features using vectorization and fast approximations of transcendental functions. A performance model and numerical examples verify the high degree of optimization. We demonstrate the applicability and predictive power of the approach and the influence of scan strategy and geometry. Depending on the specific example, results can be obtained with moderate computational resources in a few hours to days. The numerical examples include a prediction of the microstructure on the full NIST AM Benchmark cantilever specimen.","sentences":["Fast and efficient simulations of metal additive manufacturing (AM) processes are highly relevant to exploring the full potential of this promising manufacturing technique.","The microstructure composition plays an important role in characterizing the part quality and deriving mechanical properties.","When complete parts are simulated, one often needs to resort to strong simplifications such as layer-wise heating due to the large number of simulated time steps compared to the small time step sizes.","This article proposes a scan-resolved approach to the coupled thermo-microstructural problem.","Building on a highly efficient thermal model, we discuss the implementation of a phenomenological microstructure model for the evolution of the three main constituents of Ti-6Al-4V: stable $\\alpha_s$-phase, martensite $\\alpha_m$-phase and $\\beta$-phase.","The implementation is tailored to modern hardware features using vectorization and fast approximations of transcendental functions.","A performance model and numerical examples verify the high degree of optimization.","We demonstrate the applicability and predictive power of the approach and the influence of scan strategy and geometry.","Depending on the specific example, results can be obtained with moderate computational resources in a few hours to days.","The numerical examples include a prediction of the microstructure on the full NIST AM Benchmark cantilever specimen."],"url":"http://arxiv.org/abs/2402.17580v1"}
{"created":"2024-02-27 15:09:20","title":"Hyperdimensional computing: a fast, robust and interpretable paradigm for biological data","abstract":"Advances in bioinformatics are primarily due to new algorithms for processing diverse biological data sources. While sophisticated alignment algorithms have been pivotal in analyzing biological sequences, deep learning has substantially transformed bioinformatics, addressing sequence, structure, and functional analyses. However, these methods are incredibly data-hungry, compute-intensive and hard to interpret. Hyperdimensional computing (HDC) has recently emerged as an intriguing alternative. The key idea is that random vectors of high dimensionality can represent concepts such as sequence identity or phylogeny. These vectors can then be combined using simple operators for learning, reasoning or querying by exploiting the peculiar properties of high-dimensional spaces. Our work reviews and explores the potential of HDC for bioinformatics, emphasizing its efficiency, interpretability, and adeptness in handling multimodal and structured data. HDC holds a lot of potential for various omics data searching, biosignal analysis and health applications.","sentences":["Advances in bioinformatics are primarily due to new algorithms for processing diverse biological data sources.","While sophisticated alignment algorithms have been pivotal in analyzing biological sequences, deep learning has substantially transformed bioinformatics, addressing sequence, structure, and functional analyses.","However, these methods are incredibly data-hungry, compute-intensive and hard to interpret.","Hyperdimensional computing (HDC) has recently emerged as an intriguing alternative.","The key idea is that random vectors of high dimensionality can represent concepts such as sequence identity or phylogeny.","These vectors can then be combined using simple operators for learning, reasoning or querying by exploiting the peculiar properties of high-dimensional spaces.","Our work reviews and explores the potential of HDC for bioinformatics, emphasizing its efficiency, interpretability, and adeptness in handling multimodal and structured data.","HDC holds a lot of potential for various omics data searching, biosignal analysis and health applications."],"url":"http://arxiv.org/abs/2402.17572v1"}
{"created":"2024-02-27 15:09:20","title":"HBF MU-MIMO with Interference-Aware Beam Pair Link Allocation for Beyond-5G mm-Wave Networks","abstract":"Hybrid beamforming (HBF) multi-user multiple-input multiple-output (MU-MIMO) is a key technology for unlocking the directional millimeter-wave (mm-wave) nature for spatial multiplexing beyond current codebook-based 5G-NR networks. In order to suppress co-scheduled users' interference, HBF MU-MIMO is predicated on having sufficient radio frequency chains and accurate channel state information (CSI), which can otherwise lead to performance losses due to imperfect interference cancellation. In this work, we propose IABA, a 5G-NR standard-compliant beam pair link (BPL) allocation scheme for mitigating spatial interference in practical HBF MU-MIMO networks. IABA solves the network sum throughput optimization via either a distributed or a centralized BPL allocation using dedicated CSI reference signals for candidate BPL monitoring. We present a comprehensive study of practical multi-cell mm-wave networks and demonstrate that HBF MU-MIMO without interference-aware BPL allocation experiences strong residual interference which limits the achievable network performance. Our results show that IABA offers significant performance gains over the default interference-agnostic 5G-NR BPL allocation, and even allows HBF MU-MIMO to outperform the fully digital MU-MIMO baseline, by facilitating allocation of secondary BPLs other than the strongest BPL found during initial access. We further demonstrate the scalability of IABA with increased gNB antennas and densification for beyond-5G mm-wave networks.","sentences":["Hybrid beamforming (HBF) multi-user multiple-input multiple-output (MU-MIMO) is a key technology for unlocking the directional millimeter-wave (mm-wave) nature for spatial multiplexing beyond current codebook-based 5G-NR networks.","In order to suppress co-scheduled users' interference, HBF MU-MIMO is predicated on having sufficient radio frequency chains and accurate channel state information (CSI), which can otherwise lead to performance losses due to imperfect interference cancellation.","In this work, we propose IABA, a 5G-NR standard-compliant beam pair link (BPL) allocation scheme for mitigating spatial interference in practical HBF MU-MIMO networks.","IABA solves the network sum throughput optimization via either a distributed or a centralized BPL allocation using dedicated CSI reference signals for candidate BPL monitoring.","We present a comprehensive study of practical multi-cell mm-wave networks and demonstrate that HBF MU-MIMO without interference-aware BPL allocation experiences strong residual interference which limits the achievable network performance.","Our results show that IABA offers significant performance gains over the default interference-agnostic 5G-NR BPL allocation, and even allows HBF MU-MIMO to outperform the fully digital MU-MIMO baseline, by facilitating allocation of secondary BPLs other than the strongest BPL found during initial access.","We further demonstrate the scalability of IABA with increased gNB antennas and densification for beyond-5G mm-wave networks."],"url":"http://arxiv.org/abs/2402.17573v1"}
{"created":"2024-02-27 15:09:20","title":"Agent-Pro: Learning to Evolve via Policy-Level Reflection and Optimization","abstract":"Large Language Models exhibit robust problem-solving capabilities for diverse tasks. However, most LLM-based agents are designed as specific task solvers with sophisticated prompt engineering, rather than agents capable of learning and evolving through interactions. These task solvers necessitate manually crafted prompts to inform task rules and regulate LLM behaviors, inherently incapacitating to address complex dynamic scenarios e.g., large interactive games. In light of this, we propose Agent-Pro: an LLM-based Agent with Policy-level Reflection and Optimization that can learn a wealth of expertise from interactive experiences and progressively elevate its behavioral policy. Specifically, it involves a dynamic belief generation and reflection process for policy evolution. Rather than action-level reflection, Agent-Pro iteratively reflects on past trajectories and beliefs, fine-tuning its irrational beliefs for a better policy. Moreover, a depth-first search is employed for policy optimization, ensuring continual enhancement in policy payoffs. Agent-Pro is evaluated across two games: Blackjack and Texas Hold'em, outperforming vanilla LLM and specialized models. Our results show Agent-Pro can learn and evolve in complex and dynamic scenes, which also benefits numerous LLM-based applications.","sentences":["Large Language Models exhibit robust problem-solving capabilities for diverse tasks.","However, most LLM-based agents are designed as specific task solvers with sophisticated prompt engineering, rather than agents capable of learning and evolving through interactions.","These task solvers necessitate manually crafted prompts to inform task rules and regulate LLM behaviors, inherently incapacitating to address complex dynamic scenarios e.g., large interactive games.","In light of this, we propose Agent-Pro: an LLM-based Agent with Policy-level Reflection and Optimization that can learn a wealth of expertise from interactive experiences and progressively elevate its behavioral policy.","Specifically, it involves a dynamic belief generation and reflection process for policy evolution.","Rather than action-level reflection, Agent-Pro iteratively reflects on past trajectories and beliefs, fine-tuning its irrational beliefs for a better policy.","Moreover, a depth-first search is employed for policy optimization, ensuring continual enhancement in policy payoffs.","Agent-Pro is evaluated across two games: Blackjack and Texas Hold'em, outperforming vanilla LLM and specialized models.","Our results show Agent-Pro can learn and evolve in complex and dynamic scenes, which also benefits numerous LLM-based applications."],"url":"http://arxiv.org/abs/2402.17574v1"}
{"created":"2024-02-27 15:08:57","title":"Sparse Variational Contaminated Noise Gaussian Process Regression for Forecasting Geomagnetic Perturbations","abstract":"Gaussian Processes (GP) have become popular machine learning methods for kernel based learning on datasets with complicated covariance structures. In this paper, we present a novel extension to the GP framework using a contaminated normal likelihood function to better account for heteroscedastic variance and outlier noise. We propose a scalable inference algorithm based on the Sparse Variational Gaussian Process (SVGP) method for fitting sparse Gaussian process regression models with contaminated normal noise on large datasets. We examine an application to geomagnetic ground perturbations, where the state-of-art prediction model is based on neural networks. We show that our approach yields shorter predictions intervals for similar coverage and accuracy when compared to an artificial dense neural network baseline.","sentences":["Gaussian Processes (GP) have become popular machine learning methods for kernel based learning on datasets with complicated covariance structures.","In this paper, we present a novel extension to the GP framework using a contaminated normal likelihood function to better account for heteroscedastic variance and outlier noise.","We propose a scalable inference algorithm based on the Sparse Variational Gaussian Process (SVGP) method for fitting sparse Gaussian process regression models with contaminated normal noise on large datasets.","We examine an application to geomagnetic ground perturbations, where the state-of-art prediction model is based on neural networks.","We show that our approach yields shorter predictions intervals for similar coverage and accuracy when compared to an artificial dense neural network baseline."],"url":"http://arxiv.org/abs/2402.17570v1"}
{"created":"2024-02-27 15:08:14","title":"Backpropagation-Based Analytical Derivatives of EKF Covariance for Active Sensing","abstract":"To enhance accuracy of robot state estimation, perception-aware (or active sensing) methods seek trajectories that minimize uncertainty. To this aim, one possibility is to seek trajectories that minimize the final covariance of an extended Kalman filter (EKF), w.r.t. its control inputs over a given horizon. However, this can be computationally demanding. In this article, we derive novel backpropagation analytical formulas for the derivatives of the final covariance of an EKF w.r.t. its inputs. We then leverage the obtained gradients as an enabling technology to derive perception-aware optimal motion plans. Simulations validate the approach, showcasing improvements in both estimation accuracy and execution time. Experimental results on a real large ground vehicle also support the method.","sentences":["To enhance accuracy of robot state estimation, perception-aware (or active sensing) methods seek trajectories that minimize uncertainty.","To this aim, one possibility is to seek trajectories that minimize the final covariance of an extended Kalman filter (EKF), w.r.t.","its control inputs over a given horizon.","However, this can be computationally demanding.","In this article, we derive novel backpropagation analytical formulas for the derivatives of the final covariance of an EKF w.r.t.","its inputs.","We then leverage the obtained gradients as an enabling technology to derive perception-aware optimal motion plans.","Simulations validate the approach, showcasing improvements in both estimation accuracy and execution time.","Experimental results on a real large ground vehicle also support the method."],"url":"http://arxiv.org/abs/2402.17569v1"}
