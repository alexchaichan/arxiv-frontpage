{"created":"2024-07-03 17:59:53","title":"Planetarium: A Rigorous Benchmark for Translating Text to Structured Planning Languages","abstract":"Many recent works have explored using language models for planning problems. One line of research focuses on translating natural language descriptions of planning tasks into structured planning languages, such as the planning domain definition language (PDDL). While this approach is promising, accurately measuring the quality of generated PDDL code continues to pose significant challenges. First, generated PDDL code is typically evaluated using planning validators that check whether the problem can be solved with a planner. This method is insufficient because a language model might generate valid PDDL code that does not align with the natural language description of the task. Second, existing evaluation sets often have natural language descriptions of the planning task that closely resemble the ground truth PDDL, reducing the challenge of the task. To bridge this gap, we introduce \\benchmarkName, a benchmark designed to evaluate language models' ability to generate PDDL code from natural language descriptions of planning tasks. We begin by creating a PDDL equivalence algorithm that rigorously evaluates the correctness of PDDL code generated by language models by flexibly comparing it against a ground truth PDDL. Then, we present a dataset of $132,037$ text-to-PDDL pairs across 13 different tasks, with varying levels of difficulty. Finally, we evaluate several API-access and open-weight language models that reveal this task's complexity. For example, $87.6\\%$ of the PDDL problem descriptions generated by GPT-4o are syntactically parseable, $82.2\\%$ are valid, solve-able problems, but only $35.1\\%$ are semantically correct, highlighting the need for a more rigorous benchmark for this problem.","sentences":["Many recent works have explored using language models for planning problems.","One line of research focuses on translating natural language descriptions of planning tasks into structured planning languages, such as the planning domain definition language (PDDL).","While this approach is promising, accurately measuring the quality of generated PDDL code continues to pose significant challenges.","First, generated PDDL code is typically evaluated using planning validators that check whether the problem can be solved with a planner.","This method is insufficient because a language model might generate valid PDDL code that does not align with the natural language description of the task.","Second, existing evaluation sets often have natural language descriptions of the planning task that closely resemble the ground truth PDDL, reducing the challenge of the task.","To bridge this gap, we introduce \\benchmarkName, a benchmark designed to evaluate language models' ability to generate PDDL code from natural language descriptions of planning tasks.","We begin by creating a PDDL equivalence algorithm that rigorously evaluates the correctness of PDDL code generated by language models by flexibly comparing it against a ground truth PDDL.","Then, we present a dataset of $132,037$ text-to-PDDL pairs across 13 different tasks, with varying levels of difficulty.","Finally, we evaluate several API-access and open-weight language models that reveal this task's complexity.","For example, $87.6\\%$ of the PDDL problem descriptions generated by GPT-4o are syntactically parseable, $82.2\\%$ are valid, solve-able problems, but only $35.1\\%$ are semantically correct, highlighting the need for a more rigorous benchmark for this problem."],"url":"http://arxiv.org/abs/2407.03321v1"}
{"created":"2024-07-03 17:59:21","title":"InternLM-XComposer-2.5: A Versatile Large Vision Language Model Supporting Long-Contextual Input and Output","abstract":"We present InternLM-XComposer-2.5 (IXC-2.5), a versatile large-vision language model that supports long-contextual input and output. IXC-2.5 excels in various text-image comprehension and composition applications, achieving GPT-4V level capabilities with merely 7B LLM backend. Trained with 24K interleaved image-text contexts, it can seamlessly extend to 96K long contexts via RoPE extrapolation. This long-context capability allows IXC-2.5 to excel in tasks requiring extensive input and output contexts. Compared to its previous 2.0 version, InternLM-XComposer-2.5 features three major upgrades in vision-language comprehension: (1) Ultra-High Resolution Understanding, (2) Fine-Grained Video Understanding, and (3) Multi-Turn Multi-Image Dialogue. In addition to comprehension, IXC-2.5 extends to two compelling applications using extra LoRA parameters for text-image composition: (1) Crafting Webpages and (2) Composing High-Quality Text-Image Articles. IXC-2.5 has been evaluated on 28 benchmarks, outperforming existing open-source state-of-the-art models on 16 benchmarks. It also surpasses or competes closely with GPT-4V and Gemini Pro on 16 key tasks. The InternLM-XComposer-2.5 is publicly available at https://github.com/InternLM/InternLM-XComposer.","sentences":["We present InternLM-XComposer-2.5 (IXC-2.5), a versatile large-vision language model that supports long-contextual input and output.","IXC-2.5 excels in various text-image comprehension and composition applications, achieving GPT-4V level capabilities with merely 7B LLM backend.","Trained with 24K interleaved image-text contexts, it can seamlessly extend to 96K long contexts via RoPE extrapolation.","This long-context capability allows IXC-2.5 to excel in tasks requiring extensive input and output contexts.","Compared to its previous 2.0 version, InternLM-XComposer-2.5 features three major upgrades in vision-language comprehension: (1) Ultra-High Resolution Understanding, (2) Fine-Grained Video Understanding, and (3) Multi-Turn Multi-Image Dialogue.","In addition to comprehension, IXC-2.5 extends to two compelling applications using extra LoRA parameters for text-image composition: (1) Crafting Webpages and (2) Composing High-Quality Text-Image Articles.","IXC-2.5 has been evaluated on 28 benchmarks, outperforming existing open-source state-of-the-art models on 16 benchmarks.","It also surpasses or competes closely with GPT-4V and Gemini Pro on 16 key tasks.","The InternLM-XComposer-2.5 is publicly available at https://github.com/InternLM/InternLM-XComposer."],"url":"http://arxiv.org/abs/2407.03320v1"}
{"created":"2024-07-03 17:58:22","title":"Fair Division of Indivisible Chores via Earning Restricted Equilibria","abstract":"We study fair division of $m$ indivisible chores among $n$ agents with additive preferences. We consider the desirable fairness notions of envy-freeness up to any chore (EFX) and envy-freeness up to $k$ chores (EF$k$), alongside the efficiency notion of Pareto optimality (PO). We present the first constant approximations of these notions, showing the existence of:   - 5-EFX allocations, which improve the best-known factor of $O(n^2)$-EFX.   - 3-EFX and PO allocations for the special case of bivalued instances, which improve the best-known factor of $O(n)$-EFX without any efficiency guarantees.   - 2-EF2 + PO allocations, which improve the best-known factor of EF$m$ + PO.   A notable contribution of our work is the introduction of the novel concept of earning-restricted (ER) competitive equilibrium for fractional allocations, which limits agents' earnings from each chore. Technically, our work addresses two main challenges: proving the existence of an ER equilibrium and designing algorithms that leverage ER equilibria to achieve the above results. To tackle the first challenge, we formulate a linear complementarity problem (LCP) formulation that captures all ER equilibria and show that the classic complementary pivot algorithm on the LCP must terminate at an ER equilibrium. For the second challenge, we carefully set the earning limits and use properties of ER equilibria to design sophisticated procedures that involve swapping and merging bundles to meet the desired fairness and efficiency criteria. We expect that the concept of ER equilibrium will be instrumental in deriving further results on related problems.","sentences":["We study fair division of $m$ indivisible chores among $n$ agents with additive preferences.","We consider the desirable fairness notions of envy-freeness up to any chore (EFX) and envy-freeness up to $k$ chores (EF$k$), alongside the efficiency notion of Pareto optimality (PO).","We present the first constant approximations of these notions, showing the existence of:   - 5-EFX allocations, which improve the best-known factor of $O(n^2)$-EFX.   ","- 3-EFX and PO allocations for the special case of bivalued instances, which improve the best-known factor of $O(n)$-EFX without any efficiency guarantees.   ","- 2-EF2 + PO allocations, which improve the best-known factor of EF$m$ + PO.   ","A notable contribution of our work is the introduction of the novel concept of earning-restricted (ER) competitive equilibrium for fractional allocations, which limits agents' earnings from each chore.","Technically, our work addresses two main challenges: proving the existence of an ER equilibrium and designing algorithms that leverage ER equilibria to achieve the above results.","To tackle the first challenge, we formulate a linear complementarity problem (LCP) formulation that captures all ER equilibria and show that the classic complementary pivot algorithm on the LCP must terminate at an ER equilibrium.","For the second challenge, we carefully set the earning limits and use properties of ER equilibria to design sophisticated procedures that involve swapping and merging bundles to meet the desired fairness and efficiency criteria.","We expect that the concept of ER equilibrium will be instrumental in deriving further results on related problems."],"url":"http://arxiv.org/abs/2407.03318v1"}
{"created":"2024-07-03 17:55:27","title":"BACON: Supercharge Your VLM with Bag-of-Concept Graph to Mitigate Hallucinations","abstract":"This paper presents Bag-of-Concept Graph (BACON) to gift models with limited linguistic abilities to taste the privilege of Vision Language Models (VLMs) and boost downstream tasks such as detection, visual question answering (VQA), and image generation. Since the visual scenes in physical worlds are structured with complex relations between objects, BACON breaks down annotations into basic minimum elements and presents them in a graph structure. Element-wise style enables easy understanding, and structural composition liberates difficult locating. Careful prompt design births the BACON captions with the help of public-available VLMs and segmentation methods. In this way, we gather a dataset with 100K annotated images, which endow VLMs with remarkable capabilities, such as accurately generating BACON, transforming prompts into BACON format, envisioning scenarios in the style of BACONr, and dynamically modifying elements within BACON through interactive dialogue and more. Wide representative experiments, including detection, VQA, and image generation tasks, tell BACON as a lifeline to achieve previous out-of-reach tasks or excel in their current cutting-edge solutions.","sentences":["This paper presents Bag-of-Concept Graph (BACON) to gift models with limited linguistic abilities to taste the privilege of Vision Language Models (VLMs) and boost downstream tasks such as detection, visual question answering (VQA), and image generation.","Since the visual scenes in physical worlds are structured with complex relations between objects, BACON breaks down annotations into basic minimum elements and presents them in a graph structure.","Element-wise style enables easy understanding, and structural composition liberates difficult locating.","Careful prompt design births the BACON captions with the help of public-available VLMs and segmentation methods.","In this way, we gather a dataset with 100K annotated images, which endow VLMs with remarkable capabilities, such as accurately generating BACON, transforming prompts into BACON format, envisioning scenarios in the style of BACONr, and dynamically modifying elements within BACON through interactive dialogue and more.","Wide representative experiments, including detection, VQA, and image generation tasks, tell BACON as a lifeline to achieve previous out-of-reach tasks or excel in their current cutting-edge solutions."],"url":"http://arxiv.org/abs/2407.03314v1"}
{"created":"2024-07-03 17:54:11","title":"Value-Penalized Auxiliary Control from Examples for Learning without Rewards or Demonstrations","abstract":"Learning from examples of success is an appealing approach to reinforcement learning that eliminates many of the disadvantages of using hand-crafted reward functions or full expert-demonstration trajectories, both of which can be difficult to acquire, biased, or suboptimal. However, learning from examples alone dramatically increases the exploration challenge, especially for complex tasks. This work introduces value-penalized auxiliary control from examples (VPACE); we significantly improve exploration in example-based control by adding scheduled auxiliary control and examples of auxiliary tasks. Furthermore, we identify a value-calibration problem, where policy value estimates can exceed their theoretical limits based on successful data. We resolve this problem, which is exacerbated by learning auxiliary tasks, through the addition of an above-success-level value penalty. Across three simulated and one real robotic manipulation environment, and 21 different main tasks, we show that our approach substantially improves learning efficiency. Videos, code, and datasets are available at https://papers.starslab.ca/vpace.","sentences":["Learning from examples of success is an appealing approach to reinforcement learning that eliminates many of the disadvantages of using hand-crafted reward functions or full expert-demonstration trajectories, both of which can be difficult to acquire, biased, or suboptimal.","However, learning from examples alone dramatically increases the exploration challenge, especially for complex tasks.","This work introduces value-penalized auxiliary control from examples (VPACE); we significantly improve exploration in example-based control by adding scheduled auxiliary control and examples of auxiliary tasks.","Furthermore, we identify a value-calibration problem, where policy value estimates can exceed their theoretical limits based on successful data.","We resolve this problem, which is exacerbated by learning auxiliary tasks, through the addition of an above-success-level value penalty.","Across three simulated and one real robotic manipulation environment, and 21 different main tasks, we show that our approach substantially improves learning efficiency.","Videos, code, and datasets are available at https://papers.starslab.ca/vpace."],"url":"http://arxiv.org/abs/2407.03311v1"}
{"created":"2024-07-03 17:53:44","title":"Universal Length Generalization with Turing Programs","abstract":"Length generalization refers to the ability to extrapolate from short training sequences to long test sequences and is a challenge for current large language models. While prior work has proposed some architecture or data format changes to achieve length generalization, these proposals typically apply to a limited set of tasks. Building on prior scratchpad and Chain-of-Thought (CoT) techniques, we propose Turing Programs, a novel CoT strategy that decomposes an algorithmic task into steps mimicking the computation of a Turing Machine. This framework is both universal, as it can accommodate any algorithmic task, and simple, requiring only copying text from the context with small modifications. We show that by using Turing Programs, we obtain robust length generalization on a range of algorithmic tasks: addition, multiplication and in-context SGD. We then demonstrate that transformers achieve length generalization on random Turing Programs, suggesting that length generalization is possible for any algorithmic task. Finally, we theoretically prove that transformers can implement Turing Programs, constructing a simple RASP (Weiss et al.) program that simulates an arbitrary Turing machine.","sentences":["Length generalization refers to the ability to extrapolate from short training sequences to long test sequences and is a challenge for current large language models.","While prior work has proposed some architecture or data format changes to achieve length generalization, these proposals typically apply to a limited set of tasks.","Building on prior scratchpad and Chain-of-Thought (CoT) techniques, we propose Turing Programs, a novel CoT strategy that decomposes an algorithmic task into steps mimicking the computation of a Turing Machine.","This framework is both universal, as it can accommodate any algorithmic task, and simple, requiring only copying text from the context with small modifications.","We show that by using Turing Programs, we obtain robust length generalization on a range of algorithmic tasks: addition, multiplication and in-context SGD.","We then demonstrate that transformers achieve length generalization on random Turing Programs, suggesting that length generalization is possible for any algorithmic task.","Finally, we theoretically prove that transformers can implement Turing Programs, constructing a simple RASP (Weiss et al.)","program that simulates an arbitrary Turing machine."],"url":"http://arxiv.org/abs/2407.03310v1"}
{"created":"2024-07-03 17:47:59","title":"Smart City Surveillance Unveiling Indian Person Attributes in Real Time","abstract":"This project focuses on creating a smart surveillance system for Indian cities that can identify and analyze people's attributes in real time. Using advanced technologies like artificial intelligence and machine learning, the system can recognize attributes such as upper body color, what the person is wearing, accessories they are wearing, headgear, etc., and analyze behavior through cameras installed around the city.","sentences":["This project focuses on creating a smart surveillance system for Indian cities that can identify and analyze people's attributes in real time.","Using advanced technologies like artificial intelligence and machine learning, the system can recognize attributes such as upper body color, what the person is wearing, accessories they are wearing, headgear, etc., and analyze behavior through cameras installed around the city."],"url":"http://arxiv.org/abs/2407.03305v1"}
{"created":"2024-07-03 17:43:54","title":"A Review of the Applications of Deep Learning-Based Emergent Communication","abstract":"Emergent communication, or emergent language, is the field of research which studies how human language-like communication systems emerge de novo in deep multi-agent reinforcement learning environments. The possibilities of replicating the emergence of a complex behavior like language have strong intuitive appeal, yet it is necessary to complement this with clear notions of how such research can be applicable to other fields of science, technology, and engineering. This paper comprehensively reviews the applications of emergent communication research across machine learning, natural language processing, linguistics, and cognitive science. Each application is illustrated with a description of its scope, an explication of emergent communication's unique role in addressing it, a summary of the extant literature working towards the application, and brief recommendations for near-term research directions.","sentences":["Emergent communication, or emergent language, is the field of research which studies how human language-like communication systems emerge de novo in deep multi-agent reinforcement learning environments.","The possibilities of replicating the emergence of a complex behavior like language have strong intuitive appeal, yet it is necessary to complement this with clear notions of how such research can be applicable to other fields of science, technology, and engineering.","This paper comprehensively reviews the applications of emergent communication research across machine learning, natural language processing, linguistics, and cognitive science.","Each application is illustrated with a description of its scope, an explication of emergent communication's unique role in addressing it, a summary of the extant literature working towards the application, and brief recommendations for near-term research directions."],"url":"http://arxiv.org/abs/2407.03302v1"}
{"created":"2024-07-03 17:42:46","title":"DisCo-Diff: Enhancing Continuous Diffusion Models with Discrete Latents","abstract":"Diffusion models (DMs) have revolutionized generative learning. They utilize a diffusion process to encode data into a simple Gaussian distribution. However, encoding a complex, potentially multimodal data distribution into a single continuous Gaussian distribution arguably represents an unnecessarily challenging learning problem. We propose Discrete-Continuous Latent Variable Diffusion Models (DisCo-Diff) to simplify this task by introducing complementary discrete latent variables. We augment DMs with learnable discrete latents, inferred with an encoder, and train DM and encoder end-to-end. DisCo-Diff does not rely on pre-trained networks, making the framework universally applicable. The discrete latents significantly simplify learning the DM's complex noise-to-data mapping by reducing the curvature of the DM's generative ODE. An additional autoregressive transformer models the distribution of the discrete latents, a simple step because DisCo-Diff requires only few discrete variables with small codebooks. We validate DisCo-Diff on toy data, several image synthesis tasks as well as molecular docking, and find that introducing discrete latents consistently improves model performance. For example, DisCo-Diff achieves state-of-the-art FID scores on class-conditioned ImageNet-64/128 datasets with ODE sampler.","sentences":["Diffusion models (DMs) have revolutionized generative learning.","They utilize a diffusion process to encode data into a simple Gaussian distribution.","However, encoding a complex, potentially multimodal data distribution into a single continuous Gaussian distribution arguably represents an unnecessarily challenging learning problem.","We propose Discrete-Continuous Latent Variable Diffusion Models (DisCo-Diff) to simplify this task by introducing complementary discrete latent variables.","We augment DMs with learnable discrete latents, inferred with an encoder, and train DM and encoder end-to-end.","DisCo-Diff does not rely on pre-trained networks, making the framework universally applicable.","The discrete latents significantly simplify learning the DM's complex noise-to-data mapping by reducing the curvature of the DM's generative ODE.","An additional autoregressive transformer models the distribution of the discrete latents, a simple step because DisCo-Diff requires only few discrete variables with small codebooks.","We validate DisCo-Diff on toy data, several image synthesis tasks as well as molecular docking, and find that introducing discrete latents consistently improves model performance.","For example, DisCo-Diff achieves state-of-the-art FID scores on class-conditioned ImageNet-64/128 datasets with ODE sampler."],"url":"http://arxiv.org/abs/2407.03300v1"}
{"created":"2024-07-03 17:37:19","title":"Eyes on the Game: Deciphering Implicit Human Signals to Infer Human Proficiency, Trust, and Intent","abstract":"Effective collaboration between humans and AIs hinges on transparent communication and alignment of mental models. However, explicit, verbal communication is not always feasible. Under such circumstances, human-human teams often depend on implicit, nonverbal cues to glean important information about their teammates such as intent and expertise, thereby bolstering team alignment and adaptability. Among these implicit cues, two of the most salient and fundamental are a human's actions in the environment and their visual attention. In this paper, we present a novel method to combine eye gaze data and behavioral data, and evaluate their respective predictive power for human proficiency, trust, and intent. We first collect a dataset of paired eye gaze and gameplay data in the fast-paced collaborative \"Overcooked\" environment. We then train models on this dataset to compare how the predictive powers differ between gaze data, gameplay data, and their combination. We additionally compare our method to prior works that aggregate eye gaze data and demonstrate how these aggregation methods can substantially reduce the predictive ability of eye gaze. Our results indicate that, while eye gaze data and gameplay data excel in different situations, a model that integrates both types consistently outperforms all baselines. This work paves the way for developing intuitive and responsive agents that can efficiently adapt to new teammates.","sentences":["Effective collaboration between humans and AIs hinges on transparent communication and alignment of mental models.","However, explicit, verbal communication is not always feasible.","Under such circumstances, human-human teams often depend on implicit, nonverbal cues to glean important information about their teammates such as intent and expertise, thereby bolstering team alignment and adaptability.","Among these implicit cues, two of the most salient and fundamental are a human's actions in the environment and their visual attention.","In this paper, we present a novel method to combine eye gaze data and behavioral data, and evaluate their respective predictive power for human proficiency, trust, and intent.","We first collect a dataset of paired eye gaze and gameplay data in the fast-paced collaborative \"Overcooked\" environment.","We then train models on this dataset to compare how the predictive powers differ between gaze data, gameplay data, and their combination.","We additionally compare our method to prior works that aggregate eye gaze data and demonstrate how these aggregation methods can substantially reduce the predictive ability of eye gaze.","Our results indicate that, while eye gaze data and gameplay data excel in different situations, a model that integrates both types consistently outperforms all baselines.","This work paves the way for developing intuitive and responsive agents that can efficiently adapt to new teammates."],"url":"http://arxiv.org/abs/2407.03298v1"}
{"created":"2024-07-03 17:34:55","title":"Improved Noise Schedule for Diffusion Training","abstract":"Diffusion models have emerged as the de facto choice for generating visual signals. However, training a single model to predict noise across various levels poses significant challenges, necessitating numerous iterations and incurring significant computational costs. Various approaches, such as loss weighting strategy design and architectural refinements, have been introduced to expedite convergence. In this study, we propose a novel approach to design the noise schedule for enhancing the training of diffusion models. Our key insight is that the importance sampling of the logarithm of the Signal-to-Noise ratio (logSNR), theoretically equivalent to a modified noise schedule, is particularly beneficial for training efficiency when increasing the sample frequency around $\\log \\text{SNR}=0$. We empirically demonstrate the superiority of our noise schedule over the standard cosine schedule. Furthermore, we highlight the advantages of our noise schedule design on the ImageNet benchmark, showing that the designed schedule consistently benefits different prediction targets.","sentences":["Diffusion models have emerged as the de facto choice for generating visual signals.","However, training a single model to predict noise across various levels poses significant challenges, necessitating numerous iterations and incurring significant computational costs.","Various approaches, such as loss weighting strategy design and architectural refinements, have been introduced to expedite convergence.","In this study, we propose a novel approach to design the noise schedule for enhancing the training of diffusion models.","Our key insight is that the importance sampling of the logarithm of the Signal-to-Noise ratio (logSNR), theoretically equivalent to a modified noise schedule, is particularly beneficial for training efficiency when increasing the sample frequency around $\\log \\text{SNR}=0$. We empirically demonstrate the superiority of our noise schedule over the standard cosine schedule.","Furthermore, we highlight the advantages of our noise schedule design on the ImageNet benchmark, showing that the designed schedule consistently benefits different prediction targets."],"url":"http://arxiv.org/abs/2407.03297v1"}
{"created":"2024-07-03 17:26:07","title":"Biomechanics-informed Non-rigid Medical Image Registration and its Inverse Material Property Estimation with Linear and Nonlinear Elasticity","abstract":"This paper investigates both biomechanical-constrained non-rigid medical image registrations and accurate identifications of material properties for soft tissues, using physics-informed neural networks (PINNs). The complex nonlinear elasticity theory is leveraged to formally establish the partial differential equations (PDEs) representing physics laws of biomechanical constraints that need to be satisfied, with which registration and identification tasks are treated as forward (i.e., data-driven solutions of PDEs) and inverse (i.e., parameter estimation) problems under PINNs respectively. Two net configurations (i.e., Cfg1 and Cfg2) have also been compared for both linear and nonlinear physics model. Two sets of experiments have been conducted, using pairs of undeformed and deformed MR images from clinical cases of prostate cancer biopsy.   Our contributions are summarised as follows. 1) We developed a learning-based biomechanical-constrained non-rigid registration algorithm using PINNs, where linear elasticity is generalised to the nonlinear version. 2) We demonstrated extensively that nonlinear elasticity shows no statistical significance against linear models in computing point-wise displacement vectors but their respective benefits may depend on specific patients, with finite-element (FE) computed ground-truth. 3) We formulated and solved the inverse parameter estimation problem, under the joint optimisation scheme of registration and parameter identification using PINNs, whose solutions can be accurately found by locating saddle points.","sentences":["This paper investigates both biomechanical-constrained non-rigid medical image registrations and accurate identifications of material properties for soft tissues, using physics-informed neural networks (PINNs).","The complex nonlinear elasticity theory is leveraged to formally establish the partial differential equations (PDEs) representing physics laws of biomechanical constraints that need to be satisfied, with which registration and identification tasks are treated as forward (i.e., data-driven solutions of PDEs) and inverse (i.e., parameter estimation) problems under PINNs respectively.","Two net configurations (i.e., Cfg1 and Cfg2) have also been compared for both linear and nonlinear physics model.","Two sets of experiments have been conducted, using pairs of undeformed and deformed MR images from clinical cases of prostate cancer biopsy.   ","Our contributions are summarised as follows.","1) We developed a learning-based biomechanical-constrained non-rigid registration algorithm using PINNs, where linear elasticity is generalised to the nonlinear version.","2) We demonstrated extensively that nonlinear elasticity shows no statistical significance against linear models in computing point-wise displacement vectors but their respective benefits may depend on specific patients, with finite-element (FE) computed ground-truth.","3) We formulated and solved the inverse parameter estimation problem, under the joint optimisation scheme of registration and parameter identification using PINNs, whose solutions can be accurately found by locating saddle points."],"url":"http://arxiv.org/abs/2407.03292v1"}
{"created":"2024-07-03 17:24:36","title":"VCHAR:Variance-Driven Complex Human Activity Recognition framework with Generative Representation","abstract":"Complex human activity recognition (CHAR) remains a pivotal challenge within ubiquitous computing, especially in the context of smart environments. Existing studies typically require meticulous labeling of both atomic and complex activities, a task that is labor-intensive and prone to errors due to the scarcity and inaccuracies of available datasets. Most prior research has focused on datasets that either precisely label atomic activities or, at minimum, their sequence approaches that are often impractical in real world settings.In response, we introduce VCHAR (Variance-Driven Complex Human Activity Recognition), a novel framework that treats the outputs of atomic activities as a distribution over specified intervals. Leveraging generative methodologies, VCHAR elucidates the reasoning behind complex activity classifications through video-based explanations, accessible to users without prior machine learning expertise. Our evaluation across three publicly available datasets demonstrates that VCHAR enhances the accuracy of complex activity recognition without necessitating precise temporal or sequential labeling of atomic activities. Furthermore, user studies confirm that VCHAR's explanations are more intelligible compared to existing methods, facilitating a broader understanding of complex activity recognition among non-experts.","sentences":["Complex human activity recognition (CHAR) remains a pivotal challenge within ubiquitous computing, especially in the context of smart environments.","Existing studies typically require meticulous labeling of both atomic and complex activities, a task that is labor-intensive and prone to errors due to the scarcity and inaccuracies of available datasets.","Most prior research has focused on datasets that either precisely label atomic activities or, at minimum, their sequence approaches that are often impractical in real world settings.","In response, we introduce VCHAR (Variance-Driven Complex Human Activity Recognition), a novel framework that treats the outputs of atomic activities as a distribution over specified intervals.","Leveraging generative methodologies, VCHAR elucidates the reasoning behind complex activity classifications through video-based explanations, accessible to users without prior machine learning expertise.","Our evaluation across three publicly available datasets demonstrates that VCHAR enhances the accuracy of complex activity recognition without necessitating precise temporal or sequential labeling of atomic activities.","Furthermore, user studies confirm that VCHAR's explanations are more intelligible compared to existing methods, facilitating a broader understanding of complex activity recognition among non-experts."],"url":"http://arxiv.org/abs/2407.03291v1"}
{"created":"2024-07-03 17:22:33","title":"Correlated Privacy Mechanisms for Differentially Private Distributed Mean Estimation","abstract":"Differentially private distributed mean estimation (DP-DME) is a fundamental building block in privacy-preserving federated learning, where a central server estimates the mean of $d$-dimensional vectors held by $n$ users while ensuring $(\\epsilon,\\delta)$-DP. Local differential privacy (LDP) and distributed DP with secure aggregation (SecAgg) are the most common notions of DP used in DP-DME settings with an untrusted server. LDP provides strong resilience to dropouts, colluding users, and malicious server attacks, but suffers from poor utility. In contrast, SecAgg-based DP-DME achieves an $O(n)$ utility gain over LDP in DME, but requires increased communication and computation overheads and complex multi-round protocols to handle dropouts and malicious attacks. In this work, we propose CorDP-DME, a novel DP-DME mechanism that spans the gap between DME with LDP and distributed DP, offering a favorable balance between utility and resilience to dropout and collusion. CorDP-DME is based on correlated Gaussian noise, ensuring DP without the perfect conditional privacy guarantees of SecAgg-based approaches. We provide an information-theoretic analysis of CorDP-DME, and derive theoretical guarantees for utility under any given privacy parameters and dropout/colluding user thresholds. Our results demonstrate that (anti) correlated Gaussian DP mechanisms can significantly improve utility in mean estimation tasks compared to LDP -- even in adversarial settings -- while maintaining better resilience to dropouts and attacks compared to distributed DP.","sentences":["Differentially private distributed mean estimation (DP-DME) is a fundamental building block in privacy-preserving federated learning, where a central server estimates the mean of $d$-dimensional vectors held by $n$ users while ensuring $(\\epsilon,\\delta)$-DP.","Local differential privacy (LDP) and distributed DP with secure aggregation (SecAgg) are the most common notions of DP used in DP-DME settings with an untrusted server.","LDP provides strong resilience to dropouts, colluding users, and malicious server attacks, but suffers from poor utility.","In contrast, SecAgg-based DP-DME achieves an $O(n)$ utility gain over LDP in DME, but requires increased communication and computation overheads and complex multi-round protocols to handle dropouts and malicious attacks.","In this work, we propose CorDP-DME, a novel DP-DME mechanism that spans the gap between DME with LDP and distributed DP, offering a favorable balance between utility and resilience to dropout and collusion.","CorDP-DME is based on correlated Gaussian noise, ensuring DP without the perfect conditional privacy guarantees of SecAgg-based approaches.","We provide an information-theoretic analysis of CorDP-DME, and derive theoretical guarantees for utility under any given privacy parameters and dropout/colluding user thresholds.","Our results demonstrate that (anti) correlated Gaussian DP mechanisms can significantly improve utility in mean estimation tasks compared to LDP -- even in adversarial settings -- while maintaining better resilience to dropouts and attacks compared to distributed DP."],"url":"http://arxiv.org/abs/2407.03289v1"}
{"created":"2024-07-03 17:17:37","title":"Large Language Models for JSON Schema Discovery","abstract":"Semi-structured data formats such as JSON have proved to be useful data models for applications that require flexibility in the format of data stored. However, JSON data often come without the schemas that are typically available with relational data. This has resulted in a number of tools for discovering schemas from a collection of data. Although such tools can be useful, existing approaches focus on the syntax of documents and ignore semantic information.   In this work, we explore the automatic addition of meaningful semantic information to discovered schemas similar to information that is added by human schema authors. We leverage large language models and a corpus of manually authored JSON Schema documents to generate natural language descriptions of schema elements, meaningful names for reusable definitions, and identify which discovered properties are most useful and which can be considered \"noise\". Our approach performs well on existing metrics for text generation that have been previously shown to correlate well with human judgement.","sentences":["Semi-structured data formats such as JSON have proved to be useful data models for applications that require flexibility in the format of data stored.","However, JSON data often come without the schemas that are typically available with relational data.","This has resulted in a number of tools for discovering schemas from a collection of data.","Although such tools can be useful, existing approaches focus on the syntax of documents and ignore semantic information.   ","In this work, we explore the automatic addition of meaningful semantic information to discovered schemas similar to information that is added by human schema authors.","We leverage large language models and a corpus of manually authored JSON Schema documents to generate natural language descriptions of schema elements, meaningful names for reusable definitions, and identify which discovered properties are most useful and which can be considered \"noise\".","Our approach performs well on existing metrics for text generation that have been previously shown to correlate well with human judgement."],"url":"http://arxiv.org/abs/2407.03286v1"}
{"created":"2024-07-03 17:09:31","title":"From B Specifications to $\\{log$\\}$ Forgrams","abstract":"In this class notes students can learn how B specifications can be translated into $\\{log$\\}$ forgrams, how these forgrams can be executed and how they can be proved to verify some properties.","sentences":["In this class notes students can learn how B specifications can be translated into $\\{log$\\}$ forgrams, how these forgrams can be executed and how they can be proved to verify some properties."],"url":"http://arxiv.org/abs/2407.03283v1"}
{"created":"2024-07-03 17:08:52","title":"LLM Internal States Reveal Hallucination Risk Faced With a Query","abstract":"The hallucination problem of Large Language Models (LLMs) significantly limits their reliability and trustworthiness. Humans have a self-awareness process that allows us to recognize what we don't know when faced with queries. Inspired by this, our paper investigates whether LLMs can estimate their own hallucination risk before response generation. We analyze the internal mechanisms of LLMs broadly both in terms of training data sources and across 15 diverse Natural Language Generation (NLG) tasks, spanning over 700 datasets. Our empirical analysis reveals two key insights: (1) LLM internal states indicate whether they have seen the query in training data or not; and (2) LLM internal states show they are likely to hallucinate or not regarding the query. Our study explores particular neurons, activation layers, and tokens that play a crucial role in the LLM perception of uncertainty and hallucination risk. By a probing estimator, we leverage LLM self-assessment, achieving an average hallucination estimation accuracy of 84.32\\% at run time.","sentences":["The hallucination problem of Large Language Models (LLMs) significantly limits their reliability and trustworthiness.","Humans have a self-awareness process that allows us to recognize what we don't know when faced with queries.","Inspired by this, our paper investigates whether LLMs can estimate their own hallucination risk before response generation.","We analyze the internal mechanisms of LLMs broadly both in terms of training data sources and across 15 diverse Natural Language Generation (NLG) tasks, spanning over 700 datasets.","Our empirical analysis reveals two key insights: (1) LLM internal states indicate whether they have seen the query in training data or not; and (2) LLM internal states show they are likely to hallucinate or not regarding the query.","Our study explores particular neurons, activation layers, and tokens that play a crucial role in the LLM perception of uncertainty and hallucination risk.","By a probing estimator, we leverage LLM self-assessment, achieving an average hallucination estimation accuracy of 84.32\\% at run time."],"url":"http://arxiv.org/abs/2407.03282v1"}
{"created":"2024-07-03 17:04:47","title":"Cooperative Multi-Agent Deep Reinforcement Learning Methods for UAV-aided Mobile Edge Computing Networks","abstract":"This paper presents a cooperative multi-agent deep reinforcement learning (MADRL) approach for unmmaned aerial vehicle (UAV)-aided mobile edge computing (MEC) networks. An UAV with computing capability can provide task offlaoding services to ground internet-of-things devices (IDs). With partial observation of the entire network state, the UAV and the IDs individually determine their MEC strategies, i.e., UAV trajectory, resource allocation, and task offloading policy. This requires joint optimization of decision-making process and coordination strategies among the UAV and the IDs. To address this difficulty, the proposed cooperative MADRL approach computes two types of action variables, namely message action and solution action, each of which is generated by dedicated actor neural networks (NNs). As a result, each agent can automatically encapsulate its coordination messages to enhance the MEC performance in the decentralized manner. The proposed actor structure is designed based on graph attention networks such that operations are possible regardless of the number of IDs. A scalable training algorithm is also proposed to train a group of NNs for arbitrary network configurations. Numerical results demonstrate the superiority of the proposed cooperative MADRL approach over conventional methods.","sentences":["This paper presents a cooperative multi-agent deep reinforcement learning (MADRL) approach for unmmaned aerial vehicle (UAV)-aided mobile edge computing (MEC) networks.","An UAV with computing capability can provide task offlaoding services to ground internet-of-things devices (IDs).","With partial observation of the entire network state, the UAV and the IDs individually determine their MEC strategies, i.e., UAV trajectory, resource allocation, and task offloading policy.","This requires joint optimization of decision-making process and coordination strategies among the UAV and the IDs.","To address this difficulty, the proposed cooperative MADRL approach computes two types of action variables, namely message action and solution action, each of which is generated by dedicated actor neural networks (NNs).","As a result, each agent can automatically encapsulate its coordination messages to enhance the MEC performance in the decentralized manner.","The proposed actor structure is designed based on graph attention networks such that operations are possible regardless of the number of IDs.","A scalable training algorithm is also proposed to train a group of NNs for arbitrary network configurations.","Numerical results demonstrate the superiority of the proposed cooperative MADRL approach over conventional methods."],"url":"http://arxiv.org/abs/2407.03280v1"}
{"created":"2024-07-03 17:04:17","title":"Evaluating Automatic Metrics with Incremental Machine Translation Systems","abstract":"We introduce a dataset comprising commercial machine translations, gathered weekly over six years across 12 translation directions. Since human A/B testing is commonly used, we assume commercial systems improve over time, which enables us to evaluate machine translation (MT) metrics based on their preference for more recent translations. Our study confirms several previous findings in MT metrics research and demonstrates the dataset's value as a testbed for metric evaluation. We release our code at https://github.com/gjwubyron/Evo","sentences":["We introduce a dataset comprising commercial machine translations, gathered weekly over six years across 12 translation directions.","Since human A/B testing is commonly used, we assume commercial systems improve over time, which enables us to evaluate machine translation (MT) metrics based on their preference for more recent translations.","Our study confirms several previous findings in MT metrics research and demonstrates the dataset's value as a testbed for metric evaluation.","We release our code at https://github.com/gjwubyron/Evo"],"url":"http://arxiv.org/abs/2407.03277v1"}
{"created":"2024-07-03 16:57:38","title":"For a semiotic AI: Bridging computer vision and visual semiotics for computational observation of large scale facial image archives","abstract":"Social networks are creating a digital world in which the cognitive, emotional, and pragmatic value of the imagery of human faces and bodies is arguably changing. However, researchers in the digital humanities are often ill-equipped to study these phenomena at scale. This work presents FRESCO (Face Representation in E-Societies through Computational Observation), a framework designed to explore the socio-cultural implications of images on social media platforms at scale. FRESCO deconstructs images into numerical and categorical variables using state-of-the-art computer vision techniques, aligning with the principles of visual semiotics. The framework analyzes images across three levels: the plastic level, encompassing fundamental visual features like lines and colors; the figurative level, representing specific entities or concepts; and the enunciation level, which focuses particularly on constructing the point of view of the spectator and observer. These levels are analyzed to discern deeper narrative layers within the imagery. Experimental validation confirms the reliability and utility of FRESCO, and we assess its consistency and precision across two public datasets. Subsequently, we introduce the FRESCO score, a metric derived from the framework's output that serves as a reliable measure of similarity in image content.","sentences":["Social networks are creating a digital world in which the cognitive, emotional, and pragmatic value of the imagery of human faces and bodies is arguably changing.","However, researchers in the digital humanities are often ill-equipped to study these phenomena at scale.","This work presents FRESCO (Face Representation in E-Societies through Computational Observation), a framework designed to explore the socio-cultural implications of images on social media platforms at scale.","FRESCO deconstructs images into numerical and categorical variables using state-of-the-art computer vision techniques, aligning with the principles of visual semiotics.","The framework analyzes images across three levels: the plastic level, encompassing fundamental visual features like lines and colors; the figurative level, representing specific entities or concepts; and the enunciation level, which focuses particularly on constructing the point of view of the spectator and observer.","These levels are analyzed to discern deeper narrative layers within the imagery.","Experimental validation confirms the reliability and utility of FRESCO, and we assess its consistency and precision across two public datasets.","Subsequently, we introduce the FRESCO score, a metric derived from the framework's output that serves as a reliable measure of similarity in image content."],"url":"http://arxiv.org/abs/2407.03268v1"}
{"created":"2024-07-03 16:52:23","title":"Anomaly-based Framework for Detecting Power Overloading Cyberattacks in Smart Grid AMI","abstract":"The Advanced Metering Infrastructure (AMI) is one of the key components of the smart grid. It provides interactive services for managing billing and electricity consumption, but it also introduces new vectors for cyberattacks. Although, the devastating and severe impact of power overloading cyberattacks on smart grid AMI, few researches in the literature have addressed them. In the present paper, we propose a two-level anomaly detection framework based on regression decision trees. The introduced detection approach leverages the regularity and predictability of energy consumption to build reference consumption patterns for the whole neighborhood and each household within it. Using a reference consumption pattern enables detecting power overloading cyberattacks regardless of the attacker's strategy as they cause a drastic change in the consumption pattern. The continuous two-level monitoring of energy consumption load allows efficient and early detection of cyberattacks. We carried out an extensive experiment on a real-world publicly available energy consumption dataset of 500 customers in Ireland. We extracted, from the raw data, the relevant attributes for training the energy consumption patterns. The evaluation shows that our approach achieves a high detection rate, a low false alarm rate, and superior performances compared to existing solutions.","sentences":["The Advanced Metering Infrastructure (AMI) is one of the key components of the smart grid.","It provides interactive services for managing billing and electricity consumption, but it also introduces new vectors for cyberattacks.","Although, the devastating and severe impact of power overloading cyberattacks on smart grid AMI, few researches in the literature have addressed them.","In the present paper, we propose a two-level anomaly detection framework based on regression decision trees.","The introduced detection approach leverages the regularity and predictability of energy consumption to build reference consumption patterns for the whole neighborhood and each household within it.","Using a reference consumption pattern enables detecting power overloading cyberattacks regardless of the attacker's strategy as they cause a drastic change in the consumption pattern.","The continuous two-level monitoring of energy consumption load allows efficient and early detection of cyberattacks.","We carried out an extensive experiment on a real-world publicly available energy consumption dataset of 500 customers in Ireland.","We extracted, from the raw data, the relevant attributes for training the energy consumption patterns.","The evaluation shows that our approach achieves a high detection rate, a low false alarm rate, and superior performances compared to existing solutions."],"url":"http://arxiv.org/abs/2407.03264v1"}
{"created":"2024-07-03 16:50:07","title":"A Unified Framework for 3D Scene Understanding","abstract":"We propose UniSeg3D, a unified 3D segmentation framework that achieves panoptic, semantic, instance, interactive, referring, and open-vocabulary semantic segmentation tasks within a single model. Most previous 3D segmentation approaches are specialized for a specific task, thereby limiting their understanding of 3D scenes to a task-specific perspective. In contrast, the proposed method unifies six tasks into unified representations processed by the same Transformer. It facilitates inter-task knowledge sharing and, therefore, promotes comprehensive 3D scene understanding. To take advantage of multi-task unification, we enhance the performance by leveraging task connections. Specifically, we design a knowledge distillation method and a contrastive learning method to transfer task-specific knowledge across different tasks. Benefiting from extensive inter-task knowledge sharing, our UniSeg3D becomes more powerful. Experiments on three benchmarks, including the ScanNet20, ScanRefer, and ScanNet200, demonstrate that the UniSeg3D consistently outperforms current SOTA methods, even those specialized for individual tasks. We hope UniSeg3D can serve as a solid unified baseline and inspire future work. The code will be available at https://dk-liang.github.io/UniSeg3D/.","sentences":["We propose UniSeg3D, a unified 3D segmentation framework that achieves panoptic, semantic, instance, interactive, referring, and open-vocabulary semantic segmentation tasks within a single model.","Most previous 3D segmentation approaches are specialized for a specific task, thereby limiting their understanding of 3D scenes to a task-specific perspective.","In contrast, the proposed method unifies six tasks into unified representations processed by the same Transformer.","It facilitates inter-task knowledge sharing and, therefore, promotes comprehensive 3D scene understanding.","To take advantage of multi-task unification, we enhance the performance by leveraging task connections.","Specifically, we design a knowledge distillation method and a contrastive learning method to transfer task-specific knowledge across different tasks.","Benefiting from extensive inter-task knowledge sharing, our UniSeg3D becomes more powerful.","Experiments on three benchmarks, including the ScanNet20, ScanRefer, and ScanNet200, demonstrate that the UniSeg3D consistently outperforms current SOTA methods, even those specialized for individual tasks.","We hope UniSeg3D can serve as a solid unified baseline and inspire future work.","The code will be available at https://dk-liang.github.io/UniSeg3D/."],"url":"http://arxiv.org/abs/2407.03263v1"}
{"created":"2024-07-03 16:49:28","title":"Nearly Linear Sparsification of $\\ell_p$ Subspace Approximation","abstract":"The $\\ell_p$ subspace approximation problem is an NP-hard low rank approximation problem that generalizes the median hyperplane problem ($p = 1$), principal component analysis ($p = 2$), and the center hyperplane problem ($p = \\infty$). A popular approach to cope with the NP-hardness of this problem is to compute a strong coreset, which is a small weighted subset of the input points which simultaneously approximates the cost of every $k$-dimensional subspace, typically to $(1+\\varepsilon)$ relative error for a small constant $\\varepsilon$.   We obtain the first algorithm for constructing a strong coreset for $\\ell_p$ subspace approximation with a nearly optimal dependence on the rank parameter $k$, obtaining a nearly linear bound of $\\tilde O(k)\\mathrm{poly}(\\varepsilon^{-1})$ for $p<2$ and $\\tilde O(k^{p/2})\\mathrm{poly}(\\varepsilon^{-1})$ for $p>2$. Prior constructions either achieved a similar size bound but produced a coreset with a modification of the original points [SW18, FKW21], or produced a coreset of the original points but lost $\\mathrm{poly}(k)$ factors in the coreset size [HV20, WY23].   Our techniques also lead to the first nearly optimal online strong coresets for $\\ell_p$ subspace approximation with similar bounds as the offline setting, resolving a problem of [WY23]. All prior approaches lose $\\mathrm{poly}(k)$ factors in this setting, even when allowed to modify the original points.","sentences":["The $\\ell_p$ subspace approximation problem is an NP-hard low rank approximation problem that generalizes the median hyperplane problem ($p = 1$), principal component analysis ($p = 2$), and the center hyperplane problem ($p = \\infty$).","A popular approach to cope with the NP-hardness of this problem is to compute a strong coreset, which is a small weighted subset of the input points which simultaneously approximates the cost of every $k$-dimensional subspace, typically to $(1+\\varepsilon)$ relative error for a small constant $\\varepsilon$.   We obtain the first algorithm for constructing a strong coreset for $\\ell_p$ subspace approximation with a nearly optimal dependence on the rank parameter $k$, obtaining a nearly linear bound of $\\tilde O(k)\\mathrm{poly}(\\varepsilon^{-1})$ for $p<2$ and $\\tilde O(k^{p/2})\\mathrm{poly}(\\varepsilon^{-1})$ for $p>2$. Prior constructions either achieved a similar size bound but produced a coreset with a modification of the original points","[SW18, FKW21], or produced a coreset of the original points but lost $\\mathrm{poly}(k)$ factors in the coreset size [HV20, WY23].   ","Our techniques also lead to the first nearly optimal online strong coresets for $\\ell_p$ subspace approximation with similar bounds as the offline setting, resolving a problem of [WY23].","All prior approaches lose $\\mathrm{poly}(k)$ factors in this setting, even when allowed to modify the original points."],"url":"http://arxiv.org/abs/2407.03262v1"}
{"created":"2024-07-03 16:45:45","title":"Magnetic Hysteresis Modeling with Neural Operators","abstract":"Hysteresis modeling is crucial to comprehend the behavior of magnetic devices, facilitating optimal designs. Hitherto, deep learning-based methods employed to model hysteresis, face challenges in generalizing to novel input magnetic fields. This paper addresses the generalization challenge by proposing neural operators for modeling constitutive laws that exhibit magnetic hysteresis by learning a mapping between magnetic fields. In particular, two prominent neural operators -- deep operator network and Fourier neural operator -- are employed to predict novel first-order reversal curves and minor loops, where novel means they are not used to train the model. In addition, a rate-independent Fourier neural operator is proposed to predict material responses at sampling rates different from those used during training to incorporate the rate-independent characteristics of magnetic hysteresis. The presented numerical experiments demonstrate that neural operators efficiently model magnetic hysteresis, outperforming the traditional neural recurrent methods on various metrics and generalizing to novel magnetic fields. The findings emphasize the advantages of using neural operators for modeling hysteresis under varying magnetic conditions, underscoring their importance in characterizing magnetic material based devices.","sentences":["Hysteresis modeling is crucial to comprehend the behavior of magnetic devices, facilitating optimal designs.","Hitherto, deep learning-based methods employed to model hysteresis, face challenges in generalizing to novel input magnetic fields.","This paper addresses the generalization challenge by proposing neural operators for modeling constitutive laws that exhibit magnetic hysteresis by learning a mapping between magnetic fields.","In particular, two prominent neural operators -- deep operator network and Fourier neural operator -- are employed to predict novel first-order reversal curves and minor loops, where novel means they are not used to train the model.","In addition, a rate-independent Fourier neural operator is proposed to predict material responses at sampling rates different from those used during training to incorporate the rate-independent characteristics of magnetic hysteresis.","The presented numerical experiments demonstrate that neural operators efficiently model magnetic hysteresis, outperforming the traditional neural recurrent methods on various metrics and generalizing to novel magnetic fields.","The findings emphasize the advantages of using neural operators for modeling hysteresis under varying magnetic conditions, underscoring their importance in characterizing magnetic material based devices."],"url":"http://arxiv.org/abs/2407.03261v1"}
{"created":"2024-07-03 16:38:57","title":"Modern Neighborhood Components Analysis: A Deep Tabular Baseline Two Decades Later","abstract":"The growing success of deep learning in various domains has prompted investigations into its application to tabular data, where deep models have shown promising results compared to traditional tree-based methods. In this paper, we revisit Neighborhood Component Analysis (NCA), a classic tabular prediction method introduced in 2004, designed to learn a linear projection that captures semantic similarities between instances. We find that minor modifications, such as adjustments to the learning objectives and the integration of deep learning architectures, significantly enhance NCA's performance, enabling it to surpass most modern deep tabular models. Additionally, we introduce a stochastic neighbor sampling strategy that improves both the efficiency and predictive accuracy of our proposed ModernNCA -- sampling only a subset of neighbors during training, while utilizing the entire neighborhood during inference. Extensive experiments demonstrate that our ModernNCA achieves state-of-the-art results in both classification and regression tasks across various tabular datasets, outperforming both tree-based and other deep tabular models, while also reducing training time and model size.","sentences":["The growing success of deep learning in various domains has prompted investigations into its application to tabular data, where deep models have shown promising results compared to traditional tree-based methods.","In this paper, we revisit Neighborhood Component Analysis (NCA), a classic tabular prediction method introduced in 2004, designed to learn a linear projection that captures semantic similarities between instances.","We find that minor modifications, such as adjustments to the learning objectives and the integration of deep learning architectures, significantly enhance NCA's performance, enabling it to surpass most modern deep tabular models.","Additionally, we introduce a stochastic neighbor sampling strategy that improves both the efficiency and predictive accuracy of our proposed ModernNCA -- sampling only a subset of neighbors during training, while utilizing the entire neighborhood during inference.","Extensive experiments demonstrate that our ModernNCA achieves state-of-the-art results in both classification and regression tasks across various tabular datasets, outperforming both tree-based and other deep tabular models, while also reducing training time and model size."],"url":"http://arxiv.org/abs/2407.03257v1"}
{"created":"2024-07-03 16:36:26","title":"How Similar Are Elected Politicians and Their Constituents? Quantitative Evidence From Online Social Network","abstract":"How similar are politicians to those who vote for them? This is a critical question at the heart of democratic representation and particularly relevant at times when political dissatisfaction and populism are on the rise. To answer this question we compare the online discourse of elected politicians and their constituents. We collect a two and a half years (September 2020 - February 2023) constituency-level dataset for USA and UK that includes: (i) the Twitter timelines (5.6 Million tweets) of elected political representatives (595 UK Members of Parliament and 433 USA Representatives), (ii) the Nextdoor posts (21.8 Million posts) of the constituency (98.4% USA and 91.5% UK constituencies). We find that elected politicians tend to be equally similar to their constituents in terms of content and style regardless of whether a constituency elects a right or left-wing politician. The size of the electoral victory and the level of income of a constituency shows a nuanced picture. The narrower the electoral victory, the more similar the style and the more dissimilar the content is. The lower the income of a constituency, the more similar the content is. In terms of style, poorer constituencies tend to have a more similar sentiment and more dissimilar psychological text traits (i.e. measured with LIWC categories).","sentences":["How similar are politicians to those who vote for them?","This is a critical question at the heart of democratic representation and particularly relevant at times when political dissatisfaction and populism are on the rise.","To answer this question we compare the online discourse of elected politicians and their constituents.","We collect a two and a half years (September 2020 - February 2023) constituency-level dataset for USA and UK that includes: (i) the Twitter timelines (5.6 Million tweets) of elected political representatives (595 UK Members of Parliament and 433 USA Representatives), (ii) the Nextdoor posts (21.8 Million posts) of the constituency (98.4% USA and 91.5% UK constituencies).","We find that elected politicians tend to be equally similar to their constituents in terms of content and style regardless of whether a constituency elects a right or left-wing politician.","The size of the electoral victory and the level of income of a constituency shows a nuanced picture.","The narrower the electoral victory, the more similar the style and the more dissimilar the content is.","The lower the income of a constituency, the more similar the content is.","In terms of style, poorer constituencies tend to have a more similar sentiment and more dissimilar psychological text traits (i.e. measured with LIWC categories)."],"url":"http://arxiv.org/abs/2407.03255v1"}
{"created":"2024-07-03 16:34:56","title":"STF: Sentence Transformer Fine-Tuning For Topic Categorization With Limited Data","abstract":"Nowadays, topic classification from tweets attracts considerable research attention. Different classification systems have been suggested thanks to these research efforts. Nevertheless, they face major challenges owing to low performance metrics due to the limited amount of labeled data. We propose Sentence Transformers Fine-tuning (STF), a topic detection system that leverages pretrained Sentence Transformers models and fine-tuning to classify topics from tweets accurately. Moreover, extensive parameter sensitivity analyses were conducted to finetune STF parameters for our topic classification task to achieve the best performance results. Experiments on two benchmark datasets demonstrated that (1) the proposed STF can be effectively used for classifying tweet topics and outperforms the latest state-of-the-art approaches, and (2) the proposed STF does not require a huge amount of labeled tweets to achieve good accuracy, which is a limitation of many state-of-the-art approaches. Our main contribution is the achievement of promising results in tweet topic classification by applying pretrained sentence transformers language models.","sentences":["Nowadays, topic classification from tweets attracts considerable research attention.","Different classification systems have been suggested thanks to these research efforts.","Nevertheless, they face major challenges owing to low performance metrics due to the limited amount of labeled data.","We propose Sentence Transformers Fine-tuning (STF), a topic detection system that leverages pretrained Sentence Transformers models and fine-tuning to classify topics from tweets accurately.","Moreover, extensive parameter sensitivity analyses were conducted to finetune STF parameters for our topic classification task to achieve the best performance results.","Experiments on two benchmark datasets demonstrated that (1) the proposed STF can be effectively used for classifying tweet topics and outperforms the latest state-of-the-art approaches, and (2) the proposed STF does not require a huge amount of labeled tweets to achieve good accuracy, which is a limitation of many state-of-the-art approaches.","Our main contribution is the achievement of promising results in tweet topic classification by applying pretrained sentence transformers language models."],"url":"http://arxiv.org/abs/2407.03253v1"}
{"created":"2024-07-03 16:33:31","title":"ACTRESS: Active Retraining for Semi-supervised Visual Grounding","abstract":"Semi-Supervised Visual Grounding (SSVG) is a new challenge for its sparse labeled data with the need for multimodel understanding. A previous study, RefTeacher, makes the first attempt to tackle this task by adopting the teacher-student framework to provide pseudo confidence supervision and attention-based supervision. However, this approach is incompatible with current state-of-the-art visual grounding models, which follow the Transformer-based pipeline. These pipelines directly regress results without region proposals or foreground binary classification, rendering them unsuitable for fitting in RefTeacher due to the absence of confidence scores. Furthermore, the geometric difference in teacher and student inputs, stemming from different data augmentations, induces natural misalignment in attention-based constraints. To establish a compatible SSVG framework, our paper proposes the ACTive REtraining approach for Semi-Supervised Visual Grounding, abbreviated as ACTRESS. Initially, the model is enhanced by incorporating an additional quantized detection head to expose its detection confidence. Building upon this, ACTRESS consists of an active sampling strategy and a selective retraining strategy. The active sampling strategy iteratively selects high-quality pseudo labels by evaluating three crucial aspects: Faithfulness, Robustness, and Confidence, optimizing the utilization of unlabeled data. The selective retraining strategy retrains the model with periodic re-initialization of specific parameters, facilitating the model's escape from local minima. Extensive experiments demonstrates our superior performance on widely-used benchmark datasets.","sentences":["Semi-Supervised Visual Grounding (SSVG) is a new challenge for its sparse labeled data with the need for multimodel understanding.","A previous study, RefTeacher, makes the first attempt to tackle this task by adopting the teacher-student framework to provide pseudo confidence supervision and attention-based supervision.","However, this approach is incompatible with current state-of-the-art visual grounding models, which follow the Transformer-based pipeline.","These pipelines directly regress results without region proposals or foreground binary classification, rendering them unsuitable for fitting in RefTeacher due to the absence of confidence scores.","Furthermore, the geometric difference in teacher and student inputs, stemming from different data augmentations, induces natural misalignment in attention-based constraints.","To establish a compatible SSVG framework, our paper proposes the ACTive REtraining approach for Semi-Supervised Visual Grounding, abbreviated as ACTRESS.","Initially, the model is enhanced by incorporating an additional quantized detection head to expose its detection confidence.","Building upon this, ACTRESS consists of an active sampling strategy and a selective retraining strategy.","The active sampling strategy iteratively selects high-quality pseudo labels by evaluating three crucial aspects: Faithfulness, Robustness, and Confidence, optimizing the utilization of unlabeled data.","The selective retraining strategy retrains the model with periodic re-initialization of specific parameters, facilitating the model's escape from local minima.","Extensive experiments demonstrates our superior performance on widely-used benchmark datasets."],"url":"http://arxiv.org/abs/2407.03251v1"}
{"created":"2024-07-03 16:23:41","title":"Bridging Model Heterogeneity in Federated Learning via Uncertainty-based Asymmetrical Reciprocity Learning","abstract":"This paper presents FedType, a simple yet pioneering framework designed to fill research gaps in heterogeneous model aggregation within federated learning (FL). FedType introduces small identical proxy models for clients, serving as agents for information exchange, ensuring model security, and achieving efficient communication simultaneously. To transfer knowledge between large private and small proxy models on clients, we propose a novel uncertainty-based asymmetrical reciprocity learning method, eliminating the need for any public data. Comprehensive experiments conducted on benchmark datasets demonstrate the efficacy and generalization ability of FedType across diverse settings. Our approach redefines federated learning paradigms by bridging model heterogeneity, eliminating reliance on public data, prioritizing client privacy, and reducing communication costs.","sentences":["This paper presents FedType, a simple yet pioneering framework designed to fill research gaps in heterogeneous model aggregation within federated learning (FL).","FedType introduces small identical proxy models for clients, serving as agents for information exchange, ensuring model security, and achieving efficient communication simultaneously.","To transfer knowledge between large private and small proxy models on clients, we propose a novel uncertainty-based asymmetrical reciprocity learning method, eliminating the need for any public data.","Comprehensive experiments conducted on benchmark datasets demonstrate the efficacy and generalization ability of FedType across diverse settings.","Our approach redefines federated learning paradigms by bridging model heterogeneity, eliminating reliance on public data, prioritizing client privacy, and reducing communication costs."],"url":"http://arxiv.org/abs/2407.03247v1"}
{"created":"2024-07-03 16:16:41","title":"TieBot: Learning to Knot a Tie from Visual Demonstration through a Real-to-Sim-to-Real Approach","abstract":"The tie-knotting task is highly challenging due to the tie's high deformation and long-horizon manipulation actions. This work presents TieBot, a Real-to-Sim-to-Real learning from visual demonstration system for the robots to learn to knot a tie. We introduce the Hierarchical Feature Matching approach to estimate a sequence of tie's meshes from the demonstration video. With these estimated meshes used as subgoals, we first learn a teacher policy using privileged information. Then, we learn a student policy with point cloud observation by imitating teacher policy. Lastly, our pipeline learns a residual policy when the learned policy is applied to real-world execution, mitigating the Sim2Real gap. We demonstrate the effectiveness of TieBot in simulation and the real world. In the real-world experiment, a dual-arm robot successfully knots a tie, achieving 50% success rate among 10 trials. Videos can be found on our $\\href{https://tiebots.github.io/}{\\text{website}}$.","sentences":["The tie-knotting task is highly challenging due to the tie's high deformation and long-horizon manipulation actions.","This work presents TieBot, a Real-to-Sim-to-Real learning from visual demonstration system for the robots to learn to knot a tie.","We introduce the Hierarchical Feature Matching approach to estimate a sequence of tie's meshes from the demonstration video.","With these estimated meshes used as subgoals, we first learn a teacher policy using privileged information.","Then, we learn a student policy with point cloud observation by imitating teacher policy.","Lastly, our pipeline learns a residual policy when the learned policy is applied to real-world execution, mitigating the Sim2Real gap.","We demonstrate the effectiveness of TieBot in simulation and the real world.","In the real-world experiment, a dual-arm robot successfully knots a tie, achieving 50% success rate among 10 trials.","Videos can be found on our $\\href{https://tiebots.github.io/}{\\text{website}}$."],"url":"http://arxiv.org/abs/2407.03245v1"}
{"created":"2024-07-03 16:14:09","title":"Visual Grounding with Attention-Driven Constraint Balancing","abstract":"Unlike Object Detection, Visual Grounding task necessitates the detection of an object described by complex free-form language. To simultaneously model such complex semantic and visual representations, recent state-of-the-art studies adopt transformer-based models to fuse features from both modalities, further introducing various modules that modulate visual features to align with the language expressions and eliminate the irrelevant redundant information. However, their loss function, still adopting common Object Detection losses, solely governs the bounding box regression output, failing to fully optimize for the above objectives. To tackle this problem, in this paper, we first analyze the attention mechanisms of transformer-based models. Building upon this, we further propose a novel framework named Attention-Driven Constraint Balancing (AttBalance) to optimize the behavior of visual features within language-relevant regions. Extensive experimental results show that our method brings impressive improvements. Specifically, we achieve constant improvements over five different models evaluated on four different benchmarks. Moreover, we attain a new state-of-the-art performance by integrating our method into QRNet.","sentences":["Unlike Object Detection, Visual Grounding task necessitates the detection of an object described by complex free-form language.","To simultaneously model such complex semantic and visual representations, recent state-of-the-art studies adopt transformer-based models to fuse features from both modalities, further introducing various modules that modulate visual features to align with the language expressions and eliminate the irrelevant redundant information.","However, their loss function, still adopting common Object Detection losses, solely governs the bounding box regression output, failing to fully optimize for the above objectives.","To tackle this problem, in this paper, we first analyze the attention mechanisms of transformer-based models.","Building upon this, we further propose a novel framework named Attention-Driven Constraint Balancing (AttBalance) to optimize the behavior of visual features within language-relevant regions.","Extensive experimental results show that our method brings impressive improvements.","Specifically, we achieve constant improvements over five different models evaluated on four different benchmarks.","Moreover, we attain a new state-of-the-art performance by integrating our method into QRNet."],"url":"http://arxiv.org/abs/2407.03243v1"}
{"created":"2024-07-03 16:10:50","title":"Terrain Classification Enhanced with Uncertainty for Space Exploration Robots from Proprioceptive Data","abstract":"Terrain Classification is an essential task in space exploration, where unpredictable environments are difficult to observe using only exteroceptive sensors such as vision. Implementing Neural Network classifiers can have high performance but can be deemed untrustworthy as they lack transparency, which makes them unreliable for taking high-stakes decisions during mission planning. We address this by proposing Neural Networks with Uncertainty Quantification in Terrain Classification. We enable our Neural Networks with Monte Carlo Dropout, DropConnect, and Flipout in time series-capable architectures using only proprioceptive data as input. We use Bayesian Optimization with Hyperband for efficient hyperparameter optimization to find optimal models for trustworthy terrain classification.","sentences":["Terrain Classification is an essential task in space exploration, where unpredictable environments are difficult to observe using only exteroceptive sensors such as vision.","Implementing Neural Network classifiers can have high performance but can be deemed untrustworthy as they lack transparency, which makes them unreliable for taking high-stakes decisions during mission planning.","We address this by proposing Neural Networks with Uncertainty Quantification in Terrain Classification.","We enable our Neural Networks with Monte Carlo Dropout, DropConnect, and Flipout in time series-capable architectures using only proprioceptive data as input.","We use Bayesian Optimization with Hyperband for efficient hyperparameter optimization to find optimal models for trustworthy terrain classification."],"url":"http://arxiv.org/abs/2407.03241v1"}
{"created":"2024-07-03 16:10:19","title":"Cyclic Refiner: Object-Aware Temporal Representation Learning for Multi-View 3D Detection and Tracking","abstract":"We propose a unified object-aware temporal learning framework for multi-view 3D detection and tracking tasks. Having observed that the efficacy of the temporal fusion strategy in recent multi-view perception methods may be weakened by distractors and background clutters in historical frames, we propose a cyclic learning mechanism to improve the robustness of multi-view representation learning. The essence is constructing a backward bridge to propagate information from model predictions (e.g., object locations and sizes) to image and BEV features, which forms a circle with regular inference. After backward refinement, the responses of target-irrelevant regions in historical frames would be suppressed, decreasing the risk of polluting future frames and improving the object awareness ability of temporal fusion. We further tailor an object-aware association strategy for tracking based on the cyclic learning model. The cyclic learning model not only provides refined features, but also delivers finer clues (e.g., scale level) for tracklet association. The proposed cycle learning method and association module together contribute a novel and unified multi-task framework. Experiments on nuScenes show that the proposed model achieves consistent performance gains over baselines of different designs (i.e., dense query-based BEVFormer, sparse query-based SparseBEV and LSS-based BEVDet4D) on both detection and tracking evaluation.","sentences":["We propose a unified object-aware temporal learning framework for multi-view 3D detection and tracking tasks.","Having observed that the efficacy of the temporal fusion strategy in recent multi-view perception methods may be weakened by distractors and background clutters in historical frames, we propose a cyclic learning mechanism to improve the robustness of multi-view representation learning.","The essence is constructing a backward bridge to propagate information from model predictions (e.g., object locations and sizes) to image and BEV features, which forms a circle with regular inference.","After backward refinement, the responses of target-irrelevant regions in historical frames would be suppressed, decreasing the risk of polluting future frames and improving the object awareness ability of temporal fusion.","We further tailor an object-aware association strategy for tracking based on the cyclic learning model.","The cyclic learning model not only provides refined features, but also delivers finer clues (e.g., scale level) for tracklet association.","The proposed cycle learning method and association module together contribute a novel and unified multi-task framework.","Experiments on nuScenes show that the proposed model achieves consistent performance gains over baselines of different designs (i.e., dense query-based BEVFormer, sparse query-based SparseBEV and LSS-based BEVDet4D) on both detection and tracking evaluation."],"url":"http://arxiv.org/abs/2407.03240v1"}
{"created":"2024-07-03 16:08:05","title":"Reconsidering utility: unveiling the limitations of synthetic mobility data generation algorithms in real-life scenarios","abstract":"In recent years, there has been a surge in the development of models for the generation of synthetic mobility data. These models aim to facilitate the sharing of data while safeguarding privacy, all while ensuring high utility and flexibility regarding potential applications. However, current utility evaluation methods fail to fully account for real-life requirements. We evaluate the utility of five state-of-the-art synthesis approaches, each with and without the incorporation of differential privacy (DP) guarantees, in terms of real-world applicability. Specifically, we focus on so-called trip data that encode fine granular urban movements such as GPS-tracked taxi rides. Such data prove particularly valuable for downstream tasks at the road network level. Thus, our initial step involves appropriately map matching the synthetic data and subsequently comparing the resulting trips with those generated by the routing algorithm implemented in OpenStreetMap, which serves as an efficient and privacy-friendly baseline. Out of the five evaluated models, one fails to produce data within reasonable computation time and another generates too many jumps to meet the requirements for map matching. The remaining three models succeed to a certain degree in maintaining spatial distribution, one even with DP guarantees. However, all models struggle to produce meaningful sequences of geo-locations with reasonable trip lengths and to model traffic flow at intersections accurately. It is important to note that trip data encompasses various relevant characteristics beyond spatial distribution, such as temporal information, all of which are discarded by these models. Consequently, our results imply that current synthesis models fall short in their promise of high utility and flexibility.","sentences":["In recent years, there has been a surge in the development of models for the generation of synthetic mobility data.","These models aim to facilitate the sharing of data while safeguarding privacy, all while ensuring high utility and flexibility regarding potential applications.","However, current utility evaluation methods fail to fully account for real-life requirements.","We evaluate the utility of five state-of-the-art synthesis approaches, each with and without the incorporation of differential privacy (DP) guarantees, in terms of real-world applicability.","Specifically, we focus on so-called trip data that encode fine granular urban movements such as GPS-tracked taxi rides.","Such data prove particularly valuable for downstream tasks at the road network level.","Thus, our initial step involves appropriately map matching the synthetic data and subsequently comparing the resulting trips with those generated by the routing algorithm implemented in OpenStreetMap, which serves as an efficient and privacy-friendly baseline.","Out of the five evaluated models, one fails to produce data within reasonable computation time and another generates too many jumps to meet the requirements for map matching.","The remaining three models succeed to a certain degree in maintaining spatial distribution, one even with DP guarantees.","However, all models struggle to produce meaningful sequences of geo-locations with reasonable trip lengths and to model traffic flow at intersections accurately.","It is important to note that trip data encompasses various relevant characteristics beyond spatial distribution, such as temporal information, all of which are discarded by these models.","Consequently, our results imply that current synthesis models fall short in their promise of high utility and flexibility."],"url":"http://arxiv.org/abs/2407.03237v1"}
{"created":"2024-07-03 16:05:20","title":"CATT: Character-based Arabic Tashkeel Transformer","abstract":"Tashkeel, or Arabic Text Diacritization (ATD), greatly enhances the comprehension of Arabic text by removing ambiguity and minimizing the risk of misinterpretations caused by its absence. It plays a crucial role in improving Arabic text processing, particularly in applications such as text-to-speech and machine translation. This paper introduces a new approach to training ATD models. First, we finetuned two transformers, encoder-only and encoder-decoder, that were initialized from a pretrained character-based BERT. Then, we applied the Noisy-Student approach to boost the performance of the best model. We evaluated our models alongside 11 commercial and open-source models using two manually labeled benchmark datasets: WikiNews and our CATT dataset. Our findings show that our top model surpasses all evaluated models by relative Diacritic Error Rates (DERs) of 30.83\\% and 35.21\\% on WikiNews and CATT, respectively, achieving state-of-the-art in ATD. In addition, we show that our model outperforms GPT-4-turbo on CATT dataset by a relative DER of 9.36\\%. We open-source our CATT models and benchmark dataset for the research community\\footnote{https://github.com/abjadai/catt}.","sentences":["Tashkeel, or Arabic Text Diacritization (ATD), greatly enhances the comprehension of Arabic text by removing ambiguity and minimizing the risk of misinterpretations caused by its absence.","It plays a crucial role in improving Arabic text processing, particularly in applications such as text-to-speech and machine translation.","This paper introduces a new approach to training ATD models.","First, we finetuned two transformers, encoder-only and encoder-decoder, that were initialized from a pretrained character-based BERT.","Then, we applied the Noisy-Student approach to boost the performance of the best model.","We evaluated our models alongside 11 commercial and open-source models using two manually labeled benchmark datasets: WikiNews and our CATT dataset.","Our findings show that our top model surpasses all evaluated models by relative Diacritic Error Rates (DERs) of 30.83\\% and 35.21\\% on WikiNews and CATT, respectively, achieving state-of-the-art in ATD.","In addition, we show that our model outperforms GPT-4-turbo on CATT dataset by a relative DER of 9.36\\%.","We open-source our CATT models and benchmark dataset for the research community\\footnote{https://github.com/abjadai/catt}."],"url":"http://arxiv.org/abs/2407.03236v1"}
{"created":"2024-07-03 16:03:42","title":"Self-Evaluation as a Defense Against Adversarial Attacks on LLMs","abstract":"When LLMs are deployed in sensitive, human-facing settings, it is crucial that they do not output unsafe, biased, or privacy-violating outputs. For this reason, models are both trained and instructed to refuse to answer unsafe prompts such as \"Tell me how to build a bomb.\" We find that, despite these safeguards, it is possible to break model defenses simply by appending a space to the end of a model's input. In a study of eight open-source models, we demonstrate that this acts as a strong enough attack to cause the majority of models to generate harmful outputs with very high success rates. We examine the causes of this behavior, finding that the contexts in which single spaces occur in tokenized training data encourage models to generate lists when prompted, overriding training signals to refuse to answer unsafe requests. Our findings underscore the fragile state of current model alignment and promote the importance of developing more robust alignment methods. Code and data will be made available at https://github.com/Linlt-leon/Adversarial-Alignments.","sentences":["When LLMs are deployed in sensitive, human-facing settings, it is crucial that they do not output unsafe, biased, or privacy-violating outputs.","For this reason, models are both trained and instructed to refuse to answer unsafe prompts such as \"Tell me how to build a bomb.\"","We find that, despite these safeguards, it is possible to break model defenses simply by appending a space to the end of a model's input.","In a study of eight open-source models, we demonstrate that this acts as a strong enough attack to cause the majority of models to generate harmful outputs with very high success rates.","We examine the causes of this behavior, finding that the contexts in which single spaces occur in tokenized training data encourage models to generate lists when prompted, overriding training signals to refuse to answer unsafe requests.","Our findings underscore the fragile state of current model alignment and promote the importance of developing more robust alignment methods.","Code and data will be made available at https://github.com/Linlt-leon/Adversarial-Alignments."],"url":"http://arxiv.org/abs/2407.03234v1"}
{"created":"2024-07-03 16:03:10","title":"Single Character Perturbations Break LLM Alignment","abstract":"When LLMs are deployed in sensitive, human-facing settings, it is crucial that they do not output unsafe, biased, or privacy-violating outputs. For this reason, models are both trained and instructed to refuse to answer unsafe prompts such as \"Tell me how to build a bomb.\" We find that, despite these safeguards, it is possible to break model defenses simply by appending a space to the end of a model's input. In a study of eight open-source models, we demonstrate that this acts as a strong enough attack to cause the majority of models to generate harmful outputs with very high success rates. We examine the causes of this behavior, finding that the contexts in which single spaces occur in tokenized training data encourage models to generate lists when prompted, overriding training signals to refuse to answer unsafe requests. Our findings underscore the fragile state of current model alignment and promote the importance of developing more robust alignment methods. Code and data will be available at https://github.com/hannah-aught/space_attack.","sentences":["When LLMs are deployed in sensitive, human-facing settings, it is crucial that they do not output unsafe, biased, or privacy-violating outputs.","For this reason, models are both trained and instructed to refuse to answer unsafe prompts such as \"Tell me how to build a bomb.\"","We find that, despite these safeguards, it is possible to break model defenses simply by appending a space to the end of a model's input.","In a study of eight open-source models, we demonstrate that this acts as a strong enough attack to cause the majority of models to generate harmful outputs with very high success rates.","We examine the causes of this behavior, finding that the contexts in which single spaces occur in tokenized training data encourage models to generate lists when prompted, overriding training signals to refuse to answer unsafe requests.","Our findings underscore the fragile state of current model alignment and promote the importance of developing more robust alignment methods.","Code and data will be available at https://github.com/hannah-aught/space_attack."],"url":"http://arxiv.org/abs/2407.03232v1"}
{"created":"2024-07-03 15:55:31","title":"Matroid Intersection under Minimum Rank Oracle","abstract":"In this paper, we consider the tractability of the matroid intersection problem under the minimum rank oracle. In this model, we are given an oracle that takes as its input a set of elements, and returns as its output the minimum of the ranks of the given set in the two matroids. For the unweighted matroid intersection problem, we show how to construct a necessary part of the exchangeability graph, which enables us to emulate the standard augmenting path algorithm. Furthermore, we reformulate Edmonds' min-max theorem only using the minimum rank function, providing a new perspective on this result. For the weighted problem, the tractability is open in general. Nevertheless, we describe several special cases where tractability can be achieved, and we discuss potential approaches and the challenges encountered. In particular, we present a solution for the case where no circuit of one matroid is contained within a circuit of the other. Additionally, we propose a fixed-parameter tractable algorithm, parameterized by the maximum circuit size. We also show that a lexicographically maximal common independent set can be found by the same approach, which leads to at least $1/2$-approximation for finding a maximum-weight common independent set.","sentences":["In this paper, we consider the tractability of the matroid intersection problem under the minimum rank oracle.","In this model, we are given an oracle that takes as its input a set of elements, and returns as its output the minimum of the ranks of the given set in the two matroids.","For the unweighted matroid intersection problem, we show how to construct a necessary part of the exchangeability graph, which enables us to emulate the standard augmenting path algorithm.","Furthermore, we reformulate Edmonds' min-max theorem only using the minimum rank function, providing a new perspective on this result.","For the weighted problem, the tractability is open in general.","Nevertheless, we describe several special cases where tractability can be achieved, and we discuss potential approaches and the challenges encountered.","In particular, we present a solution for the case where no circuit of one matroid is contained within a circuit of the other.","Additionally, we propose a fixed-parameter tractable algorithm, parameterized by the maximum circuit size.","We also show that a lexicographically maximal common independent set can be found by the same approach, which leads to at least $1/2$-approximation for finding a maximum-weight common independent set."],"url":"http://arxiv.org/abs/2407.03229v1"}
{"created":"2024-07-03 15:55:14","title":"Improving Retrieval-augmented Text-to-SQL with AST-based Ranking and Schema Pruning","abstract":"We focus on Text-to-SQL semantic parsing from the perspective of Large Language Models. Motivated by challenges related to the size of commercial database schemata and the deployability of business intelligence solutions, we propose an approach that dynamically retrieves input database information and uses abstract syntax trees to select few-shot examples for in-context learning.   Furthermore, we investigate the extent to which an in-parallel semantic parser can be leveraged for generating $\\textit{approximated}$ versions of the expected SQL queries, to support our retrieval. We take this approach to the extreme--we adapt a model consisting of less than $500$M parameters, to act as an extremely efficient approximator, enhancing it with the ability to process schemata in a parallelised manner. We apply our approach to monolingual and cross-lingual benchmarks for semantic parsing, showing improvements over state-of-the-art baselines. Comprehensive experiments highlight the contribution of modules involved in this retrieval-augmented generation setting, revealing interesting directions for future work.","sentences":["We focus on Text-to-SQL semantic parsing from the perspective of Large Language Models.","Motivated by challenges related to the size of commercial database schemata and the deployability of business intelligence solutions, we propose an approach that dynamically retrieves input database information and uses abstract syntax trees to select few-shot examples for in-context learning.   ","Furthermore, we investigate the extent to which an in-parallel semantic parser can be leveraged for generating $\\textit{approximated}$ versions of the expected SQL queries, to support our retrieval.","We take this approach to the extreme--we adapt a model consisting of less than $500$M parameters, to act as an extremely efficient approximator, enhancing it with the ability to process schemata in a parallelised manner.","We apply our approach to monolingual and cross-lingual benchmarks for semantic parsing, showing improvements over state-of-the-art baselines.","Comprehensive experiments highlight the contribution of modules involved in this retrieval-augmented generation setting, revealing interesting directions for future work."],"url":"http://arxiv.org/abs/2407.03227v1"}
{"created":"2024-07-03 15:51:06","title":"PPO-based Dynamic Control of Uncertain Floating Platforms in the Zero-G Environment","abstract":"In the field of space exploration, floating platforms play a crucial role in scientific investigations and technological advancements. However, controlling these platforms in zero-gravity environments presents unique challenges, including uncertainties and disturbances. This paper introduces an innovative approach that combines Proximal Policy Optimization (PPO) with Model Predictive Control (MPC) in the zero-gravity laboratory (Zero-G Lab) at the University of Luxembourg. This approach leverages PPO's reinforcement learning power and MPC's precision to navigate the complex control dynamics of floating platforms. Unlike traditional control methods, this PPO-MPC approach learns from MPC predictions, adapting to unmodeled dynamics and disturbances, resulting in a resilient control framework tailored to the zero-gravity environment. Simulations and experiments in the Zero-G Lab validate this approach, showcasing the adaptability of the PPO agent. This research opens new possibilities for controlling floating platforms in zero-gravity settings, promising advancements in space exploration.","sentences":["In the field of space exploration, floating platforms play a crucial role in scientific investigations and technological advancements.","However, controlling these platforms in zero-gravity environments presents unique challenges, including uncertainties and disturbances.","This paper introduces an innovative approach that combines Proximal Policy Optimization (PPO) with Model Predictive Control (MPC) in the zero-gravity laboratory (Zero-G Lab) at the University of Luxembourg.","This approach leverages PPO's reinforcement learning power and MPC's precision to navigate the complex control dynamics of floating platforms.","Unlike traditional control methods, this PPO-MPC approach learns from MPC predictions, adapting to unmodeled dynamics and disturbances, resulting in a resilient control framework tailored to the zero-gravity environment.","Simulations and experiments in the Zero-G Lab validate this approach, showcasing the adaptability of the PPO agent.","This research opens new possibilities for controlling floating platforms in zero-gravity settings, promising advancements in space exploration."],"url":"http://arxiv.org/abs/2407.03224v1"}
{"created":"2024-07-03 15:47:37","title":"Localization in Dynamic Planar Environments Using Few Distance Measurements","abstract":"We present a method for determining the unknown location of a sensor placed in a known 2D environment in the presence of unknown dynamic obstacles, using only few distance measurements. We present guarantees on the quality of the localization, which are robust under mild assumptions on the density of the unknown/dynamic obstacles in the known environment. We demonstrate the effectiveness of our method in simulated experiments for different environments and varying dynamic-obstacle density. Our open source software is available at https://github.com/TAU-CGL/vb-fdml2-public.","sentences":["We present a method for determining the unknown location of a sensor placed in a known 2D environment in the presence of unknown dynamic obstacles, using only few distance measurements.","We present guarantees on the quality of the localization, which are robust under mild assumptions on the density of the unknown/dynamic obstacles in the known environment.","We demonstrate the effectiveness of our method in simulated experiments for different environments and varying dynamic-obstacle density.","Our open source software is available at https://github.com/TAU-CGL/vb-fdml2-public."],"url":"http://arxiv.org/abs/2407.03219v1"}
{"created":"2024-07-03 15:45:48","title":"MHNet: Multi-view High-order Network for Diagnosing Neurodevelopmental Disorders Using Resting-state fMRI","abstract":"Background: Deep learning models have shown promise in diagnosing neurodevelopmental disorders (NDD) like ASD and ADHD. However, many models either use graph neural networks (GNN) to construct single-level brain functional networks (BFNs) or employ spatial convolution filtering for local information extraction from rs-fMRI data, often neglecting high-order features crucial for NDD classification. Methods: We introduce a Multi-view High-order Network (MHNet) to capture hierarchical and high-order features from multi-view BFNs derived from rs-fMRI data for NDD prediction. MHNet has two branches: the Euclidean Space Features Extraction (ESFE) module and the Non-Euclidean Space Features Extraction (Non-ESFE) module, followed by a Feature Fusion-based Classification (FFC) module for NDD identification. ESFE includes a Functional Connectivity Generation (FCG) module and a High-order Convolutional Neural Network (HCNN) module to extract local and high-order features from BFNs in Euclidean space. Non-ESFE comprises a Generic Internet-like Brain Hierarchical Network Generation (G-IBHN-G) module and a High-order Graph Neural Network (HGNN) module to capture topological and high-order features in non-Euclidean space. Results: Experiments on three public datasets show that MHNet outperforms state-of-the-art methods using both AAL1 and Brainnetome Atlas templates. Extensive ablation studies confirm the superiority of MHNet and the effectiveness of using multi-view fMRI information and high-order features. Our study also offers atlas options for constructing more sophisticated hierarchical networks and explains the association between key brain regions and NDD. Conclusion: MHNet leverages multi-view feature learning from both Euclidean and non-Euclidean spaces, incorporating high-order information from BFNs to enhance NDD classification performance.","sentences":["Background: Deep learning models have shown promise in diagnosing neurodevelopmental disorders (NDD) like ASD and ADHD.","However, many models either use graph neural networks (GNN) to construct single-level brain functional networks (BFNs) or employ spatial convolution filtering for local information extraction from rs-fMRI data, often neglecting high-order features crucial for NDD classification.","Methods: We introduce a Multi-view High-order Network (MHNet) to capture hierarchical and high-order features from multi-view BFNs derived from rs-fMRI data for NDD prediction.","MHNet has two branches: the Euclidean Space Features Extraction (ESFE) module and the Non-Euclidean Space Features Extraction (Non-ESFE) module, followed by a Feature Fusion-based Classification (FFC) module for NDD identification.","ESFE includes a Functional Connectivity Generation (FCG) module and a High-order Convolutional Neural Network (HCNN) module to extract local and high-order features from BFNs in Euclidean space.","Non-ESFE comprises a Generic Internet-like Brain Hierarchical Network Generation (G-IBHN-G) module and a High-order Graph Neural Network (HGNN) module to capture topological and high-order features in non-Euclidean space.","Results:","Experiments on three public datasets show that MHNet outperforms state-of-the-art methods using both AAL1 and Brainnetome Atlas templates.","Extensive ablation studies confirm the superiority of MHNet and the effectiveness of using multi-view fMRI information and high-order features.","Our study also offers atlas options for constructing more sophisticated hierarchical networks and explains the association between key brain regions and NDD.","Conclusion: MHNet leverages multi-view feature learning from both Euclidean and non-Euclidean spaces, incorporating high-order information from BFNs to enhance NDD classification performance."],"url":"http://arxiv.org/abs/2407.03217v1"}
{"created":"2024-07-03 15:43:54","title":"Learning Disentangled Representation in Object-Centric Models for Visual Dynamics Prediction via Transformers","abstract":"Recent work has shown that object-centric representations can greatly help improve the accuracy of learning dynamics while also bringing interpretability. In this work, we take this idea one step further, ask the following question: \"can learning disentangled representation further improve the accuracy of visual dynamics prediction in object-centric models?\" While there has been some attempt to learn such disentangled representations for the case of static images \\citep{nsb}, to the best of our knowledge, ours is the first work which tries to do this in a general setting for video, without making any specific assumptions about the kind of attributes that an object might have. The key building block of our architecture is the notion of a {\\em block}, where several blocks together constitute an object. Each block is represented as a linear combination of a given number of learnable concept vectors, which is iteratively refined during the learning process. The blocks in our model are discovered in an unsupervised manner, by attending over object masks, in a style similar to discovery of slots \\citep{slot_attention}, for learning a dense object-centric representation. We employ self-attention via transformers over the discovered blocks to predict the next state resulting in discovery of visual dynamics. We perform a series of experiments on several benchmark 2-D, and 3-D datasets demonstrating that our architecture (1) can discover semantically meaningful blocks (2) help improve accuracy of dynamics prediction compared to SOTA object-centric models (3) perform significantly better in OOD setting where the specific attribute combinations are not seen earlier during training. Our experiments highlight the importance discovery of disentangled representation for visual dynamics prediction.","sentences":["Recent work has shown that object-centric representations can greatly help improve the accuracy of learning dynamics while also bringing interpretability.","In this work, we take this idea one step further, ask the following question: \"can learning disentangled representation further improve the accuracy of visual dynamics prediction in object-centric models?\"","While there has been some attempt to learn such disentangled representations for the case of static images \\citep{nsb}, to the best of our knowledge, ours is the first work which tries to do this in a general setting for video, without making any specific assumptions about the kind of attributes that an object might have.","The key building block of our architecture is the notion of a {\\em block}, where several blocks together constitute an object.","Each block is represented as a linear combination of a given number of learnable concept vectors, which is iteratively refined during the learning process.","The blocks in our model are discovered in an unsupervised manner, by attending over object masks, in a style similar to discovery of slots \\citep{slot_attention}, for learning a dense object-centric representation.","We employ self-attention via transformers over the discovered blocks to predict the next state resulting in discovery of visual dynamics.","We perform a series of experiments on several benchmark 2-D, and 3-D datasets demonstrating that our architecture (1) can discover semantically meaningful blocks (2) help improve accuracy of dynamics prediction compared to SOTA object-centric models (3) perform significantly better in OOD setting where the specific attribute combinations are not seen earlier during training.","Our experiments highlight the importance discovery of disentangled representation for visual dynamics prediction."],"url":"http://arxiv.org/abs/2407.03216v1"}
{"created":"2024-07-03 15:39:40","title":"How Does Quantization Affect Multilingual LLMs?","abstract":"Quantization techniques are widely used to improve inference speed and deployment of large language models. While a wide body of work examines the impact of quantized LLMs on English tasks, none have examined the effect of quantization across languages. We conduct a thorough analysis of quantized multilingual LLMs, focusing on their performance across languages and at varying scales. We use automatic benchmarks, LLM-as-a-Judge methods, and human evaluation, finding that (1) harmful effects of quantization are apparent in human evaluation, and automatic metrics severely underestimate the detriment: a 1.7% average drop in Japanese across automatic tasks corresponds to a 16.0% drop reported by human evaluators on realistic prompts; (2) languages are disparately affected by quantization, with non-Latin script languages impacted worst; and (3) challenging tasks such as mathematical reasoning degrade fastest. As the ability to serve low-compute models is critical for wide global adoption of NLP technologies, our results urge consideration of multilingual performance as a key evaluation criterion for efficient models.","sentences":["Quantization techniques are widely used to improve inference speed and deployment of large language models.","While a wide body of work examines the impact of quantized LLMs on English tasks, none have examined the effect of quantization across languages.","We conduct a thorough analysis of quantized multilingual LLMs, focusing on their performance across languages and at varying scales.","We use automatic benchmarks, LLM-as-a-Judge methods, and human evaluation, finding that (1) harmful effects of quantization are apparent in human evaluation, and automatic metrics severely underestimate the detriment: a 1.7% average drop in Japanese across automatic tasks corresponds to a 16.0% drop reported by human evaluators on realistic prompts; (2) languages are disparately affected by quantization, with non-Latin script languages impacted worst; and (3) challenging tasks such as mathematical reasoning degrade fastest.","As the ability to serve low-compute models is critical for wide global adoption of NLP technologies, our results urge consideration of multilingual performance as a key evaluation criterion for efficient models."],"url":"http://arxiv.org/abs/2407.03211v1"}
{"created":"2024-07-03 15:38:57","title":"Combining AI Control Systems and Human Decision Support via Robustness and Criticality","abstract":"AI-enabled capabilities are reaching the requisite level of maturity to be deployed in the real world, yet do not always make correct or safe decisions. One way of addressing these concerns is to leverage AI control systems alongside and in support of human decisions, relying on the AI control system in safe situations while calling on a human co-decider for critical situations. We extend a methodology for adversarial explanations (AE) to state-of-the-art reinforcement learning frameworks, including MuZero. Multiple improvements to the base agent architecture are proposed. We demonstrate how this technology has two applications: for intelligent decision tools and to enhance training / learning frameworks. In a decision support context, adversarial explanations help a user make the correct decision by highlighting those contextual factors that would need to change for a different AI-recommended decision. As another benefit of adversarial explanations, we show that the learned AI control system demonstrates robustness against adversarial tampering. Additionally, we supplement AE by introducing strategically similar autoencoders (SSAs) to help users identify and understand all salient factors being considered by the AI system. In a training / learning framework, this technology can improve both the AI's decisions and explanations through human interaction. Finally, to identify when AI decisions would most benefit from human oversight, we tie this combined system to our prior art on statistically verified analyses of the criticality of decisions at any point in time.","sentences":["AI-enabled capabilities are reaching the requisite level of maturity to be deployed in the real world, yet do not always make correct or safe decisions.","One way of addressing these concerns is to leverage AI control systems alongside and in support of human decisions, relying on the AI control system in safe situations while calling on a human co-decider for critical situations.","We extend a methodology for adversarial explanations (AE) to state-of-the-art reinforcement learning frameworks, including MuZero.","Multiple improvements to the base agent architecture are proposed.","We demonstrate how this technology has two applications: for intelligent decision tools and to enhance training / learning frameworks.","In a decision support context, adversarial explanations help a user make the correct decision by highlighting those contextual factors that would need to change for a different AI-recommended decision.","As another benefit of adversarial explanations, we show that the learned AI control system demonstrates robustness against adversarial tampering.","Additionally, we supplement AE by introducing strategically similar autoencoders (SSAs) to help users identify and understand all salient factors being considered by the AI system.","In a training / learning framework, this technology can improve both the AI's decisions and explanations through human interaction.","Finally, to identify when AI decisions would most benefit from human oversight, we tie this combined system to our prior art on statistically verified analyses of the criticality of decisions at any point in time."],"url":"http://arxiv.org/abs/2407.03210v1"}
{"created":"2024-07-03 15:36:47","title":"Category-Aware Dynamic Label Assignment with High-Quality Oriented Proposal","abstract":"Objects in aerial images are typically embedded in complex backgrounds and exhibit arbitrary orientations. When employing oriented bounding boxes (OBB) to represent arbitrary oriented objects, the periodicity of angles could lead to discontinuities in label regression values at the boundaries, inducing abrupt fluctuations in the loss function. To address this problem, an OBB representation based on the complex plane is introduced in the oriented detection framework, and a trigonometric loss function is proposed. Moreover, leveraging prior knowledge of complex background environments and significant differences in large objects in aerial images, a conformer RPN head is constructed to predict angle information. The proposed loss function and conformer RPN head jointly generate high-quality oriented proposals. A category-aware dynamic label assignment based on predicted category feedback is proposed to address the limitations of solely relying on IoU for proposal label assignment. This method makes negative sample selection more representative, ensuring consistency between classification and regression features. Experiments were conducted on four realistic oriented detection datasets, and the results demonstrate superior performance in oriented object detection with minimal parameter tuning and time costs. Specifically, mean average precision (mAP) scores of 82.02%, 71.99%, 69.87%, and 98.77% were achieved on the DOTA-v1.0, DOTA-v1.5, DIOR-R, and HRSC2016 datasets, respectively.","sentences":["Objects in aerial images are typically embedded in complex backgrounds and exhibit arbitrary orientations.","When employing oriented bounding boxes (OBB) to represent arbitrary oriented objects, the periodicity of angles could lead to discontinuities in label regression values at the boundaries, inducing abrupt fluctuations in the loss function.","To address this problem, an OBB representation based on the complex plane is introduced in the oriented detection framework, and a trigonometric loss function is proposed.","Moreover, leveraging prior knowledge of complex background environments and significant differences in large objects in aerial images, a conformer RPN head is constructed to predict angle information.","The proposed loss function and conformer RPN head jointly generate high-quality oriented proposals.","A category-aware dynamic label assignment based on predicted category feedback is proposed to address the limitations of solely relying on IoU for proposal label assignment.","This method makes negative sample selection more representative, ensuring consistency between classification and regression features.","Experiments were conducted on four realistic oriented detection datasets, and the results demonstrate superior performance in oriented object detection with minimal parameter tuning and time costs.","Specifically, mean average precision (mAP) scores of 82.02%, 71.99%, 69.87%, and 98.77% were achieved on the DOTA-v1.0, DOTA-v1.5, DIOR-R, and HRSC2016 datasets, respectively."],"url":"http://arxiv.org/abs/2407.03205v1"}
{"created":"2024-07-03 15:36:27","title":"Expressive Gaussian Human Avatars from Monocular RGB Video","abstract":"Nuanced expressiveness, particularly through fine-grained hand and facial expressions, is pivotal for enhancing the realism and vitality of digital human representations. In this work, we focus on investigating the expressiveness of human avatars when learned from monocular RGB video; a setting that introduces new challenges in capturing and animating fine-grained details. To this end, we introduce EVA, a drivable human model that meticulously sculpts fine details based on 3D Gaussians and SMPL-X, an expressive parametric human model. Focused on enhancing expressiveness, our work makes three key contributions. First, we highlight the critical importance of aligning the SMPL-X model with RGB frames for effective avatar learning. Recognizing the limitations of current SMPL-X prediction methods for in-the-wild videos, we introduce a plug-and-play module that significantly ameliorates misalignment issues. Second, we propose a context-aware adaptive density control strategy, which is adaptively adjusting the gradient thresholds to accommodate the varied granularity across body parts. Last but not least, we develop a feedback mechanism that predicts per-pixel confidence to better guide the learning of 3D Gaussians. Extensive experiments on two benchmarks demonstrate the superiority of our framework both quantitatively and qualitatively, especially on the fine-grained hand and facial details. See the project website at \\url{https://evahuman.github.io}","sentences":["Nuanced expressiveness, particularly through fine-grained hand and facial expressions, is pivotal for enhancing the realism and vitality of digital human representations.","In this work, we focus on investigating the expressiveness of human avatars when learned from monocular RGB video; a setting that introduces new challenges in capturing and animating fine-grained details.","To this end, we introduce EVA, a drivable human model that meticulously sculpts fine details based on 3D Gaussians and SMPL-X, an expressive parametric human model.","Focused on enhancing expressiveness, our work makes three key contributions.","First, we highlight the critical importance of aligning the SMPL-X model with RGB frames for effective avatar learning.","Recognizing the limitations of current SMPL-X prediction methods for in-the-wild videos, we introduce a plug-and-play module that significantly ameliorates misalignment issues.","Second, we propose a context-aware adaptive density control strategy, which is adaptively adjusting the gradient thresholds to accommodate the varied granularity across body parts.","Last but not least, we develop a feedback mechanism that predicts per-pixel confidence to better guide the learning of 3D Gaussians.","Extensive experiments on two benchmarks demonstrate the superiority of our framework both quantitatively and qualitatively, especially on the fine-grained hand and facial details.","See the project website at \\url{https://evahuman.github.io}"],"url":"http://arxiv.org/abs/2407.03204v1"}
{"created":"2024-07-03 15:36:18","title":"TheoremLlama: Transforming General-Purpose LLMs into Lean4 Experts","abstract":"Proving mathematical theorems using computer-verifiable formal languages like Lean significantly impacts mathematical reasoning. One approach to formal theorem proving involves generating complete proofs using Large Language Models (LLMs) based on Natural Language (NL) proofs. Similar methods have shown promising results in code generation. However, most modern LLMs exhibit suboptimal performance due to the scarcity of aligned NL and Formal Language (FL) theorem-proving data. This scarcity results in a paucity of methodologies for training LLMs and techniques to fully utilize their capabilities in composing formal proofs. To address the challenges, this paper proposes **TheoremLlama**, an end-to-end framework to train a general-purpose LLM to become a Lean4 expert. This framework encompasses NL-FL aligned dataset generation methods, training approaches for the LLM formal theorem prover, and techniques for LLM Lean4 proof writing. Using the dataset generation method, we provide *Open Bootstrapped Theorems* (OBT), an NL-FL aligned and bootstrapped dataset. A key innovation in this framework is the NL-FL bootstrapping method, where NL proofs are integrated into Lean4 code for training datasets, leveraging the NL reasoning ability of LLMs for formal reasoning. The **TheoremLlama** framework achieves cumulative accuracies of 36.48% and 33.61% on MiniF2F-Valid and Test datasets respectively, surpassing the GPT-4 baseline of 22.95% and 25.41%. We have also open-sourced our model checkpoints and generated dataset, and will soon make all the code publicly available.","sentences":["Proving mathematical theorems using computer-verifiable formal languages like Lean significantly impacts mathematical reasoning.","One approach to formal theorem proving involves generating complete proofs using Large Language Models (LLMs) based on Natural Language (NL) proofs.","Similar methods have shown promising results in code generation.","However, most modern LLMs exhibit suboptimal performance due to the scarcity of aligned NL and Formal Language (FL) theorem-proving data.","This scarcity results in a paucity of methodologies for training LLMs and techniques to fully utilize their capabilities in composing formal proofs.","To address the challenges, this paper proposes **TheoremLlama**, an end-to-end framework to train a general-purpose LLM to become a Lean4 expert.","This framework encompasses NL-FL aligned dataset generation methods, training approaches for the LLM formal theorem prover, and techniques for LLM Lean4 proof writing.","Using the dataset generation method, we provide *Open Bootstrapped Theorems* (OBT), an NL-FL aligned and bootstrapped dataset.","A key innovation in this framework is the NL-FL bootstrapping method, where NL proofs are integrated into Lean4 code for training datasets, leveraging the NL reasoning ability of LLMs for formal reasoning.","The **TheoremLlama** framework achieves cumulative accuracies of 36.48% and 33.61% on MiniF2F-Valid and Test datasets respectively, surpassing the GPT-4 baseline of 22.95% and 25.41%.","We have also open-sourced our model checkpoints and generated dataset, and will soon make all the code publicly available."],"url":"http://arxiv.org/abs/2407.03203v1"}
{"created":"2024-07-03 15:30:45","title":"SegVG: Transferring Object Bounding Box to Segmentation for Visual Grounding","abstract":"Different from Object Detection, Visual Grounding deals with detecting a bounding box for each text-image pair. This one box for each text-image data provides sparse supervision signals. Although previous works achieve impressive results, their passive utilization of annotation, i.e. the sole use of the box annotation as regression ground truth, results in a suboptimal performance. In this paper, we present SegVG, a novel method transfers the box-level annotation as Segmentation signals to provide an additional pixel-level supervision for Visual Grounding. Specifically, we propose the Multi-layer Multi-task Encoder-Decoder as the target grounding stage, where we learn a regression query and multiple segmentation queries to ground the target by regression and segmentation of the box in each decoding layer, respectively. This approach allows us to iteratively exploit the annotation as signals for both box-level regression and pixel-level segmentation. Moreover, as the backbones are typically initialized by pretrained parameters learned from unimodal tasks and the queries for both regression and segmentation are static learnable embeddings, a domain discrepancy remains among these three types of features, which impairs subsequent target grounding. To mitigate this discrepancy, we introduce the Triple Alignment module, where the query, text, and vision tokens are triangularly updated to share the same space by triple attention mechanism. Extensive experiments on five widely used datasets validate our state-of-the-art (SOTA) performance.","sentences":["Different from Object Detection, Visual Grounding deals with detecting a bounding box for each text-image pair.","This one box for each text-image data provides sparse supervision signals.","Although previous works achieve impressive results, their passive utilization of annotation, i.e. the sole use of the box annotation as regression ground truth, results in a suboptimal performance.","In this paper, we present SegVG, a novel method transfers the box-level annotation as Segmentation signals to provide an additional pixel-level supervision for Visual Grounding.","Specifically, we propose the Multi-layer Multi-task Encoder-Decoder as the target grounding stage, where we learn a regression query and multiple segmentation queries to ground the target by regression and segmentation of the box in each decoding layer, respectively.","This approach allows us to iteratively exploit the annotation as signals for both box-level regression and pixel-level segmentation.","Moreover, as the backbones are typically initialized by pretrained parameters learned from unimodal tasks and the queries for both regression and segmentation are static learnable embeddings, a domain discrepancy remains among these three types of features, which impairs subsequent target grounding.","To mitigate this discrepancy, we introduce the Triple Alignment module, where the query, text, and vision tokens are triangularly updated to share the same space by triple attention mechanism.","Extensive experiments on five widely used datasets validate our state-of-the-art (SOTA) performance."],"url":"http://arxiv.org/abs/2407.03200v1"}
{"created":"2024-07-03 15:29:10","title":"DyFADet: Dynamic Feature Aggregation for Temporal Action Detection","abstract":"Recent proposed neural network-based Temporal Action Detection (TAD) models are inherently limited to extracting the discriminative representations and modeling action instances with various lengths from complex scenes by shared-weights detection heads. Inspired by the successes in dynamic neural networks, in this paper, we build a novel dynamic feature aggregation (DFA) module that can simultaneously adapt kernel weights and receptive fields at different timestamps. Based on DFA, the proposed dynamic encoder layer aggregates the temporal features within the action time ranges and guarantees the discriminability of the extracted representations. Moreover, using DFA helps to develop a Dynamic TAD head (DyHead), which adaptively aggregates the multi-scale features with adjusted parameters and learned receptive fields better to detect the action instances with diverse ranges from videos. With the proposed encoder layer and DyHead, a new dynamic TAD model, DyFADet, achieves promising performance on a series of challenging TAD benchmarks, including HACS-Segment, THUMOS14, ActivityNet-1.3, Epic-Kitchen 100, Ego4D-Moment QueriesV1.0, and FineAction. Code is released to https://github.com/yangle15/DyFADet-pytorch.","sentences":["Recent proposed neural network-based Temporal Action Detection (TAD) models are inherently limited to extracting the discriminative representations and modeling action instances with various lengths from complex scenes by shared-weights detection heads.","Inspired by the successes in dynamic neural networks, in this paper, we build a novel dynamic feature aggregation (DFA) module that can simultaneously adapt kernel weights and receptive fields at different timestamps.","Based on DFA, the proposed dynamic encoder layer aggregates the temporal features within the action time ranges and guarantees the discriminability of the extracted representations.","Moreover, using DFA helps to develop a Dynamic TAD head (DyHead), which adaptively aggregates the multi-scale features with adjusted parameters and learned receptive fields better to detect the action instances with diverse ranges from videos.","With the proposed encoder layer and DyHead, a new dynamic TAD model, DyFADet, achieves promising performance on a series of challenging TAD benchmarks, including HACS-Segment, THUMOS14, ActivityNet-1.3, Epic-Kitchen 100, Ego4D-Moment QueriesV1.0, and FineAction.","Code is released to https://github.com/yangle15/DyFADet-pytorch."],"url":"http://arxiv.org/abs/2407.03197v1"}
{"created":"2024-07-03 15:26:02","title":"Prediction Instability in Machine Learning Ensembles","abstract":"In machine learning ensembles predictions from multiple models are aggregated. Despite widespread use and strong performance of ensembles in applied problems little is known about the mathematical properties of aggregating models and associated consequences for safe, explainable use of such models. In this paper we prove a theorem that shows that any ensemble will exhibit at least one of the following forms of prediction instability. It will either ignore agreement among all underlying models, change its mind when none of the underlying models have done so, or be manipulable through inclusion or exclusion of options it would never actually predict. As a consequence, ensemble aggregation procedures will always need to balance the benefits of information use against the risk of these prediction instabilities. This analysis also sheds light on what specific forms of prediction instability to expect from particular ensemble algorithms; for example popular tree ensembles like random forest, or xgboost will violate basic, intuitive monotonicity and fairness properties.","sentences":["In machine learning ensembles predictions from multiple models are aggregated.","Despite widespread use and strong performance of ensembles in applied problems little is known about the mathematical properties of aggregating models and associated consequences for safe, explainable use of such models.","In this paper we prove a theorem that shows that any ensemble will exhibit at least one of the following forms of prediction instability.","It will either ignore agreement among all underlying models, change its mind when none of the underlying models have done so, or be manipulable through inclusion or exclusion of options it would never actually predict.","As a consequence, ensemble aggregation procedures will always need to balance the benefits of information use against the risk of these prediction instabilities.","This analysis also sheds light on what specific forms of prediction instability to expect from particular ensemble algorithms; for example popular tree ensembles like random forest, or xgboost will violate basic, intuitive monotonicity and fairness properties."],"url":"http://arxiv.org/abs/2407.03194v1"}
{"created":"2024-07-03 15:18:29","title":"CiteAssist: A System for Automated Preprint Citation and BibTeX Generation","abstract":"We present CiteAssist, a system to automate the generation of BibTeX entries for preprints, streamlining the process of bibliographic annotation. Our system extracts metadata, such as author names, titles, publication dates, and keywords, to create standardized annotations within the document. CiteAssist automatically attaches the BibTeX citation to the end of a PDF and links it on the first page of the document so other researchers gain immediate access to the correct citation of the article. This method promotes platform flexibility by ensuring that annotations remain accessible regardless of the repository used to publish or access the preprint. The annotations remain available even if the preprint is viewed externally to CiteAssist. Additionally, the system adds relevant related papers based on extracted keywords to the preprint, providing researchers with additional publications besides those in related work for further reading. Researchers can enhance their preprints organization and reference management workflows through a free and publicly available web interface.","sentences":["We present CiteAssist, a system to automate the generation of BibTeX entries for preprints, streamlining the process of bibliographic annotation.","Our system extracts metadata, such as author names, titles, publication dates, and keywords, to create standardized annotations within the document.","CiteAssist automatically attaches the BibTeX citation to the end of a PDF and links it on the first page of the document so other researchers gain immediate access to the correct citation of the article.","This method promotes platform flexibility by ensuring that annotations remain accessible regardless of the repository used to publish or access the preprint.","The annotations remain available even if the preprint is viewed externally to CiteAssist.","Additionally, the system adds relevant related papers based on extracted keywords to the preprint, providing researchers with additional publications besides those in related work for further reading.","Researchers can enhance their preprints organization and reference management workflows through a free and publicly available web interface."],"url":"http://arxiv.org/abs/2407.03192v1"}
{"created":"2024-07-03 15:12:36","title":"MuDiT & MuSiT: Alignment with Colloquial Expression in Description-to-Song Generation","abstract":"Amid the rising intersection of generative AI and human artistic processes, this study probes the critical yet less-explored terrain of alignment in human-centric automatic song composition. We propose a novel task of Colloquial Description-to-Song Generation, which focuses on aligning the generated content with colloquial human expressions. This task is aimed at bridging the gap between colloquial language understanding and auditory expression within an AI model, with the ultimate goal of creating songs that accurately satisfy human auditory expectations and structurally align with musical norms. Current datasets are limited due to their narrow descriptive scope, semantic gaps and inaccuracies. To overcome data scarcity in this domain, we present the Caichong Music Dataset (CaiMD). CaiMD is manually annotated by both professional musicians and amateurs, offering diverse perspectives and a comprehensive understanding of colloquial descriptions. Unlike existing datasets pre-set with expert annotations or auto-generated ones with inherent biases, CaiMD caters more sufficiently to our purpose of aligning AI-generated music with widespread user-desired results. Moreover, we propose an innovative single-stage framework called MuDiT/MuSiT for enabling effective human-machine alignment in song creation. This framework not only achieves cross-modal comprehension between colloquial language and auditory music perceptions but also ensures generated songs align with user-desired results. MuDiT/MuSiT employs one DiT/SiT model for end-to-end generation of musical components like melody, harmony, rhythm, vocals, and instrumentation. The approach ensures harmonious sonic cohesiveness amongst all generated musical components, facilitating better resonance with human auditory expectations.","sentences":["Amid the rising intersection of generative AI and human artistic processes, this study probes the critical yet less-explored terrain of alignment in human-centric automatic song composition.","We propose a novel task of Colloquial Description-to-Song Generation, which focuses on aligning the generated content with colloquial human expressions.","This task is aimed at bridging the gap between colloquial language understanding and auditory expression within an AI model, with the ultimate goal of creating songs that accurately satisfy human auditory expectations and structurally align with musical norms.","Current datasets are limited due to their narrow descriptive scope, semantic gaps and inaccuracies.","To overcome data scarcity in this domain, we present the Caichong Music Dataset (CaiMD).","CaiMD is manually annotated by both professional musicians and amateurs, offering diverse perspectives and a comprehensive understanding of colloquial descriptions.","Unlike existing datasets pre-set with expert annotations or auto-generated ones with inherent biases, CaiMD caters more sufficiently to our purpose of aligning AI-generated music with widespread user-desired results.","Moreover, we propose an innovative single-stage framework called MuDiT/MuSiT for enabling effective human-machine alignment in song creation.","This framework not only achieves cross-modal comprehension between colloquial language and auditory music perceptions but also ensures generated songs align with user-desired results.","MuDiT/MuSiT employs one DiT/SiT model for end-to-end generation of musical components like melody, harmony, rhythm, vocals, and instrumentation.","The approach ensures harmonious sonic cohesiveness amongst all generated musical components, facilitating better resonance with human auditory expectations."],"url":"http://arxiv.org/abs/2407.03188v1"}
{"created":"2024-07-03 15:10:05","title":"Holistic view of the road transportation system based on real-time data sharing mechanism","abstract":"Traditional manual driving and single-vehicle-based intelligent driving have limitations in real-time and accurate acquisition of the current driving status and intentions of surrounding vehicles, leading to vehicles typically maintaining appropriate safe distances from each other. Yet, accidents still frequently occur, especially in merging areas; meanwhile, it is difficult to comprehensively obtain the conditions of road infrastructure. These limitations not only restrict the further improvement of road capacity but also result in irreparable losses of life and property. To overcome this bottleneck, this paper constructs a space-time global view of the road traffic system based on a real-time sharing mechanism, enabling both road users and managers to timely access the driving intentions of nearby vehicles and the real-time status of road infrastructure.","sentences":["Traditional manual driving and single-vehicle-based intelligent driving have limitations in real-time and accurate acquisition of the current driving status and intentions of surrounding vehicles, leading to vehicles typically maintaining appropriate safe distances from each other.","Yet, accidents still frequently occur, especially in merging areas; meanwhile, it is difficult to comprehensively obtain the conditions of road infrastructure.","These limitations not only restrict the further improvement of road capacity but also result in irreparable losses of life and property.","To overcome this bottleneck, this paper constructs a space-time global view of the road traffic system based on a real-time sharing mechanism, enabling both road users and managers to timely access the driving intentions of nearby vehicles and the real-time status of road infrastructure."],"url":"http://arxiv.org/abs/2407.03187v1"}
{"created":"2024-07-03 15:07:16","title":"Multiple-Resolution Tokenization for Time Series Forecasting with an Application to Pricing","abstract":"We propose a transformer architecture for time series forecasting with a focus on time series tokenisation and apply it to a real-world prediction problem from the pricing domain. Our architecture aims to learn effective representations at many scales across all available data simultaneously. The model contains a number of novel modules: a differentiated form of time series patching which employs multiple resolutions, a multiple-resolution module for time-varying known variables, a mixer-based module for capturing cross-series information, and a novel output head with favourable scaling to account for the increased number of tokens. We present an application of this model to a real world prediction problem faced by the markdown team at a very large retailer. On the experiments conducted our model outperforms in-house models and the selected existing deep learning architectures.","sentences":["We propose a transformer architecture for time series forecasting with a focus on time series tokenisation and apply it to a real-world prediction problem from the pricing domain.","Our architecture aims to learn effective representations at many scales across all available data simultaneously.","The model contains a number of novel modules: a differentiated form of time series patching which employs multiple resolutions, a multiple-resolution module for time-varying known variables, a mixer-based module for capturing cross-series information, and a novel output head with favourable scaling to account for the increased number of tokens.","We present an application of this model to a real world prediction problem faced by the markdown team at a very large retailer.","On the experiments conducted our model outperforms in-house models and the selected existing deep learning architectures."],"url":"http://arxiv.org/abs/2407.03185v1"}
{"created":"2024-07-03 15:01:18","title":"Fine-Tuning with Divergent Chains of Thought Boosts Reasoning Through Self-Correction in Language Models","abstract":"Requiring a Large Language Model to generate intermediary reasoning steps has been shown to be an effective way of boosting performance. In fact, it has been found that instruction tuning on these intermediary reasoning steps improves model performance. In this work, we present a novel method of further improving performance by requiring models to compare multiple reasoning chains before generating a solution in a single inference step. We call this method Divergent CoT (DCoT). We find that instruction tuning on DCoT datasets boosts the performance of even smaller, and therefore more accessible, LLMs. Through a rigorous set of experiments spanning a wide range of tasks that require various reasoning types, we show that fine-tuning on DCoT consistently improves performance over the CoT baseline across model families and scales (1.3B to 70B). Through a combination of empirical and manual evaluation, we additionally show that these performance gains stem from models generating multiple divergent reasoning chains in a single inference step, indicative of the enabling of self-correction in language models. Our code and data are publicly available at https://github.com/UKPLab/arxiv2024-divergent-cot.","sentences":["Requiring a Large Language Model to generate intermediary reasoning steps has been shown to be an effective way of boosting performance.","In fact, it has been found that instruction tuning on these intermediary reasoning steps improves model performance.","In this work, we present a novel method of further improving performance by requiring models to compare multiple reasoning chains before generating a solution in a single inference step.","We call this method Divergent CoT (DCoT).","We find that instruction tuning on DCoT datasets boosts the performance of even smaller, and therefore more accessible, LLMs.","Through a rigorous set of experiments spanning a wide range of tasks that require various reasoning types, we show that fine-tuning on DCoT consistently improves performance over the CoT baseline across model families and scales (1.3B to 70B).","Through a combination of empirical and manual evaluation, we additionally show that these performance gains stem from models generating multiple divergent reasoning chains in a single inference step, indicative of the enabling of self-correction in language models.","Our code and data are publicly available at https://github.com/UKPLab/arxiv2024-divergent-cot."],"url":"http://arxiv.org/abs/2407.03181v1"}
{"created":"2024-07-03 15:01:12","title":"A multi-objective combinatorial optimisation framework for large scale hierarchical population synthesis","abstract":"In agent-based simulations, synthetic populations of agents are commonly used to represent the structure, behaviour, and interactions of individuals. However, generating a synthetic population that accurately reflects real population statistics is a challenging task, particularly when performed at scale. In this paper, we propose a multi objective combinatorial optimisation technique for large scale population synthesis. We demonstrate the effectiveness of our approach by generating a synthetic population for selected regions and validating it on contingency tables from real population data. Our approach supports complex hierarchical structures between individuals and households, is scalable to large populations and achieves minimal contigency table reconstruction error. Hence, it provides a useful tool for policymakers and researchers for simulating the dynamics of complex populations.","sentences":["In agent-based simulations, synthetic populations of agents are commonly used to represent the structure, behaviour, and interactions of individuals.","However, generating a synthetic population that accurately reflects real population statistics is a challenging task, particularly when performed at scale.","In this paper, we propose a multi objective combinatorial optimisation technique for large scale population synthesis.","We demonstrate the effectiveness of our approach by generating a synthetic population for selected regions and validating it on contingency tables from real population data.","Our approach supports complex hierarchical structures between individuals and households, is scalable to large populations and achieves minimal contigency table reconstruction error.","Hence, it provides a useful tool for policymakers and researchers for simulating the dynamics of complex populations."],"url":"http://arxiv.org/abs/2407.03180v1"}
{"created":"2024-07-03 14:59:46","title":"Motion meets Attention: Video Motion Prompts","abstract":"Videos contain rich spatio-temporal information. Traditional methods for extracting motion, used in tasks such as action recognition, often rely on visual contents rather than precise motion features. This phenomenon is referred to as 'blind motion extraction' behavior, which proves inefficient in capturing motions of interest due to a lack of motion-guided cues. Recently, attention mechanisms have enhanced many computer vision tasks by effectively highlighting salient visual areas. Inspired by this, we propose using a modified Sigmoid function with learnable slope and shift parameters as an attention mechanism to activate and modulate motion signals derived from frame differencing maps. This approach generates a sequence of attention maps that enhance the processing of motion-related video content. To ensure temporally continuity and smoothness of the attention maps, we apply pair-wise temporal attention variation regularization to remove unwanted motions (e.g., noise) while preserving important ones. We then perform Hadamard product between each pair of attention maps and the original video frames to highlight the evolving motions of interest over time. These highlighted motions, termed video motion prompts, are subsequently used as inputs to the model instead of the original video frames. We formalize this process as a motion prompt layer and incorporate the regularization term into the loss function to learn better motion prompts. This layer serves as an adapter between the model and the video data, bridging the gap between traditional 'blind motion extraction' and the extraction of relevant motions of interest.","sentences":["Videos contain rich spatio-temporal information.","Traditional methods for extracting motion, used in tasks such as action recognition, often rely on visual contents rather than precise motion features.","This phenomenon is referred to as 'blind motion extraction' behavior, which proves inefficient in capturing motions of interest due to a lack of motion-guided cues.","Recently, attention mechanisms have enhanced many computer vision tasks by effectively highlighting salient visual areas.","Inspired by this, we propose using a modified Sigmoid function with learnable slope and shift parameters as an attention mechanism to activate and modulate motion signals derived from frame differencing maps.","This approach generates a sequence of attention maps that enhance the processing of motion-related video content.","To ensure temporally continuity and smoothness of the attention maps, we apply pair-wise temporal attention variation regularization to remove unwanted motions (e.g., noise) while preserving important ones.","We then perform Hadamard product between each pair of attention maps and the original video frames to highlight the evolving motions of interest over time.","These highlighted motions, termed video motion prompts, are subsequently used as inputs to the model instead of the original video frames.","We formalize this process as a motion prompt layer and incorporate the regularization term into the loss function to learn better motion prompts.","This layer serves as an adapter between the model and the video data, bridging the gap between traditional 'blind motion extraction' and the extraction of relevant motions of interest."],"url":"http://arxiv.org/abs/2407.03179v1"}
{"created":"2024-07-03 14:58:40","title":"Relating CNN-Transformer Fusion Network for Change Detection","abstract":"While deep learning, particularly convolutional neural networks (CNNs), has revolutionized remote sensing (RS) change detection (CD), existing approaches often miss crucial features due to neglecting global context and incomplete change learning. Additionally, transformer networks struggle with low-level details. RCTNet addresses these limitations by introducing \\textbf{(1)} an early fusion backbone to exploit both spatial and temporal features early on, \\textbf{(2)} a Cross-Stage Aggregation (CSA) module for enhanced temporal representation, \\textbf{(3)} a Multi-Scale Feature Fusion (MSF) module for enriched feature extraction in the decoder, and \\textbf{(4)} an Efficient Self-deciphering Attention (ESA) module utilizing transformers to capture global information and fine-grained details for accurate change detection. Extensive experiments demonstrate RCTNet's clear superiority over traditional RS image CD methods, showing significant improvement and an optimal balance between accuracy and computational cost.","sentences":["While deep learning, particularly convolutional neural networks (CNNs), has revolutionized remote sensing (RS) change detection (CD), existing approaches often miss crucial features due to neglecting global context and incomplete change learning.","Additionally, transformer networks struggle with low-level details.","RCTNet addresses these limitations by introducing \\textbf{(1)} an early fusion backbone to exploit both spatial and temporal features early on, \\textbf{(2)} a Cross-Stage Aggregation (CSA) module for enhanced temporal representation, \\textbf{(3)} a Multi-Scale Feature Fusion (MSF) module for enriched feature extraction in the decoder, and \\textbf{(4)} an Efficient Self-deciphering Attention (ESA) module utilizing transformers to capture global information and fine-grained details for accurate change detection.","Extensive experiments demonstrate RCTNet's clear superiority over traditional RS image CD methods, showing significant improvement and an optimal balance between accuracy and computational cost."],"url":"http://arxiv.org/abs/2407.03178v1"}
{"created":"2024-07-03 14:57:22","title":"EDPNet: An Efficient Dual Prototype Network for Motor Imagery EEG Decoding","abstract":"Motor imagery electroencephalograph (MI-EEG) decoding plays a crucial role in developing motor imagery brain-computer interfaces (MI-BCIs). However, decoding intentions from MI remains challenging due to the inherent complexity of EEG signals relative to the small-sample size. In this paper, we propose an Efficient Dual Prototype Network (EDPNet) to enable accurate and fast MI decoding. EDPNet employs a lightweight adaptive spatial-spectral fusion module, which promotes more efficient information fusion between multiple EEG electrodes. Subsequently, a parameter-free multi-scale variance pooling module extracts more comprehensive temporal features. Furthermore, we introduce dual prototypical learning to optimize the feature space distribution and training process, thereby improving the model's generalization ability on small-sample MI datasets. Our experimental results show that the EDPNet outperforms state-of-the-art models with superior classification accuracy and kappa values (84.11% and 0.7881 for dataset BCI competition IV 2a, 86.65% and 0.7330 for dataset BCI competition IV 2b). Additionally, we use the BCI competition III IVa dataset with fewer training data to further validate the generalization ability of the proposed EDPNet. We also achieve superior performance with 82.03% classification accuracy. Benefiting from the lightweight parameters and superior decoding accuracy, our EDPNet shows great potential for MI-BCI applications. The code is publicly available at https://github.com/hancan16/EDPNet.","sentences":["Motor imagery electroencephalograph (MI-EEG) decoding plays a crucial role in developing motor imagery brain-computer interfaces (MI-BCIs).","However, decoding intentions from MI remains challenging due to the inherent complexity of EEG signals relative to the small-sample size.","In this paper, we propose an Efficient Dual Prototype Network (EDPNet) to enable accurate and fast MI decoding.","EDPNet employs a lightweight adaptive spatial-spectral fusion module, which promotes more efficient information fusion between multiple EEG electrodes.","Subsequently, a parameter-free multi-scale variance pooling module extracts more comprehensive temporal features.","Furthermore, we introduce dual prototypical learning to optimize the feature space distribution and training process, thereby improving the model's generalization ability on small-sample MI datasets.","Our experimental results show that the EDPNet outperforms state-of-the-art models with superior classification accuracy and kappa values (84.11% and 0.7881 for dataset BCI competition IV 2a, 86.65% and 0.7330 for dataset BCI competition IV 2b).","Additionally, we use the BCI competition III IVa dataset with fewer training data to further validate the generalization ability of the proposed EDPNet.","We also achieve superior performance with 82.03% classification accuracy.","Benefiting from the lightweight parameters and superior decoding accuracy, our EDPNet shows great potential for MI-BCI applications.","The code is publicly available at https://github.com/hancan16/EDPNet."],"url":"http://arxiv.org/abs/2407.03177v1"}
{"created":"2024-07-03 14:53:25","title":"An Improved Algorithm for Shortest Paths in Weighted Unit-Disk Graphs","abstract":"Let $V$ be a set of $n$ points in the plane. The unit-disk graph $G = (V, E)$ has vertex set $V$ and an edge $e_{uv} \\in E$ between vertices $u, v \\in V$ if the Euclidean distance between $u$ and $v$ is at most 1. The weight of each edge $e_{uv}$ is the Euclidean distance between $u$ and $v$. Given $V$ and a source point $s\\in V$, we consider the problem of computing shortest paths in $G$ from $s$ to all other vertices. The previously best algorithm for this problem runs in $O(n \\log^2 n)$ time [Wang and Xue, SoCG'19]. The problem has an $\\Omega(n\\log n)$ lower bound under the algebraic decision tree model. In this paper, we present an improved algorithm of $O(n \\log^2 n / \\log \\log n)$ time (under the standard real RAM model). Furthermore, we show that the problem can be solved using $O(n\\log n)$ comparisons under the algebraic decision tree model, matching the $\\Omega(n\\log n)$ lower bound.","sentences":["Let $V$ be a set of $n$ points in the plane.","The unit-disk graph $G = (V, E)$ has vertex set $V$ and an edge $e_{uv} \\in E$ between vertices $u, v \\in V$ if the Euclidean distance between $u$ and $v$ is at most 1.","The weight of each edge $e_{uv}$ is the Euclidean distance between $u$ and $v$. Given $V$ and a source point $s\\in V$, we consider the problem of computing shortest paths in $G$ from $s$ to all other vertices.","The previously best algorithm for this problem runs in $O(n \\log^2 n)$ time","[Wang and Xue, SoCG'19].","The problem has an $\\Omega(n\\log n)$ lower bound under the algebraic decision tree model.","In this paper, we present an improved algorithm of $O(n \\log^2 n / \\log \\log n)$ time (under the standard real RAM model).","Furthermore, we show that the problem can be solved using $O(n\\log n)$ comparisons under the algebraic decision tree model, matching the $\\Omega(n\\log n)$ lower bound."],"url":"http://arxiv.org/abs/2407.03176v1"}
{"created":"2024-07-03 14:52:31","title":"Low-Rank Toeplitz Matrix Restoration: Descent Cone Analysis and Structured Random Matrix","abstract":"This note demonstrates that we can stably recover rank $r$ Toeplitz matrix $\\pmb{X}\\in\\mathbb{R}^{n\\times n}$ from a number of rank one subgaussian measurements on the order of $r\\log^{2} n$ with an exponentially decreasing failure probability by employing a nuclear norm minimization program. Our approach utilizes descent cone analysis through Mendelson's small ball method with the Toeplitz constraint. The key ingredient is to determine the spectral norm of the random matrix of the Topelitz structure, which may be of independent interest.This improves upon earlier analyses and resolves the conjecture in Chen et al. (IEEE Transactions on Information Theory, 2015).","sentences":["This note demonstrates that we can stably recover rank $r$ Toeplitz matrix $\\pmb{X}\\in\\mathbb{R}^{n\\times n}$ from a number of rank one subgaussian measurements on the order of $r\\log^{2} n$ with an exponentially decreasing failure probability by employing a nuclear norm minimization program.","Our approach utilizes descent cone analysis through Mendelson's small ball method with the Toeplitz constraint.","The key ingredient is to determine the spectral norm of the random matrix of the Topelitz structure, which may be of independent interest.","This improves upon earlier analyses and resolves the conjecture in Chen et al.","(IEEE Transactions on Information Theory, 2015)."],"url":"http://arxiv.org/abs/2407.03175v1"}
{"created":"2024-07-03 14:47:18","title":"IMC 2024 Methods & Solutions Review","abstract":"For the past three years, Kaggle has been hosting the Image Matching Challenge, which focuses on solving a 3D image reconstruction problem using a collection of 2D images. Each year, this competition fosters the development of innovative and effective methodologies by its participants. In this paper, we introduce an advanced ensemble technique that we developed, achieving a score of 0.153449 on the private leaderboard and securing the 160th position out of over 1,000 participants. Additionally, we conduct a comprehensive review of existing methods and techniques employed by top-performing teams in the competition. Our solution, alongside the insights gathered from other leading approaches, contributes to the ongoing advancement in the field of 3D image reconstruction. This research provides valuable knowledge for future participants and researchers aiming to excel in similar image matching and reconstruction challenges.","sentences":["For the past three years, Kaggle has been hosting the Image Matching Challenge, which focuses on solving a 3D image reconstruction problem using a collection of 2D images.","Each year, this competition fosters the development of innovative and effective methodologies by its participants.","In this paper, we introduce an advanced ensemble technique that we developed, achieving a score of 0.153449 on the private leaderboard and securing the 160th position out of over 1,000 participants.","Additionally, we conduct a comprehensive review of existing methods and techniques employed by top-performing teams in the competition.","Our solution, alongside the insights gathered from other leading approaches, contributes to the ongoing advancement in the field of 3D image reconstruction.","This research provides valuable knowledge for future participants and researchers aiming to excel in similar image matching and reconstruction challenges."],"url":"http://arxiv.org/abs/2407.03172v1"}
{"created":"2024-07-03 14:42:49","title":"Investigating Decoder-only Large Language Models for Speech-to-text Translation","abstract":"Large language models (LLMs), known for their exceptional reasoning capabilities, generalizability, and fluency across diverse domains, present a promising avenue for enhancing speech-related tasks. In this paper, we focus on integrating decoder-only LLMs to the task of speech-to-text translation (S2TT). We propose a decoder-only architecture that enables the LLM to directly consume the encoded speech representation and generate the text translation. Additionally, we investigate the effects of different parameter-efficient fine-tuning techniques and task formulation. Our model achieves state-of-the-art performance on CoVoST 2 and FLEURS among models trained without proprietary data. We also conduct analyses to validate the design choices of our proposed model and bring insights to the integration of LLMs to S2TT.","sentences":["Large language models (LLMs), known for their exceptional reasoning capabilities, generalizability, and fluency across diverse domains, present a promising avenue for enhancing speech-related tasks.","In this paper, we focus on integrating decoder-only LLMs to the task of speech-to-text translation (S2TT).","We propose a decoder-only architecture that enables the LLM to directly consume the encoded speech representation and generate the text translation.","Additionally, we investigate the effects of different parameter-efficient fine-tuning techniques and task formulation.","Our model achieves state-of-the-art performance on CoVoST 2 and FLEURS among models trained without proprietary data.","We also conduct analyses to validate the design choices of our proposed model and bring insights to the integration of LLMs to S2TT."],"url":"http://arxiv.org/abs/2407.03169v1"}
{"created":"2024-07-03 14:41:39","title":"LivePortrait: Efficient Portrait Animation with Stitching and Retargeting Control","abstract":"Portrait Animation aims to synthesize a lifelike video from a single source image, using it as an appearance reference, with motion (i.e., facial expressions and head pose) derived from a driving video, audio, text, or generation. Instead of following mainstream diffusion-based methods, we explore and extend the potential of the implicit-keypoint-based framework, which effectively balances computational efficiency and controllability. Building upon this, we develop a video-driven portrait animation framework named LivePortrait with a focus on better generalization, controllability, and efficiency for practical usage. To enhance the generation quality and generalization ability, we scale up the training data to about 69 million high-quality frames, adopt a mixed image-video training strategy, upgrade the network architecture, and design better motion transformation and optimization objectives. Additionally, we discover that compact implicit keypoints can effectively represent a kind of blendshapes and meticulously propose a stitching and two retargeting modules, which utilize a small MLP with negligible computational overhead, to enhance the controllability. Experimental results demonstrate the efficacy of our framework even compared to diffusion-based methods. The generation speed remarkably reaches 12.8ms on an RTX 4090 GPU with PyTorch. The inference code and models are available at https://github.com/KwaiVGI/LivePortrait","sentences":["Portrait Animation aims to synthesize a lifelike video from a single source image, using it as an appearance reference, with motion (i.e., facial expressions and head pose) derived from a driving video, audio, text, or generation.","Instead of following mainstream diffusion-based methods, we explore and extend the potential of the implicit-keypoint-based framework, which effectively balances computational efficiency and controllability.","Building upon this, we develop a video-driven portrait animation framework named LivePortrait with a focus on better generalization, controllability, and efficiency for practical usage.","To enhance the generation quality and generalization ability, we scale up the training data to about 69 million high-quality frames, adopt a mixed image-video training strategy, upgrade the network architecture, and design better motion transformation and optimization objectives.","Additionally, we discover that compact implicit keypoints can effectively represent a kind of blendshapes and meticulously propose a stitching and two retargeting modules, which utilize a small MLP with negligible computational overhead, to enhance the controllability.","Experimental results demonstrate the efficacy of our framework even compared to diffusion-based methods.","The generation speed remarkably reaches 12.8ms on an RTX 4090 GPU with PyTorch.","The inference code and models are available at https://github.com/KwaiVGI/LivePortrait"],"url":"http://arxiv.org/abs/2407.03168v1"}
{"created":"2024-07-03 14:40:21","title":"Consistent Point Orientation for Manifold Surfaces via Boundary Integration","abstract":"This paper introduces a new approach for generating globally consistent normals for point clouds sampled from manifold surfaces. Given that the generalized winding number (GWN) field generated by a point cloud with globally consistent normals is a solution to a PDE with jump boundary conditions and possesses harmonic properties, and the Dirichlet energy of the GWN field can be defined as an integral over the boundary surface, we formulate a boundary energy derived from the Dirichlet energy of the GWN. Taking as input a point cloud with randomly oriented normals, we optimize this energy to restore the global harmonicity of the GWN field, thereby recovering the globally consistent normals. Experiments show that our method outperforms state-of-the-art approaches, exhibiting enhanced robustness to noise, outliers, complex topologies, and thin structures. Our code can be found at \\url{https://github.com/liuweizhou319/BIM}.","sentences":["This paper introduces a new approach for generating globally consistent normals for point clouds sampled from manifold surfaces.","Given that the generalized winding number (GWN) field generated by a point cloud with globally consistent normals is a solution to a PDE with jump boundary conditions and possesses harmonic properties, and the Dirichlet energy of the GWN field can be defined as an integral over the boundary surface, we formulate a boundary energy derived from the Dirichlet energy of the GWN.","Taking as input a point cloud with randomly oriented normals, we optimize this energy to restore the global harmonicity of the GWN field, thereby recovering the globally consistent normals.","Experiments show that our method outperforms state-of-the-art approaches, exhibiting enhanced robustness to noise, outliers, complex topologies, and thin structures.","Our code can be found at \\url{https://github.com/liuweizhou319/BIM}."],"url":"http://arxiv.org/abs/2407.03165v1"}
{"created":"2024-07-03 14:36:07","title":"Global Context Modeling in YOLOv8 for Pediatric Wrist Fracture Detection","abstract":"Children often suffer wrist injuries in daily life, while fracture injuring radiologists usually need to analyze and interpret X-ray images before surgical treatment by surgeons. The development of deep learning has enabled neural network models to work as computer-assisted diagnosis (CAD) tools to help doctors and experts in diagnosis. Since the YOLOv8 models have obtained the satisfactory success in object detection tasks, it has been applied to fracture detection. The Global Context (GC) block effectively models the global context in a lightweight way, and incorporating it into YOLOv8 can greatly improve the model performance. This paper proposes the YOLOv8+GC model for fracture detection, which is an improved version of the YOLOv8 model with the GC block. Experimental results demonstrate that compared to the original YOLOv8 model, the proposed YOLOv8-GC model increases the mean average precision calculated at intersection over union threshold of 0.5 (mAP 50) from 63.58% to 66.32% on the GRAZPEDWRI-DX dataset, achieving the state-of-the-art (SOTA) level. The implementation code for this work is available on GitHub at https://github.com/RuiyangJu/YOLOv8_Global_Context_Fracture_Detection.","sentences":["Children often suffer wrist injuries in daily life, while fracture injuring radiologists usually need to analyze and interpret X-ray images before surgical treatment by surgeons.","The development of deep learning has enabled neural network models to work as computer-assisted diagnosis (CAD) tools to help doctors and experts in diagnosis.","Since the YOLOv8 models have obtained the satisfactory success in object detection tasks, it has been applied to fracture detection.","The Global Context (GC) block effectively models the global context in a lightweight way, and incorporating it into YOLOv8 can greatly improve the model performance.","This paper proposes the YOLOv8+GC model for fracture detection, which is an improved version of the YOLOv8 model with the GC block.","Experimental results demonstrate that compared to the original YOLOv8 model, the proposed YOLOv8-GC model increases the mean average precision calculated at intersection over union threshold of 0.5 (mAP 50) from 63.58% to 66.32% on the GRAZPEDWRI-DX dataset, achieving the state-of-the-art (SOTA) level.","The implementation code for this work is available on GitHub at https://github.com/RuiyangJu/YOLOv8_Global_Context_Fracture_Detection."],"url":"http://arxiv.org/abs/2407.03163v1"}
{"created":"2024-07-03 14:35:35","title":"Bunny-VisionPro: Real-Time Bimanual Dexterous Teleoperation for Imitation Learning","abstract":"Teleoperation is a crucial tool for collecting human demonstrations, but controlling robots with bimanual dexterous hands remains a challenge. Existing teleoperation systems struggle to handle the complexity of coordinating two hands for intricate manipulations. We introduce Bunny-VisionPro, a real-time bimanual dexterous teleoperation system that leverages a VR headset. Unlike previous vision-based teleoperation systems, we design novel low-cost devices to provide haptic feedback to the operator, enhancing immersion. Our system prioritizes safety by incorporating collision and singularity avoidance while maintaining real-time performance through innovative designs. Bunny-VisionPro outperforms prior systems on a standard task suite, achieving higher success rates and reduced task completion times. Moreover, the high-quality teleoperation demonstrations improve downstream imitation learning performance, leading to better generalizability. Notably, Bunny-VisionPro enables imitation learning with challenging multi-stage, long-horizon dexterous manipulation tasks, which have rarely been addressed in previous work. Our system's ability to handle bimanual manipulations while prioritizing safety and real-time performance makes it a powerful tool for advancing dexterous manipulation and imitation learning.","sentences":["Teleoperation is a crucial tool for collecting human demonstrations, but controlling robots with bimanual dexterous hands remains a challenge.","Existing teleoperation systems struggle to handle the complexity of coordinating two hands for intricate manipulations.","We introduce Bunny-VisionPro, a real-time bimanual dexterous teleoperation system that leverages a VR headset.","Unlike previous vision-based teleoperation systems, we design novel low-cost devices to provide haptic feedback to the operator, enhancing immersion.","Our system prioritizes safety by incorporating collision and singularity avoidance while maintaining real-time performance through innovative designs.","Bunny-VisionPro outperforms prior systems on a standard task suite, achieving higher success rates and reduced task completion times.","Moreover, the high-quality teleoperation demonstrations improve downstream imitation learning performance, leading to better generalizability.","Notably, Bunny-VisionPro enables imitation learning with challenging multi-stage, long-horizon dexterous manipulation tasks, which have rarely been addressed in previous work.","Our system's ability to handle bimanual manipulations while prioritizing safety and real-time performance makes it a powerful tool for advancing dexterous manipulation and imitation learning."],"url":"http://arxiv.org/abs/2407.03162v1"}
{"created":"2024-07-03 14:35:16","title":"SOS! Soft Prompt Attack Against Open-Source Large Language Models","abstract":"Open-source large language models (LLMs) have become increasingly popular among both the general public and industry, as they can be customized, fine-tuned, and freely used. However, some open-source LLMs require approval before usage, which has led to third parties publishing their own easily accessible versions. Similarly, third parties have been publishing fine-tuned or quantized variants of these LLMs. These versions are particularly appealing to users because of their ease of access and reduced computational resource demands. This trend has increased the risk of training time attacks, compromising the integrity and security of LLMs. In this work, we present a new training time attack, SOS, which is designed to be low in computational demand and does not require clean data or modification of the model weights, thereby maintaining the model's utility intact. The attack addresses security issues in various scenarios, including the backdoor attack, jailbreak attack, and prompt stealing attack. Our experimental findings demonstrate that the proposed attack is effective across all evaluated targets. Furthermore, we present the other side of our SOS technique, namely the copyright token -- a novel technique that enables users to mark their copyrighted content and prevent models from using it.","sentences":["Open-source large language models (LLMs) have become increasingly popular among both the general public and industry, as they can be customized, fine-tuned, and freely used.","However, some open-source LLMs require approval before usage, which has led to third parties publishing their own easily accessible versions.","Similarly, third parties have been publishing fine-tuned or quantized variants of these LLMs.","These versions are particularly appealing to users because of their ease of access and reduced computational resource demands.","This trend has increased the risk of training time attacks, compromising the integrity and security of LLMs.","In this work, we present a new training time attack, SOS, which is designed to be low in computational demand and does not require clean data or modification of the model weights, thereby maintaining the model's utility intact.","The attack addresses security issues in various scenarios, including the backdoor attack, jailbreak attack, and prompt stealing attack.","Our experimental findings demonstrate that the proposed attack is effective across all evaluated targets.","Furthermore, we present the other side of our SOS technique, namely the copyright token -- a novel technique that enables users to mark their copyrighted content and prevent models from using it."],"url":"http://arxiv.org/abs/2407.03160v1"}
{"created":"2024-07-03 14:35:11","title":"Protection Degree and Migration in the Stochastic SIRS Model: A Queueing System Perspective","abstract":"With the prevalence of COVID-19, the modeling of epidemic propagation and its analyses have played a significant role in controlling epidemics. However, individual behaviors, in particular the self-protection and migration, which have a strong influence on epidemic propagation, were always neglected in previous studies. In this paper, we mainly propose two models from the individual and population perspectives. In the first individual model, we introduce the individual protection degree that effectively suppresses the epidemic level as a stochastic variable to the SIRS model. In the alternative population model, an open Markov queueing network is constructed to investigate the individual number of each epidemic state, and we present an evolving population network via the migration of people. Besides, stochastic methods are applied to analyze both models. In various simulations, the infected probability, the number of individuals in each state and its limited distribution are demonstrated.","sentences":["With the prevalence of COVID-19, the modeling of epidemic propagation and its analyses have played a significant role in controlling epidemics.","However, individual behaviors, in particular the self-protection and migration, which have a strong influence on epidemic propagation, were always neglected in previous studies.","In this paper, we mainly propose two models from the individual and population perspectives.","In the first individual model, we introduce the individual protection degree that effectively suppresses the epidemic level as a stochastic variable to the SIRS model.","In the alternative population model, an open Markov queueing network is constructed to investigate the individual number of each epidemic state, and we present an evolving population network via the migration of people.","Besides, stochastic methods are applied to analyze both models.","In various simulations, the infected probability, the number of individuals in each state and its limited distribution are demonstrated."],"url":"http://arxiv.org/abs/2407.03159v1"}
{"created":"2024-07-03 14:34:03","title":"Let the Code LLM Edit Itself When You Edit the Code","abstract":"In this work, we investigate a typical scenario in code generation where a developer edits existing code in real time and requests a code assistant, e.g., a large language model, to re-predict the next token or next line on the fly. Naively, the LLM needs to re-encode the entire KV cache to provide an accurate prediction. However, this process is computationally expensive, especially when the sequence length is long. Simply encoding the edited subsequence and integrating it to the original KV cache meets the temporal confusion problem, leading to significantly worse performance. We address this efficiency and accuracy trade-off by introducing \\underline{\\textbf{Positional \\textbf{I}ntegrity \\textbf{E}ncoding} (PIE). Building upon the rotary positional encoding, PIE first removes the rotary matrices in the Key cache that introduce temporal confusion and then reapplies the correct rotary matrices. This process ensures that positional relationships between tokens are correct and requires only a single round of matrix multiplication. We validate the effectiveness of PIE through extensive experiments on the RepoBench-C-8k dataset, utilizing DeepSeek-Coder models with 1.3B, 6.7B, and 33B parameters. Our evaluation includes three real-world coding tasks: code insertion, code deletion, and multi-place code editing. Results demonstrate that PIE reduces computational overhead by over 85% compared to the standard full recomputation approach across all model sizes and tasks while well approximating the model performance.","sentences":["In this work, we investigate a typical scenario in code generation where a developer edits existing code in real time and requests a code assistant, e.g., a large language model, to re-predict the next token or next line on the fly.","Naively, the LLM needs to re-encode the entire KV cache to provide an accurate prediction.","However, this process is computationally expensive, especially when the sequence length is long.","Simply encoding the edited subsequence and integrating it to the original KV cache meets the temporal confusion problem, leading to significantly worse performance.","We address this efficiency and accuracy trade-off by introducing \\underline{\\textbf{Positional \\textbf{I}ntegrity \\textbf{E}ncoding} (PIE).","Building upon the rotary positional encoding, PIE first removes the rotary matrices in the Key cache that introduce temporal confusion and then reapplies the correct rotary matrices.","This process ensures that positional relationships between tokens are correct and requires only a single round of matrix multiplication.","We validate the effectiveness of PIE through extensive experiments on the RepoBench-C-8k dataset, utilizing DeepSeek-Coder models with 1.3B, 6.7B, and 33B parameters.","Our evaluation includes three real-world coding tasks: code insertion, code deletion, and multi-place code editing.","Results demonstrate that PIE reduces computational overhead by over 85% compared to the standard full recomputation approach across all model sizes and tasks while well approximating the model performance."],"url":"http://arxiv.org/abs/2407.03157v1"}
{"created":"2024-07-03 14:33:20","title":"The Complexity of Data-Free Nfer","abstract":"Nfer is a Runtime Verification language for the analysis of event traces that applies rules to create hierarchies of time intervals. This work examines the complexity of the evaluation and satisfiability problems for the data-free fragment of nfer. The evaluation problem asks whether a given interval is generated by applying rules to a known input, while the satisfiability problem asks if an input exists that will generate a given interval. By excluding data from the language, we obtain polynomial-time algorithms for the evaluation problem and for satisfiability when only considering inclusive rules. Furthermore, we show decidability for the satisfiability problem for cycle-free specifications and undecidability for satisfiability of full data-free nfer.","sentences":["Nfer is a Runtime Verification language for the analysis of event traces that applies rules to create hierarchies of time intervals.","This work examines the complexity of the evaluation and satisfiability problems for the data-free fragment of nfer.","The evaluation problem asks whether a given interval is generated by applying rules to a known input, while the satisfiability problem asks if an input exists that will generate a given interval.","By excluding data from the language, we obtain polynomial-time algorithms for the evaluation problem and for satisfiability when only considering inclusive rules.","Furthermore, we show decidability for the satisfiability problem for cycle-free specifications and undecidability for satisfiability of full data-free nfer."],"url":"http://arxiv.org/abs/2407.03155v1"}
{"created":"2024-07-03 14:31:36","title":"Reinforcement Learning for Sequence Design Leveraging Protein Language Models","abstract":"Protein sequence design, determined by amino acid sequences, are essential to protein engineering problems in drug discovery. Prior approaches have resorted to evolutionary strategies or Monte-Carlo methods for protein design, but often fail to exploit the structure of the combinatorial search space, to generalize to unseen sequences. In the context of discrete black box optimization over large search spaces, learning a mutation policy to generate novel sequences with reinforcement learning is appealing. Recent advances in protein language models (PLMs) trained on large corpora of protein sequences offer a potential solution to this problem by scoring proteins according to their biological plausibility (such as the TM-score). In this work, we propose to use PLMs as a reward function to generate new sequences. Yet the PLM can be computationally expensive to query due to its large size. To this end, we propose an alternative paradigm where optimization can be performed on scores from a smaller proxy model that is periodically finetuned, jointly while learning the mutation policy. We perform extensive experiments on various sequence lengths to benchmark RL-based approaches, and provide comprehensive evaluations along biological plausibility and diversity of the protein. Our experimental results include favorable evaluations of the proposed sequences, along with high diversity scores, demonstrating that RL is a strong candidate for biological sequence design. Finally, we provide a modular open source implementation can be easily integrated in most RL training loops, with support for replacing the reward model with other PLMs, to spur further research in this domain. The code for all experiments is provided in the supplementary material.","sentences":["Protein sequence design, determined by amino acid sequences, are essential to protein engineering problems in drug discovery.","Prior approaches have resorted to evolutionary strategies or Monte-Carlo methods for protein design, but often fail to exploit the structure of the combinatorial search space, to generalize to unseen sequences.","In the context of discrete black box optimization over large search spaces, learning a mutation policy to generate novel sequences with reinforcement learning is appealing.","Recent advances in protein language models (PLMs) trained on large corpora of protein sequences offer a potential solution to this problem by scoring proteins according to their biological plausibility (such as the TM-score).","In this work, we propose to use PLMs as a reward function to generate new sequences.","Yet the PLM can be computationally expensive to query due to its large size.","To this end, we propose an alternative paradigm where optimization can be performed on scores from a smaller proxy model that is periodically finetuned, jointly while learning the mutation policy.","We perform extensive experiments on various sequence lengths to benchmark RL-based approaches, and provide comprehensive evaluations along biological plausibility and diversity of the protein.","Our experimental results include favorable evaluations of the proposed sequences, along with high diversity scores, demonstrating that RL is a strong candidate for biological sequence design.","Finally, we provide a modular open source implementation can be easily integrated in most RL training loops, with support for replacing the reward model with other PLMs, to spur further research in this domain.","The code for all experiments is provided in the supplementary material."],"url":"http://arxiv.org/abs/2407.03154v1"}
{"created":"2024-07-03 14:30:47","title":"Stereo Risk: A Continuous Modeling Approach to Stereo Matching","abstract":"We introduce Stereo Risk, a new deep-learning approach to solve the classical stereo-matching problem in computer vision. As it is well-known that stereo matching boils down to a per-pixel disparity estimation problem, the popular state-of-the-art stereo-matching approaches widely rely on regressing the scene disparity values, yet via discretization of scene disparity values. Such discretization often fails to capture the nuanced, continuous nature of scene depth. Stereo Risk departs from the conventional discretization approach by formulating the scene disparity as an optimal solution to a continuous risk minimization problem, hence the name \"stereo risk\". We demonstrate that $L^1$ minimization of the proposed continuous risk function enhances stereo-matching performance for deep networks, particularly for disparities with multi-modal probability distributions. Furthermore, to enable the end-to-end network training of the non-differentiable $L^1$ risk optimization, we exploited the implicit function theorem, ensuring a fully differentiable network. A comprehensive analysis demonstrates our method's theoretical soundness and superior performance over the state-of-the-art methods across various benchmark datasets, including KITTI 2012, KITTI 2015, ETH3D, SceneFlow, and Middlebury 2014.","sentences":["We introduce Stereo Risk, a new deep-learning approach to solve the classical stereo-matching problem in computer vision.","As it is well-known that stereo matching boils down to a per-pixel disparity estimation problem, the popular state-of-the-art stereo-matching approaches widely rely on regressing the scene disparity values, yet via discretization of scene disparity values.","Such discretization often fails to capture the nuanced, continuous nature of scene depth.","Stereo Risk departs from the conventional discretization approach by formulating the scene disparity as an optimal solution to a continuous risk minimization problem, hence the name \"stereo risk\".","We demonstrate that $L^1$ minimization of the proposed continuous risk function enhances stereo-matching performance for deep networks, particularly for disparities with multi-modal probability distributions.","Furthermore, to enable the end-to-end network training of the non-differentiable $L^1$ risk optimization, we exploited the implicit function theorem, ensuring a fully differentiable network.","A comprehensive analysis demonstrates our method's theoretical soundness and superior performance over the state-of-the-art methods across various benchmark datasets, including KITTI 2012, KITTI 2015, ETH3D, SceneFlow, and Middlebury 2014."],"url":"http://arxiv.org/abs/2407.03152v1"}
{"created":"2024-07-03 14:23:36","title":"Enhancing Translation Accuracy of Large Language Models through Continual Pre-Training on Parallel Data","abstract":"In this paper, we propose a two-phase training approach where pre-trained large language models are continually pre-trained on parallel data and then supervised fine-tuned with a small amount of high-quality parallel data. To investigate the effectiveness of our proposed approach, we conducted continual pre-training with a 3.8B-parameter model and parallel data across eight different formats. We evaluate these methods on thirteen test sets for Japanese-to-English and English-to-Japanese translation. The results demonstrate that when utilizing parallel data in continual pre-training, it is essential to alternate between source and target sentences. Additionally, we demonstrated that the translation accuracy improves only for translation directions where the order of source and target sentences aligns between continual pre-training data and inference. In addition, we demonstrate that the LLM-based translation model is more robust in translating spoken language and achieves higher accuracy with less training data compared to supervised encoder-decoder models. We also show that the highest accuracy is achieved when the data for continual pre-training consists of interleaved source and target sentences and when tags are added to the source sentences.","sentences":["In this paper, we propose a two-phase training approach where pre-trained large language models are continually pre-trained on parallel data and then supervised fine-tuned with a small amount of high-quality parallel data.","To investigate the effectiveness of our proposed approach, we conducted continual pre-training with a 3.8B-parameter model and parallel data across eight different formats.","We evaluate these methods on thirteen test sets for Japanese-to-English and English-to-Japanese translation.","The results demonstrate that when utilizing parallel data in continual pre-training, it is essential to alternate between source and target sentences.","Additionally, we demonstrated that the translation accuracy improves only for translation directions where the order of source and target sentences aligns between continual pre-training data and inference.","In addition, we demonstrate that the LLM-based translation model is more robust in translating spoken language and achieves higher accuracy with less training data compared to supervised encoder-decoder models.","We also show that the highest accuracy is achieved when the data for continual pre-training consists of interleaved source and target sentences and when tags are added to the source sentences."],"url":"http://arxiv.org/abs/2407.03145v1"}
{"created":"2024-07-03 14:22:51","title":"Venomancer: Towards Imperceptible and Target-on-Demand Backdoor Attacks in Federated Learning","abstract":"Federated Learning (FL) is a distributed machine learning approach that maintains data privacy by training on decentralized data sources. Similar to centralized machine learning, FL is also susceptible to backdoor attacks. Most backdoor attacks in FL assume a predefined target class and require control over a large number of clients or knowledge of benign clients' information. Furthermore, they are not imperceptible and are easily detected by human inspection due to clear artifacts left on the poison data. To overcome these challenges, we propose Venomancer, an effective backdoor attack that is imperceptible and allows target-on-demand. Specifically, imperceptibility is achieved by using a visual loss function to make the poison data visually indistinguishable from the original data. Target-on-demand property allows the attacker to choose arbitrary target classes via conditional adversarial training. Additionally, experiments showed that the method is robust against state-of-the-art defenses such as Norm Clipping, Weak DP, Krum, and Multi-Krum. The source code is available at https://anonymous.4open.science/r/Venomancer-3426.","sentences":["Federated Learning (FL) is a distributed machine learning approach that maintains data privacy by training on decentralized data sources.","Similar to centralized machine learning, FL is also susceptible to backdoor attacks.","Most backdoor attacks in FL assume a predefined target class and require control over a large number of clients or knowledge of benign clients' information.","Furthermore, they are not imperceptible and are easily detected by human inspection due to clear artifacts left on the poison data.","To overcome these challenges, we propose Venomancer, an effective backdoor attack that is imperceptible and allows target-on-demand.","Specifically, imperceptibility is achieved by using a visual loss function to make the poison data visually indistinguishable from the original data.","Target-on-demand property allows the attacker to choose arbitrary target classes via conditional adversarial training.","Additionally, experiments showed that the method is robust against state-of-the-art defenses such as Norm Clipping, Weak DP, Krum, and Multi-Krum.","The source code is available at https://anonymous.4open.science/r/Venomancer-3426."],"url":"http://arxiv.org/abs/2407.03144v1"}
{"created":"2024-07-03 14:20:24","title":"Machine Learning Models for Improved Tracking from Range-Doppler Map Images","abstract":"Statistical tracking filters depend on accurate target measurements and uncertainty estimates for good tracking performance. In this work, we propose novel machine learning models for target detection and uncertainty estimation in range-Doppler map (RDM) images for Ground Moving Target Indicator (GMTI) radars. We show that by using the outputs of these models, we can significantly improve the performance of a multiple hypothesis tracker for complex multi-target air-to-ground tracking scenarios.","sentences":["Statistical tracking filters depend on accurate target measurements and uncertainty estimates for good tracking performance.","In this work, we propose novel machine learning models for target detection and uncertainty estimation in range-Doppler map (RDM) images for Ground Moving Target Indicator (GMTI) radars.","We show that by using the outputs of these models, we can significantly improve the performance of a multiple hypothesis tracker for complex multi-target air-to-ground tracking scenarios."],"url":"http://arxiv.org/abs/2407.03140v1"}
{"created":"2024-07-03 14:14:26","title":"Ultra-Lightweight Collaborative Mapping for Robot Swarms","abstract":"A key requirement in robotics is the ability to simultaneously self-localize and map a previously unknown environment, relying primarily on onboard sensing and computation. Achieving fully onboard accurate simultaneous localization and mapping (SLAM) is feasible for high-end robotic platforms, whereas small and inexpensive robots face challenges due to constrained hardware, therefore frequently resorting to external infrastructure for sensing and computation. The challenge is further exacerbated in swarms of robots, where coordination, scalability, and latency are crucial concerns. This work introduces a decentralized and lightweight collaborative SLAM approach that enables mapping on virtually any robot, even those equipped with low-cost hardware, including miniaturized insect-size devices. Moreover, the proposed solution supports large swarm formations with the capability to coordinate hundreds of agents. To substantiate our claims, we have successfully implemented collaborative SLAM on centimeter-size drones weighing only 46 grams. Remarkably, we achieve results comparable to high-end state-of-the-art solutions while reducing the cost, memory, and computation requirements by two orders of magnitude. Our approach is innovative in three main aspects. First, it enables onboard infrastructure-less collaborative mapping with a lightweight and cost-effective solution in terms of sensing and computation. Second, we optimize the data traffic within the swarm to support hundreds of cooperative agents using standard wireless protocols such as ultra-wideband (UWB), Bluetooth, or WiFi. Last, we implement a distributed swarm coordination policy to decrease mapping latency and enhance accuracy.","sentences":["A key requirement in robotics is the ability to simultaneously self-localize and map a previously unknown environment, relying primarily on onboard sensing and computation.","Achieving fully onboard accurate simultaneous localization and mapping (SLAM) is feasible for high-end robotic platforms, whereas small and inexpensive robots face challenges due to constrained hardware, therefore frequently resorting to external infrastructure for sensing and computation.","The challenge is further exacerbated in swarms of robots, where coordination, scalability, and latency are crucial concerns.","This work introduces a decentralized and lightweight collaborative SLAM approach that enables mapping on virtually any robot, even those equipped with low-cost hardware, including miniaturized insect-size devices.","Moreover, the proposed solution supports large swarm formations with the capability to coordinate hundreds of agents.","To substantiate our claims, we have successfully implemented collaborative SLAM on centimeter-size drones weighing only 46 grams.","Remarkably, we achieve results comparable to high-end state-of-the-art solutions while reducing the cost, memory, and computation requirements by two orders of magnitude.","Our approach is innovative in three main aspects.","First, it enables onboard infrastructure-less collaborative mapping with a lightweight and cost-effective solution in terms of sensing and computation.","Second, we optimize the data traffic within the swarm to support hundreds of cooperative agents using standard wireless protocols such as ultra-wideband (UWB), Bluetooth, or WiFi.","Last, we implement a distributed swarm coordination policy to decrease mapping latency and enhance accuracy."],"url":"http://arxiv.org/abs/2407.03136v1"}
{"created":"2024-07-03 14:14:18","title":"GMM-ResNext: Combining Generative and Discriminative Models for Speaker Verification","abstract":"With the development of deep learning, many different network architectures have been explored in speaker verification. However, most network architectures rely on a single deep learning architecture, and hybrid networks combining different architectures have been little studied in ASV tasks. In this paper, we propose the GMM-ResNext model for speaker verification. Conventional GMM does not consider the score distribution of each frame feature over all Gaussian components and ignores the relationship between neighboring speech frames. So, we extract the log Gaussian probability features based on the raw acoustic features and use ResNext-based network as the backbone to extract the speaker embedding. GMM-ResNext combines Generative and Discriminative Models to improve the generalization ability of deep learning models and allows one to more easily specify meaningful priors on model parameters. A two-path GMM-ResNext model based on two gender-related GMMs has also been proposed. The Experimental results show that the proposed GMM-ResNext achieves relative improvements of 48.1\\% and 11.3\\% in EER compared with ResNet34 and ECAPA-TDNN on VoxCeleb1-O test set.","sentences":["With the development of deep learning, many different network architectures have been explored in speaker verification.","However, most network architectures rely on a single deep learning architecture, and hybrid networks combining different architectures have been little studied in ASV tasks.","In this paper, we propose the GMM-ResNext model for speaker verification.","Conventional GMM does not consider the score distribution of each frame feature over all Gaussian components and ignores the relationship between neighboring speech frames.","So, we extract the log Gaussian probability features based on the raw acoustic features and use ResNext-based network as the backbone to extract the speaker embedding.","GMM-ResNext combines Generative and Discriminative Models to improve the generalization ability of deep learning models and allows one to more easily specify meaningful priors on model parameters.","A two-path GMM-ResNext model based on two gender-related GMMs has also been proposed.","The Experimental results show that the proposed GMM-ResNext achieves relative improvements of 48.1\\% and 11.3\\% in EER compared with ResNet34 and ECAPA-TDNN on VoxCeleb1-O test set."],"url":"http://arxiv.org/abs/2407.03135v1"}
{"created":"2024-07-03 14:13:04","title":"Speaker- and Text-Independent Estimation of Articulatory Movements and Phoneme Alignments from Speech","abstract":"This paper introduces a novel combination of two tasks, previously treated separately: acoustic-to-articulatory speech inversion (AAI) and phoneme-to-articulatory (PTA) motion estimation. We refer to this joint task as acoustic phoneme-to-articulatory speech inversion (APTAI) and explore two different approaches, both working speaker- and text-independently during inference. We use a multi-task learning setup, with the end-to-end goal of taking raw speech as input and estimating the corresponding articulatory movements, phoneme sequence, and phoneme alignment. While both proposed approaches share these same requirements, they differ in their way of achieving phoneme-related predictions: one is based on frame classification, the other on a two-staged training procedure and forced alignment. We reach competitive performance of 0.73 mean correlation for the AAI task and achieve up to approximately 87% frame overlap compared to a state-of-the-art text-dependent phoneme force aligner.","sentences":["This paper introduces a novel combination of two tasks, previously treated separately: acoustic-to-articulatory speech inversion (AAI) and phoneme-to-articulatory (PTA) motion estimation.","We refer to this joint task as acoustic phoneme-to-articulatory speech inversion (APTAI) and explore two different approaches, both working speaker- and text-independently during inference.","We use a multi-task learning setup, with the end-to-end goal of taking raw speech as input and estimating the corresponding articulatory movements, phoneme sequence, and phoneme alignment.","While both proposed approaches share these same requirements, they differ in their way of achieving phoneme-related predictions: one is based on frame classification, the other on a two-staged training procedure and forced alignment.","We reach competitive performance of 0.73 mean correlation for the AAI task and achieve up to approximately 87% frame overlap compared to a state-of-the-art text-dependent phoneme force aligner."],"url":"http://arxiv.org/abs/2407.03132v1"}
{"created":"2024-07-03 14:13:00","title":"MVGT: A Multi-view Graph Transformer Based on Spatial Relations for EEG Emotion Recognition","abstract":"Electroencephalography (EEG), a medical imaging technique that captures scalp electrical activity of brain structures via electrodes, has been widely used in affective computing. The spatial domain of EEG is rich in affective information.However, few of the existing studies have simultaneously analyzed EEG signals from multiple perspectives of geometric and anatomical structures in spatial domain. In this paper, we propose a multi-view Graph Transformer (MVGT) based on spatial relations, which integrates information from the temporal, frequency and spatial domains, including geometric and anatomical structures, so as to enhance the expressive power of the model comprehensively.We incorporate the spatial information of EEG channels into the model as encoding, thereby improving its ability to perceive the spatial structure of the channels. Meanwhile, experimental results based on publicly available datasets demonstrate that our proposed model outperforms state-of-the-art methods in recent years. In addition, the results also show that the MVGT could extract information from multiple domains and capture inter-channel relationships in EEG emotion recognition tasks effectively.","sentences":["Electroencephalography (EEG), a medical imaging technique that captures scalp electrical activity of brain structures via electrodes, has been widely used in affective computing.","The spatial domain of EEG is rich in affective information.","However, few of the existing studies have simultaneously analyzed EEG signals from multiple perspectives of geometric and anatomical structures in spatial domain.","In this paper, we propose a multi-view Graph Transformer (MVGT) based on spatial relations, which integrates information from the temporal, frequency and spatial domains, including geometric and anatomical structures, so as to enhance the expressive power of the model comprehensively.","We incorporate the spatial information of EEG channels into the model as encoding, thereby improving its ability to perceive the spatial structure of the channels.","Meanwhile, experimental results based on publicly available datasets demonstrate that our proposed model outperforms state-of-the-art methods in recent years.","In addition, the results also show that the MVGT could extract information from multiple domains and capture inter-channel relationships in EEG emotion recognition tasks effectively."],"url":"http://arxiv.org/abs/2407.03131v1"}
{"created":"2024-07-03 14:12:43","title":"Towards Efficient Pixel Labeling for Industrial Anomaly Detection and Localization","abstract":"In the realm of practical Anomaly Detection (AD) tasks, manual labeling of anomalous pixels proves to be a costly endeavor. Consequently, many AD methods are crafted as one-class classifiers, tailored for training sets completely devoid of anomalies, ensuring a more cost-effective approach. While some pioneering work has demonstrated heightened AD accuracy by incorporating real anomaly samples in training, this enhancement comes at the price of labor-intensive labeling processes. This paper strikes the balance between AD accuracy and labeling expenses by introducing ADClick, a novel Interactive Image Segmentation (IIS) algorithm. ADClick efficiently generates \"ground-truth\" anomaly masks for real defective images, leveraging innovative residual features and meticulously crafted language prompts. Notably, ADClick showcases a significantly elevated generalization capacity compared to existing state-of-the-art IIS approaches. Functioning as an anomaly labeling tool, ADClick generates high-quality anomaly labels (AP $= 94.1\\%$ on MVTec AD) based on only $3$ to $5$ manual click annotations per training image. Furthermore, we extend the capabilities of ADClick into ADClick-Seg, an enhanced model designed for anomaly detection and localization. By fine-tuning the ADClick-Seg model using the weak labels inferred by ADClick, we establish the state-of-the-art performances in supervised AD tasks (AP $= 86.4\\%$ on MVTec AD and AP $= 78.4\\%$, PRO $= 98.6\\%$ on KSDD2).","sentences":["In the realm of practical Anomaly Detection (AD) tasks, manual labeling of anomalous pixels proves to be a costly endeavor.","Consequently, many AD methods are crafted as one-class classifiers, tailored for training sets completely devoid of anomalies, ensuring a more cost-effective approach.","While some pioneering work has demonstrated heightened AD accuracy by incorporating real anomaly samples in training, this enhancement comes at the price of labor-intensive labeling processes.","This paper strikes the balance between AD accuracy and labeling expenses by introducing ADClick, a novel Interactive Image Segmentation (IIS) algorithm.","ADClick efficiently generates \"ground-truth\" anomaly masks for real defective images, leveraging innovative residual features and meticulously crafted language prompts.","Notably, ADClick showcases a significantly elevated generalization capacity compared to existing state-of-the-art IIS approaches.","Functioning as an anomaly labeling tool, ADClick generates high-quality anomaly labels (AP $= 94.1\\%$ on MVTec AD) based on only $3$ to $5$ manual click annotations per training image.","Furthermore, we extend the capabilities of ADClick into ADClick-Seg, an enhanced model designed for anomaly detection and localization.","By fine-tuning the ADClick-Seg model using the weak labels inferred by ADClick, we establish the state-of-the-art performances in supervised AD tasks (AP $= 86.4\\%$ on MVTec AD and AP $= 78.4\\%$, PRO $= 98.6\\%$ on KSDD2)."],"url":"http://arxiv.org/abs/2407.03130v1"}
{"created":"2024-07-03 14:12:04","title":"Social Bias Evaluation for Large Language Models Requires Prompt Variations","abstract":"Warning: This paper contains examples of stereotypes and biases. Large Language Models (LLMs) exhibit considerable social biases, and various studies have tried to evaluate and mitigate these biases accurately. Previous studies use downstream tasks as prompts to examine the degree of social biases for evaluation and mitigation. While LLMs' output highly depends on prompts, previous studies evaluating and mitigating bias have often relied on a limited variety of prompts. In this paper, we investigate the sensitivity of LLMs when changing prompt variations (task instruction and prompt, few-shot examples, debias-prompt) by analyzing task performance and social bias of LLMs. Our experimental results reveal that LLMs are highly sensitive to prompts to the extent that the ranking of LLMs fluctuates when comparing models for task performance and social bias. Additionally, we show that LLMs have tradeoffs between performance and social bias caused by the prompts. Less bias from prompt setting may result in reduced performance. Moreover, the ambiguity of instances is one of the reasons for this sensitivity to prompts in advanced LLMs, leading to various outputs. We recommend using diverse prompts, as in this study, to compare the effects of prompts on social bias in LLMs.","sentences":["Warning:","This paper contains examples of stereotypes and biases.","Large Language Models (LLMs) exhibit considerable social biases, and various studies have tried to evaluate and mitigate these biases accurately.","Previous studies use downstream tasks as prompts to examine the degree of social biases for evaluation and mitigation.","While LLMs' output highly depends on prompts, previous studies evaluating and mitigating bias have often relied on a limited variety of prompts.","In this paper, we investigate the sensitivity of LLMs when changing prompt variations (task instruction and prompt, few-shot examples, debias-prompt) by analyzing task performance and social bias of LLMs.","Our experimental results reveal that LLMs are highly sensitive to prompts to the extent that the ranking of LLMs fluctuates when comparing models for task performance and social bias.","Additionally, we show that LLMs have tradeoffs between performance and social bias caused by the prompts.","Less bias from prompt setting may result in reduced performance.","Moreover, the ambiguity of instances is one of the reasons for this sensitivity to prompts in advanced LLMs, leading to various outputs.","We recommend using diverse prompts, as in this study, to compare the effects of prompts on social bias in LLMs."],"url":"http://arxiv.org/abs/2407.03129v1"}
{"created":"2024-07-03 14:07:41","title":"Foundations and Frontiers of Graph Learning Theory","abstract":"Recent advancements in graph learning have revolutionized the way to understand and analyze data with complex structures. Notably, Graph Neural Networks (GNNs), i.e. neural network architectures designed for learning graph representations, have become a popular paradigm. With these models being usually characterized by intuition-driven design or highly intricate components, placing them within the theoretical analysis framework to distill the core concepts, helps understand the key principles that drive the functionality better and guide further development. Given this surge in interest, this article provides a comprehensive summary of the theoretical foundations and breakthroughs concerning the approximation and learning behaviors intrinsic to prevalent graph learning models. Encompassing discussions on fundamental aspects such as expressiveness power, generalization, optimization, and unique phenomena such as over-smoothing and over-squashing, this piece delves into the theoretical foundations and frontier driving the evolution of graph learning. In addition, this article also presents several challenges and further initiates discussions on possible solutions.","sentences":["Recent advancements in graph learning have revolutionized the way to understand and analyze data with complex structures.","Notably, Graph Neural Networks (GNNs), i.e. neural network architectures designed for learning graph representations, have become a popular paradigm.","With these models being usually characterized by intuition-driven design or highly intricate components, placing them within the theoretical analysis framework to distill the core concepts, helps understand the key principles that drive the functionality better and guide further development.","Given this surge in interest, this article provides a comprehensive summary of the theoretical foundations and breakthroughs concerning the approximation and learning behaviors intrinsic to prevalent graph learning models.","Encompassing discussions on fundamental aspects such as expressiveness power, generalization, optimization, and unique phenomena such as over-smoothing and over-squashing, this piece delves into the theoretical foundations and frontier driving the evolution of graph learning.","In addition, this article also presents several challenges and further initiates discussions on possible solutions."],"url":"http://arxiv.org/abs/2407.03125v1"}
{"created":"2024-07-03 14:06:14","title":"IntentionNet: Map-Lite Visual Navigation at the Kilometre Scale","abstract":"This work explores the challenges of creating a scalable and robust robot navigation system that can traverse both indoor and outdoor environments to reach distant goals. We propose a navigation system architecture called IntentionNet that employs a monolithic neural network as the low-level planner/controller, and uses a general interface that we call intentions to steer the controller. The paper proposes two types of intentions, Local Path and Environment (LPE) and Discretised Local Move (DLM), and shows that DLM is robust to significant metric positioning and mapping errors. The paper also presents Kilo-IntentionNet, an instance of the IntentionNet system using the DLM intention that is deployed on a Boston Dynamics Spot robot, and which successfully navigates through complex indoor and outdoor environments over distances of up to a kilometre with only noisy odometry.","sentences":["This work explores the challenges of creating a scalable and robust robot navigation system that can traverse both indoor and outdoor environments to reach distant goals.","We propose a navigation system architecture called IntentionNet that employs a monolithic neural network as the low-level planner/controller, and uses a general interface that we call intentions to steer the controller.","The paper proposes two types of intentions, Local Path and Environment (LPE) and Discretised Local Move (DLM), and shows that DLM is robust to significant metric positioning and mapping errors.","The paper also presents Kilo-IntentionNet, an instance of the IntentionNet system using the DLM intention that is deployed on a Boston Dynamics Spot robot, and which successfully navigates through complex indoor and outdoor environments over distances of up to a kilometre with only noisy odometry."],"url":"http://arxiv.org/abs/2407.03122v1"}
{"created":"2024-07-03 14:04:05","title":"Can machine learning solve the challenge of adaptive learning and the individualization of learning paths? A field experiment in an online learning platform","abstract":"The individualization of learning contents based on digital technologies promises large individual and social benefits. However, it remains an open question how this individualization can be implemented. To tackle this question we conduct a randomized controlled trial on a large digital self-learning platform. We develop an algorithm based on two convolutional neural networks that assigns tasks to $4,365$ learners according to their learning paths. Learners are randomized into three groups: two treatment groups -- a group-based adaptive treatment group and an individual adaptive treatment group -- and one control group. We analyze the difference between the three groups with respect to effort learners provide and their performance on the platform. Our null results shed light on the multiple challenges associated with the individualization of learning paths.","sentences":["The individualization of learning contents based on digital technologies promises large individual and social benefits.","However, it remains an open question how this individualization can be implemented.","To tackle this question we conduct a randomized controlled trial on a large digital self-learning platform.","We develop an algorithm based on two convolutional neural networks that assigns tasks to $4,365$ learners according to their learning paths.","Learners are randomized into three groups: two treatment groups -- a group-based adaptive treatment group and an individual adaptive treatment group -- and one control group.","We analyze the difference between the three groups with respect to effort learners provide and their performance on the platform.","Our null results shed light on the multiple challenges associated with the individualization of learning paths."],"url":"http://arxiv.org/abs/2407.03118v1"}
{"created":"2024-07-03 14:00:33","title":"$L_p$-norm Distortion-Efficient Adversarial Attack","abstract":"Adversarial examples have shown a powerful ability to make a well-trained model misclassified. Current mainstream adversarial attack methods only consider one of the distortions among $L_0$-norm, $L_2$-norm, and $L_\\infty$-norm. $L_0$-norm based methods cause large modification on a single pixel, resulting in naked-eye visible detection, while $L_2$-norm and $L_\\infty$-norm based methods suffer from weak robustness against adversarial defense since they always diffuse tiny perturbations to all pixels. A more realistic adversarial perturbation should be sparse and imperceptible. In this paper, we propose a novel $L_p$-norm distortion-efficient adversarial attack, which not only owns the least $L_2$-norm loss but also significantly reduces the $L_0$-norm distortion. To this aim, we design a new optimization scheme, which first optimizes an initial adversarial perturbation under $L_2$-norm constraint, and then constructs a dimension unimportance matrix for the initial perturbation. Such a dimension unimportance matrix can indicate the adversarial unimportance of each dimension of the initial perturbation. Furthermore, we introduce a new concept of adversarial threshold for the dimension unimportance matrix. The dimensions of the initial perturbation whose unimportance is higher than the threshold will be all set to zero, greatly decreasing the $L_0$-norm distortion. Experimental results on three benchmark datasets show that under the same query budget, the adversarial examples generated by our method have lower $L_0$-norm and $L_2$-norm distortion than the state-of-the-art. Especially for the MNIST dataset, our attack reduces 8.1$\\%$ $L_2$-norm distortion meanwhile remaining 47$\\%$ pixels unattacked. This demonstrates the superiority of the proposed method over its competitors in terms of adversarial robustness and visual imperceptibility.","sentences":["Adversarial examples have shown a powerful ability to make a well-trained model misclassified.","Current mainstream adversarial attack methods only consider one of the distortions among $L_0$-norm, $L_2$-norm, and $L_\\infty$-norm.","$L_0$-norm based methods cause large modification on a single pixel, resulting in naked-eye visible detection, while $L_2$-norm and $L_\\infty$-norm based methods suffer from weak robustness against adversarial defense since they always diffuse tiny perturbations to all pixels.","A more realistic adversarial perturbation should be sparse and imperceptible.","In this paper, we propose a novel $L_p$-norm distortion-efficient adversarial attack, which not only owns the least $L_2$-norm loss but also significantly reduces the $L_0$-norm distortion.","To this aim, we design a new optimization scheme, which first optimizes an initial adversarial perturbation under $L_2$-norm constraint, and then constructs a dimension unimportance matrix for the initial perturbation.","Such a dimension unimportance matrix can indicate the adversarial unimportance of each dimension of the initial perturbation.","Furthermore, we introduce a new concept of adversarial threshold for the dimension unimportance matrix.","The dimensions of the initial perturbation whose unimportance is higher than the threshold will be all set to zero, greatly decreasing the $L_0$-norm distortion.","Experimental results on three benchmark datasets show that under the same query budget, the adversarial examples generated by our method have lower $L_0$-norm and $L_2$-norm distortion than the state-of-the-art.","Especially for the MNIST dataset, our attack reduces 8.1$\\%$ $L_2$-norm distortion meanwhile remaining 47$\\%$ pixels unattacked.","This demonstrates the superiority of the proposed method over its competitors in terms of adversarial robustness and visual imperceptibility."],"url":"http://arxiv.org/abs/2407.03115v1"}
