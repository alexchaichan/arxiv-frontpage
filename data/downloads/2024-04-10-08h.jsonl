{"created":"2024-04-09 17:59:47","title":"Superpolynomial Lower Bounds for Smooth 3-LCCs and Sharp Bounds for Designs","abstract":"We give improved lower bounds for binary $3$-query locally correctable codes (3-LCCs) $C \\colon \\{0,1\\}^k \\rightarrow \\{0,1\\}^n$. Specifically, we prove:   (1) If $C$ is a linear design 3-LCC, then $n \\geq 2^{(1 - o(1))\\sqrt{k} }$. A design 3-LCC has the additional property that the correcting sets for every codeword bit form a perfect matching and every pair of codeword bits is queried an equal number of times across all matchings. Our bound is tight up to a factor $\\sqrt{8}$ in the exponent of $2$, as the best construction of binary $3$-LCCs (obtained by taking Reed-Muller codes on $\\mathbb{F}_4$ and applying a natural projection map) is a design $3$-LCC with $n \\leq 2^{\\sqrt{8 k}}$. Up to a $\\sqrt{8}$ factor, this resolves the Hamada conjecture on the maximum $\\mathbb{F}_2$-codimension of a $4$-design.   (2) If $C$ is a smooth, non-linear $3$-LCC with near-perfect completeness, then, $n \\geq k^{\\Omega(\\log k)}$.   (3) If $C$ is a smooth, non-linear $3$-LCC with completeness $1 - \\varepsilon$, then $n \\geq \\tilde{\\Omega}(k^{\\frac{1}{2\\varepsilon}})$. In particular, when $\\varepsilon$ is a small constant, this implies a lower bound for general non-linear LCCs that beats the prior best $n \\geq \\tilde{\\Omega}(k^3)$ lower bound of [AGKM23] by a polynomial factor.   Our design LCC lower bound is obtained via a fine-grained analysis of the Kikuchi matrix method applied to a variant of the matrix used in [KM23]. Our lower bounds for non-linear codes are obtained by designing a from-scratch reduction from nonlinear $3$-LCCs to a system of \"chain polynomial equations\": polynomial equations with similar structure to the long chain derivations that arise in the lower bounds for linear $3$-LCCs [KM23].","sentences":["We give improved lower bounds for binary $3$-query locally correctable codes (3-LCCs) $C \\colon \\{0,1\\}^k \\rightarrow \\{0,1\\}^n$. Specifically, we prove:   (1) If $C$ is a linear design 3-LCC, then $n \\geq 2^{(1 - o(1))\\sqrt{k} }$.","A design 3-LCC has the additional property that the correcting sets for every codeword bit form a perfect matching and every pair of codeword bits is queried an equal number of times across all matchings.","Our bound is tight up to a factor $\\sqrt{8}$ in the exponent of $2$, as the best construction of binary $3$-LCCs (obtained by taking Reed-Muller codes on $\\mathbb{F}_4$ and applying a natural projection map) is a design $3$-LCC with $n \\leq","2^{\\sqrt{8 k}}$.","Up to a $\\sqrt{8}$ factor, this resolves the Hamada conjecture on the maximum $\\mathbb{F}_2$-codimension of a $4$-design.   ","(2) If $C$ is a smooth, non-linear $3$-LCC with near-perfect completeness, then, $n \\geq k^{\\Omega(\\log k)}$.   (3) If $C$ is a smooth, non-linear $3$-LCC with completeness $1 - \\varepsilon$, then $n \\geq \\tilde{\\Omega}(k^{\\frac{1}{2\\varepsilon}})$. In particular, when $\\varepsilon$ is a small constant, this implies a lower bound for general non-linear LCCs that beats the prior best $n \\geq \\tilde{\\Omega}(k^3)$ lower bound of [AGKM23] by a polynomial factor.   ","Our design LCC lower bound is obtained via a fine-grained analysis of the Kikuchi matrix method applied to a variant of the matrix used in [KM23].","Our lower bounds for non-linear codes are obtained by designing a from-scratch reduction from nonlinear $3$-LCCs to a system of \"chain polynomial equations\": polynomial equations with similar structure to the long chain derivations that arise in the lower bounds for linear $3$-LCCs","[KM23]."],"url":"http://arxiv.org/abs/2404.06513v1"}
{"created":"2024-04-09 17:59:32","title":"InternLM-XComposer2-4KHD: A Pioneering Large Vision-Language Model Handling Resolutions from 336 Pixels to 4K HD","abstract":"The Large Vision-Language Model (LVLM) field has seen significant advancements, yet its progression has been hindered by challenges in comprehending fine-grained visual content due to limited resolution. Recent efforts have aimed to enhance the high-resolution understanding capabilities of LVLMs, yet they remain capped at approximately 1500 x 1500 pixels and constrained to a relatively narrow resolution range. This paper represents InternLM-XComposer2-4KHD, a groundbreaking exploration into elevating LVLM resolution capabilities up to 4K HD (3840 x 1600) and beyond. Concurrently, considering the ultra-high resolution may not be necessary in all scenarios, it supports a wide range of diverse resolutions from 336 pixels to 4K standard, significantly broadening its scope of applicability. Specifically, this research advances the patch division paradigm by introducing a novel extension: dynamic resolution with automatic patch configuration. It maintains the training image aspect ratios while automatically varying patch counts and configuring layouts based on a pre-trained Vision Transformer (ViT) (336 x 336), leading to dynamic training resolution from 336 pixels to 4K standard. Our research demonstrates that scaling training resolution up to 4K HD leads to consistent performance enhancements without hitting the ceiling of potential improvements. InternLM-XComposer2-4KHD shows superb capability that matches or even surpasses GPT-4V and Gemini Pro in 10 of the 16 benchmarks. The InternLM-XComposer2-4KHD model series with 7B parameters are publicly available at https://github.com/InternLM/InternLM-XComposer.","sentences":["The Large Vision-Language Model (LVLM) field has seen significant advancements, yet its progression has been hindered by challenges in comprehending fine-grained visual content due to limited resolution.","Recent efforts have aimed to enhance the high-resolution understanding capabilities of LVLMs, yet they remain capped at approximately 1500 x 1500 pixels and constrained to a relatively narrow resolution range.","This paper represents InternLM-XComposer2-4KHD, a groundbreaking exploration into elevating LVLM resolution capabilities up to 4K HD (3840 x 1600) and beyond.","Concurrently, considering the ultra-high resolution may not be necessary in all scenarios, it supports a wide range of diverse resolutions from 336 pixels to 4K standard, significantly broadening its scope of applicability.","Specifically, this research advances the patch division paradigm by introducing a novel extension: dynamic resolution with automatic patch configuration.","It maintains the training image aspect ratios while automatically varying patch counts and configuring layouts based on a pre-trained Vision Transformer (ViT) (336 x 336), leading to dynamic training resolution from 336 pixels to 4K standard.","Our research demonstrates that scaling training resolution up to 4K HD leads to consistent performance enhancements without hitting the ceiling of potential improvements.","InternLM-XComposer2-4KHD shows superb capability that matches or even surpasses GPT-4V and Gemini Pro in 10 of the 16 benchmarks.","The InternLM-XComposer2-4KHD model series with 7B parameters are publicly available at https://github.com/InternLM/InternLM-XComposer."],"url":"http://arxiv.org/abs/2404.06512v1"}
{"created":"2024-04-09 17:59:31","title":"MoReVQA: Exploring Modular Reasoning Models for Video Question Answering","abstract":"This paper addresses the task of video question answering (videoQA) via a decomposed multi-stage, modular reasoning framework. Previous modular methods have shown promise with a single planning stage ungrounded in visual content. However, through a simple and effective baseline, we find that such systems can lead to brittle behavior in practice for challenging videoQA settings. Thus, unlike traditional single-stage planning methods, we propose a multi-stage system consisting of an event parser, a grounding stage, and a final reasoning stage in conjunction with an external memory. All stages are training-free, and performed using few-shot prompting of large models, creating interpretable intermediate outputs at each stage. By decomposing the underlying planning and task complexity, our method, MoReVQA, improves over prior work on standard videoQA benchmarks (NExT-QA, iVQA, EgoSchema, ActivityNet-QA) with state-of-the-art results, and extensions to related tasks (grounded videoQA, paragraph captioning).","sentences":["This paper addresses the task of video question answering (videoQA) via a decomposed multi-stage, modular reasoning framework.","Previous modular methods have shown promise with a single planning stage ungrounded in visual content.","However, through a simple and effective baseline, we find that such systems can lead to brittle behavior in practice for challenging videoQA settings.","Thus, unlike traditional single-stage planning methods, we propose a multi-stage system consisting of an event parser, a grounding stage, and a final reasoning stage in conjunction with an external memory.","All stages are training-free, and performed using few-shot prompting of large models, creating interpretable intermediate outputs at each stage.","By decomposing the underlying planning and task complexity, our method, MoReVQA, improves over prior work on standard videoQA benchmarks (NExT-QA, iVQA, EgoSchema, ActivityNet-QA) with state-of-the-art results, and extensions to related tasks (grounded videoQA, paragraph captioning)."],"url":"http://arxiv.org/abs/2404.06511v1"}
{"created":"2024-04-09 17:59:04","title":"Can Feedback Enhance Semantic Grounding in Large Vision-Language Models?","abstract":"Enhancing semantic grounding abilities in Vision-Language Models (VLMs) often involves collecting domain-specific training data, refining the network architectures, or modifying the training recipes. In this work, we venture into an orthogonal direction and explore whether VLMs can improve their semantic grounding by \"receiving\" feedback, without requiring in-domain data, fine-tuning, or modifications to the network architectures. We systematically analyze this hypothesis using a feedback mechanism composed of a binary signal. We find that if prompted appropriately, VLMs can utilize feedback both in a single step and iteratively, showcasing the potential of feedback as an alternative technique to improve grounding in internet-scale VLMs. Furthermore, VLMs, like LLMs, struggle to self-correct errors out-of-the-box. However, we find that this issue can be mitigated via a binary verification mechanism. Finally, we explore the potential and limitations of amalgamating these findings and applying them iteratively to automatically enhance VLMs' grounding performance, showing grounding accuracy consistently improves using automated feedback across all models in all settings investigated. Overall, our iterative framework improves semantic grounding in VLMs by more than 15 accuracy points under noise-free feedback and up to 5 accuracy points under a simple automated binary verification mechanism. The project website is hosted at https://andrewliao11.github.io/vlms_feedback","sentences":["Enhancing semantic grounding abilities in Vision-Language Models (VLMs) often involves collecting domain-specific training data, refining the network architectures, or modifying the training recipes.","In this work, we venture into an orthogonal direction and explore whether VLMs can improve their semantic grounding by \"receiving\" feedback, without requiring in-domain data, fine-tuning, or modifications to the network architectures.","We systematically analyze this hypothesis using a feedback mechanism composed of a binary signal.","We find that if prompted appropriately, VLMs can utilize feedback both in a single step and iteratively, showcasing the potential of feedback as an alternative technique to improve grounding in internet-scale VLMs.","Furthermore, VLMs, like LLMs, struggle to self-correct errors out-of-the-box.","However, we find that this issue can be mitigated via a binary verification mechanism.","Finally, we explore the potential and limitations of amalgamating these findings and applying them iteratively to automatically enhance VLMs' grounding performance, showing grounding accuracy consistently improves using automated feedback across all models in all settings investigated.","Overall, our iterative framework improves semantic grounding in VLMs by more than 15 accuracy points under noise-free feedback and up to 5 accuracy points under a simple automated binary verification mechanism.","The project website is hosted at https://andrewliao11.github.io/vlms_feedback"],"url":"http://arxiv.org/abs/2404.06510v1"}
{"created":"2024-04-09 17:57:29","title":"On the Effect of (Near) Duplicate Subwords in Language Modelling","abstract":"Tokenisation is a core part of language models (LMs). It involves splitting a character sequence into subwords which are assigned arbitrary indices before being served to the LM. While typically lossless, however, this process may lead to less sample efficient LM training: as it removes character-level information, it could make it harder for LMs to generalise across similar subwords, such as now and Now. We refer to such subwords as near duplicates. In this paper, we study the impact of near duplicate subwords on LM training efficiency. First, we design an experiment that gives us an upper bound to how much we should expect a model to improve if we could perfectly generalise across near duplicates. We do this by duplicating each subword in our LM's vocabulary, creating perfectly equivalent classes of subwords. Experimentally, we find that LMs need roughly 17% more data when trained in a fully duplicated setting. Second, we investigate the impact of naturally occurring near duplicates on LMs. Here, we see that merging them considerably hurts LM performance. Therefore, although subword duplication negatively impacts LM training efficiency, naturally occurring near duplicates may not be as similar as anticipated, limiting the potential for performance improvements.","sentences":["Tokenisation is a core part of language models (LMs).","It involves splitting a character sequence into subwords which are assigned arbitrary indices before being served to the LM.","While typically lossless, however, this process may lead to less sample efficient LM training: as it removes character-level information, it could make it harder for LMs to generalise across similar subwords, such as now and Now.","We refer to such subwords as near duplicates.","In this paper, we study the impact of near duplicate subwords on LM training efficiency.","First, we design an experiment that gives us an upper bound to how much we should expect a model to improve if we could perfectly generalise across near duplicates.","We do this by duplicating each subword in our LM's vocabulary, creating perfectly equivalent classes of subwords.","Experimentally, we find that LMs need roughly 17% more data when trained in a fully duplicated setting.","Second, we investigate the impact of naturally occurring near duplicates on LMs.","Here, we see that merging them considerably hurts LM performance.","Therefore, although subword duplication negatively impacts LM training efficiency, naturally occurring near duplicates may not be as similar as anticipated, limiting the potential for performance improvements."],"url":"http://arxiv.org/abs/2404.06508v1"}
{"created":"2024-04-09 17:55:41","title":"Reconstructing Hand-Held Objects in 3D","abstract":"Objects manipulated by the hand (i.e., manipulanda) are particularly challenging to reconstruct from in-the-wild RGB images or videos. Not only does the hand occlude much of the object, but also the object is often only visible in a small number of image pixels. At the same time, two strong anchors emerge in this setting: (1) estimated 3D hands help disambiguate the location and scale of the object, and (2) the set of manipulanda is small relative to all possible objects. With these insights in mind, we present a scalable paradigm for handheld object reconstruction that builds on recent breakthroughs in large language/vision models and 3D object datasets. Our model, MCC-Hand-Object (MCC-HO), jointly reconstructs hand and object geometry given a single RGB image and inferred 3D hand as inputs. Subsequently, we use GPT-4(V) to retrieve a 3D object model that matches the object in the image and rigidly align the model to the network-inferred geometry; we call this alignment Retrieval-Augmented Reconstruction (RAR). Experiments demonstrate that MCC-HO achieves state-of-the-art performance on lab and Internet datasets, and we show how RAR can be used to automatically obtain 3D labels for in-the-wild images of hand-object interactions.","sentences":["Objects manipulated by the hand (i.e., manipulanda) are particularly challenging to reconstruct from in-the-wild RGB images or videos.","Not only does the hand occlude much of the object, but also the object is often only visible in a small number of image pixels.","At the same time, two strong anchors emerge in this setting: (1) estimated 3D hands help disambiguate the location and scale of the object, and (2) the set of manipulanda is small relative to all possible objects.","With these insights in mind, we present a scalable paradigm for handheld object reconstruction that builds on recent breakthroughs in large language/vision models and 3D object datasets.","Our model, MCC-Hand-Object (MCC-HO), jointly reconstructs hand and object geometry given a single RGB image and inferred 3D hand as inputs.","Subsequently, we use GPT-4(V) to retrieve a 3D object model that matches the object in the image and rigidly align the model to the network-inferred geometry; we call this alignment Retrieval-Augmented Reconstruction (RAR).","Experiments demonstrate that MCC-HO achieves state-of-the-art performance on lab and Internet datasets, and we show how RAR can be used to automatically obtain 3D labels for in-the-wild images of hand-object interactions."],"url":"http://arxiv.org/abs/2404.06507v1"}
{"created":"2024-04-09 17:54:10","title":"Comparing Two Model Designs for Clinical Note Generation; Is an LLM a Useful Evaluator of Consistency?","abstract":"Following an interaction with a patient, physicians are responsible for the submission of clinical documentation, often organized as a SOAP note. A clinical note is not simply a summary of the conversation but requires the use of appropriate medical terminology. The relevant information can then be extracted and organized according to the structure of the SOAP note. In this paper we analyze two different approaches to generate the different sections of a SOAP note based on the audio recording of the conversation, and specifically examine them in terms of note consistency. The first approach generates the sections independently, while the second method generates them all together. In this work we make use of PEGASUS-X Transformer models and observe that both methods lead to similar ROUGE values (less than 1% difference) and have no difference in terms of the Factuality metric. We perform a human evaluation to measure aspects of consistency and demonstrate that LLMs like Llama2 can be used to perform the same tasks with roughly the same agreement as the human annotators. Between the Llama2 analysis and the human reviewers we observe a Cohen Kappa inter-rater reliability of 0.79, 1.00, and 0.32 for consistency of age, gender, and body part injury, respectively. With this we demonstrate the usefulness of leveraging an LLM to measure quality indicators that can be identified by humans but are not currently captured by automatic metrics. This allows scaling evaluation to larger data sets, and we find that clinical note consistency improves by generating each new section conditioned on the output of all previously generated sections.","sentences":["Following an interaction with a patient, physicians are responsible for the submission of clinical documentation, often organized as a SOAP note.","A clinical note is not simply a summary of the conversation but requires the use of appropriate medical terminology.","The relevant information can then be extracted and organized according to the structure of the SOAP note.","In this paper we analyze two different approaches to generate the different sections of a SOAP note based on the audio recording of the conversation, and specifically examine them in terms of note consistency.","The first approach generates the sections independently, while the second method generates them all together.","In this work we make use of PEGASUS-X Transformer models and observe that both methods lead to similar ROUGE values (less than 1% difference) and have no difference in terms of the Factuality metric.","We perform a human evaluation to measure aspects of consistency and demonstrate that LLMs like Llama2 can be used to perform the same tasks with roughly the same agreement as the human annotators.","Between the Llama2 analysis and the human reviewers we observe a Cohen Kappa inter-rater reliability of 0.79, 1.00, and 0.32 for consistency of age, gender, and body part injury, respectively.","With this we demonstrate the usefulness of leveraging an LLM to measure quality indicators that can be identified by humans but are not currently captured by automatic metrics.","This allows scaling evaluation to larger data sets, and we find that clinical note consistency improves by generating each new section conditioned on the output of all previously generated sections."],"url":"http://arxiv.org/abs/2404.06503v1"}
{"created":"2024-04-09 17:51:42","title":"The Rise and Fall of the Initial Era","abstract":"Bibliographic data is a rich source of information that goes beyond the use cases of location and citation -- it also encodes both cultural and technological context. For most of its existence, the scholarly record has changed slowly and hence provides an opportunity to gain insight through its reflection of the cultural norms of the research community over the last four centuries. While it is often difficult to distinguish the originating driver of change, it is still valuable to consider the motivating influences that have led to changes in the structure of the scholarly record. An ``initial era'' is identified during which initials were used in preference to full names by authors on scholarly communications. Causes of the emergence and demise of this era are considered as well as the implications of this era on research culture and practice.","sentences":["Bibliographic data is a rich source of information that goes beyond the use cases of location and citation -- it also encodes both cultural and technological context.","For most of its existence, the scholarly record has changed slowly and hence provides an opportunity to gain insight through its reflection of the cultural norms of the research community over the last four centuries.","While it is often difficult to distinguish the originating driver of change, it is still valuable to consider the motivating influences that have led to changes in the structure of the scholarly record.","An ``initial era'' is identified during which initials were used in preference to full names by authors on scholarly communications.","Causes of the emergence and demise of this era are considered as well as the implications of this era on research culture and practice."],"url":"http://arxiv.org/abs/2404.06500v1"}
{"created":"2024-04-09 17:50:38","title":"Simultaneous linear connectivity of neural networks modulo permutation","abstract":"Neural networks typically exhibit permutation symmetries which contribute to the non-convexity of the networks' loss landscapes, since linearly interpolating between two permuted versions of a trained network tends to encounter a high loss barrier. Recent work has argued that permutation symmetries are the only sources of non-convexity, meaning there are essentially no such barriers between trained networks if they are permuted appropriately. In this work, we refine these arguments into three distinct claims of increasing strength. We show that existing evidence only supports \"weak linear connectivity\"-that for each pair of networks belonging to a set of SGD solutions, there exist (multiple) permutations that linearly connect it with the other networks. In contrast, the claim \"strong linear connectivity\"-that for each network, there exists one permutation that simultaneously connects it with the other networks-is both intuitively and practically more desirable. This stronger claim would imply that the loss landscape is convex after accounting for permutation, and enable linear interpolation between three or more independently trained models without increased loss. In this work, we introduce an intermediate claim-that for certain sequences of networks, there exists one permutation that simultaneously aligns matching pairs of networks from these sequences. Specifically, we discover that a single permutation aligns sequences of iteratively trained as well as iteratively pruned networks, meaning that two networks exhibit low loss barriers at each step of their optimization and sparsification trajectories respectively. Finally, we provide the first evidence that strong linear connectivity may be possible under certain conditions, by showing that barriers decrease with increasing network width when interpolating among three networks.","sentences":["Neural networks typically exhibit permutation symmetries which contribute to the non-convexity of the networks' loss landscapes, since linearly interpolating between two permuted versions of a trained network tends to encounter a high loss barrier.","Recent work has argued that permutation symmetries are the only sources of non-convexity, meaning there are essentially no such barriers between trained networks if they are permuted appropriately.","In this work, we refine these arguments into three distinct claims of increasing strength.","We show that existing evidence only supports \"weak linear connectivity\"-that for each pair of networks belonging to a set of SGD solutions, there exist (multiple) permutations that linearly connect it with the other networks.","In contrast, the claim \"strong linear connectivity\"-that for each network, there exists one permutation that simultaneously connects it with the other networks-is both intuitively and practically more desirable.","This stronger claim would imply that the loss landscape is convex after accounting for permutation, and enable linear interpolation between three or more independently trained models without increased loss.","In this work, we introduce an intermediate claim-that for certain sequences of networks, there exists one permutation that simultaneously aligns matching pairs of networks from these sequences.","Specifically, we discover that a single permutation aligns sequences of iteratively trained as well as iteratively pruned networks, meaning that two networks exhibit low loss barriers at each step of their optimization and sparsification trajectories respectively.","Finally, we provide the first evidence that strong linear connectivity may be possible under certain conditions, by showing that barriers decrease with increasing network width when interpolating among three networks."],"url":"http://arxiv.org/abs/2404.06498v1"}
{"created":"2024-04-09 17:49:18","title":"Mechanism Design for ZK-Rollup Prover Markets","abstract":"In ZK-Rollups, provers spend significant computational resources to generate validity proofs. Their costs should be compensated properly, so a sustainable prover market can form over time. Existing transaction fee mechanisms (TFMs) such as EIP-1559, however, do not work in this setting, as EIP-1559 only generates negligible revenue because of burning, while provers often create or purchase specialized hardware in hopes of creating long-term revenue from proving, somewhat reminiscent of proof-of-work miners in the case of chains like Bitcoin. In this paper, we explore the design of transaction fee mechanisms for prover markets. The desiderata for such mechanisms include efficiency (social welfare is maximized), incentive compatibility (dominant bidding strategy exists), collusion resistance (no profitable collusion among provers exists), and off-chain agreement proofness (no profitable collusion between users and provers exists). This paper presents a prover market mechanism called Proo{\\phi} (pronounced proo-fee), and show the conditions under which it can satisfy these desired properties. In our mechanism, the users bid their fee for transaction inclusion, and the user transactions are selected through a first-price auction; the provers bid their proof generation capacity and cost, and the lowest-cost provers are selected. We add an upper bound of the total capacity of the included transactions each round to forestall off-chain agreements. Proo{\\phi} is Bayesian incentive compatible for users and off-chain agreement proof, and is also incentive-compatible for provers, when assuming the latter cannot employ Sybil attacks. We present a preliminary analysis of the Proo{\\phi} mechanism and its limitations, and raise open questions about the feasibility of an ideal mechanism for prover markets. This work is in progress, and this manuscript will be updated.","sentences":["In ZK-Rollups, provers spend significant computational resources to generate validity proofs.","Their costs should be compensated properly, so a sustainable prover market can form over time.","Existing transaction fee mechanisms (TFMs) such as EIP-1559, however, do not work in this setting, as EIP-1559 only generates negligible revenue because of burning, while provers often create or purchase specialized hardware in hopes of creating long-term revenue from proving, somewhat reminiscent of proof-of-work miners in the case of chains like Bitcoin.","In this paper, we explore the design of transaction fee mechanisms for prover markets.","The desiderata for such mechanisms include efficiency (social welfare is maximized), incentive compatibility (dominant bidding strategy exists), collusion resistance (no profitable collusion among provers exists), and off-chain agreement proofness (no profitable collusion between users and provers exists).","This paper presents a prover market mechanism called Proo{\\phi} (pronounced proo-fee), and show the conditions under which it can satisfy these desired properties.","In our mechanism, the users bid their fee for transaction inclusion, and the user transactions are selected through a first-price auction; the provers bid their proof generation capacity and cost, and the lowest-cost provers are selected.","We add an upper bound of the total capacity of the included transactions each round to forestall off-chain agreements.","Proo{\\phi} is Bayesian incentive compatible for users and off-chain agreement proof, and is also incentive-compatible for provers, when assuming the latter cannot employ Sybil attacks.","We present a preliminary analysis of the Proo{\\phi} mechanism and its limitations, and raise open questions about the feasibility of an ideal mechanism for prover markets.","This work is in progress, and this manuscript will be updated."],"url":"http://arxiv.org/abs/2404.06495v1"}
{"created":"2024-04-09 17:48:52","title":"Flying With Photons: Rendering Novel Views of Propagating Light","abstract":"We present an imaging and neural rendering technique that seeks to synthesize videos of light propagating through a scene from novel, moving camera viewpoints. Our approach relies on a new ultrafast imaging setup to capture a first-of-its kind, multi-viewpoint video dataset with picosecond-level temporal resolution. Combined with this dataset, we introduce an efficient neural volume rendering framework based on the transient field. This field is defined as a mapping from a 3D point and 2D direction to a high-dimensional, discrete-time signal that represents time-varying radiance at ultrafast timescales. Rendering with transient fields naturally accounts for effects due to the finite speed of light, including viewpoint-dependent appearance changes caused by light propagation delays to the camera. We render a range of complex effects, including scattering, specular reflection, refraction, and diffraction. Additionally, we demonstrate removing viewpoint-dependent propagation delays using a time warping procedure, rendering of relativistic effects, and video synthesis of direct and global components of light transport.","sentences":["We present an imaging and neural rendering technique that seeks to synthesize videos of light propagating through a scene from novel, moving camera viewpoints.","Our approach relies on a new ultrafast imaging setup to capture a first-of-its kind, multi-viewpoint video dataset with picosecond-level temporal resolution.","Combined with this dataset, we introduce an efficient neural volume rendering framework based on the transient field.","This field is defined as a mapping from a 3D point and 2D direction to a high-dimensional, discrete-time signal that represents time-varying radiance at ultrafast timescales.","Rendering with transient fields naturally accounts for effects due to the finite speed of light, including viewpoint-dependent appearance changes caused by light propagation delays to the camera.","We render a range of complex effects, including scattering, specular reflection, refraction, and diffraction.","Additionally, we demonstrate removing viewpoint-dependent propagation delays using a time warping procedure, rendering of relativistic effects, and video synthesis of direct and global components of light transport."],"url":"http://arxiv.org/abs/2404.06493v1"}
{"created":"2024-04-09 17:45:25","title":"Graph Reinforcement Learning for Combinatorial Optimization: A Survey and Unifying Perspective","abstract":"Graphs are a natural representation for systems based on relations between connected entities. Combinatorial optimization problems, which arise when considering an objective function related to a process of interest on discrete structures, are often challenging due to the rapid growth of the solution space. The trial-and-error paradigm of Reinforcement Learning has recently emerged as a promising alternative to traditional methods, such as exact algorithms and (meta)heuristics, for discovering better decision-making strategies in a variety of disciplines including chemistry, computer science, and statistics. Despite the fact that they arose in markedly different fields, these techniques share significant commonalities. Therefore, we set out to synthesize this work in a unifying perspective that we term Graph Reinforcement Learning, interpreting it as a constructive decision-making method for graph problems. After covering the relevant technical background, we review works along the dividing line of whether the goal is to optimize graph structure given a process of interest, or to optimize the outcome of the process itself under fixed graph structure. Finally, we discuss the common challenges facing the field and open research questions. In contrast with other surveys, the present work focuses on non-canonical graph problems for which performant algorithms are typically not known and Reinforcement Learning is able to provide efficient and effective solutions.","sentences":["Graphs are a natural representation for systems based on relations between connected entities.","Combinatorial optimization problems, which arise when considering an objective function related to a process of interest on discrete structures, are often challenging due to the rapid growth of the solution space.","The trial-and-error paradigm of Reinforcement Learning has recently emerged as a promising alternative to traditional methods, such as exact algorithms and (meta)heuristics, for discovering better decision-making strategies in a variety of disciplines including chemistry, computer science, and statistics.","Despite the fact that they arose in markedly different fields, these techniques share significant commonalities.","Therefore, we set out to synthesize this work in a unifying perspective that we term Graph Reinforcement Learning, interpreting it as a constructive decision-making method for graph problems.","After covering the relevant technical background, we review works along the dividing line of whether the goal is to optimize graph structure given a process of interest, or to optimize the outcome of the process itself under fixed graph structure.","Finally, we discuss the common challenges facing the field and open research questions.","In contrast with other surveys, the present work focuses on non-canonical graph problems for which performant algorithms are typically not known and Reinforcement Learning is able to provide efficient and effective solutions."],"url":"http://arxiv.org/abs/2404.06492v1"}
{"created":"2024-04-09 17:42:59","title":"Pitfalls of Conversational LLMs on News Debiasing","abstract":"This paper addresses debiasing in news editing and evaluates the effectiveness of conversational Large Language Models in this task. We designed an evaluation checklist tailored to news editors' perspectives, obtained generated texts from three popular conversational models using a subset of a publicly available dataset in media bias, and evaluated the texts according to the designed checklist. Furthermore, we examined the models as evaluator for checking the quality of debiased model outputs. Our findings indicate that none of the LLMs are perfect in debiasing. Notably, some models, including ChatGPT, introduced unnecessary changes that may impact the author's style and create misinformation. Lastly, we show that the models do not perform as proficiently as domain experts in evaluating the quality of debiased outputs.","sentences":["This paper addresses debiasing in news editing and evaluates the effectiveness of conversational Large Language Models in this task.","We designed an evaluation checklist tailored to news editors' perspectives, obtained generated texts from three popular conversational models using a subset of a publicly available dataset in media bias, and evaluated the texts according to the designed checklist.","Furthermore, we examined the models as evaluator for checking the quality of debiased model outputs.","Our findings indicate that none of the LLMs are perfect in debiasing.","Notably, some models, including ChatGPT, introduced unnecessary changes that may impact the author's style and create misinformation.","Lastly, we show that the models do not perform as proficiently as domain experts in evaluating the quality of debiased outputs."],"url":"http://arxiv.org/abs/2404.06488v1"}
{"created":"2024-04-09 17:35:11","title":"Public-private funding models in open source software development: A case study on scikit-learn","abstract":"Governments are increasingly allocating funding for open source software (OSS) development in order to address concerns related to software security, digital sovereignty, and the competitiveness of domestic software markets, amongst others. While such funding is generally welcomed by OSS practitioners, how OSS developers perceive the relative benefits and drawbacks of governmental funding remains an open question. This paper explores this question through a case study on scikit-learn, a Python library for machine learning, whose funding model combines research grants, commercial sponsorship, community donations, and a 32 million EUR grant from the French government's artificial intelligence strategy. Through 25 interviews with scikit-learn maintainers and funders, this study makes two key contributions with implications for research and practice. First, it provides novel insights into the role of a public-private funding model in a successful, community-led OSS project and how maintainers evaluate their funding model. Furthermore, it highlights the governance mechanisms employed by maintainers to safeguard the community ethos of the project. Second, it offers practical implications for OSS developer communities, companies, and governments. For OSS communities, the study illustrates the benefits of a diversified funding model in balancing the merits and drawbacks of different funding sources. For companies, it serves as a reminder that sponsoring developers or directly funding OSS projects can significantly support OSS maintainers, who often struggle with limited resources and towering workloads. For governments, the findings emphasise the importance of funding the maintenance of existing OSS projects in addition to or exclusively funding new innovations. The paper concludes with suggestions for future research on OSS funding models.","sentences":["Governments are increasingly allocating funding for open source software (OSS) development in order to address concerns related to software security, digital sovereignty, and the competitiveness of domestic software markets, amongst others.","While such funding is generally welcomed by OSS practitioners, how OSS developers perceive the relative benefits and drawbacks of governmental funding remains an open question.","This paper explores this question through a case study on scikit-learn, a Python library for machine learning, whose funding model combines research grants, commercial sponsorship, community donations, and a 32 million EUR grant from the French government's artificial intelligence strategy.","Through 25 interviews with scikit-learn maintainers and funders, this study makes two key contributions with implications for research and practice.","First, it provides novel insights into the role of a public-private funding model in a successful, community-led OSS project and how maintainers evaluate their funding model.","Furthermore, it highlights the governance mechanisms employed by maintainers to safeguard the community ethos of the project.","Second, it offers practical implications for OSS developer communities, companies, and governments.","For OSS communities, the study illustrates the benefits of a diversified funding model in balancing the merits and drawbacks of different funding sources.","For companies, it serves as a reminder that sponsoring developers or directly funding OSS projects can significantly support OSS maintainers, who often struggle with limited resources and towering workloads.","For governments, the findings emphasise the importance of funding the maintenance of existing OSS projects in addition to or exclusively funding new innovations.","The paper concludes with suggestions for future research on OSS funding models."],"url":"http://arxiv.org/abs/2404.06484v1"}
{"created":"2024-04-09 17:34:19","title":"RhythmMamba: Fast Remote Physiological Measurement with Arbitrary Length Videos","abstract":"Remote photoplethysmography (rPPG) is a non-contact method for detecting physiological signals from facial videos, holding great potential in various applications such as healthcare, affective computing, and anti-spoofing. Existing deep learning methods struggle to address two core issues of rPPG simultaneously: extracting weak rPPG signals from video segments with large spatiotemporal redundancy and understanding the periodic patterns of rPPG among long contexts. This represents a trade-off between computational complexity and the ability to capture long-range dependencies, posing a challenge for rPPG that is suitable for deployment on mobile devices. Based on the in-depth exploration of Mamba's comprehension of spatial and temporal information, this paper introduces RhythmMamba, an end-to-end Mamba-based method that employs multi-temporal Mamba to constrain both periodic patterns and short-term trends, coupled with frequency domain feed-forward to enable Mamba to robustly understand the quasi-periodic patterns of rPPG. Extensive experiments show that RhythmMamba achieves state-of-the-art performance with reduced parameters and lower computational complexity. The proposed RhythmMamba can be applied to video segments of any length without performance degradation. The codes are available at https://github.com/zizheng-guo/RhythmMamba.","sentences":["Remote photoplethysmography (rPPG) is a non-contact method for detecting physiological signals from facial videos, holding great potential in various applications such as healthcare, affective computing, and anti-spoofing.","Existing deep learning methods struggle to address two core issues of rPPG simultaneously: extracting weak rPPG signals from video segments with large spatiotemporal redundancy and understanding the periodic patterns of rPPG among long contexts.","This represents a trade-off between computational complexity and the ability to capture long-range dependencies, posing a challenge for rPPG that is suitable for deployment on mobile devices.","Based on the in-depth exploration of Mamba's comprehension of spatial and temporal information, this paper introduces RhythmMamba, an end-to-end Mamba-based method that employs multi-temporal Mamba to constrain both periodic patterns and short-term trends, coupled with frequency domain feed-forward to enable Mamba to robustly understand the quasi-periodic patterns of rPPG.","Extensive experiments show that RhythmMamba achieves state-of-the-art performance with reduced parameters and lower computational complexity.","The proposed RhythmMamba can be applied to video segments of any length without performance degradation.","The codes are available at https://github.com/zizheng-guo/RhythmMamba."],"url":"http://arxiv.org/abs/2404.06483v1"}
{"created":"2024-04-09 17:30:48","title":"Ada-LEval: Evaluating long-context LLMs with length-adaptable benchmarks","abstract":"Recently, the large language model (LLM) community has shown increasing interest in enhancing LLMs' capability to handle extremely long documents. As various long-text techniques and model architectures emerge, the precise and detailed evaluation of models' long-text capabilities has become increasingly important. Existing long-text evaluation benchmarks, such as L-Eval and LongBench, construct long-text test sets based on open-source datasets, focusing mainly on QA and summarization tasks. These datasets include test samples of varying lengths (from 2k to 32k+) entangled together, making it challenging to assess model capabilities across different length ranges. Moreover, they do not cover the ultralong settings (100k+ tokens) that the latest LLMs claim to achieve. In this paper, we introduce Ada-LEval, a length-adaptable benchmark for evaluating the long-context understanding of LLMs. Ada-LEval includes two challenging subsets, TSort and BestAnswer, which enable a more reliable evaluation of LLMs' long context capabilities. These benchmarks support intricate manipulation of the length of test cases, and can easily produce text samples up to 128k tokens. We evaluate 4 state-of-the-art closed-source API models and 6 open-source models with Ada-LEval. The evaluation results demonstrate the limitations of current LLMs, especially in ultra-long-context settings. Our code is available at https://github.com/open-compass/Ada-LEval.","sentences":["Recently, the large language model (LLM) community has shown increasing interest in enhancing LLMs' capability to handle extremely long documents.","As various long-text techniques and model architectures emerge, the precise and detailed evaluation of models' long-text capabilities has become increasingly important.","Existing long-text evaluation benchmarks, such as L-Eval and LongBench, construct long-text test sets based on open-source datasets, focusing mainly on QA and summarization tasks.","These datasets include test samples of varying lengths (from 2k to 32k+) entangled together, making it challenging to assess model capabilities across different length ranges.","Moreover, they do not cover the ultralong settings (100k+ tokens) that the latest LLMs claim to achieve.","In this paper, we introduce Ada-LEval, a length-adaptable benchmark for evaluating the long-context understanding of LLMs.","Ada-LEval includes two challenging subsets, TSort and BestAnswer, which enable a more reliable evaluation of LLMs' long context capabilities.","These benchmarks support intricate manipulation of the length of test cases, and can easily produce text samples up to 128k tokens.","We evaluate 4 state-of-the-art closed-source API models and 6 open-source models with Ada-LEval.","The evaluation results demonstrate the limitations of current LLMs, especially in ultra-long-context settings.","Our code is available at https://github.com/open-compass/Ada-LEval."],"url":"http://arxiv.org/abs/2404.06480v1"}
{"created":"2024-04-09 17:30:18","title":"Text-Based Reasoning About Vector Graphics","abstract":"While large multimodal models excel in broad vision-language benchmarks, they often struggle with tasks requiring precise perception of low-level visual details, such as comparing line lengths or solving simple mazes. In particular, this failure mode persists in question-answering tasks about vector graphics -- images composed purely of 2D objects and shapes. To address this challenge, we propose the Visually Descriptive Language Model (VDLM), which performs text-based reasoning about vector graphics. VDLM leverages Scalable Vector Graphics (SVG) for a more precise visual description and first uses an off-the-shelf raster-to-SVG algorithm for encoding. Since existing language models cannot understand raw SVGs in a zero-shot setting, VDLM then bridges SVG with pretrained language models through a newly introduced intermediate symbolic representation, Primal Visual Description (PVD), comprising primitive attributes (e.g., shape, position, measurement) with their corresponding predicted values. PVD is task-agnostic and represents visual primitives that are universal across all vector graphics. It can be learned with procedurally generated (SVG, PVD) pairs and also enables the direct use of LLMs for generalization to complex reasoning tasks. By casting an image to a text-based representation, we can leverage the power of language models to learn alignment from SVG to visual primitives and generalize to unseen question-answering tasks. Empirical results show that VDLM achieves stronger zero-shot performance compared to state-of-the-art LMMs, such as GPT-4V, in various low-level multimodal perception and reasoning tasks on vector graphics. We additionally present extensive analyses on VDLM's performance, demonstrating that our framework offers better interpretability due to its disentangled perception and reasoning processes. Project page: https://mikewangwzhl.github.io/VDLM/","sentences":["While large multimodal models excel in broad vision-language benchmarks, they often struggle with tasks requiring precise perception of low-level visual details, such as comparing line lengths or solving simple mazes.","In particular, this failure mode persists in question-answering tasks about vector graphics -- images composed purely of 2D objects and shapes.","To address this challenge, we propose the Visually Descriptive Language Model (VDLM), which performs text-based reasoning about vector graphics.","VDLM leverages Scalable Vector Graphics (SVG) for a more precise visual description and first uses an off-the-shelf raster-to-SVG algorithm for encoding.","Since existing language models cannot understand raw SVGs in a zero-shot setting, VDLM then bridges SVG with pretrained language models through a newly introduced intermediate symbolic representation, Primal Visual Description (PVD), comprising primitive attributes (e.g., shape, position, measurement) with their corresponding predicted values.","PVD is task-agnostic and represents visual primitives that are universal across all vector graphics.","It can be learned with procedurally generated (SVG, PVD) pairs and also enables the direct use of LLMs for generalization to complex reasoning tasks.","By casting an image to a text-based representation, we can leverage the power of language models to learn alignment from SVG to visual primitives and generalize to unseen question-answering tasks.","Empirical results show that VDLM achieves stronger zero-shot performance compared to state-of-the-art LMMs, such as GPT-4V, in various low-level multimodal perception and reasoning tasks on vector graphics.","We additionally present extensive analyses on VDLM's performance, demonstrating that our framework offers better interpretability due to its disentangled perception and reasoning processes.","Project page: https://mikewangwzhl.github.io/VDLM/"],"url":"http://arxiv.org/abs/2404.06479v1"}
{"created":"2024-04-09 17:28:07","title":"Mechanised Hypersafety Proofs about Structured Data: Extended Version","abstract":"Arrays are a fundamental abstraction to represent collections of data. It is often possible to exploit structural properties of the data stored in an array (e.g., repetition or sparsity) to develop a specialised representation optimised for space efficiency. Formally reasoning about correctness of manipulations with such structured data is challenging, as they are often composed of multiple loops with non-trivial invariants.   In this work, we observe that specifications for structured data manipulations can be phrased as hypersafety properties, i.e., predicates that relate traces of $k$ programs. To turn this observation into an effective verification methodology, we developed the Logic for Graceful Tensor Manipulation (LGTM), a new Hoare-style relational separation logic for specifying and verifying computations over structured data. The key enabling idea of LGTM is that of parametrised hypersafety specifications that allow the number $k$ of the program components to depend on the program variables. We implemented LGTM as a foundational embedding into Coq, mechanising its rules, meta-theory, and the proof of soundness. Furthermore, we developed a library of domain-specific tactics that automate computer-aided hypersafety reasoning, resulting in pleasantly short proof scripts that enjoy a high degree of reuse. We argue for the effectiveness of relational reasoning about structured data in LGTM by specifying and mechanically proving correctness of 13 case studies including computations on compressed arrays and efficient operations over multiple kinds of sparse tensors.","sentences":["Arrays are a fundamental abstraction to represent collections of data.","It is often possible to exploit structural properties of the data stored in an array (e.g., repetition or sparsity) to develop a specialised representation optimised for space efficiency.","Formally reasoning about correctness of manipulations with such structured data is challenging, as they are often composed of multiple loops with non-trivial invariants.   ","In this work, we observe that specifications for structured data manipulations can be phrased as hypersafety properties, i.e., predicates that relate traces of $k$ programs.","To turn this observation into an effective verification methodology, we developed the Logic for Graceful Tensor Manipulation (LGTM), a new Hoare-style relational separation logic for specifying and verifying computations over structured data.","The key enabling idea of LGTM is that of parametrised hypersafety specifications that allow the number $k$ of the program components to depend on the program variables.","We implemented LGTM as a foundational embedding into Coq, mechanising its rules, meta-theory, and the proof of soundness.","Furthermore, we developed a library of domain-specific tactics that automate computer-aided hypersafety reasoning, resulting in pleasantly short proof scripts that enjoy a high degree of reuse.","We argue for the effectiveness of relational reasoning about structured data in LGTM by specifying and mechanically proving correctness of 13 case studies including computations on compressed arrays and efficient operations over multiple kinds of sparse tensors."],"url":"http://arxiv.org/abs/2404.06477v1"}
{"created":"2024-04-09 17:25:47","title":"Autonomous Evaluation and Refinement of Digital Agents","abstract":"We show that domain-general automatic evaluators can significantly improve the performance of agents for web navigation and device control. We experiment with multiple evaluation models that trade off between inference cost, modularity of design, and accuracy. We validate the performance of these models in several popular benchmarks for digital agents, finding between 74.4 and 92.9% agreement with oracle evaluation metrics. Finally, we use these evaluators to improve the performance of existing agents via fine-tuning and inference-time guidance. Without any additional supervision, we improve state-of-the-art performance by 29% on the popular benchmark WebArena, and achieve a 75% relative improvement in a challenging domain transfer scenario.","sentences":["We show that domain-general automatic evaluators can significantly improve the performance of agents for web navigation and device control.","We experiment with multiple evaluation models that trade off between inference cost, modularity of design, and accuracy.","We validate the performance of these models in several popular benchmarks for digital agents, finding between 74.4 and 92.9% agreement with oracle evaluation metrics.","Finally, we use these evaluators to improve the performance of existing agents via fine-tuning and inference-time guidance.","Without any additional supervision, we improve state-of-the-art performance by 29% on the popular benchmark WebArena, and achieve a 75% relative improvement in a challenging domain transfer scenario."],"url":"http://arxiv.org/abs/2404.06474v1"}
{"created":"2024-04-09 17:17:48","title":"Learning State-Invariant Representations of Objects from Image Collections with State, Pose, and Viewpoint Changes","abstract":"We add one more invariance - state invariance - to the more commonly used other invariances for learning object representations for recognition and retrieval. By state invariance, we mean robust with respect to changes in the structural form of the object, such as when an umbrella is folded, or when an item of clothing is tossed on the floor. Since humans generally have no difficulty in recognizing objects despite such state changes, we are naturally faced with the question of whether it is possible to devise a neural architecture with similar abilities. To that end, we present a novel dataset, ObjectsWithStateChange, that captures state and pose variations in the object images recorded from arbitrary viewpoints. We believe that this dataset will facilitate research in fine-grained object recognition and retrieval of objects that are capable of state changes. The goal of such research would be to train models capable of generating object embeddings that remain invariant to state changes while also staying invariant to transformations induced by changes in viewpoint, pose, illumination, etc. To demonstrate the usefulness of the ObjectsWithStateChange dataset, we also propose a curriculum learning strategy that uses the similarity relationships in the learned embedding space after each epoch to guide the training process. The model learns discriminative features by comparing visually similar objects within and across different categories, encouraging it to differentiate between objects that may be challenging to distinguish due to changes in their state. We believe that this strategy enhances the model's ability to capture discriminative features for fine-grained tasks that may involve objects with state changes, leading to performance improvements on object-level tasks not only on our new dataset, but also on two other challenging multi-view datasets such as ModelNet40 and ObjectPI.","sentences":["We add one more invariance - state invariance - to the more commonly used other invariances for learning object representations for recognition and retrieval.","By state invariance, we mean robust with respect to changes in the structural form of the object, such as when an umbrella is folded, or when an item of clothing is tossed on the floor.","Since humans generally have no difficulty in recognizing objects despite such state changes, we are naturally faced with the question of whether it is possible to devise a neural architecture with similar abilities.","To that end, we present a novel dataset, ObjectsWithStateChange, that captures state and pose variations in the object images recorded from arbitrary viewpoints.","We believe that this dataset will facilitate research in fine-grained object recognition and retrieval of objects that are capable of state changes.","The goal of such research would be to train models capable of generating object embeddings that remain invariant to state changes while also staying invariant to transformations induced by changes in viewpoint, pose, illumination, etc.","To demonstrate the usefulness of the ObjectsWithStateChange dataset, we also propose a curriculum learning strategy that uses the similarity relationships in the learned embedding space after each epoch to guide the training process.","The model learns discriminative features by comparing visually similar objects within and across different categories, encouraging it to differentiate between objects that may be challenging to distinguish due to changes in their state.","We believe that this strategy enhances the model's ability to capture discriminative features for fine-grained tasks that may involve objects with state changes, leading to performance improvements on object-level tasks not only on our new dataset, but also on two other challenging multi-view datasets such as ModelNet40 and ObjectPI."],"url":"http://arxiv.org/abs/2404.06470v1"}
{"created":"2024-04-09 17:15:37","title":"Scaling to 32 GPUs on a Novel Composable System Architecture","abstract":"The development of composable systems architecture marks a significant shift in resource allocation and utilization within data centers. This paper presents a composable architecture scaling up to 32 GPUs on a single node, addressing the technical challenges encountered and the innovative solutions implemented. This design introduces a flexible and dynamic resource distribution mechanism, particularly for GPUs, enabling tailored allocation to meet varying node demands. The architecture's dynamic nature allows for the flexible assignment and reassignment of hardware resources, such as GPUs, to different nodes as required, offering unprecedented capability and flexibility.","sentences":["The development of composable systems architecture marks a significant shift in resource allocation and utilization within data centers.","This paper presents a composable architecture scaling up to 32 GPUs on a single node, addressing the technical challenges encountered and the innovative solutions implemented.","This design introduces a flexible and dynamic resource distribution mechanism, particularly for GPUs, enabling tailored allocation to meet varying node demands.","The architecture's dynamic nature allows for the flexible assignment and reassignment of hardware resources, such as GPUs, to different nodes as required, offering unprecedented capability and flexibility."],"url":"http://arxiv.org/abs/2404.06467v1"}
{"created":"2024-04-09 17:14:41","title":"Hyperparameter Selection in Continual Learning","abstract":"In continual learning (CL) -- where a learner trains on a stream of data -- standard hyperparameter optimisation (HPO) cannot be applied, as a learner does not have access to all of the data at the same time. This has prompted the development of CL-specific HPO frameworks. The most popular way to tune hyperparameters in CL is to repeatedly train over the whole data stream with different hyperparameter settings. However, this end-of-training HPO is unrealistic as in practice a learner can only see the stream once. Hence, there is an open question: what HPO framework should a practitioner use for a CL problem in reality? This paper answers this question by evaluating several realistic HPO frameworks. We find that all the HPO frameworks considered, including end-of-training HPO, perform similarly. We therefore advocate using the realistic and most computationally efficient method: fitting the hyperparameters on the first task and then fixing them throughout training.","sentences":["In continual learning (CL) -- where a learner trains on a stream of data -- standard hyperparameter optimisation (HPO) cannot be applied, as a learner does not have access to all of the data at the same time.","This has prompted the development of CL-specific HPO frameworks.","The most popular way to tune hyperparameters in CL is to repeatedly train over the whole data stream with different hyperparameter settings.","However, this end-of-training HPO is unrealistic as in practice a learner can only see the stream once.","Hence, there is an open question: what HPO framework should a practitioner use for a CL problem in reality?","This paper answers this question by evaluating several realistic HPO frameworks.","We find that all the HPO frameworks considered, including end-of-training HPO, perform similarly.","We therefore advocate using the realistic and most computationally efficient method: fitting the hyperparameters on the first task and then fixing them throughout training."],"url":"http://arxiv.org/abs/2404.06466v1"}
{"created":"2024-04-09 17:00:53","title":"Analysis of Distributed Algorithms for Big-data","abstract":"The parallel and distributed processing are becoming de facto industry standard, and a large part of the current research is targeted on how to make computing scalable and distributed, dynamically, without allocating the resources on permanent basis. The present article focuses on the study and performance of distributed and parallel algorithms their file systems, to achieve scalability at local level (OpenMP platform), and at global level where computing and file systems are distributed. Various applications, algorithms,file systems have been used to demonstrate the areas, and their performance studies have been presented. The systems and applications chosen here are of open-source nature, due to their wider applicability.","sentences":["The parallel and distributed processing are becoming de facto industry standard, and a large part of the current research is targeted on how to make computing scalable and distributed, dynamically, without allocating the resources on permanent basis.","The present article focuses on the study and performance of distributed and parallel algorithms their file systems, to achieve scalability at local level (OpenMP platform), and at global level where computing and file systems are distributed.","Various applications, algorithms,file systems have been used to demonstrate the areas, and their performance studies have been presented.","The systems and applications chosen here are of open-source nature, due to their wider applicability."],"url":"http://arxiv.org/abs/2404.06461v1"}
{"created":"2024-04-09 16:54:19","title":"PURE: Turning Polysemantic Neurons Into Pure Features by Identifying Relevant Circuits","abstract":"The field of mechanistic interpretability aims to study the role of individual neurons in Deep Neural Networks. Single neurons, however, have the capability to act polysemantically and encode for multiple (unrelated) features, which renders their interpretation difficult. We present a method for disentangling polysemanticity of any Deep Neural Network by decomposing a polysemantic neuron into multiple monosemantic \"virtual\" neurons. This is achieved by identifying the relevant sub-graph (\"circuit\") for each \"pure\" feature. We demonstrate how our approach allows us to find and disentangle various polysemantic units of ResNet models trained on ImageNet. While evaluating feature visualizations using CLIP, our method effectively disentangles representations, improving upon methods based on neuron activations. Our code is available at https://github.com/maxdreyer/PURE.","sentences":["The field of mechanistic interpretability aims to study the role of individual neurons in Deep Neural Networks.","Single neurons, however, have the capability to act polysemantically and encode for multiple (unrelated) features, which renders their interpretation difficult.","We present a method for disentangling polysemanticity of any Deep Neural Network by decomposing a polysemantic neuron into multiple monosemantic \"virtual\" neurons.","This is achieved by identifying the relevant sub-graph (\"circuit\") for each \"pure\" feature.","We demonstrate how our approach allows us to find and disentangle various polysemantic units of ResNet models trained on ImageNet.","While evaluating feature visualizations using CLIP, our method effectively disentangles representations, improving upon methods based on neuron activations.","Our code is available at https://github.com/maxdreyer/PURE."],"url":"http://arxiv.org/abs/2404.06453v1"}
{"created":"2024-04-09 16:53:52","title":"PAAM: A Framework for Coordinated and Priority-Driven Accelerator Management in ROS 2","abstract":"This paper proposes a Priority-driven Accelerator Access Management (PAAM) framework for multi-process robotic applications built on top of the Robot Operating System (ROS) 2 middleware platform. The framework addresses the issue of predictable execution of time- and safety-critical callback chains that require hardware accelerators such as GPUs and TPUs. PAAM provides a standalone ROS executor that acts as an accelerator resource server, arbitrating accelerator access requests from all other callbacks at the application layer. This approach enables coordinated and priority-driven accelerator access management in multi-process robotic systems. The framework design is directly applicable to all types of accelerators and enables granular control over how specific chains access accelerators, making it possible to achieve predictable real-time support for accelerators used by safety-critical callback chains without making changes to underlying accelerator device drivers. The paper shows that PAAM also offers a theoretical analysis that can upper bound the worst-case response time of safety-critical callback chains that necessitate accelerator access. This paper also demonstrates that complex robotic systems with extensive accelerator usage that are integrated with PAAM may achieve up to a 91\\% reduction in end-to-end response time of their critical callback chains.","sentences":["This paper proposes a Priority-driven Accelerator Access Management (PAAM) framework for multi-process robotic applications built on top of the Robot Operating System (ROS) 2 middleware platform.","The framework addresses the issue of predictable execution of time- and safety-critical callback chains that require hardware accelerators such as GPUs and TPUs.","PAAM provides a standalone ROS executor that acts as an accelerator resource server, arbitrating accelerator access requests from all other callbacks at the application layer.","This approach enables coordinated and priority-driven accelerator access management in multi-process robotic systems.","The framework design is directly applicable to all types of accelerators and enables granular control over how specific chains access accelerators, making it possible to achieve predictable real-time support for accelerators used by safety-critical callback chains without making changes to underlying accelerator device drivers.","The paper shows that PAAM also offers a theoretical analysis that can upper bound the worst-case response time of safety-critical callback chains that necessitate accelerator access.","This paper also demonstrates that complex robotic systems with extensive accelerator usage that are integrated with PAAM may achieve up to a 91\\% reduction in end-to-end response time of their critical callback chains."],"url":"http://arxiv.org/abs/2404.06452v1"}
{"created":"2024-04-09 16:53:43","title":"SmartControl: Enhancing ControlNet for Handling Rough Visual Conditions","abstract":"Human visual imagination usually begins with analogies or rough sketches. For example, given an image with a girl playing guitar before a building, one may analogously imagine how it seems like if Iron Man playing guitar before Pyramid in Egypt. Nonetheless, visual condition may not be precisely aligned with the imaginary result indicated by text prompt, and existing layout-controllable text-to-image (T2I) generation models is prone to producing degraded generated results with obvious artifacts. To address this issue, we present a novel T2I generation method dubbed SmartControl, which is designed to modify the rough visual conditions for adapting to text prompt. The key idea of our SmartControl is to relax the visual condition on the areas that are conflicted with text prompts. In specific, a Control Scale Predictor (CSP) is designed to identify the conflict regions and predict the local control scales, while a dataset with text prompts and rough visual conditions is constructed for training CSP. It is worth noting that, even with a limited number (e.g., 1,000~2,000) of training samples, our SmartControl can generalize well to unseen objects. Extensive experiments on four typical visual condition types clearly show the efficacy of our SmartControl against state-of-the-arts. Source code, pre-trained models, and datasets are available at https://github.com/liuxiaoyu1104/SmartControl.","sentences":["Human visual imagination usually begins with analogies or rough sketches.","For example, given an image with a girl playing guitar before a building, one may analogously imagine how it seems like if Iron Man playing guitar before Pyramid in Egypt.","Nonetheless, visual condition may not be precisely aligned with the imaginary result indicated by text prompt, and existing layout-controllable text-to-image (T2I) generation models is prone to producing degraded generated results with obvious artifacts.","To address this issue, we present a novel T2I generation method dubbed SmartControl, which is designed to modify the rough visual conditions for adapting to text prompt.","The key idea of our SmartControl is to relax the visual condition on the areas that are conflicted with text prompts.","In specific, a Control Scale Predictor (CSP) is designed to identify the conflict regions and predict the local control scales, while a dataset with text prompts and rough visual conditions is constructed for training CSP.","It is worth noting that, even with a limited number (e.g., 1,000~2,000) of training samples, our SmartControl can generalize well to unseen objects.","Extensive experiments on four typical visual condition types clearly show the efficacy of our SmartControl against state-of-the-arts.","Source code, pre-trained models, and datasets are available at https://github.com/liuxiaoyu1104/SmartControl."],"url":"http://arxiv.org/abs/2404.06451v1"}
{"created":"2024-04-09 16:50:30","title":"Automated Federated Pipeline for Parameter-Efficient Fine-Tuning of Large Language Models","abstract":"Recently, there has been a surge in the development of advanced intelligent generative content (AIGC), especially large language models (LLMs). However, for many downstream tasks, it is necessary to fine-tune LLMs using private data. While federated learning offers a promising privacy-preserving solution to LLM fine-tuning, the substantial size of an LLM, combined with high computational and communication demands, makes it hard to apply to downstream tasks. More importantly, private edge servers often possess varying computing and network resources in real-world scenarios, introducing additional complexities to LLM fine-tuning. To tackle these problems, we design and implement an automated federated pipeline, named FedPipe, to fine-tune LLMs with minimal training cost but without adding any inference latency. FedPipe firstly identifies the weights to be fine-tuned based on their contributions to the LLM training. It then configures a low-rank adapter for each selected weight to train local low-rank adapters on an edge server, and aggregate local adapters of all edge servers to fine-tune the whole LLM. Finally, it appropriately quantizes the parameters of LLM to reduce memory space according to the requirements of edge servers. Extensive experiments demonstrate that FedPipe expedites the model training and achieves higher accuracy than state-of-the-art benchmarks.","sentences":["Recently, there has been a surge in the development of advanced intelligent generative content (AIGC), especially large language models (LLMs).","However, for many downstream tasks, it is necessary to fine-tune LLMs using private data.","While federated learning offers a promising privacy-preserving solution to LLM fine-tuning, the substantial size of an LLM, combined with high computational and communication demands, makes it hard to apply to downstream tasks.","More importantly, private edge servers often possess varying computing and network resources in real-world scenarios, introducing additional complexities to LLM fine-tuning.","To tackle these problems, we design and implement an automated federated pipeline, named FedPipe, to fine-tune LLMs with minimal training cost but without adding any inference latency.","FedPipe firstly identifies the weights to be fine-tuned based on their contributions to the LLM training.","It then configures a low-rank adapter for each selected weight to train local low-rank adapters on an edge server, and aggregate local adapters of all edge servers to fine-tune the whole LLM.","Finally, it appropriately quantizes the parameters of LLM to reduce memory space according to the requirements of edge servers.","Extensive experiments demonstrate that FedPipe expedites the model training and achieves higher accuracy than state-of-the-art benchmarks."],"url":"http://arxiv.org/abs/2404.06448v1"}
{"created":"2024-04-09 16:49:42","title":"The Central Spanning Tree Problem","abstract":"Spanning trees are an important primitive in many data analysis tasks, when a data set needs to be summarized in terms of its \"skeleton\", or when a tree-shaped graph over all observations is required for downstream processing. Popular definitions of spanning trees include the minimum spanning tree and the optimum distance spanning tree, a.k.a. the minimum routing cost tree. When searching for the shortest spanning tree but admitting additional branching points, even shorter spanning trees can be realized: Steiner trees. Unfortunately, both minimum spanning and Steiner trees are not robust with respect to noise in the observations; that is, small perturbations of the original data set often lead to drastic changes in the associated spanning trees. In response, we make two contributions when the data lies in a Euclidean space: on the theoretical side, we introduce a new optimization problem, the \"(branched) central spanning tree\", which subsumes all previously mentioned definitions as special cases. On the practical side, we show empirically that the (branched) central spanning tree is more robust to noise in the data, and as such is better suited to summarize a data set in terms of its skeleton. We also propose a heuristic to address the NP-hard optimization problem, and illustrate its use on single cell RNA expression data from biology and 3D point clouds of plants.","sentences":["Spanning trees are an important primitive in many data analysis tasks, when a data set needs to be summarized in terms of its \"skeleton\", or when a tree-shaped graph over all observations is required for downstream processing.","Popular definitions of spanning trees include the minimum spanning tree and the optimum distance spanning tree, a.k.a.","the minimum routing cost tree.","When searching for the shortest spanning tree but admitting additional branching points, even shorter spanning trees can be realized: Steiner trees.","Unfortunately, both minimum spanning and Steiner trees are not robust with respect to noise in the observations; that is, small perturbations of the original data set often lead to drastic changes in the associated spanning trees.","In response, we make two contributions when the data lies in a Euclidean space: on the theoretical side, we introduce a new optimization problem, the \"(branched) central spanning tree\", which subsumes all previously mentioned definitions as special cases.","On the practical side, we show empirically that the (branched) central spanning tree is more robust to noise in the data, and as such is better suited to summarize a data set in terms of its skeleton.","We also propose a heuristic to address the NP-hard optimization problem, and illustrate its use on single cell RNA expression data from biology and 3D point clouds of plants."],"url":"http://arxiv.org/abs/2404.06447v1"}
{"created":"2024-04-09 16:45:34","title":"Multi-scale Dynamic and Hierarchical Relationship Modeling for Facial Action Units Recognition","abstract":"Human facial action units (AUs) are mutually related in a hierarchical manner, as not only they are associated with each other in both spatial and temporal domains but also AUs located in the same/close facial regions show stronger relationships than those of different facial regions. While none of existing approach thoroughly model such hierarchical inter-dependencies among AUs, this paper proposes to comprehensively model multi-scale AU-related dynamic and hierarchical spatio-temporal relationship among AUs for their occurrences recognition. Specifically, we first propose a novel multi-scale temporal differencing network with an adaptive weighting block to explicitly capture facial dynamics across frames at different spatial scales, which specifically considers the heterogeneity of range and magnitude in different AUs' activation. Then, a two-stage strategy is introduced to hierarchically model the relationship among AUs based on their spatial distribution (i.e., local and cross-region AU relationship modelling). Experimental results achieved on BP4D and DISFA show that our approach is the new state-of-the-art in the field of AU occurrence recognition. Our code is publicly available at https://github.com/CVI-SZU/MDHR.","sentences":["Human facial action units (AUs) are mutually related in a hierarchical manner, as not only they are associated with each other in both spatial and temporal domains but also AUs located in the same/close facial regions show stronger relationships than those of different facial regions.","While none of existing approach thoroughly model such hierarchical inter-dependencies among AUs, this paper proposes to comprehensively model multi-scale AU-related dynamic and hierarchical spatio-temporal relationship among AUs for their occurrences recognition.","Specifically, we first propose a novel multi-scale temporal differencing network with an adaptive weighting block to explicitly capture facial dynamics across frames at different spatial scales, which specifically considers the heterogeneity of range and magnitude in different AUs' activation.","Then, a two-stage strategy is introduced to hierarchically model the relationship among AUs based on their spatial distribution (i.e., local and cross-region AU relationship modelling).","Experimental results achieved on BP4D and DISFA show that our approach is the new state-of-the-art in the field of AU occurrence recognition.","Our code is publicly available at https://github.com/CVI-SZU/MDHR."],"url":"http://arxiv.org/abs/2404.06443v1"}
{"created":"2024-04-09 16:42:54","title":"QueSTMaps: Queryable Semantic Topological Maps for 3D Scene Understanding","abstract":"Understanding the structural organisation of 3D indoor scenes in terms of rooms is often accomplished via floorplan extraction. Robotic tasks such as planning and navigation require a semantic understanding of the scene as well. This is typically achieved via object-level semantic segmentation. However, such methods struggle to segment out topological regions like \"kitchen\" in the scene. In this work, we introduce a two-step pipeline. First, we extract a topological map, i.e., floorplan of the indoor scene using a novel multi-channel occupancy representation. Then, we generate CLIP-aligned features and semantic labels for every room instance based on the objects it contains using a self-attention transformer. Our language-topology alignment supports natural language querying, e.g., a \"place to cook\" locates the \"kitchen\". We outperform the current state-of-the-art on room segmentation by ~20% and room classification by ~12%. Our detailed qualitative analysis and ablation studies provide insights into the problem of joint structural and semantic 3D scene understanding.","sentences":["Understanding the structural organisation of 3D indoor scenes in terms of rooms is often accomplished via floorplan extraction.","Robotic tasks such as planning and navigation require a semantic understanding of the scene as well.","This is typically achieved via object-level semantic segmentation.","However, such methods struggle to segment out topological regions like \"kitchen\" in the scene.","In this work, we introduce a two-step pipeline.","First, we extract a topological map, i.e., floorplan of the indoor scene using a novel multi-channel occupancy representation.","Then, we generate CLIP-aligned features and semantic labels for every room instance based on the objects it contains using a self-attention transformer.","Our language-topology alignment supports natural language querying, e.g., a \"place to cook\" locates the \"kitchen\".","We outperform the current state-of-the-art on room segmentation by ~20% and room classification by ~12%.","Our detailed qualitative analysis and ablation studies provide insights into the problem of joint structural and semantic 3D scene understanding."],"url":"http://arxiv.org/abs/2404.06442v1"}
{"created":"2024-04-09 16:28:54","title":"Seasonal Fire Prediction using Spatio-Temporal Deep Neural Networks","abstract":"With climate change expected to exacerbate fire weather conditions, the accurate anticipation of wildfires on a global scale becomes increasingly crucial for disaster mitigation. In this study, we utilize SeasFire, a comprehensive global wildfire dataset with climate, vegetation, oceanic indices, and human-related variables, to enable seasonal wildfire forecasting with machine learning. For the predictive analysis, we train deep learning models with different architectures that capture the spatio-temporal context leading to wildfires. Our investigation focuses on assessing the effectiveness of these models in predicting the presence of burned areas at varying forecasting time horizons globally, extending up to six months into the future, and on how different spatial or/and temporal context affects the performance of the models. Our findings demonstrate the great potential of deep learning models in seasonal fire forecasting; longer input time-series leads to more robust predictions across varying forecasting horizons, while integrating spatial information to capture wildfire spatio-temporal dynamics boosts performance. Finally, our results hint that in order to enhance performance at longer forecasting horizons, a larger receptive field spatially needs to be considered.","sentences":["With climate change expected to exacerbate fire weather conditions, the accurate anticipation of wildfires on a global scale becomes increasingly crucial for disaster mitigation.","In this study, we utilize SeasFire, a comprehensive global wildfire dataset with climate, vegetation, oceanic indices, and human-related variables, to enable seasonal wildfire forecasting with machine learning.","For the predictive analysis, we train deep learning models with different architectures that capture the spatio-temporal context leading to wildfires.","Our investigation focuses on assessing the effectiveness of these models in predicting the presence of burned areas at varying forecasting time horizons globally, extending up to six months into the future, and on how different spatial or/and temporal context affects the performance of the models.","Our findings demonstrate the great potential of deep learning models in seasonal fire forecasting; longer input time-series leads to more robust predictions across varying forecasting horizons, while integrating spatial information to capture wildfire spatio-temporal dynamics boosts performance.","Finally, our results hint that in order to enhance performance at longer forecasting horizons, a larger receptive field spatially needs to be considered."],"url":"http://arxiv.org/abs/2404.06437v1"}
{"created":"2024-04-09 16:25:13","title":"Software-based Security Framework for Edge and Mobile IoT","abstract":"With the proliferation of Internet of Things (IoT) devices, ensuring secure communications has become imperative. Due to their low cost and embedded nature, many of these devices operate with computational and energy constraints, neglecting the potential security vulnerabilities that they may bring. This work-in-progress is focused on designing secure communication among remote servers and embedded IoT devices to balance security robustness and energy efficiency. The proposed approach uses lightweight cryptography, optimizing device performance and security without overburdening their limited resources. Our architecture stands out for integrating Edge servers and a central Name Server, allowing secure and decentralized authentication and efficient connection transitions between different Edge servers. This architecture enhances the scalability of the IoT network and reduces the load on each server, distributing the responsibility for authentication and key management.","sentences":["With the proliferation of Internet of Things (IoT) devices, ensuring secure communications has become imperative.","Due to their low cost and embedded nature, many of these devices operate with computational and energy constraints, neglecting the potential security vulnerabilities that they may bring.","This work-in-progress is focused on designing secure communication among remote servers and embedded IoT devices to balance security robustness and energy efficiency.","The proposed approach uses lightweight cryptography, optimizing device performance and security without overburdening their limited resources.","Our architecture stands out for integrating Edge servers and a central Name Server, allowing secure and decentralized authentication and efficient connection transitions between different Edge servers.","This architecture enhances the scalability of the IoT network and reduces the load on each server, distributing the responsibility for authentication and key management."],"url":"http://arxiv.org/abs/2404.06435v1"}
{"created":"2024-04-09 16:25:06","title":"A New Hotplug Coded Caching Scheme Using PDAs","abstract":"In the original coded caching model introduced by Maddah-Ali and Niesen in 2014, the server starts broadcasting only after it receives demands from all the users. So, all the users must be active during the delivery phase. In this work, we consider a coded caching model called hotplug coded caching in which some of the users are offline during the delivery phase. This model was first introduced by Ma and Tuninetti (``On Coded Caching Systems with Offline Users,\" 2022 IEEE International Symposium on Information Theory). The concept of Hotplug Placement Delivery Arrays (HpPDAs) for the hotplug coded caching systems was introduced in (``Improved Hotplug Caching Schemes Using PDAs and $t$-Designs,\" \\emph{arXiv:2311.02856}, 2024), in which the authors have constructed HpPDAs from $t$-designs. This work provides a new hotplug coded caching scheme from the existing HpPDAs. The performance comparison of the proposed scheme with the existing schemes is presented. When applied for HpPDAs from $t$-designs, our scheme outperforms the baseline scheme by Ma and Tuninetti, and the Improved $t$-scheme by Rajput and Rajan in some memory regimes.","sentences":["In the original coded caching model introduced by Maddah-Ali and Niesen in 2014, the server starts broadcasting only after it receives demands from all the users.","So, all the users must be active during the delivery phase.","In this work, we consider a coded caching model called hotplug coded caching in which some of the users are offline during the delivery phase.","This model was first introduced by Ma and Tuninetti (``On Coded Caching Systems with Offline Users,\" 2022 IEEE International Symposium on Information Theory).","The concept of Hotplug Placement Delivery Arrays (HpPDAs) for the hotplug coded caching systems was introduced in (``Improved Hotplug Caching Schemes Using PDAs and $t$-Designs,\" \\emph{arXiv:2311.02856}, 2024), in which the authors have constructed HpPDAs from $t$-designs.","This work provides a new hotplug coded caching scheme from the existing HpPDAs.","The performance comparison of the proposed scheme with the existing schemes is presented.","When applied for HpPDAs from $t$-designs, our scheme outperforms the baseline scheme by Ma and Tuninetti, and the Improved $t$-scheme by Rajput and Rajan in some memory regimes."],"url":"http://arxiv.org/abs/2404.06433v1"}
{"created":"2024-04-09 16:25:02","title":"Missing Pieces: How Framing Uncertainty Impacts Longitudinal Trust in AI Decision Aids -- A Gig Driver Case Study","abstract":"Decision aids based on artificial intelligence (AI) are becoming increasingly common. When such systems are deployed in environments with inherent uncertainty, following AI-recommended decisions may lead to a wide range of outcomes. In this work, we investigate how the framing of uncertainty in outcomes impacts users' longitudinal trust in AI decision aids, which is crucial to ensuring that these systems achieve their intended purposes. More specifically, we use gig driving as a representative domain to address the question: how does exposing uncertainty at different levels of granularity affect the evolution of users' trust and their willingness to rely on recommended decisions? We report on a longitudinal mixed-methods study $(n = 51)$ where we measured the trust of gig drivers as they interacted with an AI-based schedule recommendation tool. Statistically significant quantitative results indicate that participants' trust in and willingness to rely on the tool for planning depended on the perceived accuracy of the tool's estimates; that providing ranged estimates improved trust; and that increasing prediction granularity and using hedging language improved willingness to rely on the tool even when trust was low. Additionally, we report on interviews with participants which revealed a diversity of experiences with the tool, suggesting that AI systems must build trust by going beyond general designs to calibrate the expectations of individual users.","sentences":["Decision aids based on artificial intelligence (AI) are becoming increasingly common.","When such systems are deployed in environments with inherent uncertainty, following AI-recommended decisions may lead to a wide range of outcomes.","In this work, we investigate how the framing of uncertainty in outcomes impacts users' longitudinal trust in AI decision aids, which is crucial to ensuring that these systems achieve their intended purposes.","More specifically, we use gig driving as a representative domain to address the question: how does exposing uncertainty at different levels of granularity affect the evolution of users' trust and their willingness to rely on recommended decisions?","We report on a longitudinal mixed-methods study $(n = 51)$ where we measured the trust of gig drivers as they interacted with an AI-based schedule recommendation tool.","Statistically significant quantitative results indicate that participants' trust in and willingness to rely on the tool for planning depended on the perceived accuracy of the tool's estimates; that providing ranged estimates improved trust; and that increasing prediction granularity and using hedging language improved willingness to rely on the tool even when trust was low.","Additionally, we report on interviews with participants which revealed a diversity of experiences with the tool, suggesting that AI systems must build trust by going beyond general designs to calibrate the expectations of individual users."],"url":"http://arxiv.org/abs/2404.06432v1"}
{"created":"2024-04-09 16:23:01","title":"pfl-research: simulation framework for accelerating research in Private Federated Learning","abstract":"Federated learning (FL) is an emerging machine learning (ML) training paradigm where clients own their data and collaborate to train a global model, without revealing any data to the server and other participants. Researchers commonly perform experiments in a simulation environment to quickly iterate on ideas. However, existing open-source tools do not offer the efficiency required to simulate FL on larger and more realistic FL datasets. We introduce pfl-research, a fast, modular, and easy-to-use Python framework for simulating FL. It supports TensorFlow, PyTorch, and non-neural network models, and is tightly integrated with state-of-the-art privacy algorithms. We study the speed of open-source FL frameworks and show that pfl-research is 7-72$\\times$ faster than alternative open-source frameworks on common cross-device setups. Such speedup will significantly boost the productivity of the FL research community and enable testing hypotheses on realistic FL datasets that were previously too resource intensive. We release a suite of benchmarks that evaluates an algorithm's overall performance on a diverse set of realistic scenarios. The code is available on GitHub at https://github.com/apple/pfl-research.","sentences":["Federated learning (FL) is an emerging machine learning (ML) training paradigm where clients own their data and collaborate to train a global model, without revealing any data to the server and other participants.","Researchers commonly perform experiments in a simulation environment to quickly iterate on ideas.","However, existing open-source tools do not offer the efficiency required to simulate FL on larger and more realistic FL datasets.","We introduce pfl-research, a fast, modular, and easy-to-use Python framework for simulating FL.","It supports TensorFlow, PyTorch, and non-neural network models, and is tightly integrated with state-of-the-art privacy algorithms.","We study the speed of open-source FL frameworks and show that pfl-research is 7-72$\\times$ faster than alternative open-source frameworks on common cross-device setups.","Such speedup will significantly boost the productivity of the FL research community and enable testing hypotheses on realistic FL datasets that were previously too resource intensive.","We release a suite of benchmarks that evaluates an algorithm's overall performance on a diverse set of realistic scenarios.","The code is available on GitHub at https://github.com/apple/pfl-research."],"url":"http://arxiv.org/abs/2404.06430v1"}
{"created":"2024-04-09 16:20:03","title":"Magic-Boost: Boost 3D Generation with Mutli-View Conditioned Diffusion","abstract":"Benefiting from the rapid development of 2D diffusion models, 3D content creation has made significant progress recently. One promising solution involves the fine-tuning of pre-trained 2D diffusion models to harness their capacity for producing multi-view images, which are then lifted into accurate 3D models via methods like fast-NeRFs or large reconstruction models. However, as inconsistency still exists and limited generated resolution, the generation results of such methods still lack intricate textures and complex geometries. To solve this problem, we propose Magic-Boost, a multi-view conditioned diffusion model that significantly refines coarse generative results through a brief period of SDS optimization ($\\sim15$min). Compared to the previous text or single image based diffusion models, Magic-Boost exhibits a robust capability to generate images with high consistency from pseudo synthesized multi-view images. It provides precise SDS guidance that well aligns with the identity of the input images, enriching the local detail in both geometry and texture of the initial generative results. Extensive experiments show Magic-Boost greatly enhances the coarse inputs and generates high-quality 3D assets with rich geometric and textural details. (Project Page: https://magic-research.github.io/magic-boost/)","sentences":["Benefiting from the rapid development of 2D diffusion models, 3D content creation has made significant progress recently.","One promising solution involves the fine-tuning of pre-trained 2D diffusion models to harness their capacity for producing multi-view images, which are then lifted into accurate 3D models via methods like fast-NeRFs or large reconstruction models.","However, as inconsistency still exists and limited generated resolution, the generation results of such methods still lack intricate textures and complex geometries.","To solve this problem, we propose Magic-Boost, a multi-view conditioned diffusion model that significantly refines coarse generative results through a brief period of SDS optimization ($\\sim15$min).","Compared to the previous text or single image based diffusion models, Magic-Boost exhibits a robust capability to generate images with high consistency from pseudo synthesized multi-view images.","It provides precise SDS guidance that well aligns with the identity of the input images, enriching the local detail in both geometry and texture of the initial generative results.","Extensive experiments show Magic-Boost greatly enhances the coarse inputs and generates high-quality 3D assets with rich geometric and textural details.","(Project Page: https://magic-research.github.io/magic-boost/)"],"url":"http://arxiv.org/abs/2404.06429v1"}
{"created":"2024-04-09 16:18:13","title":"A universal sequence of tensors for the asymptotic rank conjecture","abstract":"The exponent $\\sigma(T)$ of a tensor $T\\in\\mathbb{F}^d\\otimes\\mathbb{F}^d\\otimes\\mathbb{F}^d$ over a field $\\mathbb{F}$ captures the base of the exponential growth rate of the tensor rank of $T$ under Kronecker powers. Tensor exponents are fundamental from the standpoint of algorithms and computational complexity theory; for example, the exponent $\\omega$ of matrix multiplication can be characterized as $\\omega=2\\sigma(\\mathrm{MM}_2)$, where $\\mathrm{MM}_2\\in\\mathbb{F}^4\\otimes\\mathbb{F}^4\\otimes\\mathbb{F}^4$ is the tensor that represents $2\\times 2$ matrix multiplication.   Our main result is an explicit construction of a sequence $\\mathcal{U}_d$ of zero-one-valued tensors that is universal for the worst-case tensor exponent; more precisely, we show that $\\sigma(\\mathcal{U}_d)=\\sigma(d)$ where $\\sigma(d)=\\sup_{T\\in\\mathbb{F}^d\\otimes\\mathbb{F}^d\\otimes\\mathbb{F}^d}\\sigma(T)$. We also supply an explicit universal sequence $\\mathcal{U}_\\Delta$ localised to capture the worst-case exponent $\\sigma(\\Delta)$ of tensors with support contained in $\\Delta\\subseteq [d]\\times[d]\\times [d]$; by combining such sequences, we obtain a universal sequence $\\mathcal{T}_d$ such that $\\sigma(\\mathcal{T}_d)=1$ holds if and only if Strassen's asymptotic rank conjecture [Progr. Math. 120 (1994)] holds for $d$. Finally, we show that the limit $\\lim_{d\\rightarrow\\infty}\\sigma(d)$ exists and can be captured as $\\lim_{d\\rightarrow\\infty} \\sigma(D_d)$ for an explicit sequence $(D_d)_{d=1}^\\infty$ of tensors obtained by diagonalisation of the sequences $\\mathcal{U}_d$. As our second result we relate the absence of polynomials of fixed degree vanishing on tensors of low rank, or more generally asymptotic rank, with upper bounds on the exponent $\\sigma(d)$. Using this technique, one may bound asymptotic rank for all tensors of a given format, knowing enough specific tensors of low asymptotic rank.","sentences":["The exponent $\\sigma(T)$ of a tensor $T\\in\\mathbb{F}^d\\otimes\\mathbb{F}^d\\otimes\\mathbb{F}^d$ over a field $\\mathbb{F}$ captures the base of the exponential growth rate of the tensor rank of $T$ under Kronecker powers.","Tensor exponents are fundamental from the standpoint of algorithms and computational complexity theory; for example, the exponent $\\omega$ of matrix multiplication can be characterized as $\\omega=2\\sigma(\\mathrm{MM}_2)$, where $\\mathrm{MM}_2\\in\\mathbb{F}^4\\otimes\\mathbb{F}^4\\otimes\\mathbb{F}^4$ is the tensor that represents $2\\times 2$ matrix multiplication.   ","Our main result is an explicit construction of a sequence $\\mathcal{U}_d$ of zero-one-valued tensors that is universal for the worst-case tensor exponent; more precisely, we show that $\\sigma(\\mathcal{U}_d)=\\sigma(d)$ where $\\sigma(d)=\\sup_{T\\in\\mathbb{F}^d\\otimes\\mathbb{F}^d\\otimes\\mathbb{F}^d}\\sigma(T)$. We also supply an explicit universal sequence $\\mathcal{U}_\\Delta$ localised to capture the worst-case exponent $\\sigma(\\Delta)$ of tensors with support contained in $\\Delta\\subseteq [d]\\times[d]\\times [d]$; by combining such sequences, we obtain a universal sequence $\\mathcal{T}_d$ such that $\\sigma(\\mathcal{T}_d)=1$ holds if and only if Strassen's asymptotic rank conjecture","[Progr.","Math. 120 (1994)] holds for $d$. Finally, we show that the limit $\\lim_{d\\rightarrow\\infty}\\sigma(d)$ exists and can be captured as $\\lim_{d\\rightarrow\\infty} \\sigma(D_d)$ for an explicit sequence $(D_d)_{d=1}^\\infty$ of tensors obtained by diagonalisation of the sequences $\\mathcal{U}_d$. As our second result we relate the absence of polynomials of fixed degree vanishing on tensors of low rank, or more generally asymptotic rank, with upper bounds on the exponent $\\sigma(d)$. Using this technique, one may bound asymptotic rank for all tensors of a given format, knowing enough specific tensors of low asymptotic rank."],"url":"http://arxiv.org/abs/2404.06427v1"}
{"created":"2024-04-09 16:15:03","title":"ZeST: Zero-Shot Material Transfer from a Single Image","abstract":"We propose ZeST, a method for zero-shot material transfer to an object in the input image given a material exemplar image. ZeST leverages existing diffusion adapters to extract implicit material representation from the exemplar image. This representation is used to transfer the material using pre-trained inpainting diffusion model on the object in the input image using depth estimates as geometry cue and grayscale object shading as illumination cues. The method works on real images without any training resulting a zero-shot approach. Both qualitative and quantitative results on real and synthetic datasets demonstrate that ZeST outputs photorealistic images with transferred materials. We also show the application of ZeST to perform multiple edits and robust material assignment under different illuminations. Project Page: https://ttchengab.github.io/zest","sentences":["We propose ZeST, a method for zero-shot material transfer to an object in the input image given a material exemplar image.","ZeST leverages existing diffusion adapters to extract implicit material representation from the exemplar image.","This representation is used to transfer the material using pre-trained inpainting diffusion model on the object in the input image using depth estimates as geometry cue and grayscale object shading as illumination cues.","The method works on real images without any training resulting a zero-shot approach.","Both qualitative and quantitative results on real and synthetic datasets demonstrate that ZeST outputs photorealistic images with transferred materials.","We also show the application of ZeST to perform multiple edits and robust material assignment under different illuminations.","Project Page: https://ttchengab.github.io/zest"],"url":"http://arxiv.org/abs/2404.06425v1"}
{"created":"2024-04-09 16:14:03","title":"Deep Reinforcement Learning-Based Approach for a Single Vehicle Persistent Surveillance Problem with Fuel Constraints","abstract":"This article presents a deep reinforcement learning-based approach to tackle a persistent surveillance mission requiring a single unmanned aerial vehicle initially stationed at a depot with fuel or time-of-flight constraints to repeatedly visit a set of targets with equal priority. Owing to the vehicle's fuel or time-of-flight constraints, the vehicle must be regularly refueled, or its battery must be recharged at the depot. The objective of the problem is to determine an optimal sequence of visits to the targets that minimizes the maximum time elapsed between successive visits to any target while ensuring that the vehicle never runs out of fuel or charge. We present a deep reinforcement learning algorithm to solve this problem and present the results of numerical experiments that corroborate the effectiveness of this approach in comparison with common-sense greedy heuristics.","sentences":["This article presents a deep reinforcement learning-based approach to tackle a persistent surveillance mission requiring a single unmanned aerial vehicle initially stationed at a depot with fuel or time-of-flight constraints to repeatedly visit a set of targets with equal priority.","Owing to the vehicle's fuel or time-of-flight constraints, the vehicle must be regularly refueled, or its battery must be recharged at the depot.","The objective of the problem is to determine an optimal sequence of visits to the targets that minimizes the maximum time elapsed between successive visits to any target while ensuring that the vehicle never runs out of fuel or charge.","We present a deep reinforcement learning algorithm to solve this problem and present the results of numerical experiments that corroborate the effectiveness of this approach in comparison with common-sense greedy heuristics."],"url":"http://arxiv.org/abs/2404.06423v1"}
{"created":"2024-04-09 16:12:22","title":"Echo Chambers in the Age of Algorithms: An Audit of Twitter's Friend Recommender System","abstract":"The presence of political misinformation and ideological echo chambers on social media platforms is concerning given the important role that these sites play in the public's exposure to news and current events. Algorithmic systems employed on these platforms are presumed to play a role in these phenomena, but little is known about their mechanisms and effects. In this work, we conduct an algorithmic audit of Twitter's Who-To-Follow friend recommendation system, the first empirical audit that investigates the impact of this algorithm in-situ. We create automated Twitter accounts that initially follow left and right affiliated U.S. politicians during the 2022 U.S. midterm elections and then grow their information networks using the platform's recommender system. We pair the experiment with an observational study of Twitter users who already follow the same politicians. Broadly, we find that while following the recommendation algorithm leads accounts into dense and reciprocal neighborhoods that structurally resemble echo chambers, the recommender also results in less political homogeneity of a user's network compared to accounts growing their networks through social endorsement. Furthermore, accounts that exclusively followed users recommended by the algorithm had fewer opportunities to encounter content centered on false or misleading election narratives compared to choosing friends based on social endorsement.","sentences":["The presence of political misinformation and ideological echo chambers on social media platforms is concerning given the important role that these sites play in the public's exposure to news and current events.","Algorithmic systems employed on these platforms are presumed to play a role in these phenomena, but little is known about their mechanisms and effects.","In this work, we conduct an algorithmic audit of Twitter's Who-To-Follow friend recommendation system, the first empirical audit that investigates the impact of this algorithm in-situ.","We create automated Twitter accounts that initially follow left and right affiliated U.S. politicians during the 2022 U.S. midterm elections and then grow their information networks using the platform's recommender system.","We pair the experiment with an observational study of Twitter users who already follow the same politicians.","Broadly, we find that while following the recommendation algorithm leads accounts into dense and reciprocal neighborhoods that structurally resemble echo chambers, the recommender also results in less political homogeneity of a user's network compared to accounts growing their networks through social endorsement.","Furthermore, accounts that exclusively followed users recommended by the algorithm had fewer opportunities to encounter content centered on false or misleading election narratives compared to choosing friends based on social endorsement."],"url":"http://arxiv.org/abs/2404.06422v1"}
{"created":"2024-04-09 16:10:39","title":"Bayesian Survival Analysis by Approximate Inference of Neural Networks","abstract":"Predicting future events always comes with uncertainty, but traditional non-Bayesian methods cannot distinguish certain from uncertain predictions or explain the confidence in their predictions. In survival analysis, Bayesian methods applied to state-of-the-art solutions in the healthcare and biomedical field are still novel, and their implications have not been fully evaluated. In this paper, we study the benefits of modeling uncertainty in deep neural networks for survival analysis with a focus on prediction and calibration performance. For this, we present a Bayesian deep learning framework that consists of three Bayesian network architectures, which we train by optimizing the Cox partial likelihood and combining input-dependent aleatoric uncertainty with model-specific epistemic uncertainty. This enables us to provide uncertainty estimates as credible intervals when predicting the survival curve or as a probability density function over the predicted median survival times. For our empirical analyses, we evaluated our proposed method on four benchmark datasets and found that our method demonstrates prediction performance comparable to the state-of-the-art based on the concordance index and outperforms all other Cox-based approaches in terms of the mean absolute error. Our work explicitly compares the extent to which different Bayesian approximation techniques differ from each other and improves the prediction over traditional non-Bayesian alternatives.","sentences":["Predicting future events always comes with uncertainty, but traditional non-Bayesian methods cannot distinguish certain from uncertain predictions or explain the confidence in their predictions.","In survival analysis, Bayesian methods applied to state-of-the-art solutions in the healthcare and biomedical field are still novel, and their implications have not been fully evaluated.","In this paper, we study the benefits of modeling uncertainty in deep neural networks for survival analysis with a focus on prediction and calibration performance.","For this, we present a Bayesian deep learning framework that consists of three Bayesian network architectures, which we train by optimizing the Cox partial likelihood and combining input-dependent aleatoric uncertainty with model-specific epistemic uncertainty.","This enables us to provide uncertainty estimates as credible intervals when predicting the survival curve or as a probability density function over the predicted median survival times.","For our empirical analyses, we evaluated our proposed method on four benchmark datasets and found that our method demonstrates prediction performance comparable to the state-of-the-art based on the concordance index and outperforms all other Cox-based approaches in terms of the mean absolute error.","Our work explicitly compares the extent to which different Bayesian approximation techniques differ from each other and improves the prediction over traditional non-Bayesian alternatives."],"url":"http://arxiv.org/abs/2404.06421v1"}
{"created":"2024-04-09 16:07:35","title":"Studying the Impact of Latent Representations in Implicit Neural Networks for Scientific Continuous Field Reconstruction","abstract":"Learning a continuous and reliable representation of physical fields from sparse sampling is challenging and it affects diverse scientific disciplines. In a recent work, we present a novel model called MMGN (Multiplicative and Modulated Gabor Network) with implicit neural networks. In this work, we design additional studies leveraging explainability methods to complement the previous experiments and further enhance the understanding of latent representations generated by the model. The adopted methods are general enough to be leveraged for any latent space inspection. Preliminary results demonstrate the contextual information incorporated in the latent representations and their impact on the model performance. As a work in progress, we will continue to verify our findings and develop novel explainability approaches.","sentences":["Learning a continuous and reliable representation of physical fields from sparse sampling is challenging and it affects diverse scientific disciplines.","In a recent work, we present a novel model called MMGN (Multiplicative and Modulated Gabor Network) with implicit neural networks.","In this work, we design additional studies leveraging explainability methods to complement the previous experiments and further enhance the understanding of latent representations generated by the model.","The adopted methods are general enough to be leveraged for any latent space inspection.","Preliminary results demonstrate the contextual information incorporated in the latent representations and their impact on the model performance.","As a work in progress, we will continue to verify our findings and develop novel explainability approaches."],"url":"http://arxiv.org/abs/2404.06418v1"}
{"created":"2024-04-09 16:07:00","title":"Radon-Hurwitz Grassmannian codes","abstract":"Every equi-isoclinic tight fusion frame (EITFF) is a type of optimal code in a Grassmannian, consisting of subspaces of a finite-dimensional Hilbert space for which the smallest principal angle between any pair of them is as large as possible. EITFFs yield dictionaries with minimal block coherence and so are ideal for certain types of compressed sensing. By refining classical arguments of Lemmens and Seidel that rely upon Radon-Hurwitz theory, we fully characterize EITFFs in the special case where the dimension of the subspaces is exactly one-half of that of the ambient space. We moreover show that each such \"Radon-Hurwitz EITFF\" is highly symmetric.","sentences":["Every equi-isoclinic tight fusion frame (EITFF) is a type of optimal code in a Grassmannian, consisting of subspaces of a finite-dimensional Hilbert space for which the smallest principal angle between any pair of them is as large as possible.","EITFFs yield dictionaries with minimal block coherence and so are ideal for certain types of compressed sensing.","By refining classical arguments of Lemmens and Seidel that rely upon Radon-Hurwitz theory, we fully characterize EITFFs in the special case where the dimension of the subspaces is exactly one-half of that of the ambient space.","We moreover show that each such \"Radon-Hurwitz EITFF\" is highly symmetric."],"url":"http://arxiv.org/abs/2404.06417v1"}
{"created":"2024-04-09 16:03:26","title":"Large Language Models to the Rescue: Deadlock Resolution in Multi-Robot Systems","abstract":"Multi-agent robotic systems are prone to deadlocks in an obstacle environment where the system can get stuck away from its desired location under a smooth low-level control policy. Without an external intervention, often in terms of a high-level command, it is not possible to guarantee that just a low-level control policy can resolve such deadlocks. Utilizing the generalizability and low data requirements of large language models (LLMs), this paper explores the possibility of using LLMs for deadlock resolution. We propose a hierarchical control framework where an LLM resolves deadlocks by assigning a leader and direction for the leader to move along. A graph neural network (GNN) based low-level distributed control policy executes the assigned plan. We systematically study various prompting techniques to improve LLM's performance in resolving deadlocks. In particular, as part of prompt engineering, we provide in-context examples for LLMs. We conducted extensive experiments on various multi-robot environments with up to 15 agents and 40 obstacles. Our results demonstrate that LLM-based high-level planners are effective in resolving deadlocks in MRS.","sentences":["Multi-agent robotic systems are prone to deadlocks in an obstacle environment where the system can get stuck away from its desired location under a smooth low-level control policy.","Without an external intervention, often in terms of a high-level command, it is not possible to guarantee that just a low-level control policy can resolve such deadlocks.","Utilizing the generalizability and low data requirements of large language models (LLMs), this paper explores the possibility of using LLMs for deadlock resolution.","We propose a hierarchical control framework where an LLM resolves deadlocks by assigning a leader and direction for the leader to move along.","A graph neural network (GNN) based low-level distributed control policy executes the assigned plan.","We systematically study various prompting techniques to improve LLM's performance in resolving deadlocks.","In particular, as part of prompt engineering, we provide in-context examples for LLMs.","We conducted extensive experiments on various multi-robot environments with up to 15 agents and 40 obstacles.","Our results demonstrate that LLM-based high-level planners are effective in resolving deadlocks in MRS."],"url":"http://arxiv.org/abs/2404.06413v1"}
{"created":"2024-04-09 16:01:24","title":"AgentQuest: A Modular Benchmark Framework to Measure Progress and Improve LLM Agents","abstract":"The advances made by Large Language Models (LLMs) have led to the pursuit of LLM agents that can solve intricate, multi-step reasoning tasks. As with any research pursuit, benchmarking and evaluation are key corner stones to efficient and reliable progress. However, existing benchmarks are often narrow and simply compute overall task success. To face these issues, we propose AgentQuest -- a framework where (i) both benchmarks and metrics are modular and easily extensible through well documented and easy-to-use APIs; (ii) we offer two new evaluation metrics that can reliably track LLM agent progress while solving a task. We exemplify the utility of the metrics on two use cases wherein we identify common failure points and refine the agent architecture to obtain a significant performance increase. Together with the research community, we hope to extend AgentQuest further and therefore we make it available under https://github.com/nec-research/agentquest.","sentences":["The advances made by Large Language Models (LLMs) have led to the pursuit of LLM agents that can solve intricate, multi-step reasoning tasks.","As with any research pursuit, benchmarking and evaluation are key corner stones to efficient and reliable progress.","However, existing benchmarks are often narrow and simply compute overall task success.","To face these issues, we propose AgentQuest -- a framework where (i) both benchmarks and metrics are modular and easily extensible through well documented and easy-to-use APIs; (ii) we offer two new evaluation metrics that can reliably track LLM agent progress while solving a task.","We exemplify the utility of the metrics on two use cases wherein we identify common failure points and refine the agent architecture to obtain a significant performance increase.","Together with the research community, we hope to extend AgentQuest further and therefore we make it available under https://github.com/nec-research/agentquest."],"url":"http://arxiv.org/abs/2404.06411v1"}
{"created":"2024-04-09 15:54:16","title":"Take a Look at it! Rethinking How to Evaluate Language Model Jailbreak","abstract":"Large language models (LLMs) have become increasingly integrated with various applications. To ensure that LLMs do not generate unsafe responses, they are aligned with safeguards that specify what content is restricted. However, such alignment can be bypassed to produce prohibited content using a technique commonly referred to as jailbreak. Different systems have been proposed to perform the jailbreak automatically. These systems rely on evaluation methods to determine whether a jailbreak attempt is successful. However, our analysis reveals that current jailbreak evaluation methods have two limitations. (1) Their objectives lack clarity and do not align with the goal of identifying unsafe responses. (2) They oversimplify the jailbreak result as a binary outcome, successful or not.   In this paper, we propose three metrics, safeguard violation, informativeness, and relative truthfulness, to evaluate language model jailbreak. Additionally, we demonstrate how these metrics correlate with the goal of different malicious actors. To compute these metrics, we introduce a multifaceted approach that extends the natural language generation evaluation method after preprocessing the response. We evaluate our metrics on a benchmark dataset produced from three malicious intent datasets and three jailbreak systems. The benchmark dataset is labeled by three annotators. We compare our multifaceted approach with three existing jailbreak evaluation methods. Experiments demonstrate that our multifaceted evaluation outperforms existing methods, with F1 scores improving on average by 17% compared to existing baselines. Our findings motivate the need to move away from the binary view of the jailbreak problem and incorporate a more comprehensive evaluation to ensure the safety of the language model.","sentences":["Large language models (LLMs) have become increasingly integrated with various applications.","To ensure that LLMs do not generate unsafe responses, they are aligned with safeguards that specify what content is restricted.","However, such alignment can be bypassed to produce prohibited content using a technique commonly referred to as jailbreak.","Different systems have been proposed to perform the jailbreak automatically.","These systems rely on evaluation methods to determine whether a jailbreak attempt is successful.","However, our analysis reveals that current jailbreak evaluation methods have two limitations.","(1) Their objectives lack clarity and do not align with the goal of identifying unsafe responses.","(2) They oversimplify the jailbreak result as a binary outcome, successful or not.   ","In this paper, we propose three metrics, safeguard violation, informativeness, and relative truthfulness, to evaluate language model jailbreak.","Additionally, we demonstrate how these metrics correlate with the goal of different malicious actors.","To compute these metrics, we introduce a multifaceted approach that extends the natural language generation evaluation method after preprocessing the response.","We evaluate our metrics on a benchmark dataset produced from three malicious intent datasets and three jailbreak systems.","The benchmark dataset is labeled by three annotators.","We compare our multifaceted approach with three existing jailbreak evaluation methods.","Experiments demonstrate that our multifaceted evaluation outperforms existing methods, with F1 scores improving on average by 17% compared to existing baselines.","Our findings motivate the need to move away from the binary view of the jailbreak problem and incorporate a more comprehensive evaluation to ensure the safety of the language model."],"url":"http://arxiv.org/abs/2404.06407v1"}
{"created":"2024-04-09 15:54:03","title":"Emergent Dynamics in Neural Cellular Automata","abstract":"Neural Cellular Automata (NCA) models are trainable variations of traditional Cellular Automata (CA). Emergent motion in the patterns created by NCA has been successfully applied to synthesize dynamic textures. However, the conditions required for an NCA to display dynamic patterns remain unexplored. Here, we investigate the relationship between the NCA architecture and the emergent dynamics of the trained models. Specifically, we vary the number of channels in the cell state and the number of hidden neurons in the MultiLayer Perceptron (MLP), and draw a relationship between the combination of these two variables and the motion strength between successive frames. Our analysis reveals that the disparity and proportionality between these two variables have a strong correlation with the emergent dynamics in the NCA output. We thus propose a design principle for creating dynamic NCA.","sentences":["Neural Cellular Automata (NCA) models are trainable variations of traditional Cellular Automata (CA).","Emergent motion in the patterns created by NCA has been successfully applied to synthesize dynamic textures.","However, the conditions required for an NCA to display dynamic patterns remain unexplored.","Here, we investigate the relationship between the NCA architecture and the emergent dynamics of the trained models.","Specifically, we vary the number of channels in the cell state and the number of hidden neurons in the MultiLayer Perceptron (MLP), and draw a relationship between the combination of these two variables and the motion strength between successive frames.","Our analysis reveals that the disparity and proportionality between these two variables have a strong correlation with the emergent dynamics in the NCA output.","We thus propose a design principle for creating dynamic NCA."],"url":"http://arxiv.org/abs/2404.06406v1"}
{"created":"2024-04-09 15:54:00","title":"Wu's Method can Boost Symbolic AI to Rival Silver Medalists and AlphaGeometry to Outperform Gold Medalists at IMO Geometry","abstract":"Proving geometric theorems constitutes a hallmark of visual reasoning combining both intuitive and logical skills. Therefore, automated theorem proving of Olympiad-level geometry problems is considered a notable milestone in human-level automated reasoning. The introduction of AlphaGeometry, a neuro-symbolic model trained with 100 million synthetic samples, marked a major breakthrough. It solved 25 of 30 International Mathematical Olympiad (IMO) problems whereas the reported baseline based on Wu's method solved only ten. In this note, we revisit the IMO-AG-30 Challenge introduced with AlphaGeometry, and find that Wu's method is surprisingly strong. Wu's method alone can solve 15 problems, and some of them are not solved by any of the other methods. This leads to two key findings: (i) Combining Wu's method with the classic synthetic methods of deductive databases and angle, ratio, and distance chasing solves 21 out of 30 methods by just using a CPU-only laptop with a time limit of 5 minutes per problem. Essentially, this classic method solves just 4 problems less than AlphaGeometry and establishes the first fully symbolic baseline strong enough to rival the performance of an IMO silver medalist. (ii) Wu's method even solves 2 of the 5 problems that AlphaGeometry failed to solve. Thus, by combining AlphaGeometry with Wu's method we set a new state-of-the-art for automated theorem proving on IMO-AG-30, solving 27 out of 30 problems, the first AI method which outperforms an IMO gold medalist.","sentences":["Proving geometric theorems constitutes a hallmark of visual reasoning combining both intuitive and logical skills.","Therefore, automated theorem proving of Olympiad-level geometry problems is considered a notable milestone in human-level automated reasoning.","The introduction of AlphaGeometry, a neuro-symbolic model trained with 100 million synthetic samples, marked a major breakthrough.","It solved 25 of 30 International Mathematical Olympiad (IMO) problems whereas the reported baseline based on Wu's method solved only ten.","In this note, we revisit the IMO-AG-30 Challenge introduced with AlphaGeometry, and find that Wu's method is surprisingly strong.","Wu's method alone can solve 15 problems, and some of them are not solved by any of the other methods.","This leads to two key findings: (i) Combining Wu's method with the classic synthetic methods of deductive databases and angle, ratio, and distance chasing solves 21 out of 30 methods by just using a CPU-only laptop with a time limit of 5 minutes per problem.","Essentially, this classic method solves just 4 problems less than AlphaGeometry and establishes the first fully symbolic baseline strong enough to rival the performance of an IMO silver medalist.","(ii) Wu's method even solves 2 of the 5 problems that AlphaGeometry failed to solve.","Thus, by combining AlphaGeometry with Wu's method we set a new state-of-the-art for automated theorem proving on IMO-AG-30, solving 27 out of 30 problems, the first AI method which outperforms an IMO gold medalist."],"url":"http://arxiv.org/abs/2404.06405v1"}
{"created":"2024-04-09 15:53:06","title":"Apprentices to Research Assistants: Advancing Research with Large Language Models","abstract":"Large Language Models (LLMs) have emerged as powerful tools in various research domains. This article examines their potential through a literature review and firsthand experimentation. While LLMs offer benefits like cost-effectiveness and efficiency, challenges such as prompt tuning, biases, and subjectivity must be addressed. The study presents insights from experiments utilizing LLMs for qualitative analysis, highlighting successes and limitations. Additionally, it discusses strategies for mitigating challenges, such as prompt optimization techniques and leveraging human expertise. This study aligns with the 'LLMs as Research Tools' workshop's focus on integrating LLMs into HCI data work critically and ethically. By addressing both opportunities and challenges, our work contributes to the ongoing dialogue on their responsible application in research.","sentences":["Large Language Models (LLMs) have emerged as powerful tools in various research domains.","This article examines their potential through a literature review and firsthand experimentation.","While LLMs offer benefits like cost-effectiveness and efficiency, challenges such as prompt tuning, biases, and subjectivity must be addressed.","The study presents insights from experiments utilizing LLMs for qualitative analysis, highlighting successes and limitations.","Additionally, it discusses strategies for mitigating challenges, such as prompt optimization techniques and leveraging human expertise.","This study aligns with the 'LLMs as Research Tools' workshop's focus on integrating LLMs into HCI data work critically and ethically.","By addressing both opportunities and challenges, our work contributes to the ongoing dialogue on their responsible application in research."],"url":"http://arxiv.org/abs/2404.06404v1"}
{"created":"2024-04-09 15:53:02","title":"Online Learning of Decision Trees with Thompson Sampling","abstract":"Decision Trees are prominent prediction models for interpretable Machine Learning. They have been thoroughly researched, mostly in the batch setting with a fixed labelled dataset, leading to popular algorithms such as C4.5, ID3 and CART. Unfortunately, these methods are of heuristic nature, they rely on greedy splits offering no guarantees of global optimality and often leading to unnecessarily complex and hard-to-interpret Decision Trees. Recent breakthroughs addressed this suboptimality issue in the batch setting, but no such work has considered the online setting with data arriving in a stream. To this end, we devise a new Monte Carlo Tree Search algorithm, Thompson Sampling Decision Trees (TSDT), able to produce optimal Decision Trees in an online setting. We analyse our algorithm and prove its almost sure convergence to the optimal tree. Furthermore, we conduct extensive experiments to validate our findings empirically. The proposed TSDT outperforms existing algorithms on several benchmarks, all while presenting the practical advantage of being tailored to the online setting.","sentences":["Decision Trees are prominent prediction models for interpretable Machine Learning.","They have been thoroughly researched, mostly in the batch setting with a fixed labelled dataset, leading to popular algorithms such as C4.5, ID3 and CART.","Unfortunately, these methods are of heuristic nature, they rely on greedy splits offering no guarantees of global optimality and often leading to unnecessarily complex and hard-to-interpret Decision Trees.","Recent breakthroughs addressed this suboptimality issue in the batch setting, but no such work has considered the online setting with data arriving in a stream.","To this end, we devise a new Monte Carlo Tree Search algorithm, Thompson Sampling Decision Trees (TSDT), able to produce optimal Decision Trees in an online setting.","We analyse our algorithm and prove its almost sure convergence to the optimal tree.","Furthermore, we conduct extensive experiments to validate our findings empirically.","The proposed TSDT outperforms existing algorithms on several benchmarks, all while presenting the practical advantage of being tailored to the online setting."],"url":"http://arxiv.org/abs/2404.06403v1"}
{"created":"2024-04-09 15:48:31","title":"Bounded Edit Distance: Optimal Static and Dynamic Algorithms for Small Integer Weights","abstract":"The edit distance of two strings is the minimum number of insertions, deletions, and substitutions needed to transform one string into the other. The textbook algorithm determines the edit distance of length-$n$ strings in $O(n^2)$ time, which is optimal up to subpolynomial factors under Orthogonal Vectors Hypothesis. In the bounded version of the problem, parameterized by the edit distance $k$, the algorithm of Landau and Vishkin [JCSS'88] achieves $O(n+k^2)$ time, which is optimal as a function of $n$ and $k$.   The dynamic version of the problem asks to maintain the edit distance of two strings that change dynamically, with each update modeled as an edit. A folklore approach supports updates in $\\tilde O(k^2)$ time, where $\\tilde O(\\cdot)$ hides polylogarithmic factors. Recently, Charalampopoulos, Kociumaka, and Mozes [CPM'20] showed an algorithm with update time $\\tilde O(n)$, which is optimal under OVH in terms of $n$. The update time of $\\tilde O(\\min\\{n,k^2\\})$ raised an exciting open question of whether $\\tilde O(k)$ is possible; we answer it affirmatively.   Our solution relies on tools originating from weighted edit distance, where the weight of each edit depends on the edit type and the characters involved. The textbook algorithm supports weights, but the Landau-Vishkin approach does not, and a simple $O(nk)$-time procedure long remained the fastest for bounded weighted edit distance. Only recently, Das et al. [STOC'23] provided an $O(n+k^5)$-time algorithm, whereas Cassis, Kociumaka, and Wellnitz [FOCS'23] presented an $\\tilde O(n+\\sqrt{nk^3})$-time solution and a matching conditional lower bound. In this paper, we show that, for integer edit weights between $0$ and $W$, weighted edit distance can be computed in $\\tilde O(n+Wk^2)$ time and maintained dynamically in $\\tilde O(W^2k)$ time per update. Our static algorithm can also be implemented in $\\tilde O(n+k^{2.5})$ time.","sentences":["The edit distance of two strings is the minimum number of insertions, deletions, and substitutions needed to transform one string into the other.","The textbook algorithm determines the edit distance of length-$n$ strings in $O(n^2)$ time, which is optimal up to subpolynomial factors under Orthogonal Vectors Hypothesis.","In the bounded version of the problem, parameterized by the edit distance $k$, the algorithm of Landau and Vishkin","[JCSS'88] achieves $O(n+k^2)$ time, which is optimal as a function of $n$ and $k$.   The dynamic version of the problem asks to maintain the edit distance of two strings that change dynamically, with each update modeled as an edit.","A folklore approach supports updates in $\\tilde O(k^2)$ time, where $\\tilde O(\\cdot)$ hides polylogarithmic factors.","Recently, Charalampopoulos, Kociumaka, and Mozes [CPM'20] showed an algorithm with update time $\\tilde O(n)$, which is optimal under OVH in terms of $n$. The update time of $\\tilde O(\\min\\{n,k^2\\})$ raised an exciting open question of whether $\\tilde O(k)$ is possible; we answer it affirmatively.   ","Our solution relies on tools originating from weighted edit distance, where the weight of each edit depends on the edit type and the characters involved.","The textbook algorithm supports weights, but the Landau-Vishkin approach does not, and a simple $O(nk)$-time procedure long remained the fastest for bounded weighted edit distance.","Only recently, Das et al.","[STOC'23] provided an $O(n+k^5)$-time algorithm, whereas Cassis, Kociumaka, and Wellnitz","[FOCS'23] presented an $\\tilde O(n+\\sqrt{nk^3})$-time solution and a matching conditional lower bound.","In this paper, we show that, for integer edit weights between $0$ and $W$, weighted edit distance can be computed in $\\tilde O(n+Wk^2)$ time and maintained dynamically in $\\tilde O(W^2k)$ time per update.","Our static algorithm can also be implemented in $\\tilde O(n+k^{2.5})$ time."],"url":"http://arxiv.org/abs/2404.06401v1"}
{"created":"2024-04-09 15:46:00","title":"Dynamic Deep Learning Based Super-Resolution For The Shallow Water Equations","abstract":"Using the nonlinear shallow water equations as benchmark, we demonstrate that a simulation with the ICON-O ocean model with a 20km resolution that is frequently corrected by a U-net-type neural network can achieve discretization errors of a simulation with 10km resolution. The network, originally developed for image-based super-resolution in post-processing, is trained to compute the difference between solutions on both meshes and is used to correct the coarse mesh every 12h. Our setup is the Galewsky test case, modeling transition of a barotropic instability into turbulent flow. We show that the ML-corrected coarse resolution run correctly maintains a balance flow and captures the transition to turbulence in line with the higher resolution simulation. After 8 day of simulation, the $L_2$-error of the corrected run is similar to a simulation run on the finer mesh. While mass is conserved in the corrected runs, we observe some spurious generation of kinetic energy.","sentences":["Using the nonlinear shallow water equations as benchmark, we demonstrate that a simulation with the ICON-O ocean model with a 20km resolution that is frequently corrected by a U-net-type neural network can achieve discretization errors of a simulation with 10km resolution.","The network, originally developed for image-based super-resolution in post-processing, is trained to compute the difference between solutions on both meshes and is used to correct the coarse mesh every 12h.","Our setup is the Galewsky test case, modeling transition of a barotropic instability into turbulent flow.","We show that the ML-corrected coarse resolution run correctly maintains a balance flow and captures the transition to turbulence in line with the higher resolution simulation.","After 8 day of simulation, the $L_2$-error of the corrected run is similar to a simulation run on the finer mesh.","While mass is conserved in the corrected runs, we observe some spurious generation of kinetic energy."],"url":"http://arxiv.org/abs/2404.06400v1"}
{"created":"2024-04-09 15:36:50","title":"MiniCPM: Unveiling the Potential of Small Language Models with Scalable Training Strategies","abstract":"The burgeoning interest in developing Large Language Models (LLMs) with up to trillion parameters has been met with concerns regarding resource efficiency and practical expense, particularly given the immense cost of experimentation. This scenario underscores the importance of exploring the potential of Small Language Models (SLMs) as a resource-efficient alternative. In this context, we introduce MiniCPM, specifically the 1.2B and 2.4B non-embedding parameter variants, not only excel in their respective categories but also demonstrate capabilities on par with 7B-13B LLMs. While focusing on SLMs, our approach exhibits scalability in both model and data dimensions for future LLM research. Regarding model scaling, we employ extensive model wind tunnel experiments for stable and optimal scaling. For data scaling, we introduce a Warmup-Stable-Decay (WSD) learning rate scheduler (LRS), conducive to continuous training and domain adaptation. We present an in-depth analysis of the intriguing training dynamics that occurred in the WSD LRS. With WSD LRS, we are now able to efficiently study data-model scaling law without extensive retraining experiments on both axes of model and data, from which we derive the much higher compute optimal data-model ratio than Chinchilla Optimal. Additionally, we introduce MiniCPM family, including MiniCPM-DPO, MiniCPM-MoE and MiniCPM-128K, whose excellent performance further cementing MiniCPM's foundation in diverse SLM applications. MiniCPM models are available publicly at https://github.com/OpenBMB/MiniCPM .","sentences":["The burgeoning interest in developing Large Language Models (LLMs) with up to trillion parameters has been met with concerns regarding resource efficiency and practical expense, particularly given the immense cost of experimentation.","This scenario underscores the importance of exploring the potential of Small Language Models (SLMs) as a resource-efficient alternative.","In this context, we introduce MiniCPM, specifically the 1.2B and 2.4B non-embedding parameter variants, not only excel in their respective categories but also demonstrate capabilities on par with 7B-13B LLMs.","While focusing on SLMs, our approach exhibits scalability in both model and data dimensions for future LLM research.","Regarding model scaling, we employ extensive model wind tunnel experiments for stable and optimal scaling.","For data scaling, we introduce a Warmup-Stable-Decay (WSD) learning rate scheduler (LRS), conducive to continuous training and domain adaptation.","We present an in-depth analysis of the intriguing training dynamics that occurred in the WSD LRS.","With WSD LRS, we are now able to efficiently study data-model scaling law without extensive retraining experiments on both axes of model and data, from which we derive the much higher compute optimal data-model ratio than Chinchilla Optimal.","Additionally, we introduce MiniCPM family, including MiniCPM-DPO, MiniCPM-MoE and MiniCPM-128K, whose excellent performance further cementing MiniCPM's foundation in diverse SLM applications.","MiniCPM models are available publicly at https://github.com/OpenBMB/MiniCPM ."],"url":"http://arxiv.org/abs/2404.06395v1"}
{"created":"2024-04-09 15:35:52","title":"MuPT: A Generative Symbolic Music Pretrained Transformer","abstract":"In this paper, we explore the application of Large Language Models (LLMs) to the pre-training of music. While the prevalent use of MIDI in music modeling is well-established, our findings suggest that LLMs are inherently more compatible with ABC Notation, which aligns more closely with their design and strengths, thereby enhancing the model's performance in musical composition. To address the challenges associated with misaligned measures from different tracks during generation, we propose the development of a \\underline{S}ynchronized \\underline{M}ulti-\\underline{T}rack ABC Notation (\\textbf{SMT-ABC Notation}), which aims to preserve coherence across multiple musical tracks. Our contributions include a series of models capable of handling up to 8192 tokens, covering 90\\% of the symbolic music data in our training set. Furthermore, we explore the implications of the \\underline{S}ymbolic \\underline{M}usic \\underline{S}caling Law (\\textbf{SMS Law}) on model performance. The results indicate a promising direction for future research in music generation, offering extensive resources for community-led research through our open-source contributions.","sentences":["In this paper, we explore the application of Large Language Models (LLMs) to the pre-training of music.","While the prevalent use of MIDI in music modeling is well-established, our findings suggest that LLMs are inherently more compatible with ABC Notation, which aligns more closely with their design and strengths, thereby enhancing the model's performance in musical composition.","To address the challenges associated with misaligned measures from different tracks during generation, we propose the development of a \\underline{S}ynchronized \\underline{M}ulti-\\underline{T}rack ABC Notation (\\textbf{SMT-ABC Notation}), which aims to preserve coherence across multiple musical tracks.","Our contributions include a series of models capable of handling up to 8192 tokens, covering 90\\% of the symbolic music data in our training set.","Furthermore, we explore the implications of the \\underline{S}ymbolic \\underline{M}usic \\underline{S}caling Law (\\textbf{SMS Law}) on model performance.","The results indicate a promising direction for future research in music generation, offering extensive resources for community-led research through our open-source contributions."],"url":"http://arxiv.org/abs/2404.06393v1"}
{"created":"2024-04-09 15:35:41","title":"Event Extraction in Basque: Typologically motivated Cross-Lingual Transfer-Learning Analysis","abstract":"Cross-lingual transfer-learning is widely used in Event Extraction for low-resource languages and involves a Multilingual Language Model that is trained in a source language and applied to the target language. This paper studies whether the typological similarity between source and target languages impacts the performance of cross-lingual transfer, an under-explored topic. We first focus on Basque as the target language, which is an ideal target language because it is typologically different from surrounding languages. Our experiments on three Event Extraction tasks show that the shared linguistic characteristic between source and target languages does have an impact on transfer quality. Further analysis of 72 language pairs reveals that for tasks that involve token classification such as entity and event trigger identification, common writing script and morphological features produce higher quality cross-lingual transfer. In contrast, for tasks involving structural prediction like argument extraction, common word order is the most relevant feature. In addition, we show that when increasing the training size, not all the languages scale in the same way in the cross-lingual setting. To perform the experiments we introduce EusIE, an event extraction dataset for Basque, which follows the Multilingual Event Extraction dataset (MEE). The dataset and code are publicly available.","sentences":["Cross-lingual transfer-learning is widely used in Event Extraction for low-resource languages and involves a Multilingual Language Model that is trained in a source language and applied to the target language.","This paper studies whether the typological similarity between source and target languages impacts the performance of cross-lingual transfer, an under-explored topic.","We first focus on Basque as the target language, which is an ideal target language because it is typologically different from surrounding languages.","Our experiments on three Event Extraction tasks show that the shared linguistic characteristic between source and target languages does have an impact on transfer quality.","Further analysis of 72 language pairs reveals that for tasks that involve token classification such as entity and event trigger identification, common writing script and morphological features produce higher quality cross-lingual transfer.","In contrast, for tasks involving structural prediction like argument extraction, common word order is the most relevant feature.","In addition, we show that when increasing the training size, not all the languages scale in the same way in the cross-lingual setting.","To perform the experiments we introduce EusIE, an event extraction dataset for Basque, which follows the Multilingual Event Extraction dataset (MEE).","The dataset and code are publicly available."],"url":"http://arxiv.org/abs/2404.06392v1"}
{"created":"2024-04-09 15:35:02","title":"Exploring Neural Network Landscapes: Star-Shaped and Geodesic Connectivity","abstract":"One of the most intriguing findings in the structure of neural network landscape is the phenomenon of mode connectivity: For two typical global minima, there exists a path connecting them without barrier. This concept of mode connectivity has played a crucial role in understanding important phenomena in deep learning.   In this paper, we conduct a fine-grained analysis of this connectivity phenomenon. First, we demonstrate that in the overparameterized case, the connecting path can be as simple as a two-piece linear path, and the path length can be nearly equal to the Euclidean distance. This finding suggests that the landscape should be nearly convex in a certain sense. Second, we uncover a surprising star-shaped connectivity: For a finite number of typical minima, there exists a center on minima manifold that connects all of them simultaneously via linear paths. These results are provably valid for linear networks and two-layer ReLU networks under a teacher-student setup, and are empirically supported by models trained on MNIST and CIFAR-10.","sentences":["One of the most intriguing findings in the structure of neural network landscape is the phenomenon of mode connectivity: For two typical global minima, there exists a path connecting them without barrier.","This concept of mode connectivity has played a crucial role in understanding important phenomena in deep learning.   ","In this paper, we conduct a fine-grained analysis of this connectivity phenomenon.","First, we demonstrate that in the overparameterized case, the connecting path can be as simple as a two-piece linear path, and the path length can be nearly equal to the Euclidean distance.","This finding suggests that the landscape should be nearly convex in a certain sense.","Second, we uncover a surprising star-shaped connectivity: For a finite number of typical minima, there exists a center on minima manifold that connects all of them simultaneously via linear paths.","These results are provably valid for linear networks and two-layer ReLU networks under a teacher-student setup, and are empirically supported by models trained on MNIST and CIFAR-10."],"url":"http://arxiv.org/abs/2404.06391v1"}
{"created":"2024-04-09 15:33:09","title":"Latent Distance Guided Alignment Training for Large Language Models","abstract":"Ensuring alignment with human preferences is a crucial characteristic of large language models (LLMs). Presently, the primary alignment methods, RLHF and DPO, require extensive human annotation, which is expensive despite their efficacy. The significant expenses associated with current alignment techniques motivate researchers to investigate the development of annotation-free alignment training methods. In pursuit of improved alignment without relying on external annotation, we introduce Latent Distance Guided Alignment Training (LD-Align). This approach seeks to align the model with a high-quality supervised fine-tune dataset using guidance from a latent space. The latent space is generated through sample reconstruction, akin to auto-encoding. Consequently, we utilize the distance between sample pairs in the latent space to guide DPO-based alignment training. Extensive experimentation and evaluation show the efficacy of our proposed method in achieving notable alignment.","sentences":["Ensuring alignment with human preferences is a crucial characteristic of large language models (LLMs).","Presently, the primary alignment methods, RLHF and DPO, require extensive human annotation, which is expensive despite their efficacy.","The significant expenses associated with current alignment techniques motivate researchers to investigate the development of annotation-free alignment training methods.","In pursuit of improved alignment without relying on external annotation, we introduce Latent Distance Guided Alignment Training (LD-Align).","This approach seeks to align the model with a high-quality supervised fine-tune dataset using guidance from a latent space.","The latent space is generated through sample reconstruction, akin to auto-encoding.","Consequently, we utilize the distance between sample pairs in the latent space to guide DPO-based alignment training.","Extensive experimentation and evaluation show the efficacy of our proposed method in achieving notable alignment."],"url":"http://arxiv.org/abs/2404.06390v1"}
{"created":"2024-04-09 15:29:16","title":"The Power in Communication: Power Regularization of Communication for Autonomy in Cooperative Multi-Agent Reinforcement Learning","abstract":"Communication plays a vital role for coordination in Multi-Agent Reinforcement Learning (MARL) systems. However, misaligned agents can exploit other agents' trust and delegated power to the communication medium. In this paper, we propose power regularization as a method to limit the adverse effects of communication by misaligned agents, specifically communication which impairs the performance of cooperative agents. Power is a measure of the influence one agent's actions have over another agent's policy. By introducing power regularization, we aim to allow designers to control or reduce agents' dependency on communication when appropriate, and make them more resilient to performance deterioration due to misuses of communication. We investigate several environments in which power regularization can be a valuable capability for learning different policies that reduce the effect of power dynamics between agents during communication.","sentences":["Communication plays a vital role for coordination in Multi-Agent Reinforcement Learning (MARL) systems.","However, misaligned agents can exploit other agents' trust and delegated power to the communication medium.","In this paper, we propose power regularization as a method to limit the adverse effects of communication by misaligned agents, specifically communication which impairs the performance of cooperative agents.","Power is a measure of the influence one agent's actions have over another agent's policy.","By introducing power regularization, we aim to allow designers to control or reduce agents' dependency on communication when appropriate, and make them more resilient to performance deterioration due to misuses of communication.","We investigate several environments in which power regularization can be a valuable capability for learning different policies that reduce the effect of power dynamics between agents during communication."],"url":"http://arxiv.org/abs/2404.06387v1"}
{"created":"2024-04-09 15:28:31","title":"Assessing the Understandability and Acceptance of Attack-Defense Trees for Modelling Security Requirements","abstract":"Context and Motivation Attack-Defense Trees (ADTs) are a graphical notation used to model and assess security requirements. ADTs are widely popular, as they can facilitate communication between different stakeholders involved in system security evaluation, and they are formal enough to be verified, e.g., with model checkers. Question/Problem While the quality of this notation has been primarily assessed quantitatively, its understandability has never been evaluated despite being mentioned as a key factor for its success. Principal idea/Results In this paper, we conduct an experiment with 25 human subjects to assess the understandability and user acceptance of the ADT notation. The study focuses on performance-based variables and perception-based variables, with the aim of evaluating the relationship between these measures and how they might impact the practical use of the notation. The results confirm a good level of understandability of ADTs. Participants consider them useful, and they show intention to use them. Contribution This is the first study empirically supporting the understandability of ADTs, thereby contributing to the theory of security requirements engineering.","sentences":["Context and Motivation Attack-Defense Trees (ADTs) are a graphical notation used to model and assess security requirements.","ADTs are widely popular, as they can facilitate communication between different stakeholders involved in system security evaluation, and they are formal enough to be verified, e.g., with model checkers.","Question/Problem While the quality of this notation has been primarily assessed quantitatively, its understandability has never been evaluated despite being mentioned as a key factor for its success.","Principal idea/Results In this paper, we conduct an experiment with 25 human subjects to assess the understandability and user acceptance of the ADT notation.","The study focuses on performance-based variables and perception-based variables, with the aim of evaluating the relationship between these measures and how they might impact the practical use of the notation.","The results confirm a good level of understandability of ADTs.","Participants consider them useful, and they show intention to use them.","Contribution","This is the first study empirically supporting the understandability of ADTs, thereby contributing to the theory of security requirements engineering."],"url":"http://arxiv.org/abs/2404.06386v1"}
{"created":"2024-04-09 15:07:25","title":"Model Generation from Requirements with LLMs: an Exploratory Study","abstract":"Complementing natural language (NL) requirements with graphical models can improve stakeholders' communication and provide directions for system design. However, creating models from requirements involves manual effort. The advent of generative large language models (LLMs), ChatGPT being a notable example, offers promising avenues for automated assistance in model generation. This paper investigates the capability of ChatGPT to generate a specific type of model, i.e., UML sequence diagrams, from NL requirements. We conduct a qualitative study in which we examine the sequence diagrams generated by ChatGPT for 28 requirements documents of various types and from different domains. Observations from the analysis of the generated diagrams have systematically been captured through evaluation logs, and categorized through thematic analysis. Our results indicate that, although the models generally conform to the standard and exhibit a reasonable level of understandability, their completeness and correctness with respect to the specified requirements often present challenges. This issue is particularly pronounced in the presence of requirements smells, such as ambiguity and inconsistency. The insights derived from this study can influence the practical utilization of LLMs in the RE process, and open the door to novel RE-specific prompting strategies targeting effective model generation.","sentences":["Complementing natural language (NL) requirements with graphical models can improve stakeholders' communication and provide directions for system design.","However, creating models from requirements involves manual effort.","The advent of generative large language models (LLMs), ChatGPT being a notable example, offers promising avenues for automated assistance in model generation.","This paper investigates the capability of ChatGPT to generate a specific type of model, i.e., UML sequence diagrams, from NL requirements.","We conduct a qualitative study in which we examine the sequence diagrams generated by ChatGPT for 28 requirements documents of various types and from different domains.","Observations from the analysis of the generated diagrams have systematically been captured through evaluation logs, and categorized through thematic analysis.","Our results indicate that, although the models generally conform to the standard and exhibit a reasonable level of understandability, their completeness and correctness with respect to the specified requirements often present challenges.","This issue is particularly pronounced in the presence of requirements smells, such as ambiguity and inconsistency.","The insights derived from this study can influence the practical utilization of LLMs in the RE process, and open the door to novel RE-specific prompting strategies targeting effective model generation."],"url":"http://arxiv.org/abs/2404.06371v1"}
{"created":"2024-04-09 15:06:25","title":"Enhancing Decision Analysis with a Large Language Model: pyDecision a Comprehensive Library of MCDA Methods in Python","abstract":"Purpose: Multicriteria decision analysis (MCDA) has become increasingly essential for decision-making in complex environments. In response to this need, the pyDecision library, implemented in Python and available at https://bit.ly/3tLFGtH, has been developed to provide a comprehensive and accessible collection of MCDA methods. Methods: The pyDecision offers 70 MCDA methods, including AHP, TOPSIS, and the PROMETHEE and ELECTRE families. Beyond offering a vast range of techniques, the library provides visualization tools for more intuitive results interpretation. In addition to these features, pyDecision has integrated ChatGPT, an advanced Large Language Model, where decision-makers can use ChatGPT to discuss and compare the outcomes of different methods, providing a more interactive and intuitive understanding of the solutions. Findings: Large Language Models are undeniably potent but can sometimes be a double-edged sword. Its answers may be misleading without rigorous verification of its outputs, especially for researchers lacking deep domain expertise. It's imperative to approach its insights with a discerning eye and a solid foundation in the relevant field. Originality: With the integration of MCDA methods and ChatGPT, pyDecision is a significant contribution to the scientific community, as it is an invaluable resource for researchers, practitioners, and decision-makers navigating complex decision-making problems and seeking the most appropriate solutions based on MCDA methods.","sentences":["Purpose:","Multicriteria decision analysis (MCDA) has become increasingly essential for decision-making in complex environments.","In response to this need, the pyDecision library, implemented in Python and available at https://bit.ly/3tLFGtH, has been developed to provide a comprehensive and accessible collection of MCDA methods.","Methods: The pyDecision offers 70 MCDA methods, including AHP, TOPSIS, and the PROMETHEE and ELECTRE families.","Beyond offering a vast range of techniques, the library provides visualization tools for more intuitive results interpretation.","In addition to these features, pyDecision has integrated ChatGPT, an advanced Large Language Model, where decision-makers can use ChatGPT to discuss and compare the outcomes of different methods, providing a more interactive and intuitive understanding of the solutions.","Findings:","Large Language Models are undeniably potent but can sometimes be a double-edged sword.","Its answers may be misleading without rigorous verification of its outputs, especially for researchers lacking deep domain expertise.","It's imperative to approach its insights with a discerning eye and a solid foundation in the relevant field.","Originality: With the integration of MCDA methods and ChatGPT, pyDecision is a significant contribution to the scientific community, as it is an invaluable resource for researchers, practitioners, and decision-makers navigating complex decision-making problems and seeking the most appropriate solutions based on MCDA methods."],"url":"http://arxiv.org/abs/2404.06370v1"}
{"created":"2024-04-09 15:05:48","title":"VISION2UI: A Real-World Dataset with Layout for Code Generation from UI Designs","abstract":"Automatically generating UI code from webpage design visions can significantly alleviate the burden of developers, enabling beginner developers or designers to directly generate Web pages from design diagrams. Currently, prior research has accomplished the objective of generating UI code from rudimentary design visions or sketches through designing deep neural networks. Inspired by the groundbreaking advancements achieved by Multimodal Large Language Models (MLLMs), the automatic generation of UI code from high-fidelity design images is now emerging as a viable possibility. Nevertheless, our investigation reveals that existing MLLMs are hampered by the scarcity of authentic, high-quality, and large-scale datasets, leading to unsatisfactory performance in automated UI code generation. To mitigate this gap, we present a novel dataset, termed VISION2UI, extracted from real-world scenarios, augmented with comprehensive layout information, tailored specifically for finetuning MLLMs in UI code generation. Specifically, this dataset is derived through a series of operations, encompassing collecting, cleaning, and filtering of the open-source Common Crawl dataset. In order to uphold its quality, a neural scorer trained on labeled samples is utilized to refine the data, retaining higher-quality instances. Ultimately, this process yields a dataset comprising 2,000 (Much more is coming soon) parallel samples encompassing design visions and UI code. The dataset is available at https://huggingface.co/datasets/xcodemind/vision2ui.","sentences":["Automatically generating UI code from webpage design visions can significantly alleviate the burden of developers, enabling beginner developers or designers to directly generate Web pages from design diagrams.","Currently, prior research has accomplished the objective of generating UI code from rudimentary design visions or sketches through designing deep neural networks.","Inspired by the groundbreaking advancements achieved by Multimodal Large Language Models (MLLMs), the automatic generation of UI code from high-fidelity design images is now emerging as a viable possibility.","Nevertheless, our investigation reveals that existing MLLMs are hampered by the scarcity of authentic, high-quality, and large-scale datasets, leading to unsatisfactory performance in automated UI code generation.","To mitigate this gap, we present a novel dataset, termed VISION2UI, extracted from real-world scenarios, augmented with comprehensive layout information, tailored specifically for finetuning MLLMs in UI code generation.","Specifically, this dataset is derived through a series of operations, encompassing collecting, cleaning, and filtering of the open-source Common Crawl dataset.","In order to uphold its quality, a neural scorer trained on labeled samples is utilized to refine the data, retaining higher-quality instances.","Ultimately, this process yields a dataset comprising 2,000 (Much more is coming soon) parallel samples encompassing design visions and UI code.","The dataset is available at https://huggingface.co/datasets/xcodemind/vision2ui."],"url":"http://arxiv.org/abs/2404.06369v1"}
{"created":"2024-04-09 15:04:27","title":"ClinLinker: Medical Entity Linking of Clinical Concept Mentions in Spanish","abstract":"Advances in natural language processing techniques, such as named entity recognition and normalization to widely used standardized terminologies like UMLS or SNOMED-CT, along with the digitalization of electronic health records, have significantly advanced clinical text analysis. This study presents ClinLinker, a novel approach employing a two-phase pipeline for medical entity linking that leverages the potential of in-domain adapted language models for biomedical text mining: initial candidate retrieval using a SapBERT-based bi-encoder and subsequent re-ranking with a cross-encoder, trained by following a contrastive-learning strategy to be tailored to medical concepts in Spanish. This methodology, focused initially on content in Spanish, substantially outperforming multilingual language models designed for the same purpose. This is true even for complex scenarios involving heterogeneous medical terminologies and being trained on a subset of the original data. Our results, evaluated using top-k accuracy at 25 and other top-k metrics, demonstrate our approach's performance on two distinct clinical entity linking Gold Standard corpora, DisTEMIST (diseases) and MedProcNER (clinical procedures), outperforming previous benchmarks by 40 points in DisTEMIST and 43 points in MedProcNER, both normalized to SNOMED-CT codes. These findings highlight our approach's ability to address language-specific nuances and set a new benchmark in entity linking, offering a potent tool for enhancing the utility of digital medical records. The resulting system is of practical value, both for large scale automatic generation of structured data derived from clinical records, as well as for exhaustive extraction and harmonization of predefined clinical variables of interest.","sentences":["Advances in natural language processing techniques, such as named entity recognition and normalization to widely used standardized terminologies like UMLS or SNOMED-CT, along with the digitalization of electronic health records, have significantly advanced clinical text analysis.","This study presents ClinLinker, a novel approach employing a two-phase pipeline for medical entity linking that leverages the potential of in-domain adapted language models for biomedical text mining: initial candidate retrieval using a SapBERT-based bi-encoder and subsequent re-ranking with a cross-encoder, trained by following a contrastive-learning strategy to be tailored to medical concepts in Spanish.","This methodology, focused initially on content in Spanish, substantially outperforming multilingual language models designed for the same purpose.","This is true even for complex scenarios involving heterogeneous medical terminologies and being trained on a subset of the original data.","Our results, evaluated using top-k accuracy at 25 and other top-k metrics, demonstrate our approach's performance on two distinct clinical entity linking Gold Standard corpora, DisTEMIST (diseases) and MedProcNER (clinical procedures), outperforming previous benchmarks by 40 points in DisTEMIST and 43 points in MedProcNER, both normalized to SNOMED-CT codes.","These findings highlight our approach's ability to address language-specific nuances and set a new benchmark in entity linking, offering a potent tool for enhancing the utility of digital medical records.","The resulting system is of practical value, both for large scale automatic generation of structured data derived from clinical records, as well as for exhaustive extraction and harmonization of predefined clinical variables of interest."],"url":"http://arxiv.org/abs/2404.06367v1"}
{"created":"2024-04-09 15:02:01","title":"Dynamic Resolution Guidance for Facial Expression Recognition","abstract":"Facial expression recognition (FER) is vital for human-computer interaction and emotion analysis, yet recognizing expressions in low-resolution images remains challenging. This paper introduces a practical method called Dynamic Resolution Guidance for Facial Expression Recognition (DRGFER) to effectively recognize facial expressions in images with varying resolutions without compromising FER model accuracy. Our framework comprises two main components: the Resolution Recognition Network (RRN) and the Multi-Resolution Adaptation Facial Expression Recognition Network (MRAFER). The RRN determines image resolution, outputs a binary vector, and the MRAFER assigns images to suitable facial expression recognition networks based on resolution. We evaluated DRGFER on widely-used datasets RAFDB and FERPlus, demonstrating that our method retains optimal model performance at each resolution and outperforms alternative resolution approaches. The proposed framework exhibits robustness against resolution variations and facial expressions, offering a promising solution for real-world applications.","sentences":["Facial expression recognition (FER) is vital for human-computer interaction and emotion analysis, yet recognizing expressions in low-resolution images remains challenging.","This paper introduces a practical method called Dynamic Resolution Guidance for Facial Expression Recognition (DRGFER) to effectively recognize facial expressions in images with varying resolutions without compromising FER model accuracy.","Our framework comprises two main components: the Resolution Recognition Network (RRN) and the Multi-Resolution Adaptation Facial Expression Recognition Network (MRAFER).","The RRN determines image resolution, outputs a binary vector, and the MRAFER assigns images to suitable facial expression recognition networks based on resolution.","We evaluated DRGFER on widely-used datasets RAFDB and FERPlus, demonstrating that our method retains optimal model performance at each resolution and outperforms alternative resolution approaches.","The proposed framework exhibits robustness against resolution variations and facial expressions, offering a promising solution for real-world applications."],"url":"http://arxiv.org/abs/2404.06365v1"}
{"created":"2024-04-09 15:01:51","title":"SurveyAgent: A Conversational System for Personalized and Efficient Research Survey","abstract":"In the rapidly advancing research fields such as AI, managing and staying abreast of the latest scientific literature has become a significant challenge for researchers. Although previous efforts have leveraged AI to assist with literature searches, paper recommendations, and question-answering, a comprehensive support system that addresses the holistic needs of researchers has been lacking. This paper introduces SurveyAgent, a novel conversational system designed to provide personalized and efficient research survey assistance to researchers. SurveyAgent integrates three key modules: Knowledge Management for organizing papers, Recommendation for discovering relevant literature, and Query Answering for engaging with content on a deeper level. This system stands out by offering a unified platform that supports researchers through various stages of their literature review process, facilitated by a conversational interface that prioritizes user interaction and personalization. Our evaluation demonstrates SurveyAgent's effectiveness in streamlining research activities, showcasing its capability to facilitate how researchers interact with scientific literature.","sentences":["In the rapidly advancing research fields such as AI, managing and staying abreast of the latest scientific literature has become a significant challenge for researchers.","Although previous efforts have leveraged AI to assist with literature searches, paper recommendations, and question-answering, a comprehensive support system that addresses the holistic needs of researchers has been lacking.","This paper introduces SurveyAgent, a novel conversational system designed to provide personalized and efficient research survey assistance to researchers.","SurveyAgent integrates three key modules: Knowledge Management for organizing papers, Recommendation for discovering relevant literature, and Query Answering for engaging with content on a deeper level.","This system stands out by offering a unified platform that supports researchers through various stages of their literature review process, facilitated by a conversational interface that prioritizes user interaction and personalization.","Our evaluation demonstrates SurveyAgent's effectiveness in streamlining research activities, showcasing its capability to facilitate how researchers interact with scientific literature."],"url":"http://arxiv.org/abs/2404.06364v1"}
{"created":"2024-04-09 14:56:34","title":"Test-Time Adaptation with SaLIP: A Cascade of SAM and CLIP for Zero shot Medical Image Segmentation","abstract":"The Segment Anything Model (SAM) and CLIP are remarkable vision foundation models (VFMs). SAM, a prompt driven segmentation model, excels in segmentation tasks across diverse domains, while CLIP is renowned for its zero shot recognition capabilities. However, their unified potential has not yet been explored in medical image segmentation. To adapt SAM to medical imaging, existing methods primarily rely on tuning strategies that require extensive data or prior prompts tailored to the specific task, making it particularly challenging when only a limited number of data samples are available. This work presents an in depth exploration of integrating SAM and CLIP into a unified framework for medical image segmentation. Specifically, we propose a simple unified framework, SaLIP, for organ segmentation. Initially, SAM is used for part based segmentation within the image, followed by CLIP to retrieve the mask corresponding to the region of interest (ROI) from the pool of SAM generated masks. Finally, SAM is prompted by the retrieved ROI to segment a specific organ. Thus, SaLIP is training and fine tuning free and does not rely on domain expertise or labeled data for prompt engineering. Our method shows substantial enhancements in zero shot segmentation, showcasing notable improvements in DICE scores across diverse segmentation tasks like brain (63.46%), lung (50.11%), and fetal head (30.82%), when compared to un prompted SAM. Code and text prompts will be available online.","sentences":["The Segment Anything Model (SAM) and CLIP are remarkable vision foundation models (VFMs).","SAM, a prompt driven segmentation model, excels in segmentation tasks across diverse domains, while CLIP is renowned for its zero shot recognition capabilities.","However, their unified potential has not yet been explored in medical image segmentation.","To adapt SAM to medical imaging, existing methods primarily rely on tuning strategies that require extensive data or prior prompts tailored to the specific task, making it particularly challenging when only a limited number of data samples are available.","This work presents an in depth exploration of integrating SAM and CLIP into a unified framework for medical image segmentation.","Specifically, we propose a simple unified framework, SaLIP, for organ segmentation.","Initially, SAM is used for part based segmentation within the image, followed by CLIP to retrieve the mask corresponding to the region of interest (ROI) from the pool of SAM generated masks.","Finally, SAM is prompted by the retrieved ROI to segment a specific organ.","Thus, SaLIP is training and fine tuning free and does not rely on domain expertise or labeled data for prompt engineering.","Our method shows substantial enhancements in zero shot segmentation, showcasing notable improvements in DICE scores across diverse segmentation tasks like brain (63.46%), lung (50.11%), and fetal head (30.82%), when compared to un prompted SAM.","Code and text prompts will be available online."],"url":"http://arxiv.org/abs/2404.06362v1"}
{"created":"2024-04-09 14:55:48","title":"Meaningfulness and Genericity in a Subsuming Framework","abstract":"This paper studies the notion of meaningfulness for a unifying framework called dBang-calculus, which subsumes both call-by-name (dCbN) and call-by-value (dCbV). We first characterize meaningfulness in dBang by means of typability and inhabitation in an associated non-idempotent intersection type system previously proposed in the literature. We validate the proposed notion of meaningfulness by showing two properties (1) consistency of the theory $\\mathcal{H}$ equating meaningless terms and (2) genericity, stating that meaningless subterms have no bearing on the significance of meaningful terms. The theory $\\mathcal{H}$ is also shown to have a unique consistent and maximal extension. Last but not least, we show that the notions of meaningfulness and genericity in the literature for dCbN and dCbV are subsumed by the respectively ones proposed here for the dBang-calculus.","sentences":["This paper studies the notion of meaningfulness for a unifying framework called dBang-calculus, which subsumes both call-by-name (dCbN) and call-by-value (dCbV).","We first characterize meaningfulness in dBang by means of typability and inhabitation in an associated non-idempotent intersection type system previously proposed in the literature.","We validate the proposed notion of meaningfulness by showing two properties (1) consistency of the theory $\\mathcal{H}$ equating meaningless terms and (2) genericity, stating that meaningless subterms have no bearing on the significance of meaningful terms.","The theory $\\mathcal{H}$ is also shown to have a unique consistent and maximal extension.","Last but not least, we show that the notions of meaningfulness and genericity in the literature for dCbN and dCbV are subsumed by the respectively ones proposed here for the dBang-calculus."],"url":"http://arxiv.org/abs/2404.06361v1"}
{"created":"2024-04-09 14:53:59","title":"Towards Practical Meshlet Compression","abstract":"We propose a codec specifically designed for meshlet compression, optimized for rapid data-parallel GPU decompression within a mesh shader. Our compression strategy orders triangles in optimal generalized triangle strips (GTSs), which we generate by formulating the creation as a mixed integer linear program (MILP). Our method achieves index buffer compression rates of 16:1 compared to the vertex pipeline and crack-free vertex attribute quantization based on user preference. The 15.5 million triangles of our teaser image decompress and render in 0.59 ms on an AMD Radeon RX 7900 XTX.","sentences":["We propose a codec specifically designed for meshlet compression, optimized for rapid data-parallel GPU decompression within a mesh shader.","Our compression strategy orders triangles in optimal generalized triangle strips (GTSs), which we generate by formulating the creation as a mixed integer linear program (MILP).","Our method achieves index buffer compression rates of 16:1 compared to the vertex pipeline and crack-free vertex attribute quantization based on user preference.","The 15.5 million triangles of our teaser image decompress and render in 0.59 ms on an AMD Radeon RX 7900 XTX."],"url":"http://arxiv.org/abs/2404.06359v1"}
{"created":"2024-04-09 14:48:32","title":"Generalizable Sarcasm Detection Is Just Around The Corner, Of Course!","abstract":"We tested the robustness of sarcasm detection models by examining their behavior when fine-tuned on four sarcasm datasets containing varying characteristics of sarcasm: label source (authors vs. third-party), domain (social media/online vs. offline conversations/dialogues), style (aggressive vs. humorous mocking). We tested their prediction performance on the same dataset (intra-dataset) and across different datasets (cross-dataset). For intra-dataset predictions, models consistently performed better when fine-tuned with third-party labels rather than with author labels. For cross-dataset predictions, most models failed to generalize well to the other datasets, implying that one type of dataset cannot represent all sorts of sarcasm with different styles and domains. Compared to the existing datasets, models fine-tuned on the new dataset we release in this work showed the highest generalizability to other datasets. With a manual inspection of the datasets and post-hoc analysis, we attributed the difficulty in generalization to the fact that sarcasm actually comes in different domains and styles. We argue that future sarcasm research should take the broad scope of sarcasm into account.","sentences":["We tested the robustness of sarcasm detection models by examining their behavior when fine-tuned on four sarcasm datasets containing varying characteristics of sarcasm: label source (authors vs. third-party), domain (social media/online vs. offline conversations/dialogues), style (aggressive vs. humorous mocking).","We tested their prediction performance on the same dataset (intra-dataset) and across different datasets (cross-dataset).","For intra-dataset predictions, models consistently performed better when fine-tuned with third-party labels rather than with author labels.","For cross-dataset predictions, most models failed to generalize well to the other datasets, implying that one type of dataset cannot represent all sorts of sarcasm with different styles and domains.","Compared to the existing datasets, models fine-tuned on the new dataset we release in this work showed the highest generalizability to other datasets.","With a manual inspection of the datasets and post-hoc analysis, we attributed the difficulty in generalization to the fact that sarcasm actually comes in different domains and styles.","We argue that future sarcasm research should take the broad scope of sarcasm into account."],"url":"http://arxiv.org/abs/2404.06357v1"}
{"created":"2024-04-09 14:46:48","title":"Policy-Guided Diffusion","abstract":"In many real-world settings, agents must learn from an offline dataset gathered by some prior behavior policy. Such a setting naturally leads to distribution shift between the behavior policy and the target policy being trained - requiring policy conservatism to avoid instability and overestimation bias. Autoregressive world models offer a different solution to this by generating synthetic, on-policy experience. However, in practice, model rollouts must be severely truncated to avoid compounding error. As an alternative, we propose policy-guided diffusion. Our method uses diffusion models to generate entire trajectories under the behavior distribution, applying guidance from the target policy to move synthetic experience further on-policy. We show that policy-guided diffusion models a regularized form of the target distribution that balances action likelihood under both the target and behavior policies, leading to plausible trajectories with high target policy probability, while retaining a lower dynamics error than an offline world model baseline. Using synthetic experience from policy-guided diffusion as a drop-in substitute for real data, we demonstrate significant improvements in performance across a range of standard offline reinforcement learning algorithms and environments. Our approach provides an effective alternative to autoregressive offline world models, opening the door to the controllable generation of synthetic training data.","sentences":["In many real-world settings, agents must learn from an offline dataset gathered by some prior behavior policy.","Such a setting naturally leads to distribution shift between the behavior policy and the target policy being trained - requiring policy conservatism to avoid instability and overestimation bias.","Autoregressive world models offer a different solution to this by generating synthetic, on-policy experience.","However, in practice, model rollouts must be severely truncated to avoid compounding error.","As an alternative, we propose policy-guided diffusion.","Our method uses diffusion models to generate entire trajectories under the behavior distribution, applying guidance from the target policy to move synthetic experience further on-policy.","We show that policy-guided diffusion models a regularized form of the target distribution that balances action likelihood under both the target and behavior policies, leading to plausible trajectories with high target policy probability, while retaining a lower dynamics error than an offline world model baseline.","Using synthetic experience from policy-guided diffusion as a drop-in substitute for real data, we demonstrate significant improvements in performance across a range of standard offline reinforcement learning algorithms and environments.","Our approach provides an effective alternative to autoregressive offline world models, opening the door to the controllable generation of synthetic training data."],"url":"http://arxiv.org/abs/2404.06356v1"}
{"created":"2024-04-09 14:44:12","title":"High Noise Scheduling is a Must","abstract":"Consistency models possess high capabilities for image generation, advancing sampling steps to a single step through their advanced techniques. Current advancements move one step forward consistency training techniques and eliminates the limitation of distillation training. Even though the proposed curriculum and noise scheduling in improved training techniques yield better results than basic consistency models, it lacks well balanced noise distribution and its consistency between curriculum. In this study, it is investigated the balance between high and low noise levels in noise distribution and offered polynomial noise distribution to maintain the stability. This proposed polynomial noise distribution is also supported with a predefined Karras noises to prevent unique noise levels arises with Karras noise generation algorithm. Furthermore, by elimination of learned noisy steps with a curriculum based on sinusoidal function increase the performance of the model in denoising. To make a fair comparison with the latest released consistency model training techniques, experiments are conducted with same hyper-parameters except curriculum and noise distribution. The models utilized during experiments are determined with low depth to prove the robustness of our proposed technique. The results show that the polynomial noise distribution outperforms the model trained with log-normal noise distribution, yielding a 33.54 FID score after 100,000 training steps with constant discretization steps. Additionally, the implementation of a sinusoidal-based curriculum enhances denoising performance, resulting in a FID score of 30.48.","sentences":["Consistency models possess high capabilities for image generation, advancing sampling steps to a single step through their advanced techniques.","Current advancements move one step forward consistency training techniques and eliminates the limitation of distillation training.","Even though the proposed curriculum and noise scheduling in improved training techniques yield better results than basic consistency models, it lacks well balanced noise distribution and its consistency between curriculum.","In this study, it is investigated the balance between high and low noise levels in noise distribution and offered polynomial noise distribution to maintain the stability.","This proposed polynomial noise distribution is also supported with a predefined Karras noises to prevent unique noise levels arises with Karras noise generation algorithm.","Furthermore, by elimination of learned noisy steps with a curriculum based on sinusoidal function increase the performance of the model in denoising.","To make a fair comparison with the latest released consistency model training techniques, experiments are conducted with same hyper-parameters except curriculum and noise distribution.","The models utilized during experiments are determined with low depth to prove the robustness of our proposed technique.","The results show that the polynomial noise distribution outperforms the model trained with log-normal noise distribution, yielding a 33.54 FID score after 100,000 training steps with constant discretization steps.","Additionally, the implementation of a sinusoidal-based curriculum enhances denoising performance, resulting in a FID score of 30.48."],"url":"http://arxiv.org/abs/2404.06353v1"}
{"created":"2024-04-09 14:43:19","title":"DaF-BEVSeg: Distortion-aware Fisheye Camera based Bird's Eye View Segmentation with Occlusion Reasoning","abstract":"Semantic segmentation is an effective way to perform scene understanding. Recently, segmentation in 3D Bird's Eye View (BEV) space has become popular as its directly used by drive policy. However, there is limited work on BEV segmentation for surround-view fisheye cameras, commonly used in commercial vehicles. As this task has no real-world public dataset and existing synthetic datasets do not handle amodal regions due to occlusion, we create a synthetic dataset using the Cognata simulator comprising diverse road types, weather, and lighting conditions. We generalize the BEV segmentation to work with any camera model; this is useful for mixing diverse cameras. We implement a baseline by applying cylindrical rectification on the fisheye images and using a standard LSS-based BEV segmentation model. We demonstrate that we can achieve better performance without undistortion, which has the adverse effects of increased runtime due to pre-processing, reduced field-of-view, and resampling artifacts. Further, we introduce a distortion-aware learnable BEV pooling strategy that is more effective for the fisheye cameras. We extend the model with an occlusion reasoning module, which is critical for estimating in BEV space. Qualitative performance of DaF-BEVSeg is showcased in the video at https://streamable.com/ge4v51.","sentences":["Semantic segmentation is an effective way to perform scene understanding.","Recently, segmentation in 3D Bird's Eye View (BEV) space has become popular as its directly used by drive policy.","However, there is limited work on BEV segmentation for surround-view fisheye cameras, commonly used in commercial vehicles.","As this task has no real-world public dataset and existing synthetic datasets do not handle amodal regions due to occlusion, we create a synthetic dataset using the Cognata simulator comprising diverse road types, weather, and lighting conditions.","We generalize the BEV segmentation to work with any camera model; this is useful for mixing diverse cameras.","We implement a baseline by applying cylindrical rectification on the fisheye images and using a standard LSS-based BEV segmentation model.","We demonstrate that we can achieve better performance without undistortion, which has the adverse effects of increased runtime due to pre-processing, reduced field-of-view, and resampling artifacts.","Further, we introduce a distortion-aware learnable BEV pooling strategy that is more effective for the fisheye cameras.","We extend the model with an occlusion reasoning module, which is critical for estimating in BEV space.","Qualitative performance of DaF-BEVSeg is showcased in the video at https://streamable.com/ge4v51."],"url":"http://arxiv.org/abs/2404.06352v1"}
{"created":"2024-04-09 14:42:31","title":"HPNet: Dynamic Trajectory Forecasting with Historical Prediction Attention","abstract":"Predicting the trajectories of road agents is essential for autonomous driving systems. The recent mainstream methods follow a static paradigm, which predicts the future trajectory by using a fixed duration of historical frames. These methods make the predictions independently even at adjacent time steps, which leads to potential instability and temporal inconsistency. As successive time steps have largely overlapping historical frames, their forecasting should have intrinsic correlation, such as overlapping predicted trajectories should be consistent, or be different but share the same motion goal depending on the road situation. Motivated by this, in this work, we introduce HPNet, a novel dynamic trajectory forecasting method. Aiming for stable and accurate trajectory forecasting, our method leverages not only historical frames including maps and agent states, but also historical predictions. Specifically, we newly design a Historical Prediction Attention module to automatically encode the dynamic relationship between successive predictions. Besides, it also extends the attention range beyond the currently visible window benefitting from the use of historical predictions. The proposed Historical Prediction Attention together with the Agent Attention and Mode Attention is further formulated as the Triple Factorized Attention module, serving as the core design of HPNet.Experiments on the Argoverse and INTERACTION datasets show that HPNet achieves state-of-the-art performance, and generates accurate and stable future trajectories. Our code are available at https://github.com/XiaolongTang23/HPNet.","sentences":["Predicting the trajectories of road agents is essential for autonomous driving systems.","The recent mainstream methods follow a static paradigm, which predicts the future trajectory by using a fixed duration of historical frames.","These methods make the predictions independently even at adjacent time steps, which leads to potential instability and temporal inconsistency.","As successive time steps have largely overlapping historical frames, their forecasting should have intrinsic correlation, such as overlapping predicted trajectories should be consistent, or be different but share the same motion goal depending on the road situation.","Motivated by this, in this work, we introduce HPNet, a novel dynamic trajectory forecasting method.","Aiming for stable and accurate trajectory forecasting, our method leverages not only historical frames including maps and agent states, but also historical predictions.","Specifically, we newly design a Historical Prediction Attention module to automatically encode the dynamic relationship between successive predictions.","Besides, it also extends the attention range beyond the currently visible window benefitting from the use of historical predictions.","The proposed Historical Prediction Attention together with the Agent Attention and Mode Attention is further formulated as the Triple Factorized Attention module, serving as the core design of HPNet.","Experiments on the Argoverse and INTERACTION datasets show that HPNet achieves state-of-the-art performance, and generates accurate and stable future trajectories.","Our code are available at https://github.com/XiaolongTang23/HPNet."],"url":"http://arxiv.org/abs/2404.06351v1"}
{"created":"2024-04-09 14:40:54","title":"Rolling Shutter Correction with Intermediate Distortion Flow Estimation","abstract":"This paper proposes to correct the rolling shutter (RS) distorted images by estimating the distortion flow from the global shutter (GS) to RS directly. Existing methods usually perform correction using the undistortion flow from the RS to GS. They initially predict the flow from consecutive RS frames, subsequently rescaling it as the displacement fields from the RS frame to the underlying GS image using time-dependent scaling factors. Following this, RS-aware forward warping is employed to convert the RS image into its GS counterpart. Nevertheless, this strategy is prone to two shortcomings. First, the undistortion flow estimation is rendered inaccurate by merely linear scaling the flow, due to the complex non-linear motion nature. Second, RS-aware forward warping often results in unavoidable artifacts. To address these limitations, we introduce a new framework that directly estimates the distortion flow and rectifies the RS image with the backward warping operation. More specifically, we first propose a global correlation-based flow attention mechanism to estimate the initial distortion flow and GS feature jointly, which are then refined by the following coarse-to-fine decoder layers. Additionally, a multi-distortion flow prediction strategy is integrated to mitigate the issue of inaccurate flow estimation further. Experimental results validate the effectiveness of the proposed method, which outperforms state-of-the-art approaches on various benchmarks while maintaining high efficiency. The project is available at \\url{https://github.com/ljzycmd/DFRSC}.","sentences":["This paper proposes to correct the rolling shutter","(RS) distorted images by estimating the distortion flow from the global shutter (GS) to RS directly.","Existing methods usually perform correction using the undistortion flow from the RS to GS.","They initially predict the flow from consecutive RS frames, subsequently rescaling it as the displacement fields from the RS frame to the underlying GS image using time-dependent scaling factors.","Following this, RS-aware forward warping is employed to convert the RS image into its GS counterpart.","Nevertheless, this strategy is prone to two shortcomings.","First, the undistortion flow estimation is rendered inaccurate by merely linear scaling the flow, due to the complex non-linear motion nature.","Second, RS-aware forward warping often results in unavoidable artifacts.","To address these limitations, we introduce a new framework that directly estimates the distortion flow and rectifies the RS image with the backward warping operation.","More specifically, we first propose a global correlation-based flow attention mechanism to estimate the initial distortion flow and GS feature jointly, which are then refined by the following coarse-to-fine decoder layers.","Additionally, a multi-distortion flow prediction strategy is integrated to mitigate the issue of inaccurate flow estimation further.","Experimental results validate the effectiveness of the proposed method, which outperforms state-of-the-art approaches on various benchmarks while maintaining high efficiency.","The project is available at \\url{https://github.com/ljzycmd/DFRSC}."],"url":"http://arxiv.org/abs/2404.06350v1"}
{"created":"2024-04-09 14:40:08","title":"CausalBench: A Comprehensive Benchmark for Causal Learning Capability of Large Language Models","abstract":"Causality reveals fundamental principles behind data distributions in real-world scenarios, and the capability of large language models (LLMs) to understand causality directly impacts their efficacy across explaining outputs, adapting to new evidence, and generating counterfactuals. With the proliferation of LLMs, the evaluation of this capacity is increasingly garnering attention. However, the absence of a comprehensive benchmark has rendered existing evaluation studies being straightforward, undiversified, and homogeneous. To address these challenges, this paper proposes a comprehensive benchmark, namely CausalBench, to evaluate the causality understanding capabilities of LLMs. Originating from the causal research community, CausalBench encompasses three causal learning-related tasks, which facilitate a convenient comparison of LLMs' performance with classic causal learning algorithms. Meanwhile, causal networks of varying scales and densities are integrated in CausalBench, to explore the upper limits of LLMs' capabilities across task scenarios of varying difficulty. Notably, background knowledge and structured data are also incorporated into CausalBench to thoroughly unlock the underlying potential of LLMs for long-text comprehension and prior information utilization. Based on CausalBench, this paper evaluates nineteen leading LLMs and unveils insightful conclusions in diverse aspects. Firstly, we present the strengths and weaknesses of LLMs and quantitatively explore the upper limits of their capabilities across various scenarios. Meanwhile, we further discern the adaptability and abilities of LLMs to specific structural networks and complex chain of thought structures. Moreover, this paper quantitatively presents the differences across diverse information sources and uncovers the gap between LLMs' capabilities in causal understanding within textual contexts and numerical domains.","sentences":["Causality reveals fundamental principles behind data distributions in real-world scenarios, and the capability of large language models (LLMs) to understand causality directly impacts their efficacy across explaining outputs, adapting to new evidence, and generating counterfactuals.","With the proliferation of LLMs, the evaluation of this capacity is increasingly garnering attention.","However, the absence of a comprehensive benchmark has rendered existing evaluation studies being straightforward, undiversified, and homogeneous.","To address these challenges, this paper proposes a comprehensive benchmark, namely CausalBench, to evaluate the causality understanding capabilities of LLMs.","Originating from the causal research community, CausalBench encompasses three causal learning-related tasks, which facilitate a convenient comparison of LLMs' performance with classic causal learning algorithms.","Meanwhile, causal networks of varying scales and densities are integrated in CausalBench, to explore the upper limits of LLMs' capabilities across task scenarios of varying difficulty.","Notably, background knowledge and structured data are also incorporated into CausalBench to thoroughly unlock the underlying potential of LLMs for long-text comprehension and prior information utilization.","Based on CausalBench, this paper evaluates nineteen leading LLMs and unveils insightful conclusions in diverse aspects.","Firstly, we present the strengths and weaknesses of LLMs and quantitatively explore the upper limits of their capabilities across various scenarios.","Meanwhile, we further discern the adaptability and abilities of LLMs to specific structural networks and complex chain of thought structures.","Moreover, this paper quantitatively presents the differences across diverse information sources and uncovers the gap between LLMs' capabilities in causal understanding within textual contexts and numerical domains."],"url":"http://arxiv.org/abs/2404.06349v1"}
{"created":"2024-04-09 14:34:48","title":"RAR-b: Reasoning as Retrieval Benchmark","abstract":"Semantic textual similartiy (STS) and information retrieval tasks (IR) tasks have been the two major avenues to record the progress of embedding models in the past few years. Under the emerging Retrieval-augmented Generation (RAG) paradigm, we envision the need to evaluate next-level language understanding abilities of embedding models, and take a conscious look at the reasoning abilities stored in them. Addressing this, we pose the question: Can retrievers solve reasoning problems? By transforming reasoning tasks into retrieval tasks, we find that without specifically trained for reasoning-level language understanding, current state-of-the-art retriever models may still be far from being competent for playing the role of assisting LLMs, especially in reasoning-intensive tasks. Moreover, albeit trained to be aware of instructions, instruction-aware IR models are often better off without instructions in inference time for reasoning tasks, posing an overlooked retriever-LLM behavioral gap for the research community to align. However, recent decoder-based embedding models show great promise in narrowing the gap, highlighting the pathway for embedding models to achieve reasoning-level language understanding. We also show that, although current off-the-shelf re-ranker models fail on these tasks, injecting reasoning abilities into them through fine-tuning still appears easier than doing so to bi-encoders, and we are able to achieve state-of-the-art performance across all tasks by fine-tuning a reranking model. We release Reasoning as Retrieval Benchmark (RAR-b), a holistic suite of tasks and settings to evaluate the reasoning abilities stored in retriever models. RAR-b is available at https://github.com/gowitheflow-1998/RAR-b.","sentences":["Semantic textual similartiy (STS) and information retrieval tasks (IR) tasks have been the two major avenues to record the progress of embedding models in the past few years.","Under the emerging Retrieval-augmented Generation (RAG) paradigm, we envision the need to evaluate next-level language understanding abilities of embedding models, and take a conscious look at the reasoning abilities stored in them.","Addressing this, we pose the question: Can retrievers solve reasoning problems?","By transforming reasoning tasks into retrieval tasks, we find that without specifically trained for reasoning-level language understanding, current state-of-the-art retriever models may still be far from being competent for playing the role of assisting LLMs, especially in reasoning-intensive tasks.","Moreover, albeit trained to be aware of instructions, instruction-aware IR models are often better off without instructions in inference time for reasoning tasks, posing an overlooked retriever-LLM behavioral gap for the research community to align.","However, recent decoder-based embedding models show great promise in narrowing the gap, highlighting the pathway for embedding models to achieve reasoning-level language understanding.","We also show that, although current off-the-shelf re-ranker models fail on these tasks, injecting reasoning abilities into them through fine-tuning still appears easier than doing so to bi-encoders, and we are able to achieve state-of-the-art performance across all tasks by fine-tuning a reranking model.","We release Reasoning as Retrieval Benchmark (RAR-b), a holistic suite of tasks and settings to evaluate the reasoning abilities stored in retriever models.","RAR-b is available at https://github.com/gowitheflow-1998/RAR-b."],"url":"http://arxiv.org/abs/2404.06347v1"}
{"created":"2024-04-09 14:33:16","title":"AgentsCoDriver: Large Language Model Empowered Collaborative Driving with Lifelong Learning","abstract":"Connected and autonomous driving is developing rapidly in recent years. However, current autonomous driving systems, which are primarily based on data-driven approaches, exhibit deficiencies in interpretability, generalization, and continuing learning capabilities. In addition, the single-vehicle autonomous driving systems lack of the ability of collaboration and negotiation with other vehicles, which is crucial for the safety and efficiency of autonomous driving systems. In order to address these issues, we leverage large language models (LLMs) to develop a novel framework, AgentsCoDriver, to enable multiple vehicles to conduct collaborative driving. AgentsCoDriver consists of five modules: observation module, reasoning engine, cognitive memory module, reinforcement reflection module, and communication module. It can accumulate knowledge, lessons, and experiences over time by continuously interacting with the environment, thereby making itself capable of lifelong learning. In addition, by leveraging the communication module, different agents can exchange information and realize negotiation and collaboration in complex traffic environments. Extensive experiments are conducted and show the superiority of AgentsCoDriver.","sentences":["Connected and autonomous driving is developing rapidly in recent years.","However, current autonomous driving systems, which are primarily based on data-driven approaches, exhibit deficiencies in interpretability, generalization, and continuing learning capabilities.","In addition, the single-vehicle autonomous driving systems lack of the ability of collaboration and negotiation with other vehicles, which is crucial for the safety and efficiency of autonomous driving systems.","In order to address these issues, we leverage large language models (LLMs) to develop a novel framework, AgentsCoDriver, to enable multiple vehicles to conduct collaborative driving.","AgentsCoDriver consists of five modules: observation module, reasoning engine, cognitive memory module, reinforcement reflection module, and communication module.","It can accumulate knowledge, lessons, and experiences over time by continuously interacting with the environment, thereby making itself capable of lifelong learning.","In addition, by leveraging the communication module, different agents can exchange information and realize negotiation and collaboration in complex traffic environments.","Extensive experiments are conducted and show the superiority of AgentsCoDriver."],"url":"http://arxiv.org/abs/2404.06345v1"}
{"created":"2024-04-09 14:33:03","title":"Synaptogen: A cross-domain generative device model for large-scale neuromorphic circuit design","abstract":"We present a fast generative modeling approach for resistive memories that reproduces the complex statistical properties of real-world devices. To enable efficient modeling of analog circuits, the model is implemented in Verilog-A. By training on extensive measurement data of integrated 1T1R arrays (6,000 cycles of 512 devices), an autoregressive stochastic process accurately accounts for the cross-correlations between the switching parameters, while non-linear transformations ensure agreement with both cycle-to-cycle (C2C) and device-to-device (D2D) variability. Benchmarks show that this statistically comprehensive model achieves read/write throughputs exceeding those of even highly simplified and deterministic compact models.","sentences":["We present a fast generative modeling approach for resistive memories that reproduces the complex statistical properties of real-world devices.","To enable efficient modeling of analog circuits, the model is implemented in Verilog-A. By training on extensive measurement data of integrated 1T1R arrays (6,000 cycles of 512 devices), an autoregressive stochastic process accurately accounts for the cross-correlations between the switching parameters, while non-linear transformations ensure agreement with both cycle-to-cycle (C2C) and device-to-device (D2D) variability.","Benchmarks show that this statistically comprehensive model achieves read/write throughputs exceeding those of even highly simplified and deterministic compact models."],"url":"http://arxiv.org/abs/2404.06344v1"}
{"created":"2024-04-09 14:25:31","title":"Experimental System Design of an Active Fault-Tolerant Quadrotor","abstract":"Quadrotors have gained popularity over the last decade, aiding humans in complex tasks such as search and rescue, mapping and exploration. Despite their mechanical simplicity and versatility compared to other types of aerial vehicles, they remain vulnerable to rotor failures. In this paper, we propose an algorithmic and mechanical approach to addressing the quadrotor fault-tolerant problem in case of rotor failures. First, we present a fault-tolerant detection and control scheme that includes various attitude error metrics. The scheme transitions to a fault-tolerant control mode by surrendering the yaw control. Subsequently, to ensure compatibility with platform sensing constraints, we investigate the relationship between variations in robot rotational drag, achieved through a modular mechanical design appendage, resulting in yaw rates within sensor limits. This analysis offers a platform-agnostic framework for designing more reliable and robust quadrotors in the event of rotor failures. Extensive experimental results validate the proposed approach providing insights into successfully designing a cost-effective quadrotor capable of fault-tolerant control. The overall design enhances safety in scenarios of faulty rotors, without the need for additional sensors or computational resources.","sentences":["Quadrotors have gained popularity over the last decade, aiding humans in complex tasks such as search and rescue, mapping and exploration.","Despite their mechanical simplicity and versatility compared to other types of aerial vehicles, they remain vulnerable to rotor failures.","In this paper, we propose an algorithmic and mechanical approach to addressing the quadrotor fault-tolerant problem in case of rotor failures.","First, we present a fault-tolerant detection and control scheme that includes various attitude error metrics.","The scheme transitions to a fault-tolerant control mode by surrendering the yaw control.","Subsequently, to ensure compatibility with platform sensing constraints, we investigate the relationship between variations in robot rotational drag, achieved through a modular mechanical design appendage, resulting in yaw rates within sensor limits.","This analysis offers a platform-agnostic framework for designing more reliable and robust quadrotors in the event of rotor failures.","Extensive experimental results validate the proposed approach providing insights into successfully designing a cost-effective quadrotor capable of fault-tolerant control.","The overall design enhances safety in scenarios of faulty rotors, without the need for additional sensors or computational resources."],"url":"http://arxiv.org/abs/2404.06340v1"}
{"created":"2024-04-09 14:25:27","title":"Finding fake reviews in e-commerce platforms by using hybrid algorithms","abstract":"Sentiment analysis, a vital component in natural language processing, plays a crucial role in understanding the underlying emotions and opinions expressed in textual data. In this paper, we propose an innovative ensemble approach for sentiment analysis for finding fake reviews that amalgamate the predictive capabilities of Support Vector Machine (SVM), K-Nearest Neighbors (KNN), and Decision Tree classifiers. Our ensemble architecture strategically combines these diverse models to capitalize on their strengths while mitigating inherent weaknesses, thereby achieving superior accuracy and robustness in fake review prediction. By combining all the models of our classifiers, the predictive performance is boosted and it also fosters adaptability to varied linguistic patterns and nuances present in real-world datasets. The metrics accounted for on fake reviews demonstrate the efficacy and competitiveness of the proposed ensemble method against traditional single-model approaches. Our findings underscore the potential of ensemble techniques in advancing the state-of-the-art in finding fake reviews using hybrid algorithms, with implications for various applications in different social media and e-platforms to find the best reviews and neglect the fake ones, eliminating puffery and bluffs.","sentences":["Sentiment analysis, a vital component in natural language processing, plays a crucial role in understanding the underlying emotions and opinions expressed in textual data.","In this paper, we propose an innovative ensemble approach for sentiment analysis for finding fake reviews that amalgamate the predictive capabilities of Support Vector Machine (SVM), K-Nearest Neighbors (KNN), and Decision Tree classifiers.","Our ensemble architecture strategically combines these diverse models to capitalize on their strengths while mitigating inherent weaknesses, thereby achieving superior accuracy and robustness in fake review prediction.","By combining all the models of our classifiers, the predictive performance is boosted and it also fosters adaptability to varied linguistic patterns and nuances present in real-world datasets.","The metrics accounted for on fake reviews demonstrate the efficacy and competitiveness of the proposed ensemble method against traditional single-model approaches.","Our findings underscore the potential of ensemble techniques in advancing the state-of-the-art in finding fake reviews using hybrid algorithms, with implications for various applications in different social media and e-platforms to find the best reviews and neglect the fake ones, eliminating puffery and bluffs."],"url":"http://arxiv.org/abs/2404.06339v1"}
{"created":"2024-04-09 14:22:50","title":"Matching 2D Images in 3D: Metric Relative Pose from Metric Correspondences","abstract":"Given two images, we can estimate the relative camera pose between them by establishing image-to-image correspondences. Usually, correspondences are 2D-to-2D and the pose we estimate is defined only up to scale. Some applications, aiming at instant augmented reality anywhere, require scale-metric pose estimates, and hence, they rely on external depth estimators to recover the scale. We present MicKey, a keypoint matching pipeline that is able to predict metric correspondences in 3D camera space. By learning to match 3D coordinates across images, we are able to infer the metric relative pose without depth measurements. Depth measurements are also not required for training, nor are scene reconstructions or image overlap information. MicKey is supervised only by pairs of images and their relative poses. MicKey achieves state-of-the-art performance on the Map-Free Relocalisation benchmark while requiring less supervision than competing approaches.","sentences":["Given two images, we can estimate the relative camera pose between them by establishing image-to-image correspondences.","Usually, correspondences are 2D-to-2D and the pose we estimate is defined only up to scale.","Some applications, aiming at instant augmented reality anywhere, require scale-metric pose estimates, and hence, they rely on external depth estimators to recover the scale.","We present MicKey, a keypoint matching pipeline that is able to predict metric correspondences in 3D camera space.","By learning to match 3D coordinates across images, we are able to infer the metric relative pose without depth measurements.","Depth measurements are also not required for training, nor are scene reconstructions or image overlap information.","MicKey is supervised only by pairs of images and their relative poses.","MicKey achieves state-of-the-art performance on the Map-Free Relocalisation benchmark while requiring less supervision than competing approaches."],"url":"http://arxiv.org/abs/2404.06337v1"}
{"created":"2024-04-09 14:08:47","title":"Generative Pre-Trained Transformer for Symbolic Regression Base In-Context Reinforcement Learning","abstract":"The mathematical formula is the human language to describe nature and is the essence of scientific research. Finding mathematical formulas from observational data is a major demand of scientific research and a major challenge of artificial intelligence. This area is called symbolic regression. Originally symbolic regression was often formulated as a combinatorial optimization problem and solved using GP or reinforcement learning algorithms. These two kinds of algorithms have strong noise robustness ability and good Versatility. However, inference time usually takes a long time, so the search efficiency is relatively low. Later, based on large-scale pre-training data proposed, such methods use a large number of synthetic data points and expression pairs to train a Generative Pre-Trained Transformer(GPT). Then this GPT can only need to perform one forward propagation to obtain the results, the advantage is that the inference speed is very fast. However, its performance is very dependent on the training data and performs poorly on data outside the training set, which leads to poor noise robustness and Versatility of such methods. So, can we combine the advantages of the above two categories of SR algorithms? In this paper, we propose \\textbf{FormulaGPT}, which trains a GPT using massive sparse reward learning histories of reinforcement learning-based SR algorithms as training data. After training, the SR algorithm based on reinforcement learning is distilled into a Transformer. When new test data comes, FormulaGPT can directly generate a \"reinforcement learning process\" and automatically update the learning policy in context. Tested on more than ten datasets including SRBench, formulaGPT achieves the state-of-the-art performance in fitting ability compared with four baselines. In addition, it achieves satisfactory results in noise robustness, versatility, and inference efficiency.","sentences":["The mathematical formula is the human language to describe nature and is the essence of scientific research.","Finding mathematical formulas from observational data is a major demand of scientific research and a major challenge of artificial intelligence.","This area is called symbolic regression.","Originally symbolic regression was often formulated as a combinatorial optimization problem and solved using GP or reinforcement learning algorithms.","These two kinds of algorithms have strong noise robustness ability and good Versatility.","However, inference time usually takes a long time, so the search efficiency is relatively low.","Later, based on large-scale pre-training data proposed, such methods use a large number of synthetic data points and expression pairs to train a Generative Pre-Trained Transformer(GPT).","Then this GPT can only need to perform one forward propagation to obtain the results, the advantage is that the inference speed is very fast.","However, its performance is very dependent on the training data and performs poorly on data outside the training set, which leads to poor noise robustness and Versatility of such methods.","So, can we combine the advantages of the above two categories of SR algorithms?","In this paper, we propose \\textbf{FormulaGPT}, which trains a GPT using massive sparse reward learning histories of reinforcement learning-based SR algorithms as training data.","After training, the SR algorithm based on reinforcement learning is distilled into a Transformer.","When new test data comes, FormulaGPT can directly generate a \"reinforcement learning process\" and automatically update the learning policy in context.","Tested on more than ten datasets including SRBench, formulaGPT achieves the state-of-the-art performance in fitting ability compared with four baselines.","In addition, it achieves satisfactory results in noise robustness, versatility, and inference efficiency."],"url":"http://arxiv.org/abs/2404.06330v1"}
{"created":"2024-04-09 14:04:33","title":"Characterizations of Sparsifiability for Affine CSPs and Symmetric CSPs","abstract":"CSP sparsification, introduced by Kogan and Krauthgamer (ITCS 2015), considers the following question: when can an instance of a constraint satisfaction problem be sparsified (by retaining a weighted subset of the constraints) while still roughly capturing the weight of constraints satisfied by {\\em every} assignment. CSP sparsification generalizes and abstracts other commonly studied problems including graph cut-sparsification, hypergraph cut-sparsification and hypergraph XOR-sparsification. A central question here is to understand what properties of a constraint predicate $P:\\Sigma^r \\to \\{0,1\\}$ (where variables are assigned values in $\\Sigma$) allow for nearly linear-size sparsifiers (in the number of variables). In this work (1) we significantly extend the class of CSPs for which nearly linear-size, and other non-trivial, sparsifications exist and give classifications in some broad settings and (2) give a polynomial-time algorithm to extract this sparsification.   Our results captured in item (1) completely classify all symmetric Boolean predicates $P$ (i.e., on the Boolean domain $\\Sigma = \\{0,1\\}$) that allow nearly-linear-size sparsifications. Symmetric Boolean CSPs already capture all the special classes of sparisifcation listed above including hypergraph cut-sparsification and variants. Our study of symmetric CSPs reveals an inherent, previously undetected, number-theoretic phenomenon that determines near-linear size sparsifiability. We also completely classify the set of Boolean predicates $P$ that allow non-trivial ($o(n^r)$-size) sparsifications, thus answering an open question from the work of Kogan and Krauthgamer.","sentences":["CSP sparsification, introduced by Kogan and Krauthgamer (ITCS 2015), considers the following question: when can an instance of a constraint satisfaction problem be sparsified (by retaining a weighted subset of the constraints) while still roughly capturing the weight of constraints satisfied by {\\em every} assignment.","CSP sparsification generalizes and abstracts other commonly studied problems including graph cut-sparsification, hypergraph cut-sparsification and hypergraph XOR-sparsification.","A central question here is to understand what properties of a constraint predicate $P:\\Sigma^r \\to \\{0,1\\}$ (where variables are assigned values in $\\Sigma$) allow for nearly linear-size sparsifiers (in the number of variables).","In this work (1) we significantly extend the class of CSPs for which nearly linear-size, and other non-trivial, sparsifications exist and give classifications in some broad settings and (2) give a polynomial-time algorithm to extract this sparsification.   ","Our results captured in item (1) completely classify all symmetric Boolean predicates $P$ (i.e., on the Boolean domain $\\Sigma = \\{0,1\\}$) that allow nearly-linear-size sparsifications.","Symmetric Boolean CSPs already capture all the special classes of sparisifcation listed above including hypergraph cut-sparsification and variants.","Our study of symmetric CSPs reveals an inherent, previously undetected, number-theoretic phenomenon that determines near-linear size sparsifiability.","We also completely classify the set of Boolean predicates $P$ that allow non-trivial ($o(n^r)$-size) sparsifications, thus answering an open question from the work of Kogan and Krauthgamer."],"url":"http://arxiv.org/abs/2404.06327v1"}
{"created":"2024-04-09 14:04:26","title":"What is the $\\textit{intrinsic}$ dimension of your binary data? -- and how to compute it quickly","abstract":"Dimensionality is an important aspect for analyzing and understanding (high-dimensional) data. In their 2006 ICDM paper Tatti et al. answered the question for a (interpretable) dimension of binary data tables by introducing a normalized correlation dimension. In the present work we revisit their results and contrast them with a concept based notion of intrinsic dimension (ID) recently introduced for geometric data sets. To do this, we present a novel approximation for this ID that is based on computing concepts only up to a certain support value. We demonstrate and evaluate our approximation using all available datasets from Tatti et al., which have between 469 and 41271 extrinsic dimensions.","sentences":["Dimensionality is an important aspect for analyzing and understanding (high-dimensional) data.","In their 2006 ICDM paper Tatti et al. answered the question for a (interpretable) dimension of binary data tables by introducing a normalized correlation dimension.","In the present work we revisit their results and contrast them with a concept based notion of intrinsic dimension (ID) recently introduced for geometric data sets.","To do this, we present a novel approximation for this ID that is based on computing concepts only up to a certain support value.","We demonstrate and evaluate our approximation using all available datasets from Tatti et al., which have between 469 and 41271 extrinsic dimensions."],"url":"http://arxiv.org/abs/2404.06326v1"}
{"created":"2024-04-09 14:03:38","title":"Automatically Learning HTN Methods from Landmarks","abstract":"Hierarchical Task Network (HTN) planning usually requires a domain engineer to provide manual input about how to decompose a planning problem. Even HTN-MAKER, a well-known method-learning algorithm, requires a domain engineer to annotate the tasks with information about what to learn. We introduce CURRICULAMA, an HTN method learning algorithm that completely automates the learning process. It uses landmark analysis to compose annotated tasks and leverages curriculum learning to order the learning of methods from simpler to more complex. This eliminates the need for manual input, resolving a core issue with HTN-MAKER. We prove CURRICULAMA's soundness, and show experimentally that it has a substantially similar convergence rate in learning a complete set of methods to HTN-MAKER.","sentences":["Hierarchical Task Network (HTN) planning usually requires a domain engineer to provide manual input about how to decompose a planning problem.","Even HTN-MAKER, a well-known method-learning algorithm, requires a domain engineer to annotate the tasks with information about what to learn.","We introduce CURRICULAMA, an HTN method learning algorithm that completely automates the learning process.","It uses landmark analysis to compose annotated tasks and leverages curriculum learning to order the learning of methods from simpler to more complex.","This eliminates the need for manual input, resolving a core issue with HTN-MAKER.","We prove CURRICULAMA's soundness, and show experimentally that it has a substantially similar convergence rate in learning a complete set of methods to HTN-MAKER."],"url":"http://arxiv.org/abs/2404.06325v1"}
{"created":"2024-04-09 14:03:04","title":"Dynamic D2D-Assisted Federated Learning over O-RAN: Performance Analysis, MAC Scheduler, and Asymmetric User Selection","abstract":"Existing studies on federated learning (FL) are mostly focused on system orchestration for static snapshots of the network and making static control decisions (e.g., spectrum allocation). However, real-world wireless networks are susceptible to temporal variations of wireless channel capacity and users' datasets. In this paper, we incorporate multi-granular system dynamics (MSDs) into FL, including (M1) dynamic wireless channel capacity, captured by a set of discrete-time events, called $\\mathscr{D}$-Events, and (M2) dynamic datasets of users. The latter is characterized by (M2-a) modeling the dynamics of user's dataset size via an ordinary differential equation and (M2-b) introducing dynamic model drift}, formulated via a partial differential inequality} drawing concrete analytical connections between the dynamics of users' datasets and FL accuracy. We then conduct FL orchestration under MSDs by introducing dynamic cooperative FL with dedicated MAC schedulers (DCLM), exploiting the unique features of open radio access network (O-RAN). DCLM proposes (i) a hierarchical device-to-device (D2D)-assisted model training, (ii) dynamic control decisions through dedicated O-RAN MAC schedulers, and (iii) asymmetric user selection. We provide extensive theoretical analysis to study the convergence of DCLM. We then optimize the degrees of freedom (e.g., user selection and spectrum allocation) in DCLM through a highly non-convex optimization problem. We develop a systematic approach to obtain the solution for this problem, opening the door to solving a broad variety of network-aware FL optimization problems. We show the efficiency of DCLM via numerical simulations and provide a series of future directions.","sentences":["Existing studies on federated learning (FL) are mostly focused on system orchestration for static snapshots of the network and making static control decisions (e.g., spectrum allocation).","However, real-world wireless networks are susceptible to temporal variations of wireless channel capacity and users' datasets.","In this paper, we incorporate multi-granular system dynamics (MSDs) into FL, including (M1) dynamic wireless channel capacity, captured by a set of discrete-time events, called $\\mathscr{D}$-Events, and (M2) dynamic datasets of users.","The latter is characterized by (M2-a) modeling the dynamics of user's dataset size via an ordinary differential equation and (M2-b) introducing dynamic model drift}, formulated via a partial differential inequality} drawing concrete analytical connections between the dynamics of users' datasets and FL accuracy.","We then conduct FL orchestration under MSDs by introducing dynamic cooperative FL with dedicated MAC schedulers (DCLM), exploiting the unique features of open radio access network (O-RAN).","DCLM proposes (i) a hierarchical device-to-device (D2D)-assisted model training, (ii) dynamic control decisions through dedicated O-RAN MAC schedulers, and (iii) asymmetric user selection.","We provide extensive theoretical analysis to study the convergence of DCLM.","We then optimize the degrees of freedom (e.g., user selection and spectrum allocation) in DCLM through a highly non-convex optimization problem.","We develop a systematic approach to obtain the solution for this problem, opening the door to solving a broad variety of network-aware FL optimization problems.","We show the efficiency of DCLM via numerical simulations and provide a series of future directions."],"url":"http://arxiv.org/abs/2404.06324v1"}
{"created":"2024-04-09 13:47:37","title":"On adversarial training and the 1 Nearest Neighbor classifier","abstract":"The ability to fool deep learning classifiers with tiny perturbations of the input has lead to the development of adversarial training in which the loss with respect to adversarial examples is minimized in addition to the training examples. While adversarial training improves the robustness of the learned classifiers, the procedure is computationally expensive, sensitive to hyperparameters and may still leave the classifier vulnerable to other types of small perturbations. In this paper we analyze the adversarial robustness of the 1 Nearest Neighbor (1NN) classifier and compare its performance to adversarial training. We prove that under reasonable assumptions, the 1 NN classifier will be robust to {\\em any} small image perturbation of the training images and will give high adversarial accuracy on test images as the number of training examples goes to infinity. In experiments with 45 different binary image classification problems taken from CIFAR10, we find that 1NN outperform TRADES (a powerful adversarial training algorithm) in terms of average adversarial accuracy. In additional experiments with 69 pretrained robust models for CIFAR10, we find that 1NN outperforms almost all of them in terms of robustness to perturbations that are only slightly different from those seen during training. Taken together, our results suggest that modern adversarial training methods still fall short of the robustness of the simple 1NN classifier. our code can be found at https://github.com/amirhagai/On-Adversarial-Training-And-The-1-Nearest-Neighbor-Classifier","sentences":["The ability to fool deep learning classifiers with tiny perturbations of the input has lead to the development of adversarial training in which the loss with respect to adversarial examples is minimized in addition to the training examples.","While adversarial training improves the robustness of the learned classifiers, the procedure is computationally expensive, sensitive to hyperparameters and may still leave the classifier vulnerable to other types of small perturbations.","In this paper we analyze the adversarial robustness of the 1 Nearest Neighbor (1NN) classifier and compare its performance to adversarial training.","We prove that under reasonable assumptions, the 1 NN classifier will be robust to {\\em any} small image perturbation of the training images and will give high adversarial accuracy on test images as the number of training examples goes to infinity.","In experiments with 45 different binary image classification problems taken from CIFAR10, we find that 1NN outperform TRADES (a powerful adversarial training algorithm) in terms of average adversarial accuracy.","In additional experiments with 69 pretrained robust models for CIFAR10, we find that 1NN outperforms almost all of them in terms of robustness to perturbations that are only slightly different from those seen during training.","Taken together, our results suggest that modern adversarial training methods still fall short of the robustness of the simple 1NN classifier.","our code can be found at https://github.com/amirhagai/On-Adversarial-Training-And-The-1-Nearest-Neighbor-Classifier"],"url":"http://arxiv.org/abs/2404.06313v1"}
{"created":"2024-04-09 13:44:16","title":"DRE: Generating Recommendation Explanations by Aligning Large Language Models at Data-level","abstract":"Recommendation systems play a crucial role in various domains, suggesting items based on user behavior.However, the lack of transparency in presenting recommendations can lead to user confusion. In this paper, we introduce Data-level Recommendation Explanation (DRE), a non-intrusive explanation framework for black-box recommendation models.Different from existing methods, DRE does not require any intermediary representations of the recommendation model or latent alignment training, mitigating potential performance issues.We propose a data-level alignment method, leveraging large language models to reason relationships between user data and recommended items.Additionally, we address the challenge of enriching the details of the explanation by introducing target-aware user preference distillation, utilizing item reviews. Experimental results on benchmark datasets demonstrate the effectiveness of the DRE in providing accurate and user-centric explanations, enhancing user engagement with recommended item.","sentences":["Recommendation systems play a crucial role in various domains, suggesting items based on user behavior.","However, the lack of transparency in presenting recommendations can lead to user confusion.","In this paper, we introduce Data-level Recommendation Explanation (DRE), a non-intrusive explanation framework for black-box recommendation models.","Different from existing methods, DRE does not require any intermediary representations of the recommendation model or latent alignment training, mitigating potential performance issues.","We propose a data-level alignment method, leveraging large language models to reason relationships between user data and recommended items.","Additionally, we address the challenge of enriching the details of the explanation by introducing target-aware user preference distillation, utilizing item reviews.","Experimental results on benchmark datasets demonstrate the effectiveness of the DRE in providing accurate and user-centric explanations, enhancing user engagement with recommended item."],"url":"http://arxiv.org/abs/2404.06311v1"}
