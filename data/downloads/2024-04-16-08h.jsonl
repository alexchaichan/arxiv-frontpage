{"created":"2024-04-15 13:39:11","title":"ChainScience 2024, Conference Proceedings","abstract":"ChainScience 2024, the second edition of the interdisciplinary conference, brought together academics, practitioners, and industry experts to explore novel developments in the realm of distributed ledger technologies. The conference aimed to bridge diverse fields such as informatics, business, economics, finance, regulation, law, mathematics, physics, and complexity science. The papers presented in these conference proceedings address emerging topics such as AI/ML applications to blockchain, DLTs interoperability, decentralized financial services, and tokenomics, alongside ethical, societal, and governance aspects of blockchain and DLTs.   With a focus on promoting high-quality research and interdisciplinary collaboration, ChainScience24 aimed to unlock the collective potential of its diverse participants, embodying the ethos that the whole is greater than the sum of its parts.","sentences":["ChainScience 2024, the second edition of the interdisciplinary conference, brought together academics, practitioners, and industry experts to explore novel developments in the realm of distributed ledger technologies.","The conference aimed to bridge diverse fields such as informatics, business, economics, finance, regulation, law, mathematics, physics, and complexity science.","The papers presented in these conference proceedings address emerging topics such as AI/ML applications to blockchain, DLTs interoperability, decentralized financial services, and tokenomics, alongside ethical, societal, and governance aspects of blockchain and DLTs.   ","With a focus on promoting high-quality research and interdisciplinary collaboration, ChainScience24 aimed to unlock the collective potential of its diverse participants, embodying the ethos that the whole is greater than the sum of its parts."],"url":"http://arxiv.org/abs/2404.09782v1"}
{"created":"2024-04-15 13:30:34","title":"The Devil is in the Few Shots: Iterative Visual Knowledge Completion for Few-shot Learning","abstract":"Contrastive Language-Image Pre-training (CLIP) has shown powerful zero-shot learning performance. Few-shot learning aims to further enhance the transfer capability of CLIP by giving few images in each class, aka 'few shots'. Most existing methods either implicitly learn from the few shots by incorporating learnable prompts or adapters, or explicitly embed them in a cache model for inference. However, the narrow distribution of few shots often contains incomplete class information, leading to biased visual knowledge with high risk of misclassification. To tackle this problem, recent methods propose to supplement visual knowledge by generative models or extra databases, which can be costly and time-consuming. In this paper, we propose an Iterative Visual Knowledge CompLetion (KCL) method to complement visual knowledge by properly taking advantages of unlabeled samples without access to any auxiliary or synthetic data. Specifically, KCL first measures the similarities between unlabeled samples and each category. Then, the samples with top confidence to each category is selected and collected by a designed confidence criterion. Finally, the collected samples are treated as labeled ones and added to few shots to jointly re-estimate the remaining unlabeled ones. The above procedures will be repeated for a certain number of iterations with more and more samples being collected until convergence, ensuring a progressive and robust knowledge completion process. Extensive experiments on 11 benchmark datasets demonstrate the effectiveness and efficiency of KCL as a plug-and-play module under both few-shot and zero-shot learning settings. Code is available at https://github.com/Mark-Sky/KCL.","sentences":["Contrastive Language-Image Pre-training (CLIP) has shown powerful zero-shot learning performance.","Few-shot learning aims to further enhance the transfer capability of CLIP by giving few images in each class, aka 'few shots'.","Most existing methods either implicitly learn from the few shots by incorporating learnable prompts or adapters, or explicitly embed them in a cache model for inference.","However, the narrow distribution of few shots often contains incomplete class information, leading to biased visual knowledge with high risk of misclassification.","To tackle this problem, recent methods propose to supplement visual knowledge by generative models or extra databases, which can be costly and time-consuming.","In this paper, we propose an Iterative Visual Knowledge CompLetion (KCL) method to complement visual knowledge by properly taking advantages of unlabeled samples without access to any auxiliary or synthetic data.","Specifically, KCL first measures the similarities between unlabeled samples and each category.","Then, the samples with top confidence to each category is selected and collected by a designed confidence criterion.","Finally, the collected samples are treated as labeled ones and added to few shots to jointly re-estimate the remaining unlabeled ones.","The above procedures will be repeated for a certain number of iterations with more and more samples being collected until convergence, ensuring a progressive and robust knowledge completion process.","Extensive experiments on 11 benchmark datasets demonstrate the effectiveness and efficiency of KCL as a plug-and-play module under both few-shot and zero-shot learning settings.","Code is available at https://github.com/Mark-Sky/KCL."],"url":"http://arxiv.org/abs/2404.09778v1"}
{"created":"2024-04-15 13:28:13","title":"RandAlign: A Parameter-Free Method for Regularizing Graph Convolutional Networks","abstract":"Studies continually find that message-passing graph convolutional networks suffer from the over-smoothing issue. Basically, the issue of over-smoothing refers to the phenomenon that the learned embeddings for all nodes can become very similar to one another and therefore are uninformative after repeatedly applying message passing iterations. Intuitively, we can expect the generated embeddings become smooth asymptotically layerwisely, that is each layer of graph convolution generates a smoothed version of embeddings as compared to that generated by the previous layer. Based on this intuition, we propose RandAlign, a stochastic regularization method for graph convolutional networks. The idea of RandAlign is to randomly align the learned embedding for each node with that of the previous layer using randomly interpolation in each graph convolution layer. Through alignment, the smoothness of the generated embeddings is explicitly reduced. To better maintain the benefit yielded by the graph convolution, in the alignment step we introduce to first scale the embedding of the previous layer to the same norm as the generated embedding and then perform random interpolation for aligning the generated embedding. RandAlign is a parameter-free method and can be directly applied without introducing additional trainable weights or hyper-parameters. We experimentally evaluate RandAlign on different graph domain tasks on seven benchmark datasets. The experimental results show that RandAlign is a general method that improves the generalization performance of various graph convolutional network models and also improves the numerical stability of optimization, advancing the state of the art performance for graph representation learning.","sentences":["Studies continually find that message-passing graph convolutional networks suffer from the over-smoothing issue.","Basically, the issue of over-smoothing refers to the phenomenon that the learned embeddings for all nodes can become very similar to one another and therefore are uninformative after repeatedly applying message passing iterations.","Intuitively, we can expect the generated embeddings become smooth asymptotically layerwisely, that is each layer of graph convolution generates a smoothed version of embeddings as compared to that generated by the previous layer.","Based on this intuition, we propose RandAlign, a stochastic regularization method for graph convolutional networks.","The idea of RandAlign is to randomly align the learned embedding for each node with that of the previous layer using randomly interpolation in each graph convolution layer.","Through alignment, the smoothness of the generated embeddings is explicitly reduced.","To better maintain the benefit yielded by the graph convolution, in the alignment step we introduce to first scale the embedding of the previous layer to the same norm as the generated embedding and then perform random interpolation for aligning the generated embedding.","RandAlign is a parameter-free method and can be directly applied without introducing additional trainable weights or hyper-parameters.","We experimentally evaluate RandAlign on different graph domain tasks on seven benchmark datasets.","The experimental results show that RandAlign is a general method that improves the generalization performance of various graph convolutional network models and also improves the numerical stability of optimization, advancing the state of the art performance for graph representation learning."],"url":"http://arxiv.org/abs/2404.09774v1"}
{"created":"2024-04-15 13:19:55","title":"Eliminating Crossings in Ordered Graphs","abstract":"Drawing a graph in the plane with as few crossings as possible is one of the central problems in graph drawing and computational geometry. Another option is to remove the smallest number of vertices or edges such that the remaining graph can be drawn without crossings. We study both problems in a book-embedding setting for ordered graphs, that is, graphs with a fixed vertex order. In this setting, the vertices lie on a straight line, called the spine, in the given order, and each edge must be drawn on one of several pages of a book such that every edge has at most a fixed number of crossings. In book embeddings, there is another way to reduce or avoid crossings; namely by using more pages. The minimum number of pages needed to draw an ordered graph without any crossings is its (fixed-vertex-order) page number.   We show that the page number of an ordered graph with $n$ vertices and $m$ edges can be computed in $2^m \\cdot n^{O(1)}$ time. An $O(\\log n)$-approximation of this number can be computed efficiently. We can decide in $2^{O(d \\sqrt{k} \\log (d+k))} \\cdot n^{O(1)}$ time whether it suffices to delete $k$ edges of an ordered graph to obtain a $d$-planar layout (where every edge crosses at most $d$ other edges) on one page. As an additional parameter, we consider the size $h$ of a hitting set, that is, a set of points on the spine such that every edge, seen as an open interval, contains at least one of the points. For $h=1$, we can efficiently compute the minimum number of edges whose deletion yields fixed-vertex-order page number $p$. For $h>1$, we give an XP algorithm with respect to $h+p$. Finally, we consider spine+$t$-track drawings, where some but not all vertices lie on the spine. The vertex order on the spine is given; we must map every vertex that does not lie on the spine to one of $t$ tracks, each of which is a straight line on a separate page, parallel to the spine.","sentences":["Drawing a graph in the plane with as few crossings as possible is one of the central problems in graph drawing and computational geometry.","Another option is to remove the smallest number of vertices or edges such that the remaining graph can be drawn without crossings.","We study both problems in a book-embedding setting for ordered graphs, that is, graphs with a fixed vertex order.","In this setting, the vertices lie on a straight line, called the spine, in the given order, and each edge must be drawn on one of several pages of a book such that every edge has at most a fixed number of crossings.","In book embeddings, there is another way to reduce or avoid crossings; namely by using more pages.","The minimum number of pages needed to draw an ordered graph without any crossings is its (fixed-vertex-order) page number.   ","We show that the page number of an ordered graph with $n$ vertices and $m$ edges can be computed in $2^m \\cdot n^{O(1)}$ time.","An $O(\\log n)$-approximation of this number can be computed efficiently.","We can decide in $2^{O(d \\sqrt{k} \\log (d+k))}","\\cdot n^{O(1)}$ time whether it suffices to delete $k$ edges of an ordered graph to obtain a $d$-planar layout (where every edge crosses at most $d$ other edges) on one page.","As an additional parameter, we consider the size $h$ of a hitting set, that is, a set of points on the spine such that every edge, seen as an open interval, contains at least one of the points.","For $h=1$, we can efficiently compute the minimum number of edges whose deletion yields fixed-vertex-order page number $p$. For $h>1$, we give an XP algorithm with respect to $h+p$. Finally, we consider spine+$t$-track drawings, where some but not all vertices lie on the spine.","The vertex order on the spine is given; we must map every vertex that does not lie on the spine to one of $t$ tracks, each of which is a straight line on a separate page, parallel to the spine."],"url":"http://arxiv.org/abs/2404.09771v1"}
{"created":"2024-04-15 13:17:33","title":"Improved methodology for longitudinal Web analytics using Common Crawl","abstract":"Common Crawl is a multi-petabyte longitudinal dataset containing over 100 billion web pages which is widely used as a source of language data for sequence model training and in web science research. Each of its constituent archives is on the order of 75TB in size. Using it for research, particularly longitudinal studies, which necessarily involve multiple archives, is therefore very expensive in terms of compute time and storage space and/or web bandwidth. Two new methods for mitigating this problem are presented here, based on exploiting and extending the much smaller (<200 gigabytes (GB) compressed) _index_ which is available for each archive. By adding Last-Modified timestamps to the index we enable longitudinal exploration using only a single archive. By comparing the distribution of index features for each of the 100 segments into which archive is divided with their distribution over the whole archive, we have identified the least and most representative segments for a number of recent archives. Using this allows the segment(s) that are most representative of an archive to be used as proxies for the whole. We illustrate this approach in an analysis of changes in URI length over time, leading to an unanticipated insight into the how the creation of Web pages has changed over time.","sentences":["Common Crawl is a multi-petabyte longitudinal dataset containing over 100 billion web pages which is widely used as a source of language data for sequence model training and in web science research.","Each of its constituent archives is on the order of 75TB in size.","Using it for research, particularly longitudinal studies, which necessarily involve multiple archives, is therefore very expensive in terms of compute time and storage space and/or web bandwidth.","Two new methods for mitigating this problem are presented here, based on exploiting and extending the much smaller (<200 gigabytes (GB) compressed) _index_ which is available for each archive.","By adding Last-Modified timestamps to the index we enable longitudinal exploration using only a single archive.","By comparing the distribution of index features for each of the 100 segments into which archive is divided with their distribution over the whole archive, we have identified the least and most representative segments for a number of recent archives.","Using this allows the segment(s) that are most representative of an archive to be used as proxies for the whole.","We illustrate this approach in an analysis of changes in URI length over time, leading to an unanticipated insight into the how the creation of Web pages has changed over time."],"url":"http://arxiv.org/abs/2404.09770v1"}
{"created":"2024-04-15 13:15:27","title":"Search-Space Reduction Via Essential Vertices Revisited: Vertex Multicut and Cograph Deletion","abstract":"For an optimization problem $\\Pi$ on graphs whose solutions are vertex sets, a vertex $v$ is called $c$-essential for $\\Pi$ if all solutions of size at most $c \\cdot OPT$ contain $v$. Recent work showed that polynomial-time algorithms to detect $c$-essential vertices can be used to reduce the search space of fixed-parameter tractable algorithms solving such problems parameterized by the size $k$ of the solution. We provide several new upper- and lower bounds for detecting essential vertices. For example, we give a polynomial-time algorithm for $3$-Essential detection for Vertex Multicut, which translates into an algorithm that finds a minimum multicut of an undirected $n$-vertex graph $G$ in time $2^{O(\\ell^3)} \\cdot n^{O(1)}$, where $\\ell$ is the number of vertices in an optimal solution that are not $3$-essential. Our positive results are obtained by analyzing the integrality gaps of certain linear programs. Our lower bounds show that for sufficiently small values of $c$, the detection task becomes NP-hard assuming the Unique Games Conjecture. For example, we show that ($2-\\varepsilon$)-Essential detection for Directed Feedback Vertex Set is NP-hard under this conjecture, thereby proving that the existing algorithm that detects $2$-essential vertices is best-possible.","sentences":["For an optimization problem $\\Pi$ on graphs whose solutions are vertex sets, a vertex $v$ is called $c$-essential for $\\Pi$ if all solutions of size at most $c \\cdot OPT$ contain $v$.","Recent work showed that polynomial-time algorithms to detect $c$-essential vertices can be used to reduce the search space of fixed-parameter tractable algorithms solving such problems parameterized by the size $k$ of the solution.","We provide several new upper- and lower bounds for detecting essential vertices.","For example, we give a polynomial-time algorithm for $3$-Essential detection for Vertex Multicut, which translates into an algorithm that finds a minimum multicut of an undirected $n$-vertex graph $G$ in time $2^{O(\\ell^3)} \\cdot n^{O(1)}$, where $\\ell$ is the number of vertices in an optimal solution that are not $3$-essential.","Our positive results are obtained by analyzing the integrality gaps of certain linear programs.","Our lower bounds show that for sufficiently small values of $c$, the detection task becomes NP-hard assuming the Unique Games Conjecture.","For example, we show that ($2-\\varepsilon$)-Essential detection for Directed Feedback Vertex Set is NP-hard under this conjecture, thereby proving that the existing algorithm that detects $2$-essential vertices is best-possible."],"url":"http://arxiv.org/abs/2404.09769v1"}
{"created":"2024-04-15 13:13:56","title":"Contrastive Pretraining for Visual Concept Explanations of Socioeconomic Outcomes","abstract":"Predicting socioeconomic indicators from satellite imagery with deep learning has become an increasingly popular research direction. Post-hoc concept-based explanations can be an important step towards broader adoption of these models in policy-making as they enable the interpretation of socioeconomic outcomes based on visual concepts that are intuitive to humans. In this paper, we study the interplay between representation learning using an additional task-specific contrastive loss and post-hoc concept explainability for socioeconomic studies. Our results on two different geographical locations and tasks indicate that the task-specific pretraining imposes a continuous ordering of the latent space embeddings according to the socioeconomic outcomes. This improves the model's interpretability as it enables the latent space of the model to associate urban concepts with continuous intervals of socioeconomic outcomes. Further, we illustrate how analyzing the model's conceptual sensitivity for the intervals of socioeconomic outcomes can shed light on new insights for urban studies.","sentences":["Predicting socioeconomic indicators from satellite imagery with deep learning has become an increasingly popular research direction.","Post-hoc concept-based explanations can be an important step towards broader adoption of these models in policy-making as they enable the interpretation of socioeconomic outcomes based on visual concepts that are intuitive to humans.","In this paper, we study the interplay between representation learning using an additional task-specific contrastive loss and post-hoc concept explainability for socioeconomic studies.","Our results on two different geographical locations and tasks indicate that the task-specific pretraining imposes a continuous ordering of the latent space embeddings according to the socioeconomic outcomes.","This improves the model's interpretability as it enables the latent space of the model to associate urban concepts with continuous intervals of socioeconomic outcomes.","Further, we illustrate how analyzing the model's conceptual sensitivity for the intervals of socioeconomic outcomes can shed light on new insights for urban studies."],"url":"http://arxiv.org/abs/2404.09768v1"}
{"created":"2024-04-15 13:07:40","title":"Hilti SLAM Challenge 2023: Benchmarking Single + Multi-session SLAM across Sensor Constellations in Construction","abstract":"Simultaneous Localization and Mapping systems are a key enabler for positioning in both handheld and robotic applications. The Hilti SLAM Challenges organized over the past years have been successful at benchmarking some of the world's best SLAM Systems with high accuracy. However, more capabilities of these systems are yet to be explored, such as platform agnosticism across varying sensor suites and multi-session SLAM. These factors indirectly serve as an indicator of robustness and ease of deployment in real-world applications. There exists no dataset plus benchmark combination publicly available, which considers these factors combined. The Hilti SLAM Challenge 2023 Dataset and Benchmark addresses this issue. Additionally, we propose a novel fiducial marker design for a pre-surveyed point on the ground to be observable from an off-the-shelf LiDAR mounted on a robot, and an algorithm to estimate its position at mm-level accuracy. Results from the challenge show an increase in overall participation, single-session SLAM systems getting increasingly accurate, successfully operating across varying sensor suites, but relatively few participants performing multi-session SLAM.","sentences":["Simultaneous Localization and Mapping systems are a key enabler for positioning in both handheld and robotic applications.","The Hilti SLAM Challenges organized over the past years have been successful at benchmarking some of the world's best SLAM Systems with high accuracy.","However, more capabilities of these systems are yet to be explored, such as platform agnosticism across varying sensor suites and multi-session SLAM.","These factors indirectly serve as an indicator of robustness and ease of deployment in real-world applications.","There exists no dataset plus benchmark combination publicly available, which considers these factors combined.","The Hilti SLAM Challenge 2023 Dataset and Benchmark addresses this issue.","Additionally, we propose a novel fiducial marker design for a pre-surveyed point on the ground to be observable from an off-the-shelf LiDAR mounted on a robot, and an algorithm to estimate its position at mm-level accuracy.","Results from the challenge show an increase in overall participation, single-session SLAM systems getting increasingly accurate, successfully operating across varying sensor suites, but relatively few participants performing multi-session SLAM."],"url":"http://arxiv.org/abs/2404.09765v1"}
{"created":"2024-04-15 13:07:31","title":"Language-Agnostic Modeling of Wikipedia Articles for Content Quality Assessment across Languages","abstract":"Wikipedia is the largest web repository of free knowledge. Volunteer editors devote time and effort to creating and expanding articles in more than 300 language editions. As content quality varies from article to article, editors also spend substantial time rating articles with specific criteria. However, keeping these assessments complete and up-to-date is largely impossible given the ever-changing nature of Wikipedia. To overcome this limitation, we propose a novel computational framework for modeling the quality of Wikipedia articles.   State-of-the-art approaches to model Wikipedia article quality have leveraged machine learning techniques with language-specific features. In contrast, our framework is based on language-agnostic structural features extracted from the articles, a set of universal weights, and a language version-specific normalization criterion. Therefore, we ensure that all language editions of Wikipedia can benefit from our framework, even those that do not have their own quality assessment scheme. Using this framework, we have built datasets with the feature values and quality scores of all revisions of all articles in the existing language versions of Wikipedia. We provide a descriptive analysis of these resources and a benchmark of our framework. In addition, we discuss possible downstream tasks to be addressed with these datasets, which are released for public use.","sentences":["Wikipedia is the largest web repository of free knowledge.","Volunteer editors devote time and effort to creating and expanding articles in more than 300 language editions.","As content quality varies from article to article, editors also spend substantial time rating articles with specific criteria.","However, keeping these assessments complete and up-to-date is largely impossible given the ever-changing nature of Wikipedia.","To overcome this limitation, we propose a novel computational framework for modeling the quality of Wikipedia articles.   ","State-of-the-art approaches to model Wikipedia article quality have leveraged machine learning techniques with language-specific features.","In contrast, our framework is based on language-agnostic structural features extracted from the articles, a set of universal weights, and a language version-specific normalization criterion.","Therefore, we ensure that all language editions of Wikipedia can benefit from our framework, even those that do not have their own quality assessment scheme.","Using this framework, we have built datasets with the feature values and quality scores of all revisions of all articles in the existing language versions of Wikipedia.","We provide a descriptive analysis of these resources and a benchmark of our framework.","In addition, we discuss possible downstream tasks to be addressed with these datasets, which are released for public use."],"url":"http://arxiv.org/abs/2404.09764v1"}
{"created":"2024-04-15 13:06:32","title":"KG-CTG: Citation Generation through Knowledge Graph-guided Large Language Models","abstract":"Citation Text Generation (CTG) is a task in natural language processing (NLP) that aims to produce text that accurately cites or references a cited document within a source document. In CTG, the generated text draws upon contextual cues from both the source document and the cited paper, ensuring accurate and relevant citation information is provided. Previous work in the field of citation generation is mainly based on the text summarization of documents. Following this, this paper presents a framework, and a comparative study to demonstrate the use of Large Language Models (LLMs) for the task of citation generation. Also, we have shown the improvement in the results of citation generation by incorporating the knowledge graph relations of the papers in the prompt for the LLM to better learn the relationship between the papers. To assess how well our model is performing, we have used a subset of standard S2ORC dataset, which only consists of computer science academic research papers in the English Language. Vicuna performs best for this task with 14.15 Meteor, 12.88 Rouge-1, 1.52 Rouge-2, and 10.94 Rouge-L. Also, Alpaca performs best, and improves the performance by 36.98% in Rouge-1, and 33.14% in Meteor by including knowledge graphs.","sentences":["Citation Text Generation (CTG) is a task in natural language processing (NLP) that aims to produce text that accurately cites or references a cited document within a source document.","In CTG, the generated text draws upon contextual cues from both the source document and the cited paper, ensuring accurate and relevant citation information is provided.","Previous work in the field of citation generation is mainly based on the text summarization of documents.","Following this, this paper presents a framework, and a comparative study to demonstrate the use of Large Language Models (LLMs) for the task of citation generation.","Also, we have shown the improvement in the results of citation generation by incorporating the knowledge graph relations of the papers in the prompt for the LLM to better learn the relationship between the papers.","To assess how well our model is performing, we have used a subset of standard S2ORC dataset, which only consists of computer science academic research papers in the English Language.","Vicuna performs best for this task with 14.15 Meteor, 12.88 Rouge-1, 1.52 Rouge-2, and","10.94 Rouge-L. Also, Alpaca performs best, and improves the performance by 36.98% in Rouge-1, and 33.14% in Meteor by including knowledge graphs."],"url":"http://arxiv.org/abs/2404.09763v1"}
{"created":"2024-04-15 13:02:00","title":"Effective Reinforcement Learning Based on Structural Information Principles","abstract":"Although Reinforcement Learning (RL) algorithms acquire sequential behavioral patterns through interactions with the environment, their effectiveness in noisy and high-dimensional scenarios typically relies on specific structural priors. In this paper, we propose a novel and general Structural Information principles-based framework for effective Decision-Making, namely SIDM, approached from an information-theoretic perspective. This paper presents a specific unsupervised partitioning method that forms vertex communities in the state and action spaces based on their feature similarities. An aggregation function, which utilizes structural entropy as the vertex weight, is devised within each community to obtain its embedding, thereby facilitating hierarchical state and action abstractions. By extracting abstract elements from historical trajectories, a directed, weighted, homogeneous transition graph is constructed. The minimization of this graph's high-dimensional entropy leads to the generation of an optimal encoding tree. An innovative two-layer skill-based learning mechanism is introduced to compute the common path entropy of each state transition as its identified probability, thereby obviating the requirement for expert knowledge. Moreover, SIDM can be flexibly incorporated into various single-agent and multi-agent RL algorithms, enhancing their performance. Finally, extensive evaluations on challenging benchmarks demonstrate that, compared with SOTA baselines, our framework significantly and consistently improves the policy's quality, stability, and efficiency up to 32.70%, 88.26%, and 64.86%, respectively.","sentences":["Although Reinforcement Learning (RL) algorithms acquire sequential behavioral patterns through interactions with the environment, their effectiveness in noisy and high-dimensional scenarios typically relies on specific structural priors.","In this paper, we propose a novel and general Structural Information principles-based framework for effective Decision-Making, namely SIDM, approached from an information-theoretic perspective.","This paper presents a specific unsupervised partitioning method that forms vertex communities in the state and action spaces based on their feature similarities.","An aggregation function, which utilizes structural entropy as the vertex weight, is devised within each community to obtain its embedding, thereby facilitating hierarchical state and action abstractions.","By extracting abstract elements from historical trajectories, a directed, weighted, homogeneous transition graph is constructed.","The minimization of this graph's high-dimensional entropy leads to the generation of an optimal encoding tree.","An innovative two-layer skill-based learning mechanism is introduced to compute the common path entropy of each state transition as its identified probability, thereby obviating the requirement for expert knowledge.","Moreover, SIDM can be flexibly incorporated into various single-agent and multi-agent RL algorithms, enhancing their performance.","Finally, extensive evaluations on challenging benchmarks demonstrate that, compared with SOTA baselines, our framework significantly and consistently improves the policy's quality, stability, and efficiency up to 32.70%, 88.26%, and 64.86%, respectively."],"url":"http://arxiv.org/abs/2404.09760v1"}
{"created":"2024-04-15 13:00:09","title":"Transforming a Non-Differentiable Rasterizer into a Differentiable One with Stochastic Gradient Estimation","abstract":"We show how to transform a non-differentiable rasterizer into a differentiable one with minimal engineering efforts and no external dependencies (no Pytorch/Tensorflow). We rely on Stochastic Gradient Estimation, a technique that consists of rasterizing after randomly perturbing the scene's parameters such that their gradient can be stochastically estimated and descended. This method is simple and robust but does not scale in dimensionality (number of scene parameters). Our insight is that the number of parameters contributing to a given rasterized pixel is bounded. Estimating and averaging gradients on a per-pixel basis hence bounds the dimensionality of the underlying optimization problem and makes the method scalable. Furthermore, it is simple to track per-pixel contributing parameters by rasterizing ID- and UV-buffers, which are trivial additions to a rasterization engine if not already available. With these minor modifications, we obtain an in-engine optimizer for 3D assets with millions of geometry and texture parameters.","sentences":["We show how to transform a non-differentiable rasterizer into a differentiable one with minimal engineering efforts and no external dependencies (no Pytorch/Tensorflow).","We rely on Stochastic Gradient Estimation, a technique that consists of rasterizing after randomly perturbing the scene's parameters such that their gradient can be stochastically estimated and descended.","This method is simple and robust but does not scale in dimensionality (number of scene parameters).","Our insight is that the number of parameters contributing to a given rasterized pixel is bounded.","Estimating and averaging gradients on a per-pixel basis hence bounds the dimensionality of the underlying optimization problem and makes the method scalable.","Furthermore, it is simple to track per-pixel contributing parameters by rasterizing ID- and UV-buffers, which are trivial additions to a rasterization engine if not already available.","With these minor modifications, we obtain an in-engine optimizer for 3D assets with millions of geometry and texture parameters."],"url":"http://arxiv.org/abs/2404.09758v1"}
{"created":"2024-04-15 12:55:08","title":"Resilience of Large Language Models for Noisy Instructions","abstract":"As the rapidly advancing domain of natural language processing (NLP), large language models (LLMs) have emerged as powerful tools for interpreting human commands and generating text across various tasks. Nonetheless, the resilience of LLMs to handle text containing inherent errors, stemming from human interactions and collaborative systems, has not been thoroughly explored. Our study investigates the resilience of LLMs against five common types of disruptions including 1) ASR (Automatic Speech Recognition) errors, 2) OCR (Optical Character Recognition) errors, 3) grammatical mistakes, 4) typographical errors, and 5) distractive content. We aim to investigate how these models react by deliberately embedding these errors into instructions. Our findings reveal that while some LLMs show a degree of resistance to certain types of noise, their overall performance significantly suffers. This emphasizes the importance of further investigation into enhancing model resilience. In response to the observed decline in performance, our study also evaluates a \"re-pass\" strategy, designed to purify the instructions of noise before the LLMs process them. Our analysis indicates that correcting noisy instructions, particularly for open-source LLMs, presents significant challenges.","sentences":["As the rapidly advancing domain of natural language processing (NLP), large language models (LLMs) have emerged as powerful tools for interpreting human commands and generating text across various tasks.","Nonetheless, the resilience of LLMs to handle text containing inherent errors, stemming from human interactions and collaborative systems, has not been thoroughly explored.","Our study investigates the resilience of LLMs against five common types of disruptions including 1) ASR (Automatic Speech Recognition) errors, 2) OCR (Optical Character Recognition) errors, 3) grammatical mistakes, 4) typographical errors, and 5) distractive content.","We aim to investigate how these models react by deliberately embedding these errors into instructions.","Our findings reveal that while some LLMs show a degree of resistance to certain types of noise, their overall performance significantly suffers.","This emphasizes the importance of further investigation into enhancing model resilience.","In response to the observed decline in performance, our study also evaluates a \"re-pass\" strategy, designed to purify the instructions of noise before the LLMs process them.","Our analysis indicates that correcting noisy instructions, particularly for open-source LLMs, presents significant challenges."],"url":"http://arxiv.org/abs/2404.09754v1"}
{"created":"2024-04-15 12:54:31","title":"Personalized Collaborative Fine-Tuning for On-Device Large Language Models","abstract":"We explore on-device self-supervised collaborative fine-tuning of large language models with limited local data availability. Taking inspiration from the collaborative learning community, we introduce three distinct trust-weighted gradient aggregation schemes: weight similarity-based, prediction similarity-based and validation performance-based. To minimize communication overhead, we integrate Low-Rank Adaptation (LoRA) and only exchange LoRA weight updates. Our protocols, driven by prediction and performance metrics, surpass both FedAvg and local fine-tuning methods, which is particularly evident in realistic scenarios with more diverse local data distributions. The results underscore the effectiveness of our approach in addressing heterogeneity and scarcity within local datasets.","sentences":["We explore on-device self-supervised collaborative fine-tuning of large language models with limited local data availability.","Taking inspiration from the collaborative learning community, we introduce three distinct trust-weighted gradient aggregation schemes: weight similarity-based, prediction similarity-based and validation performance-based.","To minimize communication overhead, we integrate Low-Rank Adaptation (LoRA) and only exchange LoRA weight updates.","Our protocols, driven by prediction and performance metrics, surpass both FedAvg and local fine-tuning methods, which is particularly evident in realistic scenarios with more diverse local data distributions.","The results underscore the effectiveness of our approach in addressing heterogeneity and scarcity within local datasets."],"url":"http://arxiv.org/abs/2404.09753v1"}
{"created":"2024-04-15 12:53:48","title":"Can We Break Free from Strong Data Augmentations in Self-Supervised Learning?","abstract":"Self-supervised learning (SSL) has emerged as a promising solution for addressing the challenge of limited labeled data in deep neural networks (DNNs), offering scalability potential. However, the impact of design dependencies within the SSL framework remains insufficiently investigated. In this study, we comprehensively explore SSL behavior across a spectrum of augmentations, revealing their crucial role in shaping SSL model performance and learning mechanisms. Leveraging these insights, we propose a novel learning approach that integrates prior knowledge, with the aim of curtailing the need for extensive data augmentations and thereby amplifying the efficacy of learned representations. Notably, our findings underscore that SSL models imbued with prior knowledge exhibit reduced texture bias, diminished reliance on shortcuts and augmentations, and improved robustness against both natural and adversarial corruptions. These findings not only illuminate a new direction in SSL research, but also pave the way for enhancing DNN performance while concurrently alleviating the imperative for intensive data augmentation, thereby enhancing scalability and real-world problem-solving capabilities.","sentences":["Self-supervised learning (SSL) has emerged as a promising solution for addressing the challenge of limited labeled data in deep neural networks (DNNs), offering scalability potential.","However, the impact of design dependencies within the SSL framework remains insufficiently investigated.","In this study, we comprehensively explore SSL behavior across a spectrum of augmentations, revealing their crucial role in shaping SSL model performance and learning mechanisms.","Leveraging these insights, we propose a novel learning approach that integrates prior knowledge, with the aim of curtailing the need for extensive data augmentations and thereby amplifying the efficacy of learned representations.","Notably, our findings underscore that SSL models imbued with prior knowledge exhibit reduced texture bias, diminished reliance on shortcuts and augmentations, and improved robustness against both natural and adversarial corruptions.","These findings not only illuminate a new direction in SSL research, but also pave the way for enhancing DNN performance while concurrently alleviating the imperative for intensive data augmentation, thereby enhancing scalability and real-world problem-solving capabilities."],"url":"http://arxiv.org/abs/2404.09752v1"}
{"created":"2024-04-15 12:51:03","title":"Connectivity in Symmetric Semi-Algebraic Sets","abstract":"Semi-algebraic set is a subset of the real space defined by polynomial equations and inequalities. In this paper, we consider the problem of deciding whether two given points in a semi-algebraic set are connected. We restrict to the case when all equations and inequalities are invariant under the action of the symmetric group and their degrees at most $d<n$, where $n$ is the number of variables. Additionally, we assume that the two points are in the same fundamental domain of the action of the symmetric group, by assuming that the coordinates of two given points are sorted in non-decreasing order. We construct and analyze an algorithm that solves this problem, by taking advantage of the group action, and has a complexity being polynomial in $n$.","sentences":["Semi-algebraic set is a subset of the real space defined by polynomial equations and inequalities.","In this paper, we consider the problem of deciding whether two given points in a semi-algebraic set are connected.","We restrict to the case when all equations and inequalities are invariant under the action of the symmetric group and their degrees at most $d<n$, where $n$ is the number of variables.","Additionally, we assume that the two points are in the same fundamental domain of the action of the symmetric group, by assuming that the coordinates of two given points are sorted in non-decreasing order.","We construct and analyze an algorithm that solves this problem, by taking advantage of the group action, and has a complexity being polynomial in $n$."],"url":"http://arxiv.org/abs/2404.09749v1"}
{"created":"2024-04-15 12:50:44","title":"LetsGo: Large-Scale Garage Modeling and Rendering via LiDAR-Assisted Gaussian Primitives","abstract":"Large garages are ubiquitous yet intricate scenes in our daily lives, posing challenges characterized by monotonous colors, repetitive patterns, reflective surfaces, and transparent vehicle glass. Conventional Structure from Motion (SfM) methods for camera pose estimation and 3D reconstruction fail in these environments due to poor correspondence construction. To address these challenges, this paper introduces LetsGo, a LiDAR-assisted Gaussian splatting approach for large-scale garage modeling and rendering. We develop a handheld scanner, Polar, equipped with IMU, LiDAR, and a fisheye camera, to facilitate accurate LiDAR and image data scanning. With this Polar device, we present a GarageWorld dataset consisting of five expansive garage scenes with diverse geometric structures and will release the dataset to the community for further research. We demonstrate that the collected LiDAR point cloud by the Polar device enhances a suite of 3D Gaussian splatting algorithms for garage scene modeling and rendering. We also propose a novel depth regularizer for 3D Gaussian splatting algorithm training, effectively eliminating floating artifacts in rendered images, and a lightweight Level of Detail (LOD) Gaussian renderer for real-time viewing on web-based devices. Additionally, we explore a hybrid representation that combines the advantages of traditional mesh in depicting simple geometry and colors (e.g., walls and the ground) with modern 3D Gaussian representations capturing complex details and high-frequency textures. This strategy achieves an optimal balance between memory performance and rendering quality. Experimental results on our dataset, along with ScanNet++ and KITTI-360, demonstrate the superiority of our method in rendering quality and resource efficiency.","sentences":["Large garages are ubiquitous yet intricate scenes in our daily lives, posing challenges characterized by monotonous colors, repetitive patterns, reflective surfaces, and transparent vehicle glass.","Conventional Structure from Motion (SfM) methods for camera pose estimation and 3D reconstruction fail in these environments due to poor correspondence construction.","To address these challenges, this paper introduces LetsGo, a LiDAR-assisted Gaussian splatting approach for large-scale garage modeling and rendering.","We develop a handheld scanner, Polar, equipped with IMU, LiDAR, and a fisheye camera, to facilitate accurate LiDAR and image data scanning.","With this Polar device, we present a GarageWorld dataset consisting of five expansive garage scenes with diverse geometric structures and will release the dataset to the community for further research.","We demonstrate that the collected LiDAR point cloud by the Polar device enhances a suite of 3D Gaussian splatting algorithms for garage scene modeling and rendering.","We also propose a novel depth regularizer for 3D Gaussian splatting algorithm training, effectively eliminating floating artifacts in rendered images, and a lightweight Level of Detail (LOD) Gaussian renderer for real-time viewing on web-based devices.","Additionally, we explore a hybrid representation that combines the advantages of traditional mesh in depicting simple geometry and colors (e.g., walls and the ground) with modern 3D Gaussian representations capturing complex details and high-frequency textures.","This strategy achieves an optimal balance between memory performance and rendering quality.","Experimental results on our dataset, along with ScanNet++ and KITTI-360, demonstrate the superiority of our method in rendering quality and resource efficiency."],"url":"http://arxiv.org/abs/2404.09748v1"}
{"created":"2024-04-15 12:38:46","title":"Quantization of Large Language Models with an Overdetermined Basis","abstract":"In this paper, we introduce an algorithm for data quantization based on the principles of Kashin representation. This approach hinges on decomposing any given vector, matrix, or tensor into two factors. The first factor maintains a small infinity norm, while the second exhibits a similarly constrained norm when multiplied by an orthogonal matrix. Surprisingly, the entries of factors after decomposition are well-concentrated around several peaks, which allows us to efficiently replace them with corresponding centroids for quantization purposes. We study the theoretical properties of the proposed approach and rigorously evaluate our compression algorithm in the context of next-word prediction tasks and on a set of downstream tasks for text classification. Our findings demonstrate that Kashin Quantization achieves competitive or superior quality in model performance while ensuring data compression, marking a significant advancement in the field of data quantization.","sentences":["In this paper, we introduce an algorithm for data quantization based on the principles of Kashin representation.","This approach hinges on decomposing any given vector, matrix, or tensor into two factors.","The first factor maintains a small infinity norm, while the second exhibits a similarly constrained norm when multiplied by an orthogonal matrix.","Surprisingly, the entries of factors after decomposition are well-concentrated around several peaks, which allows us to efficiently replace them with corresponding centroids for quantization purposes.","We study the theoretical properties of the proposed approach and rigorously evaluate our compression algorithm in the context of next-word prediction tasks and on a set of downstream tasks for text classification.","Our findings demonstrate that Kashin Quantization achieves competitive or superior quality in model performance while ensuring data compression, marking a significant advancement in the field of data quantization."],"url":"http://arxiv.org/abs/2404.09737v1"}
{"created":"2024-04-15 12:37:26","title":"FSRT: Facial Scene Representation Transformer for Face Reenactment from Factorized Appearance, Head-pose, and Facial Expression Features","abstract":"The task of face reenactment is to transfer the head motion and facial expressions from a driving video to the appearance of a source image, which may be of a different person (cross-reenactment). Most existing methods are CNN-based and estimate optical flow from the source image to the current driving frame, which is then inpainted and refined to produce the output animation. We propose a transformer-based encoder for computing a set-latent representation of the source image(s). We then predict the output color of a query pixel using a transformer-based decoder, which is conditioned with keypoints and a facial expression vector extracted from the driving frame. Latent representations of the source person are learned in a self-supervised manner that factorize their appearance, head pose, and facial expressions. Thus, they are perfectly suited for cross-reenactment. In contrast to most related work, our method naturally extends to multiple source images and can thus adapt to person-specific facial dynamics. We also propose data augmentation and regularization schemes that are necessary to prevent overfitting and support generalizability of the learned representations. We evaluated our approach in a randomized user study. The results indicate superior performance compared to the state-of-the-art in terms of motion transfer quality and temporal consistency.","sentences":["The task of face reenactment is to transfer the head motion and facial expressions from a driving video to the appearance of a source image, which may be of a different person (cross-reenactment).","Most existing methods are CNN-based and estimate optical flow from the source image to the current driving frame, which is then inpainted and refined to produce the output animation.","We propose a transformer-based encoder for computing a set-latent representation of the source image(s).","We then predict the output color of a query pixel using a transformer-based decoder, which is conditioned with keypoints and a facial expression vector extracted from the driving frame.","Latent representations of the source person are learned in a self-supervised manner that factorize their appearance, head pose, and facial expressions.","Thus, they are perfectly suited for cross-reenactment.","In contrast to most related work, our method naturally extends to multiple source images and can thus adapt to person-specific facial dynamics.","We also propose data augmentation and regularization schemes that are necessary to prevent overfitting and support generalizability of the learned representations.","We evaluated our approach in a randomized user study.","The results indicate superior performance compared to the state-of-the-art in terms of motion transfer quality and temporal consistency."],"url":"http://arxiv.org/abs/2404.09736v1"}
{"created":"2024-04-15 12:35:10","title":"Equipping Diffusion Models with Differentiable Spatial Entropy for Low-Light Image Enhancement","abstract":"Image restoration, which aims to recover high-quality images from their corrupted counterparts, often faces the challenge of being an ill-posed problem that allows multiple solutions for a single input. However, most deep learning based works simply employ l1 loss to train their network in a deterministic way, resulting in over-smoothed predictions with inferior perceptual quality. In this work, we propose a novel method that shifts the focus from a deterministic pixel-by-pixel comparison to a statistical perspective, emphasizing the learning of distributions rather than individual pixel values. The core idea is to introduce spatial entropy into the loss function to measure the distribution difference between predictions and targets. To make this spatial entropy differentiable, we employ kernel density estimation (KDE) to approximate the probabilities for specific intensity values of each pixel with their neighbor areas. Specifically, we equip the entropy with diffusion models and aim for superior accuracy and enhanced perceptual quality over l1 based noise matching loss. In the experiments, we evaluate the proposed method for low light enhancement on two datasets and the NTIRE challenge 2024. All these results illustrate the effectiveness of our statistic-based entropy loss. Code is available at https://github.com/shermanlian/spatial-entropy-loss.","sentences":["Image restoration, which aims to recover high-quality images from their corrupted counterparts, often faces the challenge of being an ill-posed problem that allows multiple solutions for a single input.","However, most deep learning based works simply employ l1 loss to train their network in a deterministic way, resulting in over-smoothed predictions with inferior perceptual quality.","In this work, we propose a novel method that shifts the focus from a deterministic pixel-by-pixel comparison to a statistical perspective, emphasizing the learning of distributions rather than individual pixel values.","The core idea is to introduce spatial entropy into the loss function to measure the distribution difference between predictions and targets.","To make this spatial entropy differentiable, we employ kernel density estimation (KDE) to approximate the probabilities for specific intensity values of each pixel with their neighbor areas.","Specifically, we equip the entropy with diffusion models and aim for superior accuracy and enhanced perceptual quality over l1 based noise matching loss.","In the experiments, we evaluate the proposed method for low light enhancement on two datasets and the NTIRE challenge 2024.","All these results illustrate the effectiveness of our statistic-based entropy loss.","Code is available at https://github.com/shermanlian/spatial-entropy-loss."],"url":"http://arxiv.org/abs/2404.09735v1"}
{"created":"2024-04-15 12:34:51","title":"Weighted Sum-Rate Maximization for Movable Antenna-Enhanced Wireless Networks","abstract":"This letter investigates the weighted sum rate maximization problem in movable antenna (MA)-enhanced systems. To reduce the computational complexity, we transform it into a more tractable weighted minimum mean square error (WMMSE) problem well-suited for MA. We then adopt the WMMSE algorithm and majorization-minimization algorithm to optimize the beamforming and antenna positions, respectively. Moreover, we propose a planar movement mode, which constrains each MA to a specified area, we obtain a low-complexity closed-form solution. Numerical results demonstrate that the MA-enhanced system outperforms the conventional system. Besides, the computation time for the planar movement mode is reduced by approximately 30\\% at a little performance expense.","sentences":["This letter investigates the weighted sum rate maximization problem in movable antenna (MA)-enhanced systems.","To reduce the computational complexity, we transform it into a more tractable weighted minimum mean square error (WMMSE) problem well-suited for MA.","We then adopt the WMMSE algorithm and majorization-minimization algorithm to optimize the beamforming and antenna positions, respectively.","Moreover, we propose a planar movement mode, which constrains each MA to a specified area, we obtain a low-complexity closed-form solution.","Numerical results demonstrate that the MA-enhanced system outperforms the conventional system.","Besides, the computation time for the planar movement mode is reduced by approximately 30\\% at a little performance expense."],"url":"http://arxiv.org/abs/2404.09734v1"}
{"created":"2024-04-15 12:34:21","title":"Photo-Realistic Image Restoration in the Wild with Controlled Vision-Language Models","abstract":"Though diffusion models have been successfully applied to various image restoration (IR) tasks, their performance is sensitive to the choice of training datasets. Typically, diffusion models trained in specific datasets fail to recover images that have out-of-distribution degradations. To address this problem, this work leverages a capable vision-language model and a synthetic degradation pipeline to learn image restoration in the wild (wild IR). More specifically, all low-quality images are simulated with a synthetic degradation pipeline that contains multiple common degradations such as blur, resize, noise, and JPEG compression. Then we introduce robust training for a degradation-aware CLIP model to extract enriched image content features to assist high-quality image restoration. Our base diffusion model is the image restoration SDE (IR-SDE). Built upon it, we further present a posterior sampling strategy for fast noise-free image generation. We evaluate our model on both synthetic and real-world degradation datasets. Moreover, experiments on the unified image restoration task illustrate that the proposed posterior sampling improves image generation quality for various degradations.","sentences":["Though diffusion models have been successfully applied to various image restoration (IR) tasks, their performance is sensitive to the choice of training datasets.","Typically, diffusion models trained in specific datasets fail to recover images that have out-of-distribution degradations.","To address this problem, this work leverages a capable vision-language model and a synthetic degradation pipeline to learn image restoration in the wild (wild IR).","More specifically, all low-quality images are simulated with a synthetic degradation pipeline that contains multiple common degradations such as blur, resize, noise, and JPEG compression.","Then we introduce robust training for a degradation-aware CLIP model to extract enriched image content features to assist high-quality image restoration.","Our base diffusion model is the image restoration SDE (IR-SDE).","Built upon it, we further present a posterior sampling strategy for fast noise-free image generation.","We evaluate our model on both synthetic and real-world degradation datasets.","Moreover, experiments on the unified image restoration task illustrate that the proposed posterior sampling improves image generation quality for various degradations."],"url":"http://arxiv.org/abs/2404.09732v1"}
{"created":"2024-04-15 12:29:28","title":"Convergence Analysis of Probability Flow ODE for Score-based Generative Models","abstract":"Score-based generative models have emerged as a powerful approach for sampling high-dimensional probability distributions. Despite their effectiveness, their theoretical underpinnings remain relatively underdeveloped. In this work, we study the convergence properties of deterministic samplers based on probability flow ODEs from both theoretical and numerical perspectives. Assuming access to $L^2$-accurate estimates of the score function, we prove the total variation between the target and the generated data distributions can be bounded above by $\\mathcal{O}(d\\sqrt{\\delta})$ in the continuous time level, where $d$ denotes the data dimension and $\\delta$ represents the $L^2$-score matching error. For practical implementations using a $p$-th order Runge-Kutta integrator with step size $h$, we establish error bounds of $\\mathcal{O}(d(\\sqrt{\\delta} + (dh)^p))$ at the discrete level. Finally, we present numerical studies on problems up to $128$ dimensions to verify our theory, which indicate a better score matching error and dimension dependence.","sentences":["Score-based generative models have emerged as a powerful approach for sampling high-dimensional probability distributions.","Despite their effectiveness, their theoretical underpinnings remain relatively underdeveloped.","In this work, we study the convergence properties of deterministic samplers based on probability flow ODEs from both theoretical and numerical perspectives.","Assuming access to $L^2$-accurate estimates of the score function, we prove the total variation between the target and the generated data distributions can be bounded above by $\\mathcal{O}(d\\sqrt{\\delta})$ in the continuous time level, where $d$ denotes the data dimension and $\\delta$ represents the $L^2$-score matching error.","For practical implementations using a $p$-th order Runge-Kutta integrator with step size $h$, we establish error bounds of $\\mathcal{O}(d(\\sqrt{\\delta} + (dh)^p))$ at the discrete level.","Finally, we present numerical studies on problems up to $128$ dimensions to verify our theory, which indicate a better score matching error and dimension dependence."],"url":"http://arxiv.org/abs/2404.09730v1"}
{"created":"2024-04-15 12:27:07","title":"Privacy-Preserving Federated Unlearning with Certified Client Removal","abstract":"In recent years, Federated Unlearning (FU) has gained attention for addressing the removal of a client's influence from the global model in Federated Learning (FL) systems, thereby ensuring the ``right to be forgotten\" (RTBF). State-of-the-art methods for unlearning use historical data from FL clients, such as gradients or locally trained models. However, studies have revealed significant information leakage in this setting, with the possibility of reconstructing a user's local data from their uploaded information. Addressing this, we propose Starfish, a privacy-preserving federated unlearning scheme using Two-Party Computation (2PC) techniques and shared historical client data between two non-colluding servers. Starfish builds upon existing FU methods to ensure privacy in unlearning processes. To enhance the efficiency of privacy-preserving FU evaluations, we suggest 2PC-friendly alternatives for certain FU algorithm operations. We also implement strategies to reduce costs associated with 2PC operations and lessen cumulative approximation errors. Moreover, we establish a theoretical bound for the difference between the unlearned global model via Starfish and a global model retrained from scratch for certified client removal. Our theoretical and experimental analyses demonstrate that Starfish achieves effective unlearning with reasonable efficiency, maintaining privacy and security in FL systems.","sentences":["In recent years, Federated Unlearning (FU) has gained attention for addressing the removal of a client's influence from the global model in Federated Learning (FL) systems, thereby ensuring the ``right to be forgotten\" (RTBF).","State-of-the-art methods for unlearning use historical data from FL clients, such as gradients or locally trained models.","However, studies have revealed significant information leakage in this setting, with the possibility of reconstructing a user's local data from their uploaded information.","Addressing this, we propose Starfish, a privacy-preserving federated unlearning scheme using Two-Party Computation (2PC) techniques and shared historical client data between two non-colluding servers.","Starfish builds upon existing FU methods to ensure privacy in unlearning processes.","To enhance the efficiency of privacy-preserving FU evaluations, we suggest 2PC-friendly alternatives for certain FU algorithm operations.","We also implement strategies to reduce costs associated with 2PC operations and lessen cumulative approximation errors.","Moreover, we establish a theoretical bound for the difference between the unlearned global model via Starfish and a global model retrained from scratch for certified client removal.","Our theoretical and experimental analyses demonstrate that Starfish achieves effective unlearning with reasonable efficiency, maintaining privacy and security in FL systems."],"url":"http://arxiv.org/abs/2404.09724v1"}
{"created":"2024-04-15 12:25:41","title":"VFLGAN: Vertical Federated Learning-based Generative Adversarial Network for Vertically Partitioned Data Publication","abstract":"In the current artificial intelligence (AI) era, the scale and quality of the dataset play a crucial role in training a high-quality AI model. However, good data is not a free lunch and is always hard to access due to privacy regulations like the General Data Protection Regulation (GDPR). A potential solution is to release a synthetic dataset with a similar distribution to that of the private dataset. Nevertheless, in some scenarios, it has been found that the attributes needed to train an AI model belong to different parties, and they cannot share the raw data for synthetic data publication due to privacy regulations. In PETS 2023, Xue et al. proposed the first generative adversary network-based model, VertiGAN, for vertically partitioned data publication. However, after thoroughly investigating, we found that VertiGAN is less effective in preserving the correlation among the attributes of different parties. This article proposes a Vertical Federated Learning-based Generative Adversarial Network, VFLGAN, for vertically partitioned data publication to address the above issues. Our experimental results show that compared with VertiGAN, VFLGAN significantly improves the quality of synthetic data. Taking the MNIST dataset as an example, the quality of the synthetic dataset generated by VFLGAN is 3.2 times better than that generated by VertiGAN w.r.t. the Fr\\'echet Distance. We also designed a more efficient and effective Gaussian mechanism for the proposed VFLGAN to provide the synthetic dataset with a differential privacy guarantee. On the other hand, differential privacy only gives the upper bound of the worst-case privacy guarantee. This article also proposes a practical auditing scheme that applies membership inference attacks to estimate privacy leakage through the synthetic dataset.","sentences":["In the current artificial intelligence (AI) era, the scale and quality of the dataset play a crucial role in training a high-quality AI model.","However, good data is not a free lunch and is always hard to access due to privacy regulations like the General Data Protection Regulation (GDPR).","A potential solution is to release a synthetic dataset with a similar distribution to that of the private dataset.","Nevertheless, in some scenarios, it has been found that the attributes needed to train an AI model belong to different parties, and they cannot share the raw data for synthetic data publication due to privacy regulations.","In PETS 2023, Xue et al. proposed the first generative adversary network-based model, VertiGAN, for vertically partitioned data publication.","However, after thoroughly investigating, we found that VertiGAN is less effective in preserving the correlation among the attributes of different parties.","This article proposes a Vertical Federated Learning-based Generative Adversarial Network, VFLGAN, for vertically partitioned data publication to address the above issues.","Our experimental results show that compared with VertiGAN, VFLGAN significantly improves the quality of synthetic data.","Taking the MNIST dataset as an example, the quality of the synthetic dataset generated by VFLGAN is 3.2 times better than that generated by VertiGAN w.r.t.","the Fr\\'echet Distance.","We also designed a more efficient and effective Gaussian mechanism for the proposed VFLGAN to provide the synthetic dataset with a differential privacy guarantee.","On the other hand, differential privacy only gives the upper bound of the worst-case privacy guarantee.","This article also proposes a practical auditing scheme that applies membership inference attacks to estimate privacy leakage through the synthetic dataset."],"url":"http://arxiv.org/abs/2404.09722v1"}
{"created":"2024-04-15 12:20:09","title":"Unveiling Imitation Learning: Exploring the Impact of Data Falsity to Large Language Model","abstract":"Many recent studies endeavor to improve open-source language models through imitation learning, and re-training on the synthetic instruction data from state-of-the-art proprietary models like ChatGPT and GPT-4. However, the innate nature of synthetic data inherently contains noisy data, giving rise to a substantial presence of low-quality data replete with erroneous responses, and flawed reasoning. Although we intuitively grasp the potential harm of noisy data, we lack a quantitative understanding of its impact. To this end, this paper explores the correlation between the degree of noise and its impact on language models through instruction tuning. We first introduce the Falsity-Controllable (FACO) dataset, which comprises pairs of true answers with corresponding reasoning, as well as false pairs to manually control the falsity ratio of the dataset.Through our extensive experiments, we found multiple intriguing findings of the correlation between the factuality of the dataset and instruction tuning: Specifically, we verified falsity of the instruction is highly relevant to various benchmark scores. Moreover, when LLMs are trained with false instructions, they learn to lie and generate fake unfaithful answers, even though they know the correct answer for the user request. Additionally, we noted that once the language model is trained with a dataset contaminated by noise, restoring its original performance is possible, but it failed to reach full performance.","sentences":["Many recent studies endeavor to improve open-source language models through imitation learning, and re-training on the synthetic instruction data from state-of-the-art proprietary models like ChatGPT and GPT-4.","However, the innate nature of synthetic data inherently contains noisy data, giving rise to a substantial presence of low-quality data replete with erroneous responses, and flawed reasoning.","Although we intuitively grasp the potential harm of noisy data, we lack a quantitative understanding of its impact.","To this end, this paper explores the correlation between the degree of noise and its impact on language models through instruction tuning.","We first introduce the Falsity-Controllable (FACO) dataset, which comprises pairs of true answers with corresponding reasoning, as well as false pairs to manually control the falsity ratio of the dataset.","Through our extensive experiments, we found multiple intriguing findings of the correlation between the factuality of the dataset and instruction tuning: Specifically, we verified falsity of the instruction is highly relevant to various benchmark scores.","Moreover, when LLMs are trained with false instructions, they learn to lie and generate fake unfaithful answers, even though they know the correct answer for the user request.","Additionally, we noted that once the language model is trained with a dataset contaminated by noise, restoring its original performance is possible, but it failed to reach full performance."],"url":"http://arxiv.org/abs/2404.09717v1"}
{"created":"2024-04-15 12:18:09","title":"Higher Replay Ratio Empowers Sample-Efficient Multi-Agent Reinforcement Learning","abstract":"One of the notorious issues for Reinforcement Learning (RL) is poor sample efficiency. Compared to single agent RL, the sample efficiency for Multi-Agent Reinforcement Learning (MARL) is more challenging because of its inherent partial observability, non-stationary training, and enormous strategy space. Although much effort has been devoted to developing new methods and enhancing sample efficiency, we look at the widely used episodic training mechanism. In each training step, tens of frames are collected, but only one gradient step is made. We argue that this episodic training could be a source of poor sample efficiency. To better exploit the data already collected, we propose to increase the frequency of the gradient updates per environment interaction (a.k.a. Replay Ratio or Update-To-Data ratio). To show its generality, we evaluate $3$ MARL methods on $6$ SMAC tasks. The empirical results validate that a higher replay ratio significantly improves the sample efficiency for MARL algorithms. The codes to reimplement the results presented in this paper are open-sourced at https://anonymous.4open.science/r/rr_for_MARL-0D83/.","sentences":["One of the notorious issues for Reinforcement Learning (RL) is poor sample efficiency.","Compared to single agent RL, the sample efficiency for Multi-Agent Reinforcement Learning (MARL) is more challenging because of its inherent partial observability, non-stationary training, and enormous strategy space.","Although much effort has been devoted to developing new methods and enhancing sample efficiency, we look at the widely used episodic training mechanism.","In each training step, tens of frames are collected, but only one gradient step is made.","We argue that this episodic training could be a source of poor sample efficiency.","To better exploit the data already collected, we propose to increase the frequency of the gradient updates per environment interaction (a.k.a. Replay Ratio or Update-To-Data ratio).","To show its generality, we evaluate $3$ MARL methods on $6$ SMAC tasks.","The empirical results validate that a higher replay ratio significantly improves the sample efficiency for MARL algorithms.","The codes to reimplement the results presented in this paper are open-sourced at https://anonymous.4open.science/r/rr_for_MARL-0D83/."],"url":"http://arxiv.org/abs/2404.09715v1"}
{"created":"2024-04-15 12:12:25","title":"Online Multi-level Aggregation with Delays and Stochastic Arrivals","abstract":"This paper presents a new research direction for online Multi-Level Aggregation (MLA) with delays. In this problem, we are given an edge-weighted rooted tree $T$, and we have to serve a sequence of requests arriving at its vertices in an online manner. Each request $r$ is characterized by two parameters: its arrival time $t(r)$ and location $l(r)$ (a vertex). Once a request $r$ arrives, we can either serve it immediately or postpone this action until any time $t > t(r)$. We can serve several pending requests at the same time, and the service cost of a service corresponds to the weight of the subtree that contains all the requests served and the root of $T$. Postponing the service of a request $r$ to time $t > t(r)$ generates an additional delay cost of $t - t(r)$. The goal is to serve all requests in an online manner such that the total cost (i.e., the total sum of service and delay costs) is minimized. The current best algorithm for this problem achieves a competitive ratio of $O(d^2)$ (Azar and Touitou, FOCS'19), where $d$ denotes the depth of the tree.   Here, we consider a stochastic version of MLA where the requests follow a Poisson arrival process. We present a deterministic online algorithm which achieves a constant ratio of expectations, meaning that the ratio between the expected costs of the solution generated by our algorithm and the optimal offline solution is bounded by a constant. Our algorithm is obtained by carefully combining two strategies. In the first one, we plan periodic oblivious visits to the subset of frequent vertices, whereas in the second one, we greedily serve the pending requests in the remaining vertices. This problem is complex enough to demonstrate a very rare phenomenon that ``single-minded\" or ``sample-average\" strategies are not enough in stochastic optimization.","sentences":["This paper presents a new research direction for online Multi-Level Aggregation (MLA) with delays.","In this problem, we are given an edge-weighted rooted tree $T$, and we have to serve a sequence of requests arriving at its vertices in an online manner.","Each request $r$ is characterized by two parameters: its arrival time $t(r)$ and location $l(r)$ (a vertex).","Once a request $r$ arrives, we can either serve it immediately or postpone this action until any time $t > t(r)$. We can serve several pending requests at the same time, and the service cost of a service corresponds to the weight of the subtree that contains all the requests served and the root of $T$. Postponing the service of a request $r$ to time $t >","t(r)$ generates an additional delay cost of $t - t(r)$. The goal is to serve all requests in an online manner such that the total cost (i.e., the total sum of service and delay costs) is minimized.","The current best algorithm for this problem achieves a competitive ratio of $O(d^2)$ (Azar and Touitou, FOCS'19), where $d$ denotes the depth of the tree.   ","Here, we consider a stochastic version of MLA where the requests follow a Poisson arrival process.","We present a deterministic online algorithm which achieves a constant ratio of expectations, meaning that the ratio between the expected costs of the solution generated by our algorithm and the optimal offline solution is bounded by a constant.","Our algorithm is obtained by carefully combining two strategies.","In the first one, we plan periodic oblivious visits to the subset of frequent vertices, whereas in the second one, we greedily serve the pending requests in the remaining vertices.","This problem is complex enough to demonstrate a very rare phenomenon that ``single-minded\" or ``sample-average\" strategies are not enough in stochastic optimization."],"url":"http://arxiv.org/abs/2404.09711v1"}
{"created":"2024-04-15 12:06:22","title":"Kernel-based learning with guarantees for multi-agent applications","abstract":"This paper addresses a kernel-based learning problem for a network of agents locally observing a latent multidimensional, nonlinear phenomenon in a noisy environment. We propose a learning algorithm that requires only mild a priori knowledge about the phenomenon under investigation and delivers a model with corresponding non-asymptotic high probability error bounds. Both non-asymptotic analysis of the method and numerical simulation results are presented and discussed in the paper.","sentences":["This paper addresses a kernel-based learning problem for a network of agents locally observing a latent multidimensional, nonlinear phenomenon in a noisy environment.","We propose a learning algorithm that requires only mild a priori knowledge about the phenomenon under investigation and delivers a model with corresponding non-asymptotic high probability error bounds.","Both non-asymptotic analysis of the method and numerical simulation results are presented and discussed in the paper."],"url":"http://arxiv.org/abs/2404.09708v1"}
{"created":"2024-04-15 12:06:00","title":"Adaptive Patching for High-resolution Image Segmentation with Transformers","abstract":"Attention-based models are proliferating in the space of image analytics, including segmentation. The standard method of feeding images to transformer encoders is to divide the images into patches and then feed the patches to the model as a linear sequence of tokens. For high-resolution images, e.g. microscopic pathology images, the quadratic compute and memory cost prohibits the use of an attention-based model, if we are to use smaller patch sizes that are favorable in segmentation. The solution is to either use custom complex multi-resolution models or approximate attention schemes. We take inspiration from Adapative Mesh Refinement (AMR) methods in HPC by adaptively patching the images, as a pre-processing step, based on the image details to reduce the number of patches being fed to the model, by orders of magnitude. This method has a negligible overhead, and works seamlessly with any attention-based model, i.e. it is a pre-processing step that can be adopted by any attention-based model without friction. We demonstrate superior segmentation quality over SoTA segmentation models for real-world pathology datasets while gaining a geomean speedup of $6.9\\times$ for resolutions up to $64K^2$, on up to $2,048$ GPUs.","sentences":["Attention-based models are proliferating in the space of image analytics, including segmentation.","The standard method of feeding images to transformer encoders is to divide the images into patches and then feed the patches to the model as a linear sequence of tokens.","For high-resolution images, e.g. microscopic pathology images, the quadratic compute and memory cost prohibits the use of an attention-based model, if we are to use smaller patch sizes that are favorable in segmentation.","The solution is to either use custom complex multi-resolution models or approximate attention schemes.","We take inspiration from Adapative Mesh Refinement (AMR) methods in HPC by adaptively patching the images, as a pre-processing step, based on the image details to reduce the number of patches being fed to the model, by orders of magnitude.","This method has a negligible overhead, and works seamlessly with any attention-based model, i.e. it is a pre-processing step that can be adopted by any attention-based model without friction.","We demonstrate superior segmentation quality over SoTA segmentation models for real-world pathology datasets while gaining a geomean speedup of $6.9\\times$ for resolutions up to $64K^2$, on up to $2,048$ GPUs."],"url":"http://arxiv.org/abs/2404.09707v1"}
{"created":"2024-04-15 12:02:59","title":"Enhancing Robot Explanation Capabilities through Vision-Language Models: a Preliminary Study by Interpreting Visual Inputs for Improved Human-Robot Interaction","abstract":"This paper presents an improved system based on our prior work, designed to create explanations for autonomous robot actions during Human-Robot Interaction (HRI). Previously, we developed a system that used Large Language Models (LLMs) to interpret logs and produce natural language explanations. In this study, we expand our approach by incorporating Vision-Language Models (VLMs), enabling the system to analyze textual logs with the added context of visual input. This method allows for generating explanations that combine data from the robot's logs and the images it captures. We tested this enhanced system on a basic navigation task where the robot needs to avoid a human obstacle. The findings from this preliminary study indicate that adding visual interpretation improves our system's explanations by precisely identifying obstacles and increasing the accuracy of the explanations provided.","sentences":["This paper presents an improved system based on our prior work, designed to create explanations for autonomous robot actions during Human-Robot Interaction (HRI).","Previously, we developed a system that used Large Language Models (LLMs) to interpret logs and produce natural language explanations.","In this study, we expand our approach by incorporating Vision-Language Models (VLMs), enabling the system to analyze textual logs with the added context of visual input.","This method allows for generating explanations that combine data from the robot's logs and the images it captures.","We tested this enhanced system on a basic navigation task where the robot needs to avoid a human obstacle.","The findings from this preliminary study indicate that adding visual interpretation improves our system's explanations by precisely identifying obstacles and increasing the accuracy of the explanations provided."],"url":"http://arxiv.org/abs/2404.09705v1"}
{"created":"2024-04-15 12:01:42","title":"AI Competitions and Benchmarks: Dataset Development","abstract":"Machine learning is now used in many applications thanks to its ability to predict, generate, or discover patterns from large quantities of data. However, the process of collecting and transforming data for practical use is intricate. Even in today's digital era, where substantial data is generated daily, it is uncommon for it to be readily usable; most often, it necessitates meticulous manual data preparation. The haste in developing new models can frequently result in various shortcomings, potentially posing risks when deployed in real-world scenarios (eg social discrimination, critical failures), leading to the failure or substantial escalation of costs in AI-based projects. This chapter provides a comprehensive overview of established methodological tools, enriched by our practical experience, in the development of datasets for machine learning. Initially, we develop the tasks involved in dataset development and offer insights into their effective management (including requirements, design, implementation, evaluation, distribution, and maintenance). Then, we provide more details about the implementation process which includes data collection, transformation, and quality evaluation. Finally, we address practical considerations regarding dataset distribution and maintenance.","sentences":["Machine learning is now used in many applications thanks to its ability to predict, generate, or discover patterns from large quantities of data.","However, the process of collecting and transforming data for practical use is intricate.","Even in today's digital era, where substantial data is generated daily, it is uncommon for it to be readily usable; most often, it necessitates meticulous manual data preparation.","The haste in developing new models can frequently result in various shortcomings, potentially posing risks when deployed in real-world scenarios (eg social discrimination, critical failures), leading to the failure or substantial escalation of costs in AI-based projects.","This chapter provides a comprehensive overview of established methodological tools, enriched by our practical experience, in the development of datasets for machine learning.","Initially, we develop the tasks involved in dataset development and offer insights into their effective management (including requirements, design, implementation, evaluation, distribution, and maintenance).","Then, we provide more details about the implementation process which includes data collection, transformation, and quality evaluation.","Finally, we address practical considerations regarding dataset distribution and maintenance."],"url":"http://arxiv.org/abs/2404.09703v1"}
{"created":"2024-04-15 11:59:45","title":"Generative AI for Game Theory-based Mobile Networking","abstract":"With the continuous advancement of network technology, various emerging complex networking optimization problems opened up a wide range of applications utilizating of game theory. However, since game theory is a mathematical framework, game theory-based solutions often require the experience and knowledge of human experts. Recently, the remarkable advantages exhibited by generative artificial intelligence (GAI) have gained widespread attention. In this article, we propose a novel GAI-enabled game theory solution that combines the powerful reasoning and generation capabilities of GAI to the design and optimization of mobile networking. Specifically, we first outline the game theory and key technologies of GAI, and then explore the advantages of combining GAI with game theory. Then, we briefly review the advantages and limitations of existing research and demonstrate the potential application values of GAI applied to game theory in mobile networking. Subsequently, we develop a game theory framework enabled by large language models (LLMs) to realize this combination, and demonstrate the effectiveness of the proposed framework through a case study in secured UAV networks. Finally, we provide several directions for future extensions.","sentences":["With the continuous advancement of network technology, various emerging complex networking optimization problems opened up a wide range of applications utilizating of game theory.","However, since game theory is a mathematical framework, game theory-based solutions often require the experience and knowledge of human experts.","Recently, the remarkable advantages exhibited by generative artificial intelligence (GAI) have gained widespread attention.","In this article, we propose a novel GAI-enabled game theory solution that combines the powerful reasoning and generation capabilities of GAI to the design and optimization of mobile networking.","Specifically, we first outline the game theory and key technologies of GAI, and then explore the advantages of combining GAI with game theory.","Then, we briefly review the advantages and limitations of existing research and demonstrate the potential application values of GAI applied to game theory in mobile networking.","Subsequently, we develop a game theory framework enabled by large language models (LLMs) to realize this combination, and demonstrate the effectiveness of the proposed framework through a case study in secured UAV networks.","Finally, we provide several directions for future extensions."],"url":"http://arxiv.org/abs/2404.09699v1"}
{"created":"2024-04-15 11:59:19","title":"HSIDMamba: Exploring Bidirectional State-Space Models for Hyperspectral Denoising","abstract":"Effectively discerning spatial-spectral dependencies in HSI denoising is crucial, but prevailing methods using convolution or transformers still face computational efficiency limitations. Recently, the emerging Selective State Space Model(Mamba) has risen with its nearly linear computational complexity in processing natural language sequences, which inspired us to explore its potential in handling long spectral sequences. In this paper, we propose HSIDMamba(HSDM), tailored to exploit the linear complexity for effectively capturing spatial-spectral dependencies in HSI denoising. In particular, HSDM comprises multiple Hyperspectral Continuous Scan Blocks, incorporating BCSM(Bidirectional Continuous Scanning Mechanism), scale residual, and spectral attention mechanisms to enhance the capture of long-range and local spatial-spectral information. BCSM strengthens spatial-spectral interactions by linking forward and backward scans and enhancing information from eight directions through SSM, significantly enhancing the perceptual capability of HSDM and improving denoising performance more effectively. Extensive evaluations against HSI denoising benchmarks validate the superior performance of HSDM, achieving state-of-the-art results in performance and surpassing the efficiency of the latest transformer architectures by $30\\%$.","sentences":["Effectively discerning spatial-spectral dependencies in HSI denoising is crucial, but prevailing methods using convolution or transformers still face computational efficiency limitations.","Recently, the emerging Selective State Space Model(Mamba) has risen with its nearly linear computational complexity in processing natural language sequences, which inspired us to explore its potential in handling long spectral sequences.","In this paper, we propose HSIDMamba(HSDM), tailored to exploit the linear complexity for effectively capturing spatial-spectral dependencies in HSI denoising.","In particular, HSDM comprises multiple Hyperspectral Continuous Scan Blocks, incorporating BCSM(Bidirectional Continuous Scanning Mechanism), scale residual, and spectral attention mechanisms to enhance the capture of long-range and local spatial-spectral information.","BCSM strengthens spatial-spectral interactions by linking forward and backward scans and enhancing information from eight directions through SSM, significantly enhancing the perceptual capability of HSDM and improving denoising performance more effectively.","Extensive evaluations against HSI denoising benchmarks validate the superior performance of HSDM, achieving state-of-the-art results in performance and surpassing the efficiency of the latest transformer architectures by $30\\%$."],"url":"http://arxiv.org/abs/2404.09697v1"}
{"created":"2024-04-15 11:54:27","title":"Are Large Language Models Reliable Argument Quality Annotators?","abstract":"Evaluating the quality of arguments is a crucial aspect of any system leveraging argument mining. However, it is a challenge to obtain reliable and consistent annotations regarding argument quality, as this usually requires domain-specific expertise of the annotators. Even among experts, the assessment of argument quality is often inconsistent due to the inherent subjectivity of this task. In this paper, we study the potential of using state-of-the-art large language models (LLMs) as proxies for argument quality annotators. To assess the capability of LLMs in this regard, we analyze the agreement between model, human expert, and human novice annotators based on an established taxonomy of argument quality dimensions. Our findings highlight that LLMs can produce consistent annotations, with a moderately high agreement with human experts across most of the quality dimensions. Moreover, we show that using LLMs as additional annotators can significantly improve the agreement between annotators. These results suggest that LLMs can serve as a valuable tool for automated argument quality assessment, thus streamlining and accelerating the evaluation of large argument datasets.","sentences":["Evaluating the quality of arguments is a crucial aspect of any system leveraging argument mining.","However, it is a challenge to obtain reliable and consistent annotations regarding argument quality, as this usually requires domain-specific expertise of the annotators.","Even among experts, the assessment of argument quality is often inconsistent due to the inherent subjectivity of this task.","In this paper, we study the potential of using state-of-the-art large language models (LLMs) as proxies for argument quality annotators.","To assess the capability of LLMs in this regard, we analyze the agreement between model, human expert, and human novice annotators based on an established taxonomy of argument quality dimensions.","Our findings highlight that LLMs can produce consistent annotations, with a moderately high agreement with human experts across most of the quality dimensions.","Moreover, we show that using LLMs as additional annotators can significantly improve the agreement between annotators.","These results suggest that LLMs can serve as a valuable tool for automated argument quality assessment, thus streamlining and accelerating the evaluation of large argument datasets."],"url":"http://arxiv.org/abs/2404.09696v1"}
{"created":"2024-04-15 11:53:22","title":"LoRAP: Transformer Sub-Layers Deserve Differentiated Structured Compression for Large Language Models","abstract":"Large language models (LLMs) show excellent performance in difficult tasks, but they often require massive memories and computational resources. How to reduce the parameter scale of LLMs has become research hotspots. In this study, we make an important observation that the multi-head self-attention (MHA) sub-layer of Transformer exhibits noticeable low-rank structure, while the feed-forward network (FFN) sub-layer does not. With this regard, we design a mixed compression model, which organically combines Low-Rank matrix approximation And structured Pruning (LoRAP). For the MHA sub-layer, we propose an input activation weighted singular value decomposition method to strengthen the low-rank characteristic. Furthermore, we discover that the weight matrices in MHA sub-layer have different low-rank degrees. Thus, a novel parameter allocation scheme according to the discrepancy of low-rank degrees is devised. For the FFN sub-layer, we propose a gradient-free structured channel pruning method. During the pruning, we get an interesting finding that the least important 1% of parameter actually play a vital role in model performance. Extensive evaluations on zero-shot perplexity and zero-shot task classification indicate that our proposal is superior to previous structured compression rivals under multiple compression ratios.","sentences":["Large language models (LLMs) show excellent performance in difficult tasks, but they often require massive memories and computational resources.","How to reduce the parameter scale of LLMs has become research hotspots.","In this study, we make an important observation that the multi-head self-attention (MHA) sub-layer of Transformer exhibits noticeable low-rank structure, while the feed-forward network (FFN) sub-layer does not.","With this regard, we design a mixed compression model, which organically combines Low-Rank matrix approximation And structured Pruning (LoRAP).","For the MHA sub-layer, we propose an input activation weighted singular value decomposition method to strengthen the low-rank characteristic.","Furthermore, we discover that the weight matrices in MHA sub-layer have different low-rank degrees.","Thus, a novel parameter allocation scheme according to the discrepancy of low-rank degrees is devised.","For the FFN sub-layer, we propose a gradient-free structured channel pruning method.","During the pruning, we get an interesting finding that the least important 1% of parameter actually play a vital role in model performance.","Extensive evaluations on zero-shot perplexity and zero-shot task classification indicate that our proposal is superior to previous structured compression rivals under multiple compression ratios."],"url":"http://arxiv.org/abs/2404.09695v1"}
{"created":"2024-04-15 11:46:24","title":"XoFTR: Cross-modal Feature Matching Transformer","abstract":"We introduce, XoFTR, a cross-modal cross-view method for local feature matching between thermal infrared (TIR) and visible images. Unlike visible images, TIR images are less susceptible to adverse lighting and weather conditions but present difficulties in matching due to significant texture and intensity differences. Current hand-crafted and learning-based methods for visible-TIR matching fall short in handling viewpoint, scale, and texture diversities. To address this, XoFTR incorporates masked image modeling pre-training and fine-tuning with pseudo-thermal image augmentation to handle the modality differences. Additionally, we introduce a refined matching pipeline that adjusts for scale discrepancies and enhances match reliability through sub-pixel level refinement. To validate our approach, we collect a comprehensive visible-thermal dataset, and show that our method outperforms existing methods on many benchmarks.","sentences":["We introduce, XoFTR, a cross-modal cross-view method for local feature matching between thermal infrared (TIR) and visible images.","Unlike visible images, TIR images are less susceptible to adverse lighting and weather conditions but present difficulties in matching due to significant texture and intensity differences.","Current hand-crafted and learning-based methods for visible-TIR matching fall short in handling viewpoint, scale, and texture diversities.","To address this, XoFTR incorporates masked image modeling pre-training and fine-tuning with pseudo-thermal image augmentation to handle the modality differences.","Additionally, we introduce a refined matching pipeline that adjusts for scale discrepancies and enhances match reliability through sub-pixel level refinement.","To validate our approach, we collect a comprehensive visible-thermal dataset, and show that our method outperforms existing methods on many benchmarks."],"url":"http://arxiv.org/abs/2404.09692v1"}
{"created":"2024-04-15 11:46:06","title":"Dynamic Ego-Velocity estimation Using Moving mmWave Radar: A Phase-Based Approach","abstract":"Precise ego-motion measurement is crucial for various applications, including robotics, augmented reality, and autonomous navigation. In this poster, we propose mmPhase, an odometry framework based on single-chip millimetre-wave (mmWave) radar for robust ego-motion estimation in mobile platforms without requiring additional modalities like the visual, wheel, or inertial odometry. mmPhase leverages a phase-based velocity estimation approach to overcome the limitations of conventional doppler resolution. For real-world evaluations of mmPhase we have developed an ego-vehicle prototype. Compared to the state-of-the-art baselines, mmPhase shows superior performance in ego-velocity estimation.","sentences":["Precise ego-motion measurement is crucial for various applications, including robotics, augmented reality, and autonomous navigation.","In this poster, we propose mmPhase, an odometry framework based on single-chip millimetre-wave (mmWave) radar for robust ego-motion estimation in mobile platforms without requiring additional modalities like the visual, wheel, or inertial odometry.","mmPhase leverages a phase-based velocity estimation approach to overcome the limitations of conventional doppler resolution.","For real-world evaluations of mmPhase we have developed an ego-vehicle prototype.","Compared to the state-of-the-art baselines, mmPhase shows superior performance in ego-velocity estimation."],"url":"http://arxiv.org/abs/2404.09691v1"}
{"created":"2024-04-15 11:45:30","title":"Harnessing GPT-4V(ision) for Insurance: A Preliminary Exploration","abstract":"The emergence of Large Multimodal Models (LMMs) marks a significant milestone in the development of artificial intelligence. Insurance, as a vast and complex discipline, involves a wide variety of data forms in its operational processes, including text, images, and videos, thereby giving rise to diverse multimodal tasks. Despite this, there has been limited systematic exploration of multimodal tasks specific to insurance, nor a thorough investigation into how LMMs can address these challenges. In this paper, we explore GPT-4V's capabilities in the insurance domain. We categorize multimodal tasks by focusing primarily on visual aspects based on types of insurance (e.g., auto, household/commercial property, health, and agricultural insurance) and insurance stages (e.g., risk assessment, risk monitoring, and claims processing). Our experiment reveals that GPT-4V exhibits remarkable abilities in insurance-related tasks, demonstrating not only a robust understanding of multimodal content in the insurance domain but also a comprehensive knowledge of insurance scenarios. However, there are notable shortcomings: GPT-4V struggles with detailed risk rating and loss assessment, suffers from hallucination in image understanding, and shows variable support for different languages. Through this work, we aim to bridge the insurance domain with cutting-edge LMM technology, facilitate interdisciplinary exchange and development, and provide a foundation for the continued advancement and evolution of future research endeavors.","sentences":["The emergence of Large Multimodal Models (LMMs) marks a significant milestone in the development of artificial intelligence.","Insurance, as a vast and complex discipline, involves a wide variety of data forms in its operational processes, including text, images, and videos, thereby giving rise to diverse multimodal tasks.","Despite this, there has been limited systematic exploration of multimodal tasks specific to insurance, nor a thorough investigation into how LMMs can address these challenges.","In this paper, we explore GPT-4V's capabilities in the insurance domain.","We categorize multimodal tasks by focusing primarily on visual aspects based on types of insurance (e.g., auto, household/commercial property, health, and agricultural insurance) and insurance stages (e.g., risk assessment, risk monitoring, and claims processing).","Our experiment reveals that GPT-4V exhibits remarkable abilities in insurance-related tasks, demonstrating not only a robust understanding of multimodal content in the insurance domain but also a comprehensive knowledge of insurance scenarios.","However, there are notable shortcomings: GPT-4V struggles with detailed risk rating and loss assessment, suffers from hallucination in image understanding, and shows variable support for different languages.","Through this work, we aim to bridge the insurance domain with cutting-edge LMM technology, facilitate interdisciplinary exchange and development, and provide a foundation for the continued advancement and evolution of future research endeavors."],"url":"http://arxiv.org/abs/2404.09690v1"}
{"created":"2024-04-15 11:38:28","title":"UAV Navigation in Tunnels with 2D tilted LiDARs","abstract":"Navigation of UAVs in challenging environments like tunnels or mines, where it is not possible to use GNSS methods to self-localize, illumination may be uneven or nonexistent, and wall features are likely to be scarce, is a complex task, especially if the navigation has to be done at high speed. In this paper we propose a novel proof-of-concept navigation technique for UAVs based on the use of LiDAR information through the joint use of geometric and machine-learning algorithms. The perceived information is processed by a deep neural network to establish the yaw of the UAV with respect to the tunnel's longitudinal axis, in order to adjust the direction of navigation. Additionally, a geometric method is used to compute the safest location inside the tunnel (i.e. the one that maximizes the distance to the closest obstacle). This information proves to be sufficient for simple yet effective navigation in straight and curved tunnels.","sentences":["Navigation of UAVs in challenging environments like tunnels or mines, where it is not possible to use GNSS methods to self-localize, illumination may be uneven or nonexistent, and wall features are likely to be scarce, is a complex task, especially if the navigation has to be done at high speed.","In this paper we propose a novel proof-of-concept navigation technique for UAVs based on the use of LiDAR information through the joint use of geometric and machine-learning algorithms.","The perceived information is processed by a deep neural network to establish the yaw of the UAV with respect to the tunnel's longitudinal axis, in order to adjust the direction of navigation.","Additionally, a geometric method is used to compute the safest location inside the tunnel (i.e. the one that maximizes the distance to the closest obstacle).","This information proves to be sufficient for simple yet effective navigation in straight and curved tunnels."],"url":"http://arxiv.org/abs/2404.09688v1"}
{"created":"2024-04-15 11:37:47","title":"Plus Strategies are Exponentially Slower for Planted Optima of Random Height","abstract":"We compare the $(1,\\lambda)$-EA and the $(1 + \\lambda)$-EA on the recently introduced benchmark DisOM, which is the OneMax function with randomly planted local optima. Previous work showed that if all local optima have the same relative height, then the plus strategy never loses more than a factor $O(n\\log n)$ compared to the comma strategy. Here we show that even small random fluctuations in the heights of the local optima have a devastating effect for the plus strategy and lead to super-polynomial runtimes. On the other hand, due to their ability to escape local optima, comma strategies are unaffected by the height of the local optima and remain efficient. Our results hold for a broad class of possible distortions and show that the plus strategy, but not the comma strategy, is generally deceived by sparse unstructured fluctuations of a smooth landscape.","sentences":["We compare the $(1,\\lambda)$-EA and the $(1 + \\lambda)$-EA on the recently introduced benchmark DisOM, which is the OneMax function with randomly planted local optima.","Previous work showed that if all local optima have the same relative height, then the plus strategy never loses more than a factor $O(n\\log n)$ compared to the comma strategy.","Here we show that even small random fluctuations in the heights of the local optima have a devastating effect for the plus strategy and lead to super-polynomial runtimes.","On the other hand, due to their ability to escape local optima, comma strategies are unaffected by the height of the local optima and remain efficient.","Our results hold for a broad class of possible distortions and show that the plus strategy, but not the comma strategy, is generally deceived by sparse unstructured fluctuations of a smooth landscape."],"url":"http://arxiv.org/abs/2404.09687v1"}
{"created":"2024-04-15 11:37:40","title":"AntBatchInfer: Elastic Batch Inference in the Kubernetes Cluster","abstract":"Offline batch inference is a common task in the industry for deep learning applications, but it can be challenging to ensure stability and performance when dealing with large amounts of data and complicated inference pipelines. This paper demonstrated AntBatchInfer, an elastic batch inference framework, which is specially optimized for the non-dedicated cluster. AntBatchInfer addresses these challenges by providing multi-level fault-tolerant capabilities, enabling the stable execution of versatile and long-running inference tasks. It also improves inference efficiency by pipelining, intra-node, and inter-node scaling. It further optimizes the performance in complicated multiple-model batch inference scenarios. Through extensive experiments and real-world statistics, we demonstrate the superiority of our framework in terms of stability and efficiency. In the experiment, it outperforms the baseline by at least $2\\times$ and $6\\times$ in the single-model or multiple-model batch inference. Also, it is widely used at Ant Group, with thousands of daily jobs from various scenarios, including DLRM, CV, and NLP, which proves its practicability in the industry.","sentences":["Offline batch inference is a common task in the industry for deep learning applications, but it can be challenging to ensure stability and performance when dealing with large amounts of data and complicated inference pipelines.","This paper demonstrated AntBatchInfer, an elastic batch inference framework, which is specially optimized for the non-dedicated cluster.","AntBatchInfer addresses these challenges by providing multi-level fault-tolerant capabilities, enabling the stable execution of versatile and long-running inference tasks.","It also improves inference efficiency by pipelining, intra-node, and inter-node scaling.","It further optimizes the performance in complicated multiple-model batch inference scenarios.","Through extensive experiments and real-world statistics, we demonstrate the superiority of our framework in terms of stability and efficiency.","In the experiment, it outperforms the baseline by at least $2\\times$ and $6\\times$ in the single-model or multiple-model batch inference.","Also, it is widely used at Ant Group, with thousands of daily jobs from various scenarios, including DLRM, CV, and NLP, which proves its practicability in the industry."],"url":"http://arxiv.org/abs/2404.09686v1"}
{"created":"2024-04-15 11:36:58","title":"Bridging the Gap: Advancements in Technology to Support Dementia Care -- A Scoping Review","abstract":"Dementia has serious consequences for the daily life of the person affected due to the decline in the their cognitive, behavioral and functional abilities. Caring for people living with dementia can be challenging and distressing. Innovative solutions are becoming essential to enrich the lives of those impacted and alleviate caregiver burdens. This scoping review, spanning literature from 2010 to July 2023 in the field of Human-Computer Interaction (HCI), offers a comprehensive look at how interactive technology contributes to dementia care. Emphasizing technology's role in addressing the unique needs of people with dementia (PwD) and their caregivers, this review encompasses assistive devices, mobile applications, sensors, and GPS tracking. Delving into challenges encountered in clinical and home-care settings, it succinctly outlines the influence of cutting-edge technologies, such as wearables, virtual reality, robots, and artificial intelligence, in supporting individuals with dementia and their caregivers. We categorize current dementia-related technologies into six groups based on their intended use and function: 1) daily life monitoring, 2) daily life support, 3) social interaction and communication, 4) well-being enhancement, 5) cognitive support, and 6) caregiver support.","sentences":["Dementia has serious consequences for the daily life of the person affected due to the decline in the their cognitive, behavioral and functional abilities.","Caring for people living with dementia can be challenging and distressing.","Innovative solutions are becoming essential to enrich the lives of those impacted and alleviate caregiver burdens.","This scoping review, spanning literature from 2010 to July 2023 in the field of Human-Computer Interaction (HCI), offers a comprehensive look at how interactive technology contributes to dementia care.","Emphasizing technology's role in addressing the unique needs of people with dementia (PwD) and their caregivers, this review encompasses assistive devices, mobile applications, sensors, and GPS tracking.","Delving into challenges encountered in clinical and home-care settings, it succinctly outlines the influence of cutting-edge technologies, such as wearables, virtual reality, robots, and artificial intelligence, in supporting individuals with dementia and their caregivers.","We categorize current dementia-related technologies into six groups based on their intended use and function: 1) daily life monitoring, 2) daily life support, 3) social interaction and communication, 4) well-being enhancement, 5) cognitive support, and 6) caregiver support."],"url":"http://arxiv.org/abs/2404.09685v1"}
{"created":"2024-04-15 11:36:10","title":"Multi-News+: Cost-efficient Dataset Cleansing via LLM-based Data Annotation","abstract":"The quality of the dataset is crucial for ensuring optimal performance and reliability of downstream task models. However, datasets often contain noisy data inadvertently included during the construction process. Numerous attempts have been made to correct this issue through human annotators. However, hiring and managing human annotators is expensive and time-consuming. As an alternative, recent studies are exploring the use of large language models (LLMs) for data annotation.   In this study, we present a case study that extends the application of LLM-based data annotation to enhance the quality of existing datasets through a cleansing strategy. Specifically, we leverage approaches such as chain-of-thought (CoT) and majority voting to imitate human annotation and classify unrelated documents from the Multi-News dataset, which is widely used for the multi-document summarization task. Through our proposed cleansing method, we introduce an enhanced Multi-News+. By employing LLMs for data cleansing, we demonstrate an efficient and effective approach to improving dataset quality without relying on expensive human annotation efforts.","sentences":["The quality of the dataset is crucial for ensuring optimal performance and reliability of downstream task models.","However, datasets often contain noisy data inadvertently included during the construction process.","Numerous attempts have been made to correct this issue through human annotators.","However, hiring and managing human annotators is expensive and time-consuming.","As an alternative, recent studies are exploring the use of large language models (LLMs) for data annotation.   ","In this study, we present a case study that extends the application of LLM-based data annotation to enhance the quality of existing datasets through a cleansing strategy.","Specifically, we leverage approaches such as chain-of-thought (CoT) and majority voting to imitate human annotation and classify unrelated documents from the Multi-News dataset, which is widely used for the multi-document summarization task.","Through our proposed cleansing method, we introduce an enhanced Multi-News+.","By employing LLMs for data cleansing, we demonstrate an efficient and effective approach to improving dataset quality without relying on expensive human annotation efforts."],"url":"http://arxiv.org/abs/2404.09682v1"}
{"created":"2024-04-15 11:35:15","title":"An Empirical Study of Open Edge Computing Platforms: Ecosystem, Usage, and Security Risks","abstract":"Emerging in recent years, open edge computing platforms (OECPs) claim large-scale edge nodes, the extensive usage and adoption, as well as the openness to any third parties to join as edge nodes. For instance, OneThingCloud, a major OECP operated in China, advertises 5 million edge nodes, 70TB bandwidth, and 1,500PB storage. However, little information is publicly available for such OECPs with regards to their technical mechanisms and involvement in edge computing activities. Furthermore, different from known edge computing paradigms, OECPs feature an open ecosystem wherein any third party can participate as edge nodes and earn revenue for the contribution of computing and bandwidth resources, which, however, can introduce byzantine or even malicious edge nodes and thus break the traditional threat model for edge computing. In this study, we conduct the first empirical study on two representative OECPs, which is made possible through the deployment of edge nodes across locations, the efficient and semi-automatic analysis of edge traffic as well as the carefully designed security experiments. As the results, a set of novel findings and insights have been distilled with regards to their technical mechanisms, the landscape of edge nodes, the usage and adoption, and the practical security/privacy risks. Particularly, millions of daily active edge nodes have been observed, which feature a wide distribution in the network space and the extensive adoption in content delivery towards end users of 16 popular Internet services. Also, multiple practical and concerning security risks have been identified along with acknowledgements received from relevant parties, e.g., the exposure of long-term and cross-edge-node credentials, the co-location with malicious activities of diverse categories, the failures of TLS certificate verification, the extensive information leakage against end users, etc.","sentences":["Emerging in recent years, open edge computing platforms (OECPs) claim large-scale edge nodes, the extensive usage and adoption, as well as the openness to any third parties to join as edge nodes.","For instance, OneThingCloud, a major OECP operated in China, advertises 5 million edge nodes, 70TB bandwidth, and 1,500PB storage.","However, little information is publicly available for such OECPs with regards to their technical mechanisms and involvement in edge computing activities.","Furthermore, different from known edge computing paradigms, OECPs feature an open ecosystem wherein any third party can participate as edge nodes and earn revenue for the contribution of computing and bandwidth resources, which, however, can introduce byzantine or even malicious edge nodes and thus break the traditional threat model for edge computing.","In this study, we conduct the first empirical study on two representative OECPs, which is made possible through the deployment of edge nodes across locations, the efficient and semi-automatic analysis of edge traffic as well as the carefully designed security experiments.","As the results, a set of novel findings and insights have been distilled with regards to their technical mechanisms, the landscape of edge nodes, the usage and adoption, and the practical security/privacy risks.","Particularly, millions of daily active edge nodes have been observed, which feature a wide distribution in the network space and the extensive adoption in content delivery towards end users of 16 popular Internet services.","Also, multiple practical and concerning security risks have been identified along with acknowledgements received from relevant parties, e.g., the exposure of long-term and cross-edge-node credentials, the co-location with malicious activities of diverse categories, the failures of TLS certificate verification, the extensive information leakage against end users, etc."],"url":"http://arxiv.org/abs/2404.09681v1"}
{"created":"2024-04-15 11:20:44","title":"AntDT: A Self-Adaptive Distributed Training Framework for Leader and Straggler Nodes","abstract":"Many distributed training techniques like Parameter Server and AllReduce have been proposed to take advantage of the increasingly large data and rich features. However, stragglers frequently occur in distributed training due to resource contention and hardware heterogeneity, which significantly hampers the training efficiency. Previous works only address part of the stragglers and could not adaptively solve various stragglers in practice. Additionally, it is challenging to use a systematic framework to address all stragglers because different stragglers require diverse data allocation and fault-tolerance mechanisms. Therefore, this paper proposes a unified distributed training framework called AntDT (Ant Distributed Training Framework) to adaptively solve the straggler problems. Firstly, the framework consists of four components, including the Stateful Dynamic Data Sharding service, Monitor, Controller, and Agent. These components work collaboratively to efficiently distribute workloads and provide a range of pre-defined straggler mitigation methods with fault tolerance, thereby hiding messy details of data allocation and fault handling. Secondly, the framework provides a high degree of flexibility, allowing for the customization of straggler mitigation solutions based on the specific circumstances of the cluster. Leveraging this flexibility, we introduce two straggler mitigation solutions, namely AntDT-ND for non-dedicated clusters and AntDT-DD for dedicated clusters, as practical examples to resolve various types of stragglers at Ant Group. Justified by our comprehensive experiments and industrial deployment statistics, AntDT outperforms other SOTA methods more than 3x in terms of training efficiency. Additionally, in Alipay's homepage recommendation scenario, using AntDT reduces the training duration of the ranking model from 27.8 hours to just 5.4 hours.","sentences":["Many distributed training techniques like Parameter Server and AllReduce have been proposed to take advantage of the increasingly large data and rich features.","However, stragglers frequently occur in distributed training due to resource contention and hardware heterogeneity, which significantly hampers the training efficiency.","Previous works only address part of the stragglers and could not adaptively solve various stragglers in practice.","Additionally, it is challenging to use a systematic framework to address all stragglers because different stragglers require diverse data allocation and fault-tolerance mechanisms.","Therefore, this paper proposes a unified distributed training framework called AntDT (Ant Distributed Training Framework) to adaptively solve the straggler problems.","Firstly, the framework consists of four components, including the Stateful Dynamic Data Sharding service, Monitor, Controller, and Agent.","These components work collaboratively to efficiently distribute workloads and provide a range of pre-defined straggler mitigation methods with fault tolerance, thereby hiding messy details of data allocation and fault handling.","Secondly, the framework provides a high degree of flexibility, allowing for the customization of straggler mitigation solutions based on the specific circumstances of the cluster.","Leveraging this flexibility, we introduce two straggler mitigation solutions, namely AntDT-ND for non-dedicated clusters and AntDT-DD for dedicated clusters, as practical examples to resolve various types of stragglers at Ant Group.","Justified by our comprehensive experiments and industrial deployment statistics, AntDT outperforms other SOTA methods more than 3x in terms of training efficiency.","Additionally, in Alipay's homepage recommendation scenario, using AntDT reduces the training duration of the ranking model from 27.8 hours to just 5.4 hours."],"url":"http://arxiv.org/abs/2404.09679v1"}
{"created":"2024-04-15 11:14:25","title":"A Generic Trajectory Planning Method for Constrained All-Wheel-Steering Robots","abstract":"This paper presents a trajectory planning method for wheeled robots with fixed steering axes while the steering angle of each wheel is constrained. In the past, All-Wheel-Steering(AWS) robots, incorporating modes such as rotation-free translation maneuvers, in-situ rotational maneuvers, and proportional steering, exhibited inefficient performance due to time-consuming mode switches. This inefficiency arises from wheel rotation constraints and inter-wheel cooperation requirements. The direct application of a holonomic moving strategy can lead to significant slip angles or even structural failure. Additionally, the limited steering range of AWS wheeled robots exacerbates nonlinearity issues, thereby complicating control processes. To address these challenges, we developed a novel planning method termed Constrained AWS(C-AWS), which integrates second-order discrete search with predictive control techniques. Experimental results demonstrate that our method adeptly generates feasible and smooth trajectories for C-AWS while adhering to steering angle constraints.","sentences":["This paper presents a trajectory planning method for wheeled robots with fixed steering axes while the steering angle of each wheel is constrained.","In the past, All-Wheel-Steering(AWS) robots, incorporating modes such as rotation-free translation maneuvers, in-situ rotational maneuvers, and proportional steering, exhibited inefficient performance due to time-consuming mode switches.","This inefficiency arises from wheel rotation constraints and inter-wheel cooperation requirements.","The direct application of a holonomic moving strategy can lead to significant slip angles or even structural failure.","Additionally, the limited steering range of AWS wheeled robots exacerbates nonlinearity issues, thereby complicating control processes.","To address these challenges, we developed a novel planning method termed Constrained AWS(C-AWS), which integrates second-order discrete search with predictive control techniques.","Experimental results demonstrate that our method adeptly generates feasible and smooth trajectories for C-AWS while adhering to steering angle constraints."],"url":"http://arxiv.org/abs/2404.09677v1"}
{"created":"2024-04-15 11:12:12","title":"A Circus of Circuits: Connections Between Decision Diagrams, Circuits, and Automata","abstract":"This document is an introduction to two related formalisms to define Boolean functions: binary decision diagrams, and Boolean circuits. It presents these formalisms and several of their variants studied in the setting of knowledge compilation. Last, it explains how these formalisms can be connected to the notions of automata over words and trees.","sentences":["This document is an introduction to two related formalisms to define Boolean functions: binary decision diagrams, and Boolean circuits.","It presents these formalisms and several of their variants studied in the setting of knowledge compilation.","Last, it explains how these formalisms can be connected to the notions of automata over words and trees."],"url":"http://arxiv.org/abs/2404.09674v1"}
{"created":"2024-04-15 10:56:24","title":"Demonstration of a Networked Music Performance Experience with MEVO","abstract":"In this paper we present a Networked Music Performance system currently under development at Politecnico di Torino. We demonstrate its use in a distributed concert held in June 2023, which featured three musicians in Turin (Italy) and three musicians in Wroc{\\l}aw (Poland). Although in its early stages, the system proved to be already stable enough to appear transparent to the remote audience.","sentences":["In this paper we present a Networked Music Performance system currently under development at Politecnico di Torino.","We demonstrate its use in a distributed concert held in June 2023, which featured three musicians in Turin (Italy) and three musicians in Wroc{\\l}aw (Poland).","Although in its early stages, the system proved to be already stable enough to appear transparent to the remote audience."],"url":"http://arxiv.org/abs/2404.09665v1"}
{"created":"2024-04-15 10:54:47","title":"Closing the Gap in the Trade-off between Fair Representations and Accuracy","abstract":"The rapid developments of various machine learning models and their deployments in several applications has led to discussions around the importance of looking beyond the accuracies of these models. Fairness of such models is one such aspect that is deservedly gaining more attention. In this work, we analyse the natural language representations of documents and sentences (i.e., encodings) for any embedding-level bias that could potentially also affect the fairness of the downstream tasks that rely on them. We identify bias in these encodings either towards or against different sub-groups based on the difference in their reconstruction errors along various subsets of principal components. We explore and recommend ways to mitigate such bias in the encodings while also maintaining a decent accuracy in classification models that use them.","sentences":["The rapid developments of various machine learning models and their deployments in several applications has led to discussions around the importance of looking beyond the accuracies of these models.","Fairness of such models is one such aspect that is deservedly gaining more attention.","In this work, we analyse the natural language representations of documents and sentences (i.e., encodings) for any embedding-level bias that could potentially also affect the fairness of the downstream tasks that rely on them.","We identify bias in these encodings either towards or against different sub-groups based on the difference in their reconstruction errors along various subsets of principal components.","We explore and recommend ways to mitigate such bias in the encodings while also maintaining a decent accuracy in classification models that use them."],"url":"http://arxiv.org/abs/2404.09664v1"}
{"created":"2024-04-15 10:47:59","title":"Reconstructing Curves from Sparse Samples on Riemannian Manifolds","abstract":"Reconstructing 2D curves from sample points has long been a critical challenge in computer graphics, finding essential applications in vector graphics. The design and editing of curves on surfaces has only recently begun to receive attention, primarily relying on human assistance, and where not, limited by very strict sampling conditions. In this work, we formally improve on the state-of-the-art requirements and introduce an innovative algorithm capable of reconstructing closed curves directly on surfaces from a given sparse set of sample points. We extend and adapt a state-of-the-art planar curve reconstruction method to the realm of surfaces while dealing with the challenges arising from working on non-Euclidean domains. We demonstrate the robustness of our method by reconstructing multiple curves on various surface meshes. We explore novel potential applications of our approach, allowing for automated reconstruction of curves on Riemannian manifolds.","sentences":["Reconstructing 2D curves from sample points has long been a critical challenge in computer graphics, finding essential applications in vector graphics.","The design and editing of curves on surfaces has only recently begun to receive attention, primarily relying on human assistance, and where not, limited by very strict sampling conditions.","In this work, we formally improve on the state-of-the-art requirements and introduce an innovative algorithm capable of reconstructing closed curves directly on surfaces from a given sparse set of sample points.","We extend and adapt a state-of-the-art planar curve reconstruction method to the realm of surfaces while dealing with the challenges arising from working on non-Euclidean domains.","We demonstrate the robustness of our method by reconstructing multiple curves on various surface meshes.","We explore novel potential applications of our approach, allowing for automated reconstruction of curves on Riemannian manifolds."],"url":"http://arxiv.org/abs/2404.09661v1"}
{"created":"2024-04-15 10:47:25","title":"OpenAirLink: Reproducible Wireless Channel Emulation using Software Defined Radios","abstract":"This paper presents OpenAirLink(OAL), an open-source channel emulator for reproducible testing of wireless scenarios. OAL is implemented on off-the-shelf software-defined radios (SDR) and presents a smaller-scale alternative to expensive commercially available channel emulators. Path loss and propagation delay are the fundamental aspects of emulating a wireless channel. OAL provides a simple method to change these aspects in real-time. The emulator is implemented using a finite impulse response (FIR) filter. The FIR filter is written in Verilog and flashed on the SDRs Field Programmable Gate Array (FPGA). Most processing transpires on the FPGA, so OAL does not require high-performance computing hardware and SDRs. We validate the performance of OAL and demonstrate the utility of such a channel emulation tool using two examples. We believe that open-source channel emulators such as OAL can make reproducible wireless experiments accessible to many researchers in the scientific community.","sentences":["This paper presents OpenAirLink(OAL), an open-source channel emulator for reproducible testing of wireless scenarios.","OAL is implemented on off-the-shelf software-defined radios (SDR) and presents a smaller-scale alternative to expensive commercially available channel emulators.","Path loss and propagation delay are the fundamental aspects of emulating a wireless channel.","OAL provides a simple method to change these aspects in real-time.","The emulator is implemented using a finite impulse response (FIR) filter.","The FIR filter is written in Verilog and flashed on the SDRs Field Programmable Gate Array (FPGA).","Most processing transpires on the FPGA, so OAL does not require high-performance computing hardware and SDRs.","We validate the performance of OAL and demonstrate the utility of such a channel emulation tool using two examples.","We believe that open-source channel emulators such as OAL can make reproducible wireless experiments accessible to many researchers in the scientific community."],"url":"http://arxiv.org/abs/2404.09660v1"}
{"created":"2024-04-15 10:45:12","title":"Sampling for Model Predictive Trajectory Planning in Autonomous Driving using Normalizing Flows","abstract":"Alongside optimization-based planners, sampling-based approaches are often used in trajectory planning for autonomous driving due to their simplicity. Model predictive path integral control is a framework that builds upon optimization principles while incorporating stochastic sampling of input trajectories. This paper investigates several sampling approaches for trajectory generation. In this context, normalizing flows originating from the field of variational inference are considered for the generation of sampling distributions, as they model transformations of simple to more complex distributions. Accordingly, learning-based normalizing flow models are trained for a more efficient exploration of the input domain for the task at hand. The developed algorithm and the proposed sampling distributions are evaluated in two simulation scenarios.","sentences":["Alongside optimization-based planners, sampling-based approaches are often used in trajectory planning for autonomous driving due to their simplicity.","Model predictive path integral control is a framework that builds upon optimization principles while incorporating stochastic sampling of input trajectories.","This paper investigates several sampling approaches for trajectory generation.","In this context, normalizing flows originating from the field of variational inference are considered for the generation of sampling distributions, as they model transformations of simple to more complex distributions.","Accordingly, learning-based normalizing flow models are trained for a more efficient exploration of the input domain for the task at hand.","The developed algorithm and the proposed sampling distributions are evaluated in two simulation scenarios."],"url":"http://arxiv.org/abs/2404.09657v1"}
{"created":"2024-04-15 10:44:31","title":"Learn Your Reference Model for Real Good Alignment","abstract":"The complexity of the alignment problem stems from the fact that existing methods are unstable. Researchers continuously invent various tricks to address this shortcoming. For instance, in the fundamental Reinforcement Learning From Human Feedback (RLHF) technique of Language Model alignment, in addition to reward maximization, the Kullback-Leibler divergence between the trainable policy and the SFT policy is minimized. This addition prevents the model from being overfitted to the Reward Model (RM) and generating texts that are out-of-domain for the RM. The Direct Preference Optimization (DPO) method reformulates the optimization task of RLHF and eliminates the Reward Model while tacitly maintaining the requirement for the policy to be close to the SFT policy. In our paper, we argue that this implicit limitation in the DPO method leads to sub-optimal results. We propose a new method called Trust Region DPO (TR-DPO), which updates the reference policy during training. With such a straightforward update, we demonstrate the effectiveness of TR-DPO against DPO on the Anthropic HH and TLDR datasets. We show that TR-DPO outperforms DPO by up to 19%, measured by automatic evaluation with GPT-4. The new alignment approach that we propose allows us to improve the quality of models across several parameters at once, such as coherence, correctness, level of detail, helpfulness, and harmlessness.","sentences":["The complexity of the alignment problem stems from the fact that existing methods are unstable.","Researchers continuously invent various tricks to address this shortcoming.","For instance, in the fundamental Reinforcement Learning From Human Feedback (RLHF) technique of Language Model alignment, in addition to reward maximization, the Kullback-Leibler divergence between the trainable policy and the SFT policy is minimized.","This addition prevents the model from being overfitted to the Reward Model (RM) and generating texts that are out-of-domain for the RM.","The Direct Preference Optimization (DPO) method reformulates the optimization task of RLHF and eliminates the Reward Model while tacitly maintaining the requirement for the policy to be close to the SFT policy.","In our paper, we argue that this implicit limitation in the DPO method leads to sub-optimal results.","We propose a new method called Trust Region DPO (TR-DPO), which updates the reference policy during training.","With such a straightforward update, we demonstrate the effectiveness of TR-DPO against DPO on the Anthropic HH and TLDR datasets.","We show that TR-DPO outperforms DPO by up to 19%, measured by automatic evaluation with GPT-4.","The new alignment approach that we propose allows us to improve the quality of models across several parameters at once, such as coherence, correctness, level of detail, helpfulness, and harmlessness."],"url":"http://arxiv.org/abs/2404.09656v1"}
{"created":"2024-04-15 10:42:22","title":"Do LLMs Understand Visual Anomalies? Uncovering LLM Capabilities in Zero-shot Anomaly Detection","abstract":"Large vision-language models (LVLMs) are markedly proficient in deriving visual representations guided by natural language. Recent explorations have utilized LVLMs to tackle zero-shot visual anomaly detection (VAD) challenges by pairing images with textual descriptions indicative of normal and abnormal conditions, referred to as anomaly prompts. However, existing approaches depend on static anomaly prompts that are prone to cross-semantic ambiguity, and prioritize global image-level representations over crucial local pixel-level image-to-text alignment that is necessary for accurate anomaly localization. In this paper, we present ALFA, a training-free approach designed to address these challenges via a unified model. We propose a run-time prompt adaptation strategy, which first generates informative anomaly prompts to leverage the capabilities of a large language model (LLM). This strategy is enhanced by a contextual scoring mechanism for per-image anomaly prompt adaptation and cross-semantic ambiguity mitigation. We further introduce a novel fine-grained aligner to fuse local pixel-level semantics for precise anomaly localization, by projecting the image-text alignment from global to local semantic spaces. Extensive evaluations on the challenging MVTec and VisA datasets confirm ALFA's effectiveness in harnessing the language potential for zero-shot VAD, achieving significant PRO improvements of 12.1% on MVTec AD and 8.9% on VisA compared to state-of-the-art zero-shot VAD approaches.","sentences":["Large vision-language models (LVLMs) are markedly proficient in deriving visual representations guided by natural language.","Recent explorations have utilized LVLMs to tackle zero-shot visual anomaly detection (VAD) challenges by pairing images with textual descriptions indicative of normal and abnormal conditions, referred to as anomaly prompts.","However, existing approaches depend on static anomaly prompts that are prone to cross-semantic ambiguity, and prioritize global image-level representations over crucial local pixel-level image-to-text alignment that is necessary for accurate anomaly localization.","In this paper, we present ALFA, a training-free approach designed to address these challenges via a unified model.","We propose a run-time prompt adaptation strategy, which first generates informative anomaly prompts to leverage the capabilities of a large language model (LLM).","This strategy is enhanced by a contextual scoring mechanism for per-image anomaly prompt adaptation and cross-semantic ambiguity mitigation.","We further introduce a novel fine-grained aligner to fuse local pixel-level semantics for precise anomaly localization, by projecting the image-text alignment from global to local semantic spaces.","Extensive evaluations on the challenging MVTec and VisA datasets confirm ALFA's effectiveness in harnessing the language potential for zero-shot VAD, achieving significant PRO improvements of 12.1% on MVTec AD and 8.9% on VisA compared to state-of-the-art zero-shot VAD approaches."],"url":"http://arxiv.org/abs/2404.09654v1"}
{"created":"2024-04-15 10:34:22","title":"Stiffness-Tuneable Limb Segment with Flexible Spine for Malleable Robots","abstract":"Robotic arms built from stiffness-adjustable, continuously bending segments serially connected with revolute joints have the ability to change their mechanical architecture and workspace, thus allowing high flexibility and adaptation to different tasks with less than six degrees of freedom, a concept that we call malleable robots. Known stiffening mechanisms may be used to implement suitable links for these novel robotic manipulators; however, these solutions usually show a reduced performance when bending due to structural deformation. By including an inner support structure this deformation can be minimised, resulting in an increased stiffening performance. This paper presents a new multi-material spine-inspired flexible structure for providing support in stiffness-controllable layer-jamming-based robotic links of large diameter. The proposed spine mechanism is highly movable with type and range of motions that match those of a robotic link using solely layer jamming, whilst maintaining a hollow and light structure. The mechanics and design of the flexible spine are explored, and a prototype of a link utilising it is developed and compared with limb segments based on granular jamming and layer jamming without support structure. Results of experiments verify the advantages of the proposed design, demonstrating that it maintains a constant central diameter across bending angles and presents an improvement of more than 203% of resisting force at 180 degrees.","sentences":["Robotic arms built from stiffness-adjustable, continuously bending segments serially connected with revolute joints have the ability to change their mechanical architecture and workspace, thus allowing high flexibility and adaptation to different tasks with less than six degrees of freedom, a concept that we call malleable robots.","Known stiffening mechanisms may be used to implement suitable links for these novel robotic manipulators; however, these solutions usually show a reduced performance when bending due to structural deformation.","By including an inner support structure this deformation can be minimised, resulting in an increased stiffening performance.","This paper presents a new multi-material spine-inspired flexible structure for providing support in stiffness-controllable layer-jamming-based robotic links of large diameter.","The proposed spine mechanism is highly movable with type and range of motions that match those of a robotic link using solely layer jamming, whilst maintaining a hollow and light structure.","The mechanics and design of the flexible spine are explored, and a prototype of a link utilising it is developed and compared with limb segments based on granular jamming and layer jamming without support structure.","Results of experiments verify the advantages of the proposed design, demonstrating that it maintains a constant central diameter across bending angles and presents an improvement of more than 203% of resisting force at 180 degrees."],"url":"http://arxiv.org/abs/2404.09653v1"}
{"created":"2024-04-15 10:33:39","title":"Monitoring Second-Order Hyperproperties","abstract":"Hyperproperties express the relationship between multiple executions of a system. This is needed in many AI-related fields, such as knowledge representation and planning, to capture system properties related to knowledge, information flow, and privacy. In this paper, we study the monitoring of complex hyperproperties at runtime. Previous work in this area has either focused on the simpler problem of monitoring trace properties (which are sets of traces, while hyperproperties are sets of sets of traces) or on monitoring first-order hyperproperties, which are expressible in temporal logics with first-order quantification over traces, such as HyperLTL. We present the first monitoring algorithm for the much more expressive class of second-order hyperproperties. Second-order hyperproperties include system properties like common knowledge, which cannot be expressed in first-order logics like HyperLTL.   We introduce Hyper$^2$LTL$_f$, a temporal logic over finite traces that allows for second-order quantification over sets of traces. We study the monitoring problem in two fundamental execution models: (1) the parallel model, where a fixed number of traces is monitored in parallel, and (2) the sequential model, where an unbounded number of traces is observed sequentially, one trace after the other. For the parallel model, we show that the monitoring of the second-order hyperproperties of Hyper$^2$LTL$_f$ can be reduced to monitoring first-order hyperproperties. For the sequential model, we present a monitoring algorithm that handles second-order quantification efficiently, exploiting optimizations based on the monotonicity of subformulas, graph-based storing of executions, and fixpoint hashing. We present experimental results from a range of benchmarks, including examples from common knowledge and planning.","sentences":["Hyperproperties express the relationship between multiple executions of a system.","This is needed in many AI-related fields, such as knowledge representation and planning, to capture system properties related to knowledge, information flow, and privacy.","In this paper, we study the monitoring of complex hyperproperties at runtime.","Previous work in this area has either focused on the simpler problem of monitoring trace properties (which are sets of traces, while hyperproperties are sets of sets of traces) or on monitoring first-order hyperproperties, which are expressible in temporal logics with first-order quantification over traces, such as HyperLTL.","We present the first monitoring algorithm for the much more expressive class of second-order hyperproperties.","Second-order hyperproperties include system properties like common knowledge, which cannot be expressed in first-order logics like HyperLTL.   ","We introduce Hyper$^2$LTL$_f$, a temporal logic over finite traces that allows for second-order quantification over sets of traces.","We study the monitoring problem in two fundamental execution models: (1) the parallel model, where a fixed number of traces is monitored in parallel, and (2) the sequential model, where an unbounded number of traces is observed sequentially, one trace after the other.","For the parallel model, we show that the monitoring of the second-order hyperproperties of Hyper$^2$LTL$_f$ can be reduced to monitoring first-order hyperproperties.","For the sequential model, we present a monitoring algorithm that handles second-order quantification efficiently, exploiting optimizations based on the monotonicity of subformulas, graph-based storing of executions, and fixpoint hashing.","We present experimental results from a range of benchmarks, including examples from common knowledge and planning."],"url":"http://arxiv.org/abs/2404.09652v1"}
{"created":"2024-04-15 10:25:14","title":"Object Instance Retrieval in Assistive Robotics: Leveraging Fine-Tuned SimSiam with Multi-View Images Based on 3D Semantic Map","abstract":"Robots that assist in daily life are required to locate specific instances of objects that match the user's desired object in the environment. This task is known as Instance-Specific Image Goal Navigation (InstanceImageNav), which requires a model capable of distinguishing between different instances within the same class. One significant challenge in robotics is that when a robot observes the same object from various 3D viewpoints, its appearance may differ greatly, making it difficult to recognize and locate the object accurately. In this study, we introduce a method, SimView, that leverages multi-view images based on a 3D semantic map of the environment and self-supervised learning by SimSiam to train an instance identification model on-site. The effectiveness of our approach is validated using a photorealistic simulator, Habitat Matterport 3D, created by scanning real home environments. Our results demonstrate a 1.7-fold improvement in task accuracy compared to CLIP, which is pre-trained multimodal contrastive learning for object search. This improvement highlights the benefits of our proposed fine-tuning method in enhancing the performance of assistive robots in InstanceImageNav tasks. The project website is https://emergentsystemlabstudent.github.io/MultiViewRetrieve/.","sentences":["Robots that assist in daily life are required to locate specific instances of objects that match the user's desired object in the environment.","This task is known as Instance-Specific Image Goal Navigation (InstanceImageNav), which requires a model capable of distinguishing between different instances within the same class.","One significant challenge in robotics is that when a robot observes the same object from various 3D viewpoints, its appearance may differ greatly, making it difficult to recognize and locate the object accurately.","In this study, we introduce a method, SimView, that leverages multi-view images based on a 3D semantic map of the environment and self-supervised learning by SimSiam to train an instance identification model on-site.","The effectiveness of our approach is validated using a photorealistic simulator, Habitat Matterport 3D, created by scanning real home environments.","Our results demonstrate a 1.7-fold improvement in task accuracy compared to CLIP, which is pre-trained multimodal contrastive learning for object search.","This improvement highlights the benefits of our proposed fine-tuning method in enhancing the performance of assistive robots in InstanceImageNav tasks.","The project website is https://emergentsystemlabstudent.github.io/MultiViewRetrieve/."],"url":"http://arxiv.org/abs/2404.09647v1"}
{"created":"2024-04-15 10:24:32","title":"Real-world Instance-specific Image Goal Navigation for Service Robots: Bridging the Domain Gap with Contrastive Learning","abstract":"Improving instance-specific image goal navigation (InstanceImageNav), which locates the identical object in a real-world environment from a query image, is essential for robotic systems to assist users in finding desired objects. The challenge lies in the domain gap between low-quality images observed by the moving robot, characterized by motion blur and low-resolution, and high-quality query images provided by the user. Such domain gaps could significantly reduce the task success rate but have not been the focus of previous work. To address this, we propose a novel method called Few-shot Cross-quality Instance-aware Adaptation (CrossIA), which employs contrastive learning with an instance classifier to align features between massive low- and few high-quality images. This approach effectively reduces the domain gap by bringing the latent representations of cross-quality images closer on an instance basis. Additionally, the system integrates an object image collection with a pre-trained deblurring model to enhance the observed image quality. Our method fine-tunes the SimSiam model, pre-trained on ImageNet, using CrossIA. We evaluated our method's effectiveness through an InstanceImageNav task with 20 different types of instances, where the robot identifies the same instance in a real-world environment as a high-quality query image. Our experiments showed that our method improves the task success rate by up to three times compared to the baseline, a conventional approach based on SuperGlue. These findings highlight the potential of leveraging contrastive learning and image enhancement techniques to bridge the domain gap and improve object localization in robotic applications. The project website is https://emergentsystemlabstudent.github.io/DomainBridgingNav/.","sentences":["Improving instance-specific image goal navigation (InstanceImageNav), which locates the identical object in a real-world environment from a query image, is essential for robotic systems to assist users in finding desired objects.","The challenge lies in the domain gap between low-quality images observed by the moving robot, characterized by motion blur and low-resolution, and high-quality query images provided by the user.","Such domain gaps could significantly reduce the task success rate but have not been the focus of previous work.","To address this, we propose a novel method called Few-shot Cross-quality Instance-aware Adaptation (CrossIA), which employs contrastive learning with an instance classifier to align features between massive low- and few high-quality images.","This approach effectively reduces the domain gap by bringing the latent representations of cross-quality images closer on an instance basis.","Additionally, the system integrates an object image collection with a pre-trained deblurring model to enhance the observed image quality.","Our method fine-tunes the SimSiam model, pre-trained on ImageNet, using CrossIA.","We evaluated our method's effectiveness through an InstanceImageNav task with 20 different types of instances, where the robot identifies the same instance in a real-world environment as a high-quality query image.","Our experiments showed that our method improves the task success rate by up to three times compared to the baseline, a conventional approach based on SuperGlue.","These findings highlight the potential of leveraging contrastive learning and image enhancement techniques to bridge the domain gap and improve object localization in robotic applications.","The project website is https://emergentsystemlabstudent.github.io/DomainBridgingNav/."],"url":"http://arxiv.org/abs/2404.09645v1"}
{"created":"2024-04-15 10:23:11","title":"An Origami-Inspired Variable Friction Surface for Increasing the Dexterity of Robotic Grippers","abstract":"While the grasping capability of robotic grippers has shown significant development, the ability to manipulate objects within the hand is still limited. One explanation for this limitation is the lack of controlled contact variation between the grasped object and the gripper. For instance, human hands have the ability to firmly grip object surfaces, as well as slide over object faces, an aspect that aids the enhanced manipulation of objects within the hand without losing contact. In this letter, we present a parametric, origami-inspired thin surface capable of transitioning between a high friction and a low friction state, suitable for implementation as an epidermis in robotic fingers. A numerical analysis of the proposed surface based on its design parameters, force analysis, and performance in in-hand manipulation tasks is presented. Through the development of a simple two-fingered two-degree-of-freedom gripper utilizing the proposed variable-friction surfaces with different parameters, we experimentally demonstrate the improved manipulation capabilities of the hand when compared to the same gripper without changeable friction. Results show that the pattern density and valley gap are the main parameters that effect the in-hand manipulation performance. The origami-inspired thin surface with a higher pattern density generated a smaller valley gap and smaller height change, producing a more stable improvement of the manipulation capabilities of the hand.","sentences":["While the grasping capability of robotic grippers has shown significant development, the ability to manipulate objects within the hand is still limited.","One explanation for this limitation is the lack of controlled contact variation between the grasped object and the gripper.","For instance, human hands have the ability to firmly grip object surfaces, as well as slide over object faces, an aspect that aids the enhanced manipulation of objects within the hand without losing contact.","In this letter, we present a parametric, origami-inspired thin surface capable of transitioning between a high friction and a low friction state, suitable for implementation as an epidermis in robotic fingers.","A numerical analysis of the proposed surface based on its design parameters, force analysis, and performance in in-hand manipulation tasks is presented.","Through the development of a simple two-fingered two-degree-of-freedom gripper utilizing the proposed variable-friction surfaces with different parameters, we experimentally demonstrate the improved manipulation capabilities of the hand when compared to the same gripper without changeable friction.","Results show that the pattern density and valley gap are the main parameters that effect the in-hand manipulation performance.","The origami-inspired thin surface with a higher pattern density generated a smaller valley gap and smaller height change, producing a more stable improvement of the manipulation capabilities of the hand."],"url":"http://arxiv.org/abs/2404.09644v1"}
{"created":"2024-04-15 10:19:39","title":"CREST: Cross-modal Resonance through Evidential Deep Learning for Enhanced Zero-Shot Learning","abstract":"Zero-shot learning (ZSL) enables the recognition of novel classes by leveraging semantic knowledge transfer from known to unknown categories. This knowledge, typically encapsulated in attribute descriptions, aids in identifying class-specific visual features, thus facilitating visual-semantic alignment and improving ZSL performance. However, real-world challenges such as distribution imbalances and attribute co-occurrence among instances often hinder the discernment of local variances in images, a problem exacerbated by the scarcity of fine-grained, region-specific attribute annotations. Moreover, the variability in visual presentation within categories can also skew attribute-category associations. In response, we propose a bidirectional cross-modal ZSL approach CREST. It begins by extracting representations for attribute and visual localization and employs Evidential Deep Learning (EDL) to measure underlying epistemic uncertainty, thereby enhancing the model's resilience against hard negatives. CREST incorporates dual learning pathways, focusing on both visual-category and attribute-category alignments, to ensure robust correlation between latent and observable spaces. Moreover, we introduce an uncertainty-informed cross-modal fusion technique to refine visual-attribute inference. Extensive experiments demonstrate our model's effectiveness and unique explainability across multiple datasets. Our code and data are available at: Comments: Ongoing work; 10 pages, 2 Tables, 9 Figures; Repo is available at https://github.com/JethroJames/CREST.","sentences":["Zero-shot learning (ZSL) enables the recognition of novel classes by leveraging semantic knowledge transfer from known to unknown categories.","This knowledge, typically encapsulated in attribute descriptions, aids in identifying class-specific visual features, thus facilitating visual-semantic alignment and improving ZSL performance.","However, real-world challenges such as distribution imbalances and attribute co-occurrence among instances often hinder the discernment of local variances in images, a problem exacerbated by the scarcity of fine-grained, region-specific attribute annotations.","Moreover, the variability in visual presentation within categories can also skew attribute-category associations.","In response, we propose a bidirectional cross-modal ZSL approach CREST.","It begins by extracting representations for attribute and visual localization and employs Evidential Deep Learning (EDL) to measure underlying epistemic uncertainty, thereby enhancing the model's resilience against hard negatives.","CREST incorporates dual learning pathways, focusing on both visual-category and attribute-category alignments, to ensure robust correlation between latent and observable spaces.","Moreover, we introduce an uncertainty-informed cross-modal fusion technique to refine visual-attribute inference.","Extensive experiments demonstrate our model's effectiveness and unique explainability across multiple datasets.","Our code and data are available at: Comments: Ongoing work; 10 pages, 2 Tables, 9 Figures; Repo is available at https://github.com/JethroJames/CREST."],"url":"http://arxiv.org/abs/2404.09640v1"}
{"created":"2024-04-15 10:13:05","title":"climber++: Pivot-Based Approximate Similarity Search over Big Data Series","abstract":"The generation and collection of big data series are becoming an integral part of many emerging applications in sciences, IoT, finance, and web applications among several others. The terabyte-scale of data series has motivated recent efforts to design fully distributed techniques for supporting operations such as approximate kNN similarity search, which is a building block operation in most analytics services on data series. Unfortunately, these techniques are heavily geared towards achieving scalability at the cost of sacrificing the results' accuracy. State-of-the-art systems report accuracy below 10% and 40%, respectively, which is not practical for many real-world applications. In this paper, we investigate the root problems in these existing techniques that limit their ability to achieve better a trade-off between scalability and accuracy. Then, we propose a framework, called CLIMBER, that encompasses a novel feature extraction mechanism, indexing scheme, and query processing algorithms for supporting approximate similarity search in big data series. For CLIMBER, we propose a new loss-resistant dual representation composed of rank-sensitive and ranking-insensitive signatures capturing data series objects. Based on this representation, we devise a distributed two-level index structure supported by an efficient data partitioning scheme. Our similarity metrics tailored for this dual representation enables meaningful comparison and distance evaluation between the rank-sensitive and ranking-insensitive signatures. Finally, we propose two efficient query processing algorithms, CLIMBER-kNN and CLIMBER-kNN-Adaptive, for answering approximate kNN similarity queries. Our experimental study on real-world and benchmark datasets demonstrates that CLIMBER, unlike existing techniques, features results' accuracy above 80% while retaining the desired scalability to terabytes of data.","sentences":["The generation and collection of big data series are becoming an integral part of many emerging applications in sciences, IoT, finance, and web applications among several others.","The terabyte-scale of data series has motivated recent efforts to design fully distributed techniques for supporting operations such as approximate kNN similarity search, which is a building block operation in most analytics services on data series.","Unfortunately, these techniques are heavily geared towards achieving scalability at the cost of sacrificing the results' accuracy.","State-of-the-art systems report accuracy below 10% and 40%, respectively, which is not practical for many real-world applications.","In this paper, we investigate the root problems in these existing techniques that limit their ability to achieve better a trade-off between scalability and accuracy.","Then, we propose a framework, called CLIMBER, that encompasses a novel feature extraction mechanism, indexing scheme, and query processing algorithms for supporting approximate similarity search in big data series.","For CLIMBER, we propose a new loss-resistant dual representation composed of rank-sensitive and ranking-insensitive signatures capturing data series objects.","Based on this representation, we devise a distributed two-level index structure supported by an efficient data partitioning scheme.","Our similarity metrics tailored for this dual representation enables meaningful comparison and distance evaluation between the rank-sensitive and ranking-insensitive signatures.","Finally, we propose two efficient query processing algorithms, CLIMBER-kNN and CLIMBER-kNN-Adaptive, for answering approximate kNN similarity queries.","Our experimental study on real-world and benchmark datasets demonstrates that CLIMBER, unlike existing techniques, features results' accuracy above 80% while retaining the desired scalability to terabytes of data."],"url":"http://arxiv.org/abs/2404.09637v1"}
{"created":"2024-04-15 10:12:33","title":"All-in-one simulation-based inference","abstract":"Amortized Bayesian inference trains neural networks to solve stochastic inference problems using model simulations, thereby making it possible to rapidly perform Bayesian inference for any newly observed data. However, current simulation-based amortized inference methods are simulation-hungry and inflexible: They require the specification of a fixed parametric prior, simulator, and inference tasks ahead of time. Here, we present a new amortized inference method -- the Simformer -- which overcomes these limitations. By training a probabilistic diffusion model with transformer architectures, the Simformer outperforms current state-of-the-art amortized inference approaches on benchmark tasks and is substantially more flexible: It can be applied to models with function-valued parameters, it can handle inference scenarios with missing or unstructured data, and it can sample arbitrary conditionals of the joint distribution of parameters and data, including both posterior and likelihood. We showcase the performance and flexibility of the Simformer on simulators from ecology, epidemiology, and neuroscience, and demonstrate that it opens up new possibilities and application domains for amortized Bayesian inference on simulation-based models.","sentences":["Amortized Bayesian inference trains neural networks to solve stochastic inference problems using model simulations, thereby making it possible to rapidly perform Bayesian inference for any newly observed data.","However, current simulation-based amortized inference methods are simulation-hungry and inflexible: They require the specification of a fixed parametric prior, simulator, and inference tasks ahead of time.","Here, we present a new amortized inference method -- the Simformer -- which overcomes these limitations.","By training a probabilistic diffusion model with transformer architectures, the Simformer outperforms current state-of-the-art amortized inference approaches on benchmark tasks and is substantially more flexible: It can be applied to models with function-valued parameters, it can handle inference scenarios with missing or unstructured data, and it can sample arbitrary conditionals of the joint distribution of parameters and data, including both posterior and likelihood.","We showcase the performance and flexibility of the Simformer on simulators from ecology, epidemiology, and neuroscience, and demonstrate that it opens up new possibilities and application domains for amortized Bayesian inference on simulation-based models."],"url":"http://arxiv.org/abs/2404.09636v1"}
{"created":"2024-04-15 10:05:36","title":"In-Context Translation: Towards Unifying Image Recognition, Processing, and Generation","abstract":"We propose In-Context Translation (ICT), a general learning framework to unify visual recognition (e.g., semantic segmentation), low-level image processing (e.g., denoising), and conditional image generation (e.g., edge-to-image synthesis). Thanks to unification, ICT significantly reduces the inherent inductive bias that comes with designing models for specific tasks, and it maximizes mutual enhancement across similar tasks. However, the unification across a large number of tasks is non-trivial due to various data formats and training pipelines. To this end, ICT introduces two designs. Firstly, it standardizes input-output data of different tasks into RGB image pairs, e.g., semantic segmentation data pairs an RGB image with its segmentation mask in the same RGB format. This turns different tasks into a general translation task between two RGB images. Secondly, it standardizes the training of different tasks into a general in-context learning, where \"in-context\" means the input comprises an example input-output pair of the target task and a query image. The learning objective is to generate the \"missing\" data paired with the query. The implicit translation process is thus between the query and the generated image. In experiments, ICT unifies ten vision tasks and showcases impressive performance on their respective benchmarks. Notably, compared to its competitors, e.g., Painter and PromptDiffusion, ICT trained on only 4 RTX 3090 GPUs is shown to be more efficient and less costly in training.","sentences":["We propose In-Context Translation (ICT), a general learning framework to unify visual recognition (e.g., semantic segmentation), low-level image processing (e.g., denoising), and conditional image generation (e.g., edge-to-image synthesis).","Thanks to unification, ICT significantly reduces the inherent inductive bias that comes with designing models for specific tasks, and it maximizes mutual enhancement across similar tasks.","However, the unification across a large number of tasks is non-trivial due to various data formats and training pipelines.","To this end, ICT introduces two designs.","Firstly, it standardizes input-output data of different tasks into RGB image pairs, e.g., semantic segmentation data pairs an RGB image with its segmentation mask in the same RGB format.","This turns different tasks into a general translation task between two RGB images.","Secondly, it standardizes the training of different tasks into a general in-context learning, where \"in-context\" means the input comprises an example input-output pair of the target task and a query image.","The learning objective is to generate the \"missing\" data paired with the query.","The implicit translation process is thus between the query and the generated image.","In experiments, ICT unifies ten vision tasks and showcases impressive performance on their respective benchmarks.","Notably, compared to its competitors, e.g., Painter and PromptDiffusion, ICT trained on only 4 RTX 3090 GPUs is shown to be more efficient and less costly in training."],"url":"http://arxiv.org/abs/2404.09633v1"}
{"created":"2024-04-15 10:04:15","title":"Bridging Vision and Language Spaces with Assignment Prediction","abstract":"This paper introduces VLAP, a novel approach that bridges pretrained vision models and large language models (LLMs) to make frozen LLMs understand the visual world. VLAP transforms the embedding space of pretrained vision models into the LLMs' word embedding space using a single linear layer for efficient and general-purpose visual and language understanding. Specifically, we harness well-established word embeddings to bridge two modality embedding spaces. The visual and text representations are simultaneously assigned to a set of word embeddings within pretrained LLMs by formulating the assigning procedure as an optimal transport problem. We predict the assignment of one modality from the representation of another modality data, enforcing consistent assignments for paired multimodal data. This allows vision and language representations to contain the same information, grounding the frozen LLMs' word embedding space in visual data. Moreover, a robust semantic taxonomy of LLMs can be preserved with visual data since the LLMs interpret and reason linguistic information from correlations between word embeddings. Experimental results show that VLAP achieves substantial improvements over the previous linear transformation-based approaches across a range of vision-language tasks, including image captioning, visual question answering, and cross-modal retrieval. We also demonstrate the learned visual representations hold a semantic taxonomy of LLMs, making visual semantic arithmetic possible.","sentences":["This paper introduces VLAP, a novel approach that bridges pretrained vision models and large language models (LLMs) to make frozen LLMs understand the visual world.","VLAP transforms the embedding space of pretrained vision models into the LLMs' word embedding space using a single linear layer for efficient and general-purpose visual and language understanding.","Specifically, we harness well-established word embeddings to bridge two modality embedding spaces.","The visual and text representations are simultaneously assigned to a set of word embeddings within pretrained LLMs by formulating the assigning procedure as an optimal transport problem.","We predict the assignment of one modality from the representation of another modality data, enforcing consistent assignments for paired multimodal data.","This allows vision and language representations to contain the same information, grounding the frozen LLMs' word embedding space in visual data.","Moreover, a robust semantic taxonomy of LLMs can be preserved with visual data since the LLMs interpret and reason linguistic information from correlations between word embeddings.","Experimental results show that VLAP achieves substantial improvements over the previous linear transformation-based approaches across a range of vision-language tasks, including image captioning, visual question answering, and cross-modal retrieval.","We also demonstrate the learned visual representations hold a semantic taxonomy of LLMs, making visual semantic arithmetic possible."],"url":"http://arxiv.org/abs/2404.09632v1"}
{"created":"2024-04-15 10:01:43","title":"Action Model Learning with Guarantees","abstract":"This paper studies the problem of action model learning with full observability. Following the learning by search paradigm by Mitchell, we develop a theory for action model learning based on version spaces that interprets the task as search for hypothesis that are consistent with the learning examples. Our theoretical findings are instantiated in an online algorithm that maintains a compact representation of all solutions of the problem. Among these range of solutions, we bring attention to actions models approximating the actual transition system from below (sound models) and from above (complete models). We show how to manipulate the output of our learning algorithm to build deterministic and non-deterministic formulations of the sound and complete models and prove that, given enough examples, both formulations converge into the very same true model. Our experiments reveal their usefulness over a range of planning domains.","sentences":["This paper studies the problem of action model learning with full observability.","Following the learning by search paradigm by Mitchell, we develop a theory for action model learning based on version spaces that interprets the task as search for hypothesis that are consistent with the learning examples.","Our theoretical findings are instantiated in an online algorithm that maintains a compact representation of all solutions of the problem.","Among these range of solutions, we bring attention to actions models approximating the actual transition system from below (sound models) and from above (complete models).","We show how to manipulate the output of our learning algorithm to build deterministic and non-deterministic formulations of the sound and complete models and prove that, given enough examples, both formulations converge into the very same true model.","Our experiments reveal their usefulness over a range of planning domains."],"url":"http://arxiv.org/abs/2404.09631v1"}
{"created":"2024-04-15 09:57:51","title":"Centralization in Proof-of-Stake Blockchains: A Game-Theoretic Analysis of Bootstrapping Protocols","abstract":"Proof-of-stake (PoS) has emerged as a natural alternative to the resource-intensive Proof-of-Work (PoW) blockchain, as was recently seen with the Ethereum Merge. PoS-based blockchains require an initial stake distribution among the participants. Typically, this initial stake distribution is called bootstrapping. This paper argues that existing bootstrapping protocols are prone to centralization. To address centralization due to bootstrapping, we propose a novel game $\\Gamma_\\textsf{bootstrap}$. Next, we define three conditions: (i) Individual Rationality (IR), (ii) Incentive Compatibility (IC), and (iii) $(\\tau,\\delta,\\epsilon)-$ Decentralization that an \\emph{ideal} bootstrapping protocol must satisfy. $(\\tau,\\delta,\\epsilon)$ are certain parameters to quantify decentralization. Towards this, we propose a novel centralization metric, C-NORM, to measure centralization in a PoS System. We define a centralization game -- $\\Gamma_\\textsf{cent}$, to analyze the efficacy of centralization metrics. We show that C-NORM effectively captures centralization in the presence of strategic players capable of launching Sybil attacks. With C-NORM, we analyze popular bootstrapping protocols such as Airdrop and Proof-of-Burn (PoB) and prove that they do not satisfy IC and IR, respectively. Motivated by the Ethereum Merge, we study W2SB (a PoW-based bootstrapping protocol) and prove it is ideal. In addition, we conduct synthetic simulations to empirically validate that W2SB bootstrapped PoS is decentralized.","sentences":["Proof-of-stake (PoS) has emerged as a natural alternative to the resource-intensive Proof-of-Work (PoW) blockchain, as was recently seen with the Ethereum Merge.","PoS-based blockchains require an initial stake distribution among the participants.","Typically, this initial stake distribution is called bootstrapping.","This paper argues that existing bootstrapping protocols are prone to centralization.","To address centralization due to bootstrapping, we propose a novel game $\\Gamma_\\textsf{bootstrap}$.","Next, we define three conditions: (i) Individual Rationality (IR), (ii) Incentive Compatibility (IC), and (iii) $(\\tau,\\delta,\\epsilon)-$ Decentralization that an \\emph{ideal} bootstrapping protocol must satisfy.","$(\\tau,\\delta,\\epsilon)$ are certain parameters to quantify decentralization.","Towards this, we propose a novel centralization metric, C-NORM, to measure centralization in a PoS System.","We define a centralization game -- $\\Gamma_\\textsf{cent}$, to analyze the efficacy of centralization metrics.","We show that C-NORM effectively captures centralization in the presence of strategic players capable of launching Sybil attacks.","With C-NORM, we analyze popular bootstrapping protocols such as Airdrop and Proof-of-Burn (PoB) and prove that they do not satisfy IC and IR, respectively.","Motivated by the Ethereum Merge, we study W2SB (a PoW-based bootstrapping protocol) and prove it is ideal.","In addition, we conduct synthetic simulations to empirically validate that W2SB bootstrapped PoS is decentralized."],"url":"http://arxiv.org/abs/2404.09627v1"}
{"created":"2024-04-15 09:56:36","title":"Privacy-Preserving Intrusion Detection using Convolutional Neural Networks","abstract":"Privacy-preserving analytics is designed to protect valuable assets. A common service provision involves the input data from the client and the model on the analyst's side. The importance of the privacy preservation is fuelled by legal obligations and intellectual property concerns. We explore the use case of a model owner providing an analytic service on customer's private data. No information about the data shall be revealed to the analyst and no information about the model shall be leaked to the customer. Current methods involve costs: accuracy deterioration and computational complexity. The complexity, in turn, results in a longer processing time, increased requirement on computing resources, and involves data communication between the client and the server. In order to deploy such service architecture, we need to evaluate the optimal setting that fits the constraints. And that is what this paper addresses. In this work, we enhance an attack detection system based on Convolutional Neural Networks with privacy-preserving technology based on PriMIA framework that is initially designed for medical data.","sentences":["Privacy-preserving analytics is designed to protect valuable assets.","A common service provision involves the input data from the client and the model on the analyst's side.","The importance of the privacy preservation is fuelled by legal obligations and intellectual property concerns.","We explore the use case of a model owner providing an analytic service on customer's private data.","No information about the data shall be revealed to the analyst and no information about the model shall be leaked to the customer.","Current methods involve costs: accuracy deterioration and computational complexity.","The complexity, in turn, results in a longer processing time, increased requirement on computing resources, and involves data communication between the client and the server.","In order to deploy such service architecture, we need to evaluate the optimal setting that fits the constraints.","And that is what this paper addresses.","In this work, we enhance an attack detection system based on Convolutional Neural Networks with privacy-preserving technology based on PriMIA framework that is initially designed for medical data."],"url":"http://arxiv.org/abs/2404.09625v1"}
{"created":"2024-04-15 09:56:20","title":"AesExpert: Towards Multi-modality Foundation Model for Image Aesthetics Perception","abstract":"The highly abstract nature of image aesthetics perception (IAP) poses significant challenge for current multimodal large language models (MLLMs). The lack of human-annotated multi-modality aesthetic data further exacerbates this dilemma, resulting in MLLMs falling short of aesthetics perception capabilities. To address the above challenge, we first introduce a comprehensively annotated Aesthetic Multi-Modality Instruction Tuning (AesMMIT) dataset, which serves as the footstone for building multi-modality aesthetics foundation models. Specifically, to align MLLMs with human aesthetics perception, we construct a corpus-rich aesthetic critique database with 21,904 diverse-sourced images and 88K human natural language feedbacks, which are collected via progressive questions, ranging from coarse-grained aesthetic grades to fine-grained aesthetic descriptions. To ensure that MLLMs can handle diverse queries, we further prompt GPT to refine the aesthetic critiques and assemble the large-scale aesthetic instruction tuning dataset, i.e. AesMMIT, which consists of 409K multi-typed instructions to activate stronger aesthetic capabilities. Based on the AesMMIT database, we fine-tune the open-sourced general foundation models, achieving multi-modality Aesthetic Expert models, dubbed AesExpert. Extensive experiments demonstrate that the proposed AesExpert models deliver significantly better aesthetic perception performances than the state-of-the-art MLLMs, including the most advanced GPT-4V and Gemini-Pro-Vision. Source data will be available at https://github.com/yipoh/AesExpert.","sentences":["The highly abstract nature of image aesthetics perception (IAP) poses significant challenge for current multimodal large language models (MLLMs).","The lack of human-annotated multi-modality aesthetic data further exacerbates this dilemma, resulting in MLLMs falling short of aesthetics perception capabilities.","To address the above challenge, we first introduce a comprehensively annotated Aesthetic Multi-Modality Instruction Tuning (AesMMIT) dataset, which serves as the footstone for building multi-modality aesthetics foundation models.","Specifically, to align MLLMs with human aesthetics perception, we construct a corpus-rich aesthetic critique database with 21,904 diverse-sourced images and 88K human natural language feedbacks, which are collected via progressive questions, ranging from coarse-grained aesthetic grades to fine-grained aesthetic descriptions.","To ensure that MLLMs can handle diverse queries, we further prompt GPT to refine the aesthetic critiques and assemble the large-scale aesthetic instruction tuning dataset, i.e. AesMMIT, which consists of 409K multi-typed instructions to activate stronger aesthetic capabilities.","Based on the AesMMIT database, we fine-tune the open-sourced general foundation models, achieving multi-modality Aesthetic Expert models, dubbed AesExpert.","Extensive experiments demonstrate that the proposed AesExpert models deliver significantly better aesthetic perception performances than the state-of-the-art MLLMs, including the most advanced GPT-4V and Gemini-Pro-Vision.","Source data will be available at https://github.com/yipoh/AesExpert."],"url":"http://arxiv.org/abs/2404.09624v1"}
{"created":"2024-04-15 09:49:33","title":"DIDLM:A Comprehensive Multi-Sensor Dataset with Infrared Cameras, Depth Cameras, LiDAR, and 4D Millimeter-Wave Radar in Challenging Scenarios for 3D Mapping","abstract":"This study presents a comprehensive multi-sensor dataset designed for 3D mapping in challenging indoor and outdoor environments. The dataset comprises data from infrared cameras, depth cameras, LiDAR, and 4D millimeter-wave radar, facilitating exploration of advanced perception and mapping techniques. Integration of diverse sensor data enhances perceptual capabilities in extreme conditions such as rain, snow, and uneven road surfaces. The dataset also includes interactive robot data at different speeds indoors and outdoors, providing a realistic background environment. Slam comparisons between similar routes are conducted, analyzing the influence of different complex scenes on various sensors. Various SLAM algorithms are employed to process the dataset, revealing performance differences among algorithms in different scenarios. In summary, this dataset addresses the problem of data scarcity in special environments, fostering the development of perception and mapping algorithms for extreme conditions. Leveraging multi-sensor data including infrared, depth cameras, LiDAR, 4D millimeter-wave radar, and robot interactions, the dataset advances intelligent mapping and perception capabilities.Our dataset is available at https://github.com/GongWeiSheng/DIDLM.","sentences":["This study presents a comprehensive multi-sensor dataset designed for 3D mapping in challenging indoor and outdoor environments.","The dataset comprises data from infrared cameras, depth cameras, LiDAR, and 4D millimeter-wave radar, facilitating exploration of advanced perception and mapping techniques.","Integration of diverse sensor data enhances perceptual capabilities in extreme conditions such as rain, snow, and uneven road surfaces.","The dataset also includes interactive robot data at different speeds indoors and outdoors, providing a realistic background environment.","Slam comparisons between similar routes are conducted, analyzing the influence of different complex scenes on various sensors.","Various SLAM algorithms are employed to process the dataset, revealing performance differences among algorithms in different scenarios.","In summary, this dataset addresses the problem of data scarcity in special environments, fostering the development of perception and mapping algorithms for extreme conditions.","Leveraging multi-sensor data including infrared, depth cameras, LiDAR, 4D millimeter-wave radar, and robot interactions, the dataset advances intelligent mapping and perception capabilities.","Our dataset is available at https://github.com/GongWeiSheng/DIDLM."],"url":"http://arxiv.org/abs/2404.09622v1"}
{"created":"2024-04-15 09:48:11","title":"Leveraging the Doppler Effect for Channel Charting","abstract":"Channel Charting is a dimensionality reduction technique that reconstructs a map of the radio environment from similarity relationships found in channel state information. Distances in the channel chart are often computed based on some dissimilarity metric, which can be derived from angular-domain information, channel impulse responses, measured phase differences or simply timestamps. Using such information implicitly makes strong assumptions about the level of phase and time synchronization between base station antennas or assumes approximately constant transmitter velocity. Many practical systems, however, may not provide phase and time synchronization and single-antenna base stations may not even have angular-domain information. We propose a Doppler effect-based loss function for Channel Charting that only requires frequency synchronization between spatially distributed base station antennas, which is a much weaker assumption. We use a dataset measured in an indoor environment to demonstrate that the proposed method is practically feasible with just four base station antennas, that it produces a channel chart that is suitable for localization in the global coordinate frame and that it outperforms other state-of-the-art methods under the given limitations.","sentences":["Channel Charting is a dimensionality reduction technique that reconstructs a map of the radio environment from similarity relationships found in channel state information.","Distances in the channel chart are often computed based on some dissimilarity metric, which can be derived from angular-domain information, channel impulse responses, measured phase differences or simply timestamps.","Using such information implicitly makes strong assumptions about the level of phase and time synchronization between base station antennas or assumes approximately constant transmitter velocity.","Many practical systems, however, may not provide phase and time synchronization and single-antenna base stations may not even have angular-domain information.","We propose a Doppler effect-based loss function for Channel Charting that only requires frequency synchronization between spatially distributed base station antennas, which is a much weaker assumption.","We use a dataset measured in an indoor environment to demonstrate that the proposed method is practically feasible with just four base station antennas, that it produces a channel chart that is suitable for localization in the global coordinate frame and that it outperforms other state-of-the-art methods under the given limitations."],"url":"http://arxiv.org/abs/2404.09620v1"}
{"created":"2024-04-15 09:47:48","title":"UNIAA: A Unified Multi-modal Image Aesthetic Assessment Baseline and Benchmark","abstract":"As an alternative to expensive expert evaluation, Image Aesthetic Assessment (IAA) stands out as a crucial task in computer vision. However, traditional IAA methods are typically constrained to a single data source or task, restricting the universality and broader application. In this work, to better align with human aesthetics, we propose a Unified Multi-modal Image Aesthetic Assessment (UNIAA) framework, including a Multi-modal Large Language Model (MLLM) named UNIAA-LLaVA and a comprehensive benchmark named UNIAA-Bench. We choose MLLMs with both visual perception and language ability for IAA and establish a low-cost paradigm for transforming the existing datasets into unified and high-quality visual instruction tuning data, from which the UNIAA-LLaVA is trained. To further evaluate the IAA capability of MLLMs, we construct the UNIAA-Bench, which consists of three aesthetic levels: Perception, Description, and Assessment. Extensive experiments validate the effectiveness and rationality of UNIAA. UNIAA-LLaVA achieves competitive performance on all levels of UNIAA-Bench, compared with existing MLLMs. Specifically, our model performs better than GPT-4V in aesthetic perception and even approaches the junior-level human. We find MLLMs have great potential in IAA, yet there remains plenty of room for further improvement. The UNIAA-LLaVA and UNIAA-Bench will be released.","sentences":["As an alternative to expensive expert evaluation, Image Aesthetic Assessment (IAA) stands out as a crucial task in computer vision.","However, traditional IAA methods are typically constrained to a single data source or task, restricting the universality and broader application.","In this work, to better align with human aesthetics, we propose a Unified Multi-modal Image Aesthetic Assessment (UNIAA) framework, including a Multi-modal Large Language Model (MLLM) named UNIAA-LLaVA and a comprehensive benchmark named UNIAA-Bench.","We choose MLLMs with both visual perception and language ability for IAA and establish a low-cost paradigm for transforming the existing datasets into unified and high-quality visual instruction tuning data, from which the UNIAA-LLaVA is trained.","To further evaluate the IAA capability of MLLMs, we construct the UNIAA-Bench, which consists of three aesthetic levels: Perception, Description, and Assessment.","Extensive experiments validate the effectiveness and rationality of UNIAA.","UNIAA-LLaVA achieves competitive performance on all levels of UNIAA-Bench, compared with existing MLLMs.","Specifically, our model performs better than GPT-4V in aesthetic perception and even approaches the junior-level human.","We find MLLMs have great potential in IAA, yet there remains plenty of room for further improvement.","The UNIAA-LLaVA and UNIAA-Bench will be released."],"url":"http://arxiv.org/abs/2404.09619v1"}
{"created":"2024-04-15 09:40:44","title":"A Review and Efficient Implementation of Scene Graph Generation Metrics","abstract":"Scene graph generation has emerged as a prominent research field in computer vision, witnessing significant advancements in the recent years. However, despite these strides, precise and thorough definitions for the metrics used to evaluate scene graph generation models are lacking. In this paper, we address this gap in the literature by providing a review and precise definition of commonly used metrics in scene graph generation. Our comprehensive examination clarifies the underlying principles of these metrics and can serve as a reference or introduction to scene graph metrics.   Furthermore, to facilitate the usage of these metrics, we introduce a standalone Python package called SGBench that efficiently implements all defined metrics, ensuring their accessibility to the research community. Additionally, we present a scene graph benchmarking web service, that enables researchers to compare scene graph generation methods and increase visibility of new methods in a central place.   All of our code can be found at https://lorjul.github.io/sgbench/.","sentences":["Scene graph generation has emerged as a prominent research field in computer vision, witnessing significant advancements in the recent years.","However, despite these strides, precise and thorough definitions for the metrics used to evaluate scene graph generation models are lacking.","In this paper, we address this gap in the literature by providing a review and precise definition of commonly used metrics in scene graph generation.","Our comprehensive examination clarifies the underlying principles of these metrics and can serve as a reference or introduction to scene graph metrics.   ","Furthermore, to facilitate the usage of these metrics, we introduce a standalone Python package called SGBench that efficiently implements all defined metrics, ensuring their accessibility to the research community.","Additionally, we present a scene graph benchmarking web service, that enables researchers to compare scene graph generation methods and increase visibility of new methods in a central place.   ","All of our code can be found at https://lorjul.github.io/sgbench/."],"url":"http://arxiv.org/abs/2404.09616v1"}
{"created":"2024-04-15 09:37:52","title":"If there's a Trigger Warning, then where's the Trigger? Investigating Trigger Warnings at the Passage Level","abstract":"Trigger warnings are labels that preface documents with sensitive content if this content could be perceived as harmful by certain groups of readers. Since warnings about a document intuitively need to be shown before reading it, authors usually assign trigger warnings at the document level. What parts of their writing prompted them to assign a warning, however, remains unclear. We investigate for the first time the feasibility of identifying the triggering passages of a document, both manually and computationally. We create a dataset of 4,135 English passages, each annotated with one of eight common trigger warnings. In a large-scale evaluation, we then systematically evaluate the effectiveness of fine-tuned and few-shot classifiers, and their generalizability. We find that trigger annotation belongs to the group of subjective annotation tasks in NLP, and that automatic trigger classification remains challenging but feasible.","sentences":["Trigger warnings are labels that preface documents with sensitive content if this content could be perceived as harmful by certain groups of readers.","Since warnings about a document intuitively need to be shown before reading it, authors usually assign trigger warnings at the document level.","What parts of their writing prompted them to assign a warning, however, remains unclear.","We investigate for the first time the feasibility of identifying the triggering passages of a document, both manually and computationally.","We create a dataset of 4,135 English passages, each annotated with one of eight common trigger warnings.","In a large-scale evaluation, we then systematically evaluate the effectiveness of fine-tuned and few-shot classifiers, and their generalizability.","We find that trigger annotation belongs to the group of subjective annotation tasks in NLP, and that automatic trigger classification remains challenging but feasible."],"url":"http://arxiv.org/abs/2404.09615v1"}
{"created":"2024-04-15 09:33:09","title":"Efficient and accurate neural field reconstruction using resistive memory","abstract":"Human beings construct perception of space by integrating sparse observations into massively interconnected synapses and neurons, offering a superior parallelism and efficiency. Replicating this capability in AI finds wide applications in medical imaging, AR/VR, and embodied AI, where input data is often sparse and computing resources are limited. However, traditional signal reconstruction methods on digital computers face both software and hardware challenges. On the software front, difficulties arise from storage inefficiencies in conventional explicit signal representation. Hardware obstacles include the von Neumann bottleneck, which limits data transfer between the CPU and memory, and the limitations of CMOS circuits in supporting parallel processing. We propose a systematic approach with software-hardware co-optimizations for signal reconstruction from sparse inputs. Software-wise, we employ neural field to implicitly represent signals via neural networks, which is further compressed using low-rank decomposition and structured pruning. Hardware-wise, we design a resistive memory-based computing-in-memory (CIM) platform, featuring a Gaussian Encoder (GE) and an MLP Processing Engine (PE). The GE harnesses the intrinsic stochasticity of resistive memory for efficient input encoding, while the PE achieves precise weight mapping through a Hardware-Aware Quantization (HAQ) circuit. We demonstrate the system's efficacy on a 40nm 256Kb resistive memory-based in-memory computing macro, achieving huge energy efficiency and parallelism improvements without compromising reconstruction quality in tasks like 3D CT sparse reconstruction, novel view synthesis, and novel view synthesis for dynamic scenes. This work advances the AI-driven signal restoration technology and paves the way for future efficient and robust medical AI and 3D vision applications.","sentences":["Human beings construct perception of space by integrating sparse observations into massively interconnected synapses and neurons, offering a superior parallelism and efficiency.","Replicating this capability in AI finds wide applications in medical imaging, AR/VR, and embodied AI, where input data is often sparse and computing resources are limited.","However, traditional signal reconstruction methods on digital computers face both software and hardware challenges.","On the software front, difficulties arise from storage inefficiencies in conventional explicit signal representation.","Hardware obstacles include the von Neumann bottleneck, which limits data transfer between the CPU and memory, and the limitations of CMOS circuits in supporting parallel processing.","We propose a systematic approach with software-hardware co-optimizations for signal reconstruction from sparse inputs.","Software-wise, we employ neural field to implicitly represent signals via neural networks, which is further compressed using low-rank decomposition and structured pruning.","Hardware-wise, we design a resistive memory-based computing-in-memory (CIM) platform, featuring a Gaussian Encoder (GE) and an MLP Processing Engine (PE).","The GE harnesses the intrinsic stochasticity of resistive memory for efficient input encoding, while the PE achieves precise weight mapping through a Hardware-Aware Quantization (HAQ) circuit.","We demonstrate the system's efficacy on a 40nm 256Kb resistive memory-based in-memory computing macro, achieving huge energy efficiency and parallelism improvements without compromising reconstruction quality in tasks like 3D CT sparse reconstruction, novel view synthesis, and novel view synthesis for dynamic scenes.","This work advances the AI-driven signal restoration technology and paves the way for future efficient and robust medical AI and 3D vision applications."],"url":"http://arxiv.org/abs/2404.09613v1"}
{"created":"2024-04-15 09:32:12","title":"LoRA Dropout as a Sparsity Regularizer for Overfitting Control","abstract":"Parameter-efficient fine-tuning methods, represented by LoRA, play an essential role in adapting large-scale pre-trained models to downstream tasks. However, fine-tuning LoRA-series models also faces the risk of overfitting on the training dataset, and yet there's still a lack of theoretical guidance and practical mechanism to control overfitting on LoRA-based PEFT methods. In this paper, we propose a LoRA Dropout mechanism for the LoRA-based methods by introducing random noises to the learnable low-rank matrices and increasing parameter sparsity. We then demonstrate the theoretical mechanism of our LoRA Dropout mechanism from the perspective of sparsity regularization by providing a generalization error bound under this framework. Theoretical results show that appropriate sparsity would help tighten the gap between empirical and generalization risks and thereby control overfitting. Furthermore, based on the LoRA Dropout framework, we introduce a test-time ensemble strategy and provide theoretical evidence demonstrating that the ensemble method can further compress the error bound, and lead to better performance during inference time. Extensive experiments on various NLP tasks provide practical validations of the effectiveness of our LoRA Dropout framework in improving model accuracy and calibration.","sentences":["Parameter-efficient fine-tuning methods, represented by LoRA, play an essential role in adapting large-scale pre-trained models to downstream tasks.","However, fine-tuning LoRA-series models also faces the risk of overfitting on the training dataset, and yet there's still a lack of theoretical guidance and practical mechanism to control overfitting on LoRA-based PEFT methods.","In this paper, we propose a LoRA Dropout mechanism for the LoRA-based methods by introducing random noises to the learnable low-rank matrices and increasing parameter sparsity.","We then demonstrate the theoretical mechanism of our LoRA Dropout mechanism from the perspective of sparsity regularization by providing a generalization error bound under this framework.","Theoretical results show that appropriate sparsity would help tighten the gap between empirical and generalization risks and thereby control overfitting.","Furthermore, based on the LoRA Dropout framework, we introduce a test-time ensemble strategy and provide theoretical evidence demonstrating that the ensemble method can further compress the error bound, and lead to better performance during inference time.","Extensive experiments on various NLP tasks provide practical validations of the effectiveness of our LoRA Dropout framework in improving model accuracy and calibration."],"url":"http://arxiv.org/abs/2404.09610v1"}
{"created":"2024-04-15 09:29:12","title":"Better space-time-robustness trade-offs for set reconciliation","abstract":"We consider the problem of reconstructing the symmetric difference between similar sets from their representations (sketches) of size linear in the number of differences. Exact solutions to this problem are based on error-correcting coding techniques and suffer from a large decoding time. Existing probabilistic solutions based on Invertible Bloom Lookup Tables (IBLTs) are time-efficient but offer insufficient success guarantees for many applications. Here we propose a tunable trade-off between the two approaches combining the efficiency of IBLTs with exponentially decreasing failure probability. The proof relies on a refined analysis of IBLTs proposed in (Baek Tejs Houen et al. SOSA 2023) which has an independent interest. We also propose a modification of our algorithm that enables telling apart the elements of each set in the symmetric difference.","sentences":["We consider the problem of reconstructing the symmetric difference between similar sets from their representations (sketches) of size linear in the number of differences.","Exact solutions to this problem are based on error-correcting coding techniques and suffer from a large decoding time.","Existing probabilistic solutions based on Invertible Bloom Lookup Tables (IBLTs) are time-efficient but offer insufficient success guarantees for many applications.","Here we propose a tunable trade-off between the two approaches combining the efficiency of IBLTs with exponentially decreasing failure probability.","The proof relies on a refined analysis of IBLTs proposed in (Baek Tejs Houen et al. SOSA 2023) which has an independent interest.","We also propose a modification of our algorithm that enables telling apart the elements of each set in the symmetric difference."],"url":"http://arxiv.org/abs/2404.09607v1"}
{"created":"2024-04-15 09:26:33","title":"A Self-feedback Knowledge Elicitation Approach for Chemical Reaction Predictions","abstract":"The task of chemical reaction predictions (CRPs) plays a pivotal role in advancing drug discovery and material science. However, its effectiveness is constrained by the vast and uncertain chemical reaction space and challenges in capturing reaction selectivity, particularly due to existing methods' limitations in exploiting the data's inherent knowledge. To address these challenges, we introduce a data-curated self-feedback knowledge elicitation approach. This method starts from iterative optimization of molecular representations and facilitates the extraction of knowledge on chemical reaction types (RTs). Then, we employ adaptive prompt learning to infuse the prior knowledge into the large language model (LLM). As a result, we achieve significant enhancements: a 14.2% increase in retrosynthesis prediction accuracy, a 74.2% rise in reagent prediction accuracy, and an expansion in the model's capability for handling multi-task chemical reactions. This research offers a novel paradigm for knowledge elicitation in scientific research and showcases the untapped potential of LLMs in CRPs.","sentences":["The task of chemical reaction predictions (CRPs) plays a pivotal role in advancing drug discovery and material science.","However, its effectiveness is constrained by the vast and uncertain chemical reaction space and challenges in capturing reaction selectivity, particularly due to existing methods' limitations in exploiting the data's inherent knowledge.","To address these challenges, we introduce a data-curated self-feedback knowledge elicitation approach.","This method starts from iterative optimization of molecular representations and facilitates the extraction of knowledge on chemical reaction types (RTs).","Then, we employ adaptive prompt learning to infuse the prior knowledge into the large language model (LLM).","As a result, we achieve significant enhancements: a 14.2% increase in retrosynthesis prediction accuracy, a 74.2% rise in reagent prediction accuracy, and an expansion in the model's capability for handling multi-task chemical reactions.","This research offers a novel paradigm for knowledge elicitation in scientific research and showcases the untapped potential of LLMs in CRPs."],"url":"http://arxiv.org/abs/2404.09606v1"}
{"created":"2024-04-15 09:25:29","title":"Finite-sample expansions for the optimal error probability in asymmetric binary hypothesis testing","abstract":"The problem of binary hypothesis testing between two probability measures is considered. New sharp bounds are derived for the best achievable error probability of such tests based on independent and identically distributed observations. Specifically, the asymmetric version of the problem is examined, where different requirements are placed on the two error probabilities. Accurate nonasymptotic expansions with explicit constants are obtained for the error probability, using tools from large deviations and Gaussian approximation. Examples are shown indicating that, in the asymmetric regime, the approximations suggested by the new bounds are significantly more accurate than the approximations provided by either of the two main earlier approaches -- normal approximation and error exponents.","sentences":["The problem of binary hypothesis testing between two probability measures is considered.","New sharp bounds are derived for the best achievable error probability of such tests based on independent and identically distributed observations.","Specifically, the asymmetric version of the problem is examined, where different requirements are placed on the two error probabilities.","Accurate nonasymptotic expansions with explicit constants are obtained for the error probability, using tools from large deviations and Gaussian approximation.","Examples are shown indicating that, in the asymmetric regime, the approximations suggested by the new bounds are significantly more accurate than the approximations provided by either of the two main earlier approaches -- normal approximation and error exponents."],"url":"http://arxiv.org/abs/2404.09605v1"}
{"created":"2024-04-15 09:22:46","title":"Machine learning-based optimization workflow of the homogeneity of spunbond nonwovens with human validation","abstract":"In the last ten years, the average annual growth rate of nonwoven production was 4%. In 2020 and 2021, nonwoven production has increased even further due to the huge demand for nonwoven products needed for protective clothing such as FFP2 masks to combat the COVID19 pandemic. Optimizing the production process is still a challenge due to its high nonlinearity. In this paper, we present a machine learning-based optimization workflow aimed at improving the homogeneity of spunbond nonwovens. The optimization workflow is based on a mathematical model that simulates the microstructures of nonwovens. Based on trainingy data coming from this simulator, different machine learning algorithms are trained in order to find a surrogate model for the time-consuming simulator. Human validation is employed to verify the outputs of machine learning algorithms by assessing the aesthetics of the nonwovens. We include scientific and expert knowledge into the training data to reduce the computational costs involved in the optimization process. We demonstrate the necessity and effectiveness of our workflow in optimizing the homogeneity of nonwovens.","sentences":["In the last ten years, the average annual growth rate of nonwoven production was 4%.","In 2020 and 2021, nonwoven production has increased even further due to the huge demand for nonwoven products needed for protective clothing such as FFP2 masks to combat the COVID19 pandemic.","Optimizing the production process is still a challenge due to its high nonlinearity.","In this paper, we present a machine learning-based optimization workflow aimed at improving the homogeneity of spunbond nonwovens.","The optimization workflow is based on a mathematical model that simulates the microstructures of nonwovens.","Based on trainingy data coming from this simulator, different machine learning algorithms are trained in order to find a surrogate model for the time-consuming simulator.","Human validation is employed to verify the outputs of machine learning algorithms by assessing the aesthetics of the nonwovens.","We include scientific and expert knowledge into the training data to reduce the computational costs involved in the optimization process.","We demonstrate the necessity and effectiveness of our workflow in optimizing the homogeneity of nonwovens."],"url":"http://arxiv.org/abs/2404.09604v1"}
{"created":"2024-04-15 09:16:49","title":"Reactive Model Correction: Mitigating Harm to Task-Relevant Features via Conditional Bias Suppression","abstract":"Deep Neural Networks are prone to learning and relying on spurious correlations in the training data, which, for high-risk applications, can have fatal consequences. Various approaches to suppress model reliance on harmful features have been proposed that can be applied post-hoc without additional training. Whereas those methods can be applied with efficiency, they also tend to harm model performance by globally shifting the distribution of latent features. To mitigate unintended overcorrection of model behavior, we propose a reactive approach conditioned on model-derived knowledge and eXplainable Artificial Intelligence (XAI) insights. While the reactive approach can be applied to many post-hoc methods, we demonstrate the incorporation of reactivity in particular for P-ClArC (Projective Class Artifact Compensation), introducing a new method called R-ClArC (Reactive Class Artifact Compensation). Through rigorous experiments in controlled settings (FunnyBirds) and with a real-world dataset (ISIC2019), we show that introducing reactivity can minimize the detrimental effect of the applied correction while simultaneously ensuring low reliance on spurious features.","sentences":["Deep Neural Networks are prone to learning and relying on spurious correlations in the training data, which, for high-risk applications, can have fatal consequences.","Various approaches to suppress model reliance on harmful features have been proposed that can be applied post-hoc without additional training.","Whereas those methods can be applied with efficiency, they also tend to harm model performance by globally shifting the distribution of latent features.","To mitigate unintended overcorrection of model behavior, we propose a reactive approach conditioned on model-derived knowledge and eXplainable Artificial Intelligence (XAI) insights.","While the reactive approach can be applied to many post-hoc methods, we demonstrate the incorporation of reactivity in particular for P-ClArC (Projective Class Artifact Compensation), introducing a new method called R-ClArC (Reactive Class Artifact Compensation).","Through rigorous experiments in controlled settings (FunnyBirds) and with a real-world dataset (ISIC2019), we show that introducing reactivity can minimize the detrimental effect of the applied correction while simultaneously ensuring low reliance on spurious features."],"url":"http://arxiv.org/abs/2404.09601v1"}
{"created":"2024-04-15 09:10:52","title":"Enhancing Code Vulnerability Detection via Vulnerability-Preserving Data Augmentation","abstract":"Source code vulnerability detection aims to identify inherent vulnerabilities to safeguard software systems from potential attacks. Many prior studies overlook diverse vulnerability characteristics, simplifying the problem into a binary (0-1) classification task for example determining whether it is vulnerable or not. This poses a challenge for a single deep learning-based model to effectively learn the wide array of vulnerability characteristics. Furthermore, due to the challenges associated with collecting large-scale vulnerability data, these detectors often overfit limited training datasets, resulting in lower model generalization performance.   To address the aforementioned challenges, in this work, we introduce a fine-grained vulnerability detector namely FGVulDet. Unlike previous approaches, FGVulDet employs multiple classifiers to discern characteristics of various vulnerability types and combines their outputs to identify the specific type of vulnerability. Each classifier is designed to learn type-specific vulnerability semantics. Additionally, to address the scarcity of data for some vulnerability types and enhance data diversity for learning better vulnerability semantics, we propose a novel vulnerability-preserving data augmentation technique to augment the number of vulnerabilities. Taking inspiration from recent advancements in graph neural networks for learning program semantics, we incorporate a Gated Graph Neural Network (GGNN) and extend it to an edge-aware GGNN to capture edge-type information. FGVulDet is trained on a large-scale dataset from GitHub, encompassing five different types of vulnerabilities. Extensive experiments compared with static-analysis-based approaches and learning-based approaches have demonstrated the effectiveness of FGVulDet.","sentences":["Source code vulnerability detection aims to identify inherent vulnerabilities to safeguard software systems from potential attacks.","Many prior studies overlook diverse vulnerability characteristics, simplifying the problem into a binary (0-1) classification task for example determining whether it is vulnerable or not.","This poses a challenge for a single deep learning-based model to effectively learn the wide array of vulnerability characteristics.","Furthermore, due to the challenges associated with collecting large-scale vulnerability data, these detectors often overfit limited training datasets, resulting in lower model generalization performance.   ","To address the aforementioned challenges, in this work, we introduce a fine-grained vulnerability detector namely FGVulDet.","Unlike previous approaches, FGVulDet employs multiple classifiers to discern characteristics of various vulnerability types and combines their outputs to identify the specific type of vulnerability.","Each classifier is designed to learn type-specific vulnerability semantics.","Additionally, to address the scarcity of data for some vulnerability types and enhance data diversity for learning better vulnerability semantics, we propose a novel vulnerability-preserving data augmentation technique to augment the number of vulnerabilities.","Taking inspiration from recent advancements in graph neural networks for learning program semantics, we incorporate a Gated Graph Neural Network (GGNN) and extend it to an edge-aware GGNN to capture edge-type information.","FGVulDet is trained on a large-scale dataset from GitHub, encompassing five different types of vulnerabilities.","Extensive experiments compared with static-analysis-based approaches and learning-based approaches have demonstrated the effectiveness of FGVulDet."],"url":"http://arxiv.org/abs/2404.09599v1"}
{"created":"2024-04-15 09:08:02","title":"Using Tangible Interaction to Design Musicking Artifacts for Non-musicians","abstract":"This paper presents a Research through Design exploration of the potential for using tangible interactions to enable active music experiences - musicking - for non-musicians. We present the Tubularium prototype, which aims to help non-musicians play music without requiring any initial skill. We present the initial design of the prototype and the features implemented in order to enable music-making by non-musicians, and offer some reflections based on observations of informal initial user explorations of the prototype.","sentences":["This paper presents a Research through Design exploration of the potential for using tangible interactions to enable active music experiences - musicking - for non-musicians.","We present the Tubularium prototype, which aims to help non-musicians play music without requiring any initial skill.","We present the initial design of the prototype and the features implemented in order to enable music-making by non-musicians, and offer some reflections based on observations of informal initial user explorations of the prototype."],"url":"http://arxiv.org/abs/2404.09597v1"}
{"created":"2024-04-15 09:03:05","title":"Improving Recall of Large Language Models: A Model Collaboration Approach for Relational Triple Extraction","abstract":"Relation triple extraction, which outputs a set of triples from long sentences, plays a vital role in knowledge acquisition. Large language models can accurately extract triples from simple sentences through few-shot learning or fine-tuning when given appropriate instructions. However, they often miss out when extracting from complex sentences. In this paper, we design an evaluation-filtering framework that integrates large language models with small models for relational triple extraction tasks. The framework includes an evaluation model that can extract related entity pairs with high precision. We propose a simple labeling principle and a deep neural network to build the model, embedding the outputs as prompts into the extraction process of the large model. We conduct extensive experiments to demonstrate that the proposed method can assist large language models in obtaining more accurate extraction results, especially from complex sentences containing multiple relational triples. Our evaluation model can also be embedded into traditional extraction models to enhance their extraction precision from complex sentences.","sentences":["Relation triple extraction, which outputs a set of triples from long sentences, plays a vital role in knowledge acquisition.","Large language models can accurately extract triples from simple sentences through few-shot learning or fine-tuning when given appropriate instructions.","However, they often miss out when extracting from complex sentences.","In this paper, we design an evaluation-filtering framework that integrates large language models with small models for relational triple extraction tasks.","The framework includes an evaluation model that can extract related entity pairs with high precision.","We propose a simple labeling principle and a deep neural network to build the model, embedding the outputs as prompts into the extraction process of the large model.","We conduct extensive experiments to demonstrate that the proposed method can assist large language models in obtaining more accurate extraction results, especially from complex sentences containing multiple relational triples.","Our evaluation model can also be embedded into traditional extraction models to enhance their extraction precision from complex sentences."],"url":"http://arxiv.org/abs/2404.09593v1"}
{"created":"2024-04-15 09:01:47","title":"3D Gaussian Splatting as Markov Chain Monte Carlo","abstract":"While 3D Gaussian Splatting has recently become popular for neural rendering, current methods rely on carefully engineered cloning and splitting strategies for placing Gaussians, which does not always generalize and may lead to poor-quality renderings. In addition, for real-world scenes, they rely on a good initial point cloud to perform well. In this work, we rethink 3D Gaussians as random samples drawn from an underlying probability distribution describing the physical representation of the scene -- in other words, Markov Chain Monte Carlo (MCMC) samples. Under this view, we show that the 3D Gaussian updates are strikingly similar to a Stochastic Langevin Gradient Descent (SGLD) update. As with MCMC, samples are nothing but past visit locations, adding new Gaussians under our framework can simply be realized without heuristics as placing Gaussians at existing Gaussian locations. To encourage using fewer Gaussians for efficiency, we introduce an L1-regularizer on the Gaussians. On various standard evaluation scenes, we show that our method provides improved rendering quality, easy control over the number of Gaussians, and robustness to initialization.","sentences":["While 3D Gaussian Splatting has recently become popular for neural rendering, current methods rely on carefully engineered cloning and splitting strategies for placing Gaussians, which does not always generalize and may lead to poor-quality renderings.","In addition, for real-world scenes, they rely on a good initial point cloud to perform well.","In this work, we rethink 3D Gaussians as random samples drawn from an underlying probability distribution describing the physical representation of the scene -- in other words, Markov Chain Monte Carlo (MCMC) samples.","Under this view, we show that the 3D Gaussian updates are strikingly similar to a Stochastic Langevin Gradient Descent (SGLD) update.","As with MCMC, samples are nothing but past visit locations, adding new Gaussians under our framework can simply be realized without heuristics as placing Gaussians at existing Gaussian locations.","To encourage using fewer Gaussians for efficiency, we introduce an L1-regularizer on the Gaussians.","On various standard evaluation scenes, we show that our method provides improved rendering quality, easy control over the number of Gaussians, and robustness to initialization."],"url":"http://arxiv.org/abs/2404.09591v1"}
{"created":"2024-04-15 08:56:53","title":"German Tourism Knowledge Graph","abstract":"Tourism is one of the most critical sectors of the global economy. Due to its heterogeneous and fragmented nature, it provides one of the most suitable use cases for knowledge graphs. In this poster, we introduce the German Tourism Knowledge Graph that integrates tourism-related data from 16 federal states of Germany and various other sources to provide a curated knowledge source for various applications. It is publicly available through GUIs and an API.","sentences":["Tourism is one of the most critical sectors of the global economy.","Due to its heterogeneous and fragmented nature, it provides one of the most suitable use cases for knowledge graphs.","In this poster, we introduce the German Tourism Knowledge Graph that integrates tourism-related data from 16 federal states of Germany and various other sources to provide a curated knowledge source for various applications.","It is publicly available through GUIs and an API."],"url":"http://arxiv.org/abs/2404.09587v1"}
{"created":"2024-04-15 08:54:33","title":"Mitigating the Curse of Dimensionality for Certified Robustness via Dual Randomized Smoothing","abstract":"Randomized Smoothing (RS) has been proven a promising method for endowing an arbitrary image classifier with certified robustness. However, the substantial uncertainty inherent in the high-dimensional isotropic Gaussian noise imposes the curse of dimensionality on RS. Specifically, the upper bound of ${\\ell_2}$ certified robustness radius provided by RS exhibits a diminishing trend with the expansion of the input dimension $d$, proportionally decreasing at a rate of $1/\\sqrt{d}$. This paper explores the feasibility of providing ${\\ell_2}$ certified robustness for high-dimensional input through the utilization of dual smoothing in the lower-dimensional space. The proposed Dual Randomized Smoothing (DRS) down-samples the input image into two sub-images and smooths the two sub-images in lower dimensions. Theoretically, we prove that DRS guarantees a tight ${\\ell_2}$ certified robustness radius for the original input and reveal that DRS attains a superior upper bound on the ${\\ell_2}$ robustness radius, which decreases proportionally at a rate of $(1/\\sqrt m + 1/\\sqrt n )$ with $m+n=d$. Extensive experiments demonstrate the generalizability and effectiveness of DRS, which exhibits a notable capability to integrate with established methodologies, yielding substantial improvements in both accuracy and ${\\ell_2}$ certified robustness baselines of RS on the CIFAR-10 and ImageNet datasets. Code is available at https://github.com/xiasong0501/DRS.","sentences":["Randomized Smoothing (RS) has been proven a promising method for endowing an arbitrary image classifier with certified robustness.","However, the substantial uncertainty inherent in the high-dimensional isotropic Gaussian noise imposes the curse of dimensionality on RS.","Specifically, the upper bound of ${\\ell_2}$ certified robustness radius provided by RS exhibits a diminishing trend with the expansion of the input dimension $d$, proportionally decreasing at a rate of $1/\\sqrt{d}$. This paper explores the feasibility of providing ${\\ell_2}$ certified robustness for high-dimensional input through the utilization of dual smoothing in the lower-dimensional space.","The proposed Dual Randomized Smoothing (DRS) down-samples the input image into two sub-images and smooths the two sub-images in lower dimensions.","Theoretically, we prove that DRS guarantees a tight ${\\ell_2}$ certified robustness radius for the original input and reveal that DRS attains a superior upper bound on the ${\\ell_2}$ robustness radius, which decreases proportionally at a rate of $(1/\\sqrt m + 1/\\sqrt n )$ with $m+n=d$. Extensive experiments demonstrate the generalizability and effectiveness of DRS, which exhibits a notable capability to integrate with established methodologies, yielding substantial improvements in both accuracy and ${\\ell_2}$ certified robustness baselines of RS on the CIFAR-10 and ImageNet datasets.","Code is available at https://github.com/xiasong0501/DRS."],"url":"http://arxiv.org/abs/2404.09586v1"}
{"created":"2024-04-15 08:52:51","title":"Pseudo-label Learning with Calibrated Confidence Using an Energy-based Model","abstract":"In pseudo-labeling (PL), which is a type of semi-supervised learning, pseudo-labels are assigned based on the confidence scores provided by the classifier; therefore, accurate confidence is important for successful PL. In this study, we propose a PL algorithm based on an energy-based model (EBM), which is referred to as the energy-based PL (EBPL). In EBPL, a neural network-based classifier and an EBM are jointly trained by sharing their feature extraction parts. This approach enables the model to learn both the class decision boundary and input data distribution, enhancing confidence calibration during network training. The experimental results demonstrate that EBPL outperforms the existing PL method in semi-supervised image classification tasks, with superior confidence calibration error and recognition accuracy.","sentences":["In pseudo-labeling (PL), which is a type of semi-supervised learning, pseudo-labels are assigned based on the confidence scores provided by the classifier; therefore, accurate confidence is important for successful PL.","In this study, we propose a PL algorithm based on an energy-based model (EBM), which is referred to as the energy-based PL (EBPL).","In EBPL, a neural network-based classifier and an EBM are jointly trained by sharing their feature extraction parts.","This approach enables the model to learn both the class decision boundary and input data distribution, enhancing confidence calibration during network training.","The experimental results demonstrate that EBPL outperforms the existing PL method in semi-supervised image classification tasks, with superior confidence calibration error and recognition accuracy."],"url":"http://arxiv.org/abs/2404.09585v1"}
{"created":"2024-04-15 08:51:49","title":"GeoSACS: Geometric Shared Autonomy via Canal Surfaces","abstract":"We introduce GeoSACS, a geometric framework for shared autonomy (SA). In variable environments, SA methods can be used to combine robotic capabilities with real-time human input in a way that offloads the physical task from the human. To remain intuitive, it can be helpful to simplify requirements for human input (i.e., reduce the dimensionality), which create challenges for to map low-dimensional human inputs to the higher dimensional control space of robots without requiring large amounts of data. We built GeoSACS on canal surfaces, a geometric framework that represents potential robot trajectories as a canal from as few as two demonstrations. GeoSACS maps user corrections on the cross-sections of this canal to provide an efficient SA framework. We extend canal surfaces to consider orientation and update the control frames to support intuitive mapping from user input to robot motions. Finally, we demonstrate GeoSACS in two preliminary studies, including a complex manipulation task where a robot loads laundry into a washer.","sentences":["We introduce GeoSACS, a geometric framework for shared autonomy (SA).","In variable environments, SA methods can be used to combine robotic capabilities with real-time human input in a way that offloads the physical task from the human.","To remain intuitive, it can be helpful to simplify requirements for human input (i.e., reduce the dimensionality), which create challenges for to map low-dimensional human inputs to the higher dimensional control space of robots without requiring large amounts of data.","We built GeoSACS on canal surfaces, a geometric framework that represents potential robot trajectories as a canal from as few as two demonstrations.","GeoSACS maps user corrections on the cross-sections of this canal to provide an efficient SA framework.","We extend canal surfaces to consider orientation and update the control frames to support intuitive mapping from user input to robot motions.","Finally, we demonstrate GeoSACS in two preliminary studies, including a complex manipulation task where a robot loads laundry into a washer."],"url":"http://arxiv.org/abs/2404.09584v1"}
{"created":"2024-04-15 08:40:01","title":"Modelling Language","abstract":"This paper argues that large language models have a valuable scientific role to play in serving as scientific models of a language. Linguistic study should not only be concerned with the cognitive processes behind linguistic competence, but also with language understood as an external, social entity. Once this is recognized, the value of large language models as scientific models becomes clear. This paper defends this position against a number of arguments to the effect that language models provide no linguistic insight. It also draws upon recent work in philosophy of science to show how large language models could serve as scientific models.","sentences":["This paper argues that large language models have a valuable scientific role to play in serving as scientific models of a language.","Linguistic study should not only be concerned with the cognitive processes behind linguistic competence, but also with language understood as an external, social entity.","Once this is recognized, the value of large language models as scientific models becomes clear.","This paper defends this position against a number of arguments to the effect that language models provide no linguistic insight.","It also draws upon recent work in philosophy of science to show how large language models could serve as scientific models."],"url":"http://arxiv.org/abs/2404.09579v1"}
{"created":"2024-04-15 08:39:52","title":"Recall-Augmented Ranking: Enhancing Click-Through Rate Prediction Accuracy with Cross-Stage Data","abstract":"Click-through rate (CTR) prediction plays an indispensable role in online platforms. Numerous models have been proposed to capture users' shifting preferences by leveraging user behavior sequences. However, these historical sequences often suffer from severe homogeneity and scarcity compared to the extensive item pool. Relying solely on such sequences for user representations is inherently restrictive, as user interests extend beyond the scope of items they have previously engaged with. To address this challenge, we propose a data-driven approach to enrich user representations. We recognize user profiling and recall items as two ideal data sources within the cross-stage framework, encompassing the u2u (user-to-user) and i2i (item-to-item) aspects respectively. In this paper, we propose a novel architecture named Recall-Augmented Ranking (RAR). RAR consists of two key sub-modules, which synergistically gather information from a vast pool of look-alike users and recall items, resulting in enriched user representations. Notably, RAR is orthogonal to many existing CTR models, allowing for consistent performance improvements in a plug-and-play manner. Extensive experiments are conducted, which verify the efficacy and compatibility of RAR against the SOTA methods.","sentences":["Click-through rate (CTR) prediction plays an indispensable role in online platforms.","Numerous models have been proposed to capture users' shifting preferences by leveraging user behavior sequences.","However, these historical sequences often suffer from severe homogeneity and scarcity compared to the extensive item pool.","Relying solely on such sequences for user representations is inherently restrictive, as user interests extend beyond the scope of items they have previously engaged with.","To address this challenge, we propose a data-driven approach to enrich user representations.","We recognize user profiling and recall items as two ideal data sources within the cross-stage framework, encompassing the u2u (user-to-user) and i2i (item-to-item) aspects respectively.","In this paper, we propose a novel architecture named Recall-Augmented Ranking (RAR).","RAR consists of two key sub-modules, which synergistically gather information from a vast pool of look-alike users and recall items, resulting in enriched user representations.","Notably, RAR is orthogonal to many existing CTR models, allowing for consistent performance improvements in a plug-and-play manner.","Extensive experiments are conducted, which verify the efficacy and compatibility of RAR against the SOTA methods."],"url":"http://arxiv.org/abs/2404.09578v1"}
