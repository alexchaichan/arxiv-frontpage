{"created":"2024-04-23 17:59:59","title":"SMPLer: Taming Transformers for Monocular 3D Human Shape and Pose Estimation","abstract":"Existing Transformers for monocular 3D human shape and pose estimation typically have a quadratic computation and memory complexity with respect to the feature length, which hinders the exploitation of fine-grained information in high-resolution features that is beneficial for accurate reconstruction. In this work, we propose an SMPL-based Transformer framework (SMPLer) to address this issue. SMPLer incorporates two key ingredients: a decoupled attention operation and an SMPL-based target representation, which allow effective utilization of high-resolution features in the Transformer. In addition, based on these two designs, we also introduce several novel modules including a multi-scale attention and a joint-aware attention to further boost the reconstruction performance. Extensive experiments demonstrate the effectiveness of SMPLer against existing 3D human shape and pose estimation methods both quantitatively and qualitatively. Notably, the proposed algorithm achieves an MPJPE of 45.2 mm on the Human3.6M dataset, improving upon Mesh Graphormer by more than 10% with fewer than one-third of the parameters. Code and pretrained models are available at https://github.com/xuxy09/SMPLer.","sentences":["Existing Transformers for monocular 3D human shape and pose estimation typically have a quadratic computation and memory complexity with respect to the feature length, which hinders the exploitation of fine-grained information in high-resolution features that is beneficial for accurate reconstruction.","In this work, we propose an SMPL-based Transformer framework (SMPLer) to address this issue.","SMPLer incorporates two key ingredients: a decoupled attention operation and an SMPL-based target representation, which allow effective utilization of high-resolution features in the Transformer.","In addition, based on these two designs, we also introduce several novel modules including a multi-scale attention and a joint-aware attention to further boost the reconstruction performance.","Extensive experiments demonstrate the effectiveness of SMPLer against existing 3D human shape and pose estimation methods both quantitatively and qualitatively.","Notably, the proposed algorithm achieves an MPJPE of 45.2 mm on the Human3.6M dataset, improving upon Mesh Graphormer by more than 10% with fewer than one-third of the parameters.","Code and pretrained models are available at https://github.com/xuxy09/SMPLer."],"url":"http://arxiv.org/abs/2404.15276v1"}
{"created":"2024-04-23 17:59:43","title":"ID-Animator: Zero-Shot Identity-Preserving Human Video Generation","abstract":"Generating high fidelity human video with specified identities has attracted significant attention in the content generation community. However, existing techniques struggle to strike a balance between training efficiency and identity preservation, either requiring tedious case-by-case finetuning or usually missing the identity details in video generation process. In this study, we present ID-Animator, a zero-shot human-video generation approach that can perform personalized video generation given single reference facial image without further training. ID-Animator inherits existing diffusion-based video generation backbones with a face adapter to encode the ID-relevant embeddings from learnable facial latent queries. To facilitate the extraction of identity information in video generation, we introduce an ID-oriented dataset construction pipeline, which incorporates decoupled human attribute and action captioning technique from a constructed facial image pool. Based on this pipeline, a random face reference training method is further devised to precisely capture the ID-relevant embeddings from reference images, thus improving the fidelity and generalization capacity of our model for ID-specific video generation. Extensive experiments demonstrate the superiority of ID-Animator to generate personalized human videos over previous models. Moreover, our method is highly compatible with popular pre-trained T2V models like animatediff and various community backbone models, showing high extendability in real-world applications for video generation where identity preservation is highly desired. Our codes and checkpoints will be released at https://github.com/ID-Animator/ID-Animator.","sentences":["Generating high fidelity human video with specified identities has attracted significant attention in the content generation community.","However, existing techniques struggle to strike a balance between training efficiency and identity preservation, either requiring tedious case-by-case finetuning or usually missing the identity details in video generation process.","In this study, we present ID-Animator, a zero-shot human-video generation approach that can perform personalized video generation given single reference facial image without further training.","ID-Animator inherits existing diffusion-based video generation backbones with a face adapter to encode the ID-relevant embeddings from learnable facial latent queries.","To facilitate the extraction of identity information in video generation, we introduce an ID-oriented dataset construction pipeline, which incorporates decoupled human attribute and action captioning technique from a constructed facial image pool.","Based on this pipeline, a random face reference training method is further devised to precisely capture the ID-relevant embeddings from reference images, thus improving the fidelity and generalization capacity of our model for ID-specific video generation.","Extensive experiments demonstrate the superiority of ID-Animator to generate personalized human videos over previous models.","Moreover, our method is highly compatible with popular pre-trained T2V models like animatediff and various community backbone models, showing high extendability in real-world applications for video generation where identity preservation is highly desired.","Our codes and checkpoints will be released at https://github.com/ID-Animator/ID-Animator."],"url":"http://arxiv.org/abs/2404.15275v1"}
{"created":"2024-04-23 17:59:12","title":"Metric-guided Image Reconstruction Bounds via Conformal Prediction","abstract":"Recent advancements in machine learning have led to novel imaging systems and algorithms that address ill-posed problems. Assessing their trustworthiness and understanding how to deploy them safely at test time remains an important and open problem. We propose a method that leverages conformal prediction to retrieve upper/lower bounds and statistical inliers/outliers of reconstructions based on the prediction intervals of downstream metrics. We apply our method to sparse-view CT for downstream radiotherapy planning and show 1) that metric-guided bounds have valid coverage for downstream metrics while conventional pixel-wise bounds do not and 2) anatomical differences of upper/lower bounds between metric-guided and pixel-wise methods. Our work paves the way for more meaningful reconstruction bounds. Code available at https://github.com/matthewyccheung/conformal-metric","sentences":["Recent advancements in machine learning have led to novel imaging systems and algorithms that address ill-posed problems.","Assessing their trustworthiness and understanding how to deploy them safely at test time remains an important and open problem.","We propose a method that leverages conformal prediction to retrieve upper/lower bounds and statistical inliers/outliers of reconstructions based on the prediction intervals of downstream metrics.","We apply our method to sparse-view CT for downstream radiotherapy planning and show 1) that metric-guided bounds have valid coverage for downstream metrics while conventional pixel-wise bounds do not and 2) anatomical differences of upper/lower bounds between metric-guided and pixel-wise methods.","Our work paves the way for more meaningful reconstruction bounds.","Code available at https://github.com/matthewyccheung/conformal-metric"],"url":"http://arxiv.org/abs/2404.15274v1"}
{"created":"2024-04-23 17:59:01","title":"CT-GLIP: 3D Grounded Language-Image Pretraining with CT Scans and Radiology Reports for Full-Body Scenarios","abstract":"Medical Vision-Language Pretraining (Med-VLP) establishes a connection between visual content from medical images and the relevant textual descriptions. Existing Med-VLP methods primarily focus on 2D images depicting a single body part, notably chest X-rays. In this paper, we extend the scope of Med-VLP to encompass 3D images, specifically targeting full-body scenarios, by using a multimodal dataset of CT images and reports. Compared with the 2D counterpart, 3D VLP is required to effectively capture essential semantics from significantly sparser representation in 3D imaging. In this paper, we introduce CT-GLIP (Grounded Language-Image Pretraining with CT scans), a novel method that constructs organ-level image-text pairs to enhance multimodal contrastive learning, aligning grounded visual features with precise diagnostic text. Additionally, we developed an abnormality dictionary to augment contrastive learning with diverse negative samples. Our method, trained on a multimodal CT dataset comprising 44,011 organ-level vision-text pairs from 17,702 patients across 104 organs, demonstrates it can identify organs and abnormalities in a zero-shot manner using natural languages. The performance of CT-GLIP is validated on a separate test set of 1,130 patients, focusing on the 16 most frequent abnormalities across 7 organs. The experimental results show our model's superior performance over the standard CLIP framework across zero-shot and fine-tuning scenarios, using both CNN and ViT architectures.","sentences":["Medical Vision-Language Pretraining (Med-VLP) establishes a connection between visual content from medical images and the relevant textual descriptions.","Existing Med-VLP methods primarily focus on 2D images depicting a single body part, notably chest X-rays.","In this paper, we extend the scope of Med-VLP to encompass 3D images, specifically targeting full-body scenarios, by using a multimodal dataset of CT images and reports.","Compared with the 2D counterpart, 3D VLP is required to effectively capture essential semantics from significantly sparser representation in 3D imaging.","In this paper, we introduce CT-GLIP (Grounded Language-Image Pretraining with CT scans), a novel method that constructs organ-level image-text pairs to enhance multimodal contrastive learning, aligning grounded visual features with precise diagnostic text.","Additionally, we developed an abnormality dictionary to augment contrastive learning with diverse negative samples.","Our method, trained on a multimodal CT dataset comprising 44,011 organ-level vision-text pairs from 17,702 patients across 104 organs, demonstrates it can identify organs and abnormalities in a zero-shot manner using natural languages.","The performance of CT-GLIP is validated on a separate test set of 1,130 patients, focusing on the 16 most frequent abnormalities across 7 organs.","The experimental results show our model's superior performance over the standard CLIP framework across zero-shot and fine-tuning scenarios, using both CNN and ViT architectures."],"url":"http://arxiv.org/abs/2404.15272v1"}
{"created":"2024-04-23 17:58:33","title":"Automatic Layout Planning for Visually-Rich Documents with Instruction-Following Models","abstract":"Recent advancements in instruction-following models have made user interactions with models more user-friendly and efficient, broadening their applicability. In graphic design, non-professional users often struggle to create visually appealing layouts due to limited skills and resources. In this work, we introduce a novel multimodal instruction-following framework for layout planning, allowing users to easily arrange visual elements into tailored layouts by specifying canvas size and design purpose, such as for book covers, posters, brochures, or menus. We developed three layout reasoning tasks to train the model in understanding and executing layout instructions. Experiments on two benchmarks show that our method not only simplifies the design process for non-professionals but also surpasses the performance of few-shot GPT-4V models, with mIoU higher by 12% on Crello. This progress highlights the potential of multimodal instruction-following models to automate and simplify the design process, providing an approachable solution for a wide range of design tasks on visually-rich documents.","sentences":["Recent advancements in instruction-following models have made user interactions with models more user-friendly and efficient, broadening their applicability.","In graphic design, non-professional users often struggle to create visually appealing layouts due to limited skills and resources.","In this work, we introduce a novel multimodal instruction-following framework for layout planning, allowing users to easily arrange visual elements into tailored layouts by specifying canvas size and design purpose, such as for book covers, posters, brochures, or menus.","We developed three layout reasoning tasks to train the model in understanding and executing layout instructions.","Experiments on two benchmarks show that our method not only simplifies the design process for non-professionals but also surpasses the performance of few-shot GPT-4V models, with mIoU higher by 12% on Crello.","This progress highlights the potential of multimodal instruction-following models to automate and simplify the design process, providing an approachable solution for a wide range of design tasks on visually-rich documents."],"url":"http://arxiv.org/abs/2404.15271v1"}
{"created":"2024-04-23 17:57:47","title":"Aligning LLM Agents by Learning Latent Preference from User Edits","abstract":"We study interactive learning of language agents based on user edits made to the agent's output. In a typical setting such as writing assistants, the user interacts with a language agent to generate a response given a context, and may optionally edit the agent response to personalize it based on their latent preference, in addition to improving the correctness. The edit feedback is naturally generated, making it a suitable candidate for improving the agent's alignment with the user's preference, and for reducing the cost of user edits over time. We propose a learning framework, PRELUDE that infers a description of the user's latent preference based on historic edit data and using it to define a prompt policy that drives future response generation. This avoids fine-tuning the agent, which is costly, challenging to scale with the number of users, and may even degrade its performance on other tasks. Furthermore, learning descriptive preference improves interpretability, allowing the user to view and modify the learned preference. However, user preference can be complex and vary based on context, making it challenging to learn. To address this, we propose a simple yet effective algorithm named CIPHER that leverages a large language model (LLM) to infer the user preference for a given context based on user edits. In the future, CIPHER retrieves inferred preferences from the k-closest contexts in the history, and forms an aggregate preference for response generation. We introduce two interactive environments -- summarization and email writing, for evaluation using a GPT-4 simulated user. We compare with algorithms that directly retrieve user edits but do not learn descriptive preference, and algorithms that learn context-agnostic preference. On both tasks, CIPHER achieves the lowest edit distance cost and learns preferences that show significant similarity to the ground truth preferences","sentences":["We study interactive learning of language agents based on user edits made to the agent's output.","In a typical setting such as writing assistants, the user interacts with a language agent to generate a response given a context, and may optionally edit the agent response to personalize it based on their latent preference, in addition to improving the correctness.","The edit feedback is naturally generated, making it a suitable candidate for improving the agent's alignment with the user's preference, and for reducing the cost of user edits over time.","We propose a learning framework, PRELUDE that infers a description of the user's latent preference based on historic edit data and using it to define a prompt policy that drives future response generation.","This avoids fine-tuning the agent, which is costly, challenging to scale with the number of users, and may even degrade its performance on other tasks.","Furthermore, learning descriptive preference improves interpretability, allowing the user to view and modify the learned preference.","However, user preference can be complex and vary based on context, making it challenging to learn.","To address this, we propose a simple yet effective algorithm named CIPHER that leverages a large language model (LLM) to infer the user preference for a given context based on user edits.","In the future, CIPHER retrieves inferred preferences from the k-closest contexts in the history, and forms an aggregate preference for response generation.","We introduce two interactive environments -- summarization and email writing, for evaluation using a GPT-4 simulated user.","We compare with algorithms that directly retrieve user edits but do not learn descriptive preference, and algorithms that learn context-agnostic preference.","On both tasks, CIPHER achieves the lowest edit distance cost and learns preferences that show significant similarity to the ground truth preferences"],"url":"http://arxiv.org/abs/2404.15269v1"}
{"created":"2024-04-23 17:56:08","title":"From Parts to Whole: A Unified Reference Framework for Controllable Human Image Generation","abstract":"Recent advancements in controllable human image generation have led to zero-shot generation using structural signals (e.g., pose, depth) or facial appearance. Yet, generating human images conditioned on multiple parts of human appearance remains challenging. Addressing this, we introduce Parts2Whole, a novel framework designed for generating customized portraits from multiple reference images, including pose images and various aspects of human appearance. To achieve this, we first develop a semantic-aware appearance encoder to retain details of different human parts, which processes each image based on its textual label to a series of multi-scale feature maps rather than one image token, preserving the image dimension. Second, our framework supports multi-image conditioned generation through a shared self-attention mechanism that operates across reference and target features during the diffusion process. We enhance the vanilla attention mechanism by incorporating mask information from the reference human images, allowing for the precise selection of any part. Extensive experiments demonstrate the superiority of our approach over existing alternatives, offering advanced capabilities for multi-part controllable human image customization. See our project page at https://huanngzh.github.io/Parts2Whole/.","sentences":["Recent advancements in controllable human image generation have led to zero-shot generation using structural signals (e.g., pose, depth) or facial appearance.","Yet, generating human images conditioned on multiple parts of human appearance remains challenging.","Addressing this, we introduce Parts2Whole, a novel framework designed for generating customized portraits from multiple reference images, including pose images and various aspects of human appearance.","To achieve this, we first develop a semantic-aware appearance encoder to retain details of different human parts, which processes each image based on its textual label to a series of multi-scale feature maps rather than one image token, preserving the image dimension.","Second, our framework supports multi-image conditioned generation through a shared self-attention mechanism that operates across reference and target features during the diffusion process.","We enhance the vanilla attention mechanism by incorporating mask information from the reference human images, allowing for the precise selection of any part.","Extensive experiments demonstrate the superiority of our approach over existing alternatives, offering advanced capabilities for multi-part controllable human image customization.","See our project page at https://huanngzh.github.io/Parts2Whole/."],"url":"http://arxiv.org/abs/2404.15267v1"}
{"created":"2024-04-23 17:55:07","title":"TalkingGaussian: Structure-Persistent 3D Talking Head Synthesis via Gaussian Splatting","abstract":"Radiance fields have demonstrated impressive performance in synthesizing lifelike 3D talking heads. However, due to the difficulty in fitting steep appearance changes, the prevailing paradigm that presents facial motions by directly modifying point appearance may lead to distortions in dynamic regions. To tackle this challenge, we introduce TalkingGaussian, a deformation-based radiance fields framework for high-fidelity talking head synthesis. Leveraging the point-based Gaussian Splatting, facial motions can be represented in our method by applying smooth and continuous deformations to persistent Gaussian primitives, without requiring to learn the difficult appearance change like previous methods. Due to this simplification, precise facial motions can be synthesized while keeping a highly intact facial feature. Under such a deformation paradigm, we further identify a face-mouth motion inconsistency that would affect the learning of detailed speaking motions. To address this conflict, we decompose the model into two branches separately for the face and inside mouth areas, therefore simplifying the learning tasks to help reconstruct more accurate motion and structure of the mouth region. Extensive experiments demonstrate that our method renders high-quality lip-synchronized talking head videos, with better facial fidelity and higher efficiency compared with previous methods.","sentences":["Radiance fields have demonstrated impressive performance in synthesizing lifelike 3D talking heads.","However, due to the difficulty in fitting steep appearance changes, the prevailing paradigm that presents facial motions by directly modifying point appearance may lead to distortions in dynamic regions.","To tackle this challenge, we introduce TalkingGaussian, a deformation-based radiance fields framework for high-fidelity talking head synthesis.","Leveraging the point-based Gaussian Splatting, facial motions can be represented in our method by applying smooth and continuous deformations to persistent Gaussian primitives, without requiring to learn the difficult appearance change like previous methods.","Due to this simplification, precise facial motions can be synthesized while keeping a highly intact facial feature.","Under such a deformation paradigm, we further identify a face-mouth motion inconsistency that would affect the learning of detailed speaking motions.","To address this conflict, we decompose the model into two branches separately for the face and inside mouth areas, therefore simplifying the learning tasks to help reconstruct more accurate motion and structure of the mouth region.","Extensive experiments demonstrate that our method renders high-quality lip-synchronized talking head videos, with better facial fidelity and higher efficiency compared with previous methods."],"url":"http://arxiv.org/abs/2404.15264v1"}
{"created":"2024-04-23 17:55:05","title":"Multi-Session SLAM with Differentiable Wide-Baseline Pose Optimization","abstract":"We introduce a new system for Multi-Session SLAM, which tracks camera motion across multiple disjoint videos under a single global reference. Our approach couples the prediction of optical flow with solver layers to estimate camera pose. The backbone is trained end-to-end using a novel differentiable solver for wide-baseline two-view pose. The full system can connect disjoint sequences, perform visual odometry, and global optimization. Compared to existing approaches, our design is accurate and robust to catastrophic failures. Code is available at github.com/princeton-vl/MultiSlam_DiffPose","sentences":["We introduce a new system for Multi-Session SLAM, which tracks camera motion across multiple disjoint videos under a single global reference.","Our approach couples the prediction of optical flow with solver layers to estimate camera pose.","The backbone is trained end-to-end using a novel differentiable solver for wide-baseline two-view pose.","The full system can connect disjoint sequences, perform visual odometry, and global optimization.","Compared to existing approaches, our design is accurate and robust to catastrophic failures.","Code is available at github.com/princeton-vl/MultiSlam_DiffPose"],"url":"http://arxiv.org/abs/2404.15263v1"}
{"created":"2024-04-23 17:46:50","title":"FlowMap: High-Quality Camera Poses, Intrinsics, and Depth via Gradient Descent","abstract":"This paper introduces FlowMap, an end-to-end differentiable method that solves for precise camera poses, camera intrinsics, and per-frame dense depth of a video sequence. Our method performs per-video gradient-descent minimization of a simple least-squares objective that compares the optical flow induced by depth, intrinsics, and poses against correspondences obtained via off-the-shelf optical flow and point tracking. Alongside the use of point tracks to encourage long-term geometric consistency, we introduce differentiable re-parameterizations of depth, intrinsics, and pose that are amenable to first-order optimization. We empirically show that camera parameters and dense depth recovered by our method enable photo-realistic novel view synthesis on 360-degree trajectories using Gaussian Splatting. Our method not only far outperforms prior gradient-descent based bundle adjustment methods, but surprisingly performs on par with COLMAP, the state-of-the-art SfM method, on the downstream task of 360-degree novel view synthesis (even though our method is purely gradient-descent based, fully differentiable, and presents a complete departure from conventional SfM).","sentences":["This paper introduces FlowMap, an end-to-end differentiable method that solves for precise camera poses, camera intrinsics, and per-frame dense depth of a video sequence.","Our method performs per-video gradient-descent minimization of a simple least-squares objective that compares the optical flow induced by depth, intrinsics, and poses against correspondences obtained via off-the-shelf optical flow and point tracking.","Alongside the use of point tracks to encourage long-term geometric consistency, we introduce differentiable re-parameterizations of depth, intrinsics, and pose that are amenable to first-order optimization.","We empirically show that camera parameters and dense depth recovered by our method enable photo-realistic novel view synthesis on 360-degree trajectories using Gaussian Splatting.","Our method not only far outperforms prior gradient-descent based bundle adjustment methods, but surprisingly performs on par with COLMAP, the state-of-the-art SfM method, on the downstream task of 360-degree novel view synthesis (even though our method is purely gradient-descent based, fully differentiable, and presents a complete departure from conventional SfM)."],"url":"http://arxiv.org/abs/2404.15259v1"}
{"created":"2024-04-23 17:42:45","title":"TOP-Nav: Legged Navigation Integrating Terrain, Obstacle and Proprioception Estimation","abstract":"Legged navigation is typically examined within open-world, off-road, and challenging environments. In these scenarios, estimating external disturbances requires a complex synthesis of multi-modal information. This underlines a major limitation in existing works that primarily focus on avoiding obstacles. In this work, we propose TOP-Nav, a novel legged navigation framework that integrates a comprehensive path planner with Terrain awareness, Obstacle avoidance and close-loop Proprioception. TOP-Nav underscores the synergies between vision and proprioception in both path and motion planning. Within the path planner, we present and integrate a terrain estimator that enables the robot to select waypoints on terrains with higher traversability while effectively avoiding obstacles. In the motion planning level, we not only implement a locomotion controller to track the navigation commands, but also construct a proprioception advisor to provide motion evaluations for the path planner. Based on the close-loop motion feedback, we make online corrections for the vision-based terrain and obstacle estimations. Consequently, TOP-Nav achieves open-world navigation that the robot can handle terrains or disturbances beyond the distribution of prior knowledge and overcomes constraints imposed by visual conditions. Building upon extensive experiments conducted in both simulation and real-world environments, TOP-Nav demonstrates superior performance in open-world navigation compared to existing methods.","sentences":["Legged navigation is typically examined within open-world, off-road, and challenging environments.","In these scenarios, estimating external disturbances requires a complex synthesis of multi-modal information.","This underlines a major limitation in existing works that primarily focus on avoiding obstacles.","In this work, we propose TOP-Nav, a novel legged navigation framework that integrates a comprehensive path planner with Terrain awareness, Obstacle avoidance and close-loop Proprioception.","TOP-Nav underscores the synergies between vision and proprioception in both path and motion planning.","Within the path planner, we present and integrate a terrain estimator that enables the robot to select waypoints on terrains with higher traversability while effectively avoiding obstacles.","In the motion planning level, we not only implement a locomotion controller to track the navigation commands, but also construct a proprioception advisor to provide motion evaluations for the path planner.","Based on the close-loop motion feedback, we make online corrections for the vision-based terrain and obstacle estimations.","Consequently, TOP-Nav achieves open-world navigation that the robot can handle terrains or disturbances beyond the distribution of prior knowledge and overcomes constraints imposed by visual conditions.","Building upon extensive experiments conducted in both simulation and real-world environments, TOP-Nav demonstrates superior performance in open-world navigation compared to existing methods."],"url":"http://arxiv.org/abs/2404.15256v1"}
{"created":"2024-04-23 17:42:29","title":"How to use and interpret activation patching","abstract":"Activation patching is a popular mechanistic interpretability technique, but has many subtleties regarding how it is applied and how one may interpret the results. We provide a summary of advice and best practices, based on our experience using this technique in practice. We include an overview of the different ways to apply activation patching and a discussion on how to interpret the results. We focus on what evidence patching experiments provide about circuits, and on the choice of metric and associated pitfalls.","sentences":["Activation patching is a popular mechanistic interpretability technique, but has many subtleties regarding how it is applied and how one may interpret the results.","We provide a summary of advice and best practices, based on our experience using this technique in practice.","We include an overview of the different ways to apply activation patching and a discussion on how to interpret the results.","We focus on what evidence patching experiments provide about circuits, and on the choice of metric and associated pitfalls."],"url":"http://arxiv.org/abs/2404.15255v1"}
{"created":"2024-04-23 17:39:27","title":"UniMERNet: A Universal Network for Real-World Mathematical Expression Recognition","abstract":"This paper presents the UniMER dataset to provide the first study on Mathematical Expression Recognition (MER) towards complex real-world scenarios. The UniMER dataset consists of a large-scale training set UniMER-1M offering an unprecedented scale and diversity with one million training instances and a meticulously designed test set UniMER-Test that reflects a diverse range of formula distributions prevalent in real-world scenarios. Therefore, the UniMER dataset enables the training of a robust and high-accuracy MER model and comprehensive evaluation of model performance. Moreover, we introduce the Universal Mathematical Expression Recognition Network (UniMERNet), an innovative framework designed to enhance MER in practical scenarios. UniMERNet incorporates a Length-Aware Module to process formulas of varied lengths efficiently, thereby enabling the model to handle complex mathematical expressions with greater accuracy. In addition, UniMERNet employs our UniMER-1M data and image augmentation techniques to improve the model's robustness under different noise conditions. Our extensive experiments demonstrate that UniMERNet outperforms existing MER models, setting a new benchmark in various scenarios and ensuring superior recognition quality in real-world applications. The dataset and model are available at https://github.com/opendatalab/UniMERNet.","sentences":["This paper presents the UniMER dataset to provide the first study on Mathematical Expression Recognition (MER) towards complex real-world scenarios.","The UniMER dataset consists of a large-scale training set UniMER-1M offering an unprecedented scale and diversity with one million training instances and a meticulously designed test set UniMER-Test that reflects a diverse range of formula distributions prevalent in real-world scenarios.","Therefore, the UniMER dataset enables the training of a robust and high-accuracy MER model and comprehensive evaluation of model performance.","Moreover, we introduce the Universal Mathematical Expression Recognition Network (UniMERNet), an innovative framework designed to enhance MER in practical scenarios.","UniMERNet incorporates a Length-Aware Module to process formulas of varied lengths efficiently, thereby enabling the model to handle complex mathematical expressions with greater accuracy.","In addition, UniMERNet employs our UniMER-1M data and image augmentation techniques to improve the model's robustness under different noise conditions.","Our extensive experiments demonstrate that UniMERNet outperforms existing MER models, setting a new benchmark in various scenarios and ensuring superior recognition quality in real-world applications.","The dataset and model are available at https://github.com/opendatalab/UniMERNet."],"url":"http://arxiv.org/abs/2404.15254v1"}
{"created":"2024-04-23 17:39:06","title":"Source-free Domain Adaptation for Video Object Detection Under Adverse Image Conditions","abstract":"When deploying pre-trained video object detectors in real-world scenarios, the domain gap between training and testing data caused by adverse image conditions often leads to performance degradation. Addressing this issue becomes particularly challenging when only the pre-trained model and degraded videos are available. Although various source-free domain adaptation (SFDA) methods have been proposed for single-frame object detectors, SFDA for video object detection (VOD) remains unexplored. Moreover, most unsupervised domain adaptation works for object detection rely on two-stage detectors, while SFDA for one-stage detectors, which are more vulnerable to fine-tuning, is not well addressed in the literature. In this paper, we propose Spatial-Temporal Alternate Refinement with Mean Teacher (STAR-MT), a simple yet effective SFDA method for VOD. Specifically, we aim to improve the performance of the one-stage VOD method, YOLOV, under adverse image conditions, including noise, air turbulence, and haze. Extensive experiments on the ImageNetVOD dataset and its degraded versions demonstrate that our method consistently improves video object detection performance in challenging imaging conditions, showcasing its potential for real-world applications.","sentences":["When deploying pre-trained video object detectors in real-world scenarios, the domain gap between training and testing data caused by adverse image conditions often leads to performance degradation.","Addressing this issue becomes particularly challenging when only the pre-trained model and degraded videos are available.","Although various source-free domain adaptation (SFDA) methods have been proposed for single-frame object detectors, SFDA for video object detection (VOD) remains unexplored.","Moreover, most unsupervised domain adaptation works for object detection rely on two-stage detectors, while SFDA for one-stage detectors, which are more vulnerable to fine-tuning, is not well addressed in the literature.","In this paper, we propose Spatial-Temporal Alternate Refinement with Mean Teacher (STAR-MT), a simple yet effective SFDA method for VOD.","Specifically, we aim to improve the performance of the one-stage VOD method, YOLOV, under adverse image conditions, including noise, air turbulence, and haze.","Extensive experiments on the ImageNetVOD dataset and its degraded versions demonstrate that our method consistently improves video object detection performance in challenging imaging conditions, showcasing its potential for real-world applications."],"url":"http://arxiv.org/abs/2404.15252v1"}
{"created":"2024-04-23 17:33:04","title":"A Dependency Pair Framework for Relative Termination of Term Rewriting","abstract":"Dependency pairs are one of the most powerful techniques for proving termination of term rewrite systems (TRSs), and they are used in almost all tools for termination analysis of TRSs. Problem #106 of the RTA List of Open Problems asks for an adaption of dependency pairs for relative termination. Here, infinite rewrite sequences are allowed, but one wants to prove that a certain subset of the rewrite rules cannot be used infinitely often. Dependency pairs were recently adapted to annotated dependency pairs (ADPs) to prove almost-sure termination of probabilistic TRSs. In this paper, we develop a novel adaption of ADPs for relative termination. We implemented our new ADP framework in our tool AProVE and evaluate it in comparison to state-of-the-art tools for relative termination of TRSs.","sentences":["Dependency pairs are one of the most powerful techniques for proving termination of term rewrite systems (TRSs), and they are used in almost all tools for termination analysis of TRSs.","Problem #106 of the RTA List of Open Problems asks for an adaption of dependency pairs for relative termination.","Here, infinite rewrite sequences are allowed, but one wants to prove that a certain subset of the rewrite rules cannot be used infinitely often.","Dependency pairs were recently adapted to annotated dependency pairs (ADPs) to prove almost-sure termination of probabilistic TRSs.","In this paper, we develop a novel adaption of ADPs for relative termination.","We implemented our new ADP framework in our tool AProVE and evaluate it in comparison to state-of-the-art tools for relative termination of TRSs."],"url":"http://arxiv.org/abs/2404.15248v1"}
{"created":"2024-04-23 17:32:24","title":"XFT: Unlocking the Power of Code Instruction Tuning by Simply Merging Upcycled Mixture-of-Experts","abstract":"We introduce XFT, a simple yet powerful training scheme, by simply merging upcycled Mixture-of-Experts (MoE) to unleash the performance limit of instruction-tuned code Large Language Models (LLMs). While vanilla sparse upcycling fails to improve instruction tuning, XFT introduces a shared expert mechanism with a novel routing weight normalization strategy into sparse upcycling, which significantly boosts instruction tuning. After fine-tuning the upcycled MoE model, XFT introduces a learnable model merging mechanism to compile the upcycled MoE model back to a dense model, achieving upcycled MoE-level performance with only dense-model compute. By applying XFT to a 1.3B model, we create a new state-of-the-art tiny code LLM (<3B) with 67.1 and 64.6 pass@1 on HumanEval and HumanEval+ respectively. With the same data and model architecture, XFT improves supervised fine-tuning (SFT) by 13% on HumanEval+, along with consistent improvements from 2% to 13% on MBPP+, MultiPL-E, and DS-1000, demonstrating its generalizability. XFT is fully orthogonal to existing techniques such as Evol-Instruct and OSS-Instruct, opening a new dimension for improving code instruction tuning. Codes are available at https://github.com/ise-uiuc/xft .","sentences":["We introduce XFT, a simple yet powerful training scheme, by simply merging upcycled Mixture-of-Experts (MoE) to unleash the performance limit of instruction-tuned code Large Language Models (LLMs).","While vanilla sparse upcycling fails to improve instruction tuning, XFT introduces a shared expert mechanism with a novel routing weight normalization strategy into sparse upcycling, which significantly boosts instruction tuning.","After fine-tuning the upcycled MoE model, XFT introduces a learnable model merging mechanism to compile the upcycled MoE model back to a dense model, achieving upcycled MoE-level performance with only dense-model compute.","By applying XFT to a 1.3B model, we create a new state-of-the-art tiny code LLM (<3B) with 67.1 and 64.6 pass@1 on HumanEval and HumanEval+ respectively.","With the same data and model architecture, XFT improves supervised fine-tuning (SFT) by 13% on HumanEval+, along with consistent improvements from 2% to 13% on MBPP+, MultiPL-E, and DS-1000, demonstrating its generalizability.","XFT is fully orthogonal to existing techniques such as Evol-Instruct and OSS-Instruct, opening a new dimension for improving code instruction tuning.","Codes are available at https://github.com/ise-uiuc/xft ."],"url":"http://arxiv.org/abs/2404.15247v1"}
{"created":"2024-04-23 17:28:52","title":"Efficient Multi-Processor Scheduling in Increasingly Realistic Models","abstract":"We study the problem of efficiently scheduling a computational DAG on multiple processors. The majority of previous works have developed and compared algorithms for this problem in relatively simple models; in contrast to this, we analyze this problem in a more realistic model that captures many real-world aspects, such as communication costs, synchronization costs, and the hierarchical structure of modern processing architectures. For this we extend the well-established BSP model of parallel computing with non-uniform memory access (NUMA) effects. We then develop a range of new scheduling algorithms to minimize the scheduling cost in this more complex setting: several initialization heuristics, a hill-climbing local search method, and several approaches that formulate (and solve) the scheduling problem as an Integer Linear Program (ILP). We combine these algorithms into a single framework, and conduct experiments on a diverse set of real-world computational DAGs to show that the resulting scheduler significantly outperforms both academic and practical baselines. In particular, even without NUMA effects, our scheduler finds solutions of 24%-44% smaller cost on average than the baselines, and in case of NUMA effects, it achieves up to a factor $2.5\\times$ improvement compared to the baselines. Finally, we also develop a multilevel scheduling algorithm, which provides up to almost a factor $5\\times$ improvement in the special case when the problem is dominated by very high communication costs.","sentences":["We study the problem of efficiently scheduling a computational DAG on multiple processors.","The majority of previous works have developed and compared algorithms for this problem in relatively simple models; in contrast to this, we analyze this problem in a more realistic model that captures many real-world aspects, such as communication costs, synchronization costs, and the hierarchical structure of modern processing architectures.","For this we extend the well-established BSP model of parallel computing with non-uniform memory access (NUMA) effects.","We then develop a range of new scheduling algorithms to minimize the scheduling cost in this more complex setting: several initialization heuristics, a hill-climbing local search method, and several approaches that formulate (and solve) the scheduling problem as an Integer Linear Program (ILP).","We combine these algorithms into a single framework, and conduct experiments on a diverse set of real-world computational DAGs to show that the resulting scheduler significantly outperforms both academic and practical baselines.","In particular, even without NUMA effects, our scheduler finds solutions of 24%-44% smaller cost on average than the baselines, and in case of NUMA effects, it achieves up to a factor $2.5\\times$ improvement compared to the baselines.","Finally, we also develop a multilevel scheduling algorithm, which provides up to almost a factor $5\\times$ improvement in the special case when the problem is dominated by very high communication costs."],"url":"http://arxiv.org/abs/2404.15246v1"}
{"created":"2024-04-23 17:26:34","title":"Efficient Transformer Encoders for Mask2Former-style models","abstract":"Vision transformer based models bring significant improvements for image segmentation tasks. Although these architectures offer powerful capabilities irrespective of specific segmentation tasks, their use of computational resources can be taxing on deployed devices. One way to overcome this challenge is by adapting the computation level to the specific needs of the input image rather than the current one-size-fits-all approach. To this end, we introduce ECO-M2F or EffiCient TransfOrmer Encoders for Mask2Former-style models. Noting that the encoder module of M2F-style models incur high resource-intensive computations, ECO-M2F provides a strategy to self-select the number of hidden layers in the encoder, conditioned on the input image. To enable this self-selection ability for providing a balance between performance and computational efficiency, we present a three step recipe. The first step is to train the parent architecture to enable early exiting from the encoder. The second step is to create an derived dataset of the ideal number of encoder layers required for each training example. The third step is to use the aforementioned derived dataset to train a gating network that predicts the number of encoder layers to be used, conditioned on the input image. Additionally, to change the computational-accuracy tradeoff, only steps two and three need to be repeated which significantly reduces retraining time. Experiments on the public datasets show that the proposed approach reduces expected encoder computational cost while maintaining performance, adapts to various user compute resources, is flexible in architecture configurations, and can be extended beyond the segmentation task to object detection.","sentences":["Vision transformer based models bring significant improvements for image segmentation tasks.","Although these architectures offer powerful capabilities irrespective of specific segmentation tasks, their use of computational resources can be taxing on deployed devices.","One way to overcome this challenge is by adapting the computation level to the specific needs of the input image rather than the current one-size-fits-all approach.","To this end, we introduce ECO-M2F or EffiCient TransfOrmer Encoders for Mask2Former-style models.","Noting that the encoder module of M2F-style models incur high resource-intensive computations, ECO-M2F provides a strategy to self-select the number of hidden layers in the encoder, conditioned on the input image.","To enable this self-selection ability for providing a balance between performance and computational efficiency, we present a three step recipe.","The first step is to train the parent architecture to enable early exiting from the encoder.","The second step is to create an derived dataset of the ideal number of encoder layers required for each training example.","The third step is to use the aforementioned derived dataset to train a gating network that predicts the number of encoder layers to be used, conditioned on the input image.","Additionally, to change the computational-accuracy tradeoff, only steps two and three need to be repeated which significantly reduces retraining time.","Experiments on the public datasets show that the proposed approach reduces expected encoder computational cost while maintaining performance, adapts to various user compute resources, is flexible in architecture configurations, and can be extended beyond the segmentation task to object detection."],"url":"http://arxiv.org/abs/2404.15244v1"}
{"created":"2024-04-23 17:25:35","title":"A Hybrid Kernel-Free Boundary Integral Method with Operator Learning for Solving Parametric Partial Differential Equations In Complex Domains","abstract":"The Kernel-Free Boundary Integral (KFBI) method presents an iterative solution to boundary integral equations arising from elliptic partial differential equations (PDEs). This method effectively addresses elliptic PDEs on irregular domains, including the modified Helmholtz, Stokes, and elasticity equations. The rapid evolution of neural networks and deep learning has invigorated the exploration of numerical PDEs. An increasing interest is observed in deep learning approaches that seamlessly integrate mathematical principles for investigating numerical PDEs. We propose a hybrid KFBI method, integrating the foundational principles of the KFBI method with the capabilities of deep learning. This approach, within the framework of the boundary integral method, designs a network to approximate the solution operator for the corresponding integral equations by mapping the parameters, inhomogeneous terms and boundary information of PDEs to the boundary density functions, which can be regarded as the solution of the integral equations. The models are trained using data generated by the Cartesian grid-based KFBI algorithm, exhibiting robust generalization capabilities. It accurately predicts density functions across diverse boundary conditions and parameters within the same class of equations. Experimental results demonstrate that the trained model can directly infer the boundary density function with satisfactory precision, obviating the need for iterative steps in solving boundary integral equations. Furthermore, applying the inference results of the model as initial values for iterations is also reasonable; this approach can retain the inherent second-order accuracy of the KFBI method while accelerating the traditional KFBI approach by reducing about 50% iterations.","sentences":["The Kernel-Free Boundary Integral (KFBI) method presents an iterative solution to boundary integral equations arising from elliptic partial differential equations (PDEs).","This method effectively addresses elliptic PDEs on irregular domains, including the modified Helmholtz, Stokes, and elasticity equations.","The rapid evolution of neural networks and deep learning has invigorated the exploration of numerical PDEs.","An increasing interest is observed in deep learning approaches that seamlessly integrate mathematical principles for investigating numerical PDEs.","We propose a hybrid KFBI method, integrating the foundational principles of the KFBI method with the capabilities of deep learning.","This approach, within the framework of the boundary integral method, designs a network to approximate the solution operator for the corresponding integral equations by mapping the parameters, inhomogeneous terms and boundary information of PDEs to the boundary density functions, which can be regarded as the solution of the integral equations.","The models are trained using data generated by the Cartesian grid-based KFBI algorithm, exhibiting robust generalization capabilities.","It accurately predicts density functions across diverse boundary conditions and parameters within the same class of equations.","Experimental results demonstrate that the trained model can directly infer the boundary density function with satisfactory precision, obviating the need for iterative steps in solving boundary integral equations.","Furthermore, applying the inference results of the model as initial values for iterations is also reasonable; this approach can retain the inherent second-order accuracy of the KFBI method while accelerating the traditional KFBI approach by reducing about 50% iterations."],"url":"http://arxiv.org/abs/2404.15242v1"}
{"created":"2024-04-23 17:17:52","title":"Augmented Voices: An Augmented Reality Experience Highlighting the Social Injustices of Gender-Based Violence in the Muslim South-Asian Diaspora","abstract":"This paper delves into the distressing prevalence of gender-based violence (GBV) and its deep-seated psychological ramifications, particularly among Muslim South Asian women living in diasporic communities. Despite the gravity of GBV, these women often face formidable barriers in voicing their experiences and accessing support. \"Augmented Voices\" emerges as a technological beacon, harnessing the potential of augmented reality (AR) to bridge the digital and physical realms through mobile devices, enhancing the visibility of these often-silenced voices. With its technological motivation firmly anchored in the convergence of AR and real-world interactions, \"Augmented Voices\" offers a digital platform where storytelling acts as a catalyst, bringing to the fore the experiences shared by these women. By superimposing their narratives onto physical locations via Geographic Information System (GIS) Mapping, the application \"augments their voices\" in the diaspora, providing a conduit for expression and solidarity. This project, currently at its developmental stage, aspires to elevate the stories of GBV victims to a level where their struggles are not just heard but felt, forging a powerful connection between the user and the narrative. It is designed to transcend the limitations of conventional storytelling, creating an \"augmented\" reality where voices that are often muted by societal constraints can resonate powerfully. The project underscores the urgent imperative to confront GBV, catalyzing societal transformation and fostering robust support networks for those in the margins. It is a pioneering example of how technology can become a formidable ally in the fight for social justice and the empowerment of the oppressed. Additionally, this paper delves into the AR workflow illustrating its relevance and contribution to the broader theme of site-specific AR for social justice.","sentences":["This paper delves into the distressing prevalence of gender-based violence (GBV) and its deep-seated psychological ramifications, particularly among Muslim South Asian women living in diasporic communities.","Despite the gravity of GBV, these women often face formidable barriers in voicing their experiences and accessing support.","\"Augmented Voices\" emerges as a technological beacon, harnessing the potential of augmented reality (AR) to bridge the digital and physical realms through mobile devices, enhancing the visibility of these often-silenced voices.","With its technological motivation firmly anchored in the convergence of AR and real-world interactions, \"Augmented Voices\" offers a digital platform where storytelling acts as a catalyst, bringing to the fore the experiences shared by these women.","By superimposing their narratives onto physical locations via Geographic Information System (GIS) Mapping, the application \"augments their voices\" in the diaspora, providing a conduit for expression and solidarity.","This project, currently at its developmental stage, aspires to elevate the stories of GBV victims to a level where their struggles are not just heard but felt, forging a powerful connection between the user and the narrative.","It is designed to transcend the limitations of conventional storytelling, creating an \"augmented\" reality where voices that are often muted by societal constraints can resonate powerfully.","The project underscores the urgent imperative to confront GBV, catalyzing societal transformation and fostering robust support networks for those in the margins.","It is a pioneering example of how technology can become a formidable ally in the fight for social justice and the empowerment of the oppressed.","Additionally, this paper delves into the AR workflow illustrating its relevance and contribution to the broader theme of site-specific AR for social justice."],"url":"http://arxiv.org/abs/2404.15239v1"}
{"created":"2024-04-23 17:16:08","title":"CultureBank: An Online Community-Driven Knowledge Base Towards Culturally Aware Language Technologies","abstract":"To enhance language models' cultural awareness, we design a generalizable pipeline to construct cultural knowledge bases from different online communities on a massive scale. With the pipeline, we construct CultureBank, a knowledge base built upon users' self-narratives with 12K cultural descriptors sourced from TikTok and 11K from Reddit. Unlike previous cultural knowledge resources, CultureBank contains diverse views on cultural descriptors to allow flexible interpretation of cultural knowledge, and contextualized cultural scenarios to help grounded evaluation. With CultureBank, we evaluate different LLMs' cultural awareness, and identify areas for improvement. We also fine-tune a language model on CultureBank: experiments show that it achieves better performances on two downstream cultural tasks in a zero-shot setting. Finally, we offer recommendations based on our findings for future culturally aware language technologies. The project page is https://culturebank.github.io . The code and model is at https://github.com/SALT-NLP/CultureBank . The released CultureBank dataset is at https://huggingface.co/datasets/SALT-NLP/CultureBank .","sentences":["To enhance language models' cultural awareness, we design a generalizable pipeline to construct cultural knowledge bases from different online communities on a massive scale.","With the pipeline, we construct CultureBank, a knowledge base built upon users' self-narratives with 12K cultural descriptors sourced from TikTok and 11K from Reddit.","Unlike previous cultural knowledge resources, CultureBank contains diverse views on cultural descriptors to allow flexible interpretation of cultural knowledge, and contextualized cultural scenarios to help grounded evaluation.","With CultureBank, we evaluate different LLMs' cultural awareness, and identify areas for improvement.","We also fine-tune a language model on CultureBank: experiments show that it achieves better performances on two downstream cultural tasks in a zero-shot setting.","Finally, we offer recommendations based on our findings for future culturally aware language technologies.","The project page is https://culturebank.github.io .","The code and model is at https://github.com/SALT-NLP/CultureBank .","The released CultureBank dataset is at https://huggingface.co/datasets/SALT-NLP/CultureBank ."],"url":"http://arxiv.org/abs/2404.15238v1"}
{"created":"2024-04-23 17:12:45","title":"Revisiting Unnaturalness for Automated Program Repair in the Era of Large Language Models","abstract":"Language models have improved by orders of magnitude with the recent emergence of Transformer-based Large Language Models (LLMs). LLMs have demonstrated their ability to generate natural code that is highly similar to code written by professional developers. One intermediate value an LLM can emit is entropy, which measures the naturalness of a token of code. We hypothesize that entropy can be used to improve the performance of Automated Program Repair (APR) tasks. While much progress has been made in Automated Program Repair (APR), fault localization techniques suffer from a lack of diversity in ranking scores, patch generation tools tend to be inefficient as all tests need to run before determining if a patch is likely to be correct, and patch ranking often suffers from the test-suite over-fitting problem. However, using an LLM directly for APR introduces concerns for training data leakage. In this work, we introduce a novel way of using the entropy of LLMs in combination with prior APR tools to improve all stages of APR. We show that entropy is highly complementary with prior fault localization tools. Our proposed re-ranking method achieves a 50% Top-5 score improvement over SBFL. We propose a patch-naturalness measurement, entropy-delta, to improve the efficiency of template-based repair techniques by ranking plausible patches before undergoing testing. When using entropy-delta for patch ranking and classification, our proposed method can rank correct patches more effectively than state-of-the-art machine learning tools with an 49% improvement in Top-1. Our work suggests that LLMs can be an effective addition to compliment prior APR tasks while minimizing both the test-suite overfitting problem and the LLM data leakage problem.","sentences":["Language models have improved by orders of magnitude with the recent emergence of Transformer-based Large Language Models (LLMs).","LLMs have demonstrated their ability to generate natural code that is highly similar to code written by professional developers.","One intermediate value an LLM can emit is entropy, which measures the naturalness of a token of code.","We hypothesize that entropy can be used to improve the performance of Automated Program Repair (APR) tasks.","While much progress has been made in Automated Program Repair (APR), fault localization techniques suffer from a lack of diversity in ranking scores, patch generation tools tend to be inefficient as all tests need to run before determining if a patch is likely to be correct, and patch ranking often suffers from the test-suite over-fitting problem.","However, using an LLM directly for APR introduces concerns for training data leakage.","In this work, we introduce a novel way of using the entropy of LLMs in combination with prior APR tools to improve all stages of APR.","We show that entropy is highly complementary with prior fault localization tools.","Our proposed re-ranking method achieves a 50% Top-5 score improvement over SBFL.","We propose a patch-naturalness measurement, entropy-delta, to improve the efficiency of template-based repair techniques by ranking plausible patches before undergoing testing.","When using entropy-delta for patch ranking and classification, our proposed method can rank correct patches more effectively than state-of-the-art machine learning tools with an 49% improvement in Top-1.","Our work suggests that LLMs can be an effective addition to compliment prior APR tasks while minimizing both the test-suite overfitting problem and the LLM data leakage problem."],"url":"http://arxiv.org/abs/2404.15236v1"}
{"created":"2024-04-23 17:10:49","title":"Massively Annotated Datasets for Assessment of Synthetic and Real Data in Face Recognition","abstract":"Face recognition applications have grown in parallel with the size of datasets, complexity of deep learning models and computational power. However, while deep learning models evolve to become more capable and computational power keeps increasing, the datasets available are being retracted and removed from public access. Privacy and ethical concerns are relevant topics within these domains. Through generative artificial intelligence, researchers have put efforts into the development of completely synthetic datasets that can be used to train face recognition systems. Nonetheless, the recent advances have not been sufficient to achieve performance comparable to the state-of-the-art models trained on real data. To study the drift between the performance of models trained on real and synthetic datasets, we leverage a massive attribute classifier (MAC) to create annotations for four datasets: two real and two synthetic. From these annotations, we conduct studies on the distribution of each attribute within all four datasets. Additionally, we further inspect the differences between real and synthetic datasets on the attribute set. When comparing through the Kullback-Leibler divergence we have found differences between real and synthetic samples. Interestingly enough, we have verified that while real samples suffice to explain the synthetic distribution, the opposite could not be further from being true.","sentences":["Face recognition applications have grown in parallel with the size of datasets, complexity of deep learning models and computational power.","However, while deep learning models evolve to become more capable and computational power keeps increasing, the datasets available are being retracted and removed from public access.","Privacy and ethical concerns are relevant topics within these domains.","Through generative artificial intelligence, researchers have put efforts into the development of completely synthetic datasets that can be used to train face recognition systems.","Nonetheless, the recent advances have not been sufficient to achieve performance comparable to the state-of-the-art models trained on real data.","To study the drift between the performance of models trained on real and synthetic datasets, we leverage a massive attribute classifier (MAC) to create annotations for four datasets: two real and two synthetic.","From these annotations, we conduct studies on the distribution of each attribute within all four datasets.","Additionally, we further inspect the differences between real and synthetic datasets on the attribute set.","When comparing through the Kullback-Leibler divergence we have found differences between real and synthetic samples.","Interestingly enough, we have verified that while real samples suffice to explain the synthetic distribution, the opposite could not be further from being true."],"url":"http://arxiv.org/abs/2404.15234v1"}
{"created":"2024-04-23 17:00:23","title":"Multi-Tier Non-Terrestrial Networking for Disaster Communications: A Layered Clustering Approach","abstract":"It is crucial to deploy temporary non-terrestrial networks (NTN) in disaster situations where terrestrial networks are no longer operable. Deploying uncrewed aerial vehicle base stations (UAV-BSs) can provide a radio access network (RAN); however, the backhaul link may also be damaged and unserviceable in such disaster conditions. In this regard, high-altitude platform stations (HAPS) spark attention as they can be deployed as super macro base stations (SMBS) and data centers. Therefore, in this study, we investigate a three-layer heterogeneous network with different topologies to prolong the lifespan of the temporary network by using UAV-BSs for RAN services and HAPS-SMBS as a backhaul. Furthermore, a two-layer clustering algorithm is proposed to handle the UAV-BS ad-hoc networking effectively.","sentences":["It is crucial to deploy temporary non-terrestrial networks (NTN) in disaster situations where terrestrial networks are no longer operable.","Deploying uncrewed aerial vehicle base stations (UAV-BSs) can provide a radio access network (RAN); however, the backhaul link may also be damaged and unserviceable in such disaster conditions.","In this regard, high-altitude platform stations (HAPS) spark attention as they can be deployed as super macro base stations (SMBS) and data centers.","Therefore, in this study, we investigate a three-layer heterogeneous network with different topologies to prolong the lifespan of the temporary network by using UAV-BSs for RAN services and HAPS-SMBS as a backhaul.","Furthermore, a two-layer clustering algorithm is proposed to handle the UAV-BS ad-hoc networking effectively."],"url":"http://arxiv.org/abs/2404.15229v1"}
{"created":"2024-04-23 16:59:02","title":"Re-Thinking Inverse Graphics With Large Language Models","abstract":"Inverse graphics -- the task of inverting an image into physical variables that, when rendered, enable reproduction of the observed scene -- is a fundamental challenge in computer vision and graphics. Disentangling an image into its constituent elements, such as the shape, color, and material properties of the objects of the 3D scene that produced it, requires a comprehensive understanding of the environment. This requirement limits the ability of existing carefully engineered approaches to generalize across domains. Inspired by the zero-shot ability of large language models (LLMs) to generalize to novel contexts, we investigate the possibility of leveraging the broad world knowledge encoded in such models in solving inverse-graphics problems. To this end, we propose the Inverse-Graphics Large Language Model (IG-LLM), an inverse-graphics framework centered around an LLM, that autoregressively decodes a visual embedding into a structured, compositional 3D-scene representation. We incorporate a frozen pre-trained visual encoder and a continuous numeric head to enable end-to-end training. Through our investigation, we demonstrate the potential of LLMs to facilitate inverse graphics through next-token prediction, without the use of image-space supervision. Our analysis opens up new possibilities for precise spatial reasoning about images that exploit the visual knowledge of LLMs. We will release our code and data to ensure the reproducibility of our investigation and to facilitate future research at https://ig-llm.is.tue.mpg.de/","sentences":["Inverse graphics -- the task of inverting an image into physical variables that, when rendered, enable reproduction of the observed scene -- is a fundamental challenge in computer vision and graphics.","Disentangling an image into its constituent elements, such as the shape, color, and material properties of the objects of the 3D scene that produced it, requires a comprehensive understanding of the environment.","This requirement limits the ability of existing carefully engineered approaches to generalize across domains.","Inspired by the zero-shot ability of large language models (LLMs) to generalize to novel contexts, we investigate the possibility of leveraging the broad world knowledge encoded in such models in solving inverse-graphics problems.","To this end, we propose the Inverse-Graphics Large Language Model (IG-LLM), an inverse-graphics framework centered around an LLM, that autoregressively decodes a visual embedding into a structured, compositional 3D-scene representation.","We incorporate a frozen pre-trained visual encoder and a continuous numeric head to enable end-to-end training.","Through our investigation, we demonstrate the potential of LLMs to facilitate inverse graphics through next-token prediction, without the use of image-space supervision.","Our analysis opens up new possibilities for precise spatial reasoning about images that exploit the visual knowledge of LLMs.","We will release our code and data to ensure the reproducibility of our investigation and to facilitate future research at https://ig-llm.is.tue.mpg.de/"],"url":"http://arxiv.org/abs/2404.15228v1"}
{"created":"2024-04-23 16:54:56","title":"PHLP: Sole Persistent Homology for Link Prediction -- Interpretable Feature Extraction","abstract":"Link prediction (LP), inferring the connectivity between nodes, is a significant research area in graph data, where a link represents essential information on relationships between nodes. Although graph neural network (GNN)-based models have achieved high performance in LP, understanding why they perform well is challenging because most comprise complex neural networks. We employ persistent homology (PH), a topological data analysis method that helps analyze the topological information of graphs, to explain the reasons for the high performance. We propose a novel method that employs PH for LP (PHLP) focusing on how the presence or absence of target links influences the overall topology. The PHLP utilizes the angle hop subgraph and new node labeling called degree double radius node labeling (Degree DRNL), distinguishing the information of graphs better than DRNL. Using only a classifier, PHLP performs similarly to state-of-the-art (SOTA) models on most benchmark datasets. Incorporating the outputs calculated using PHLP into the existing GNN-based SOTA models improves performance across all benchmark datasets. To the best of our knowledge, PHLP is the first method of applying PH to LP without GNNs. The proposed approach, employing PH while not relying on neural networks, enables the identification of crucial factors for improving performance.","sentences":["Link prediction (LP), inferring the connectivity between nodes, is a significant research area in graph data, where a link represents essential information on relationships between nodes.","Although graph neural network (GNN)-based models have achieved high performance in LP, understanding why they perform well is challenging because most comprise complex neural networks.","We employ persistent homology (PH), a topological data analysis method that helps analyze the topological information of graphs, to explain the reasons for the high performance.","We propose a novel method that employs PH for LP (PHLP) focusing on how the presence or absence of target links influences the overall topology.","The PHLP utilizes the angle hop subgraph and new node labeling called degree double radius node labeling (Degree DRNL), distinguishing the information of graphs better than DRNL.","Using only a classifier, PHLP performs similarly to state-of-the-art (SOTA) models on most benchmark datasets.","Incorporating the outputs calculated using PHLP into the existing GNN-based SOTA models improves performance across all benchmark datasets.","To the best of our knowledge, PHLP is the first method of applying PH to LP without GNNs.","The proposed approach, employing PH while not relying on neural networks, enables the identification of crucial factors for improving performance."],"url":"http://arxiv.org/abs/2404.15225v1"}
{"created":"2024-04-23 16:54:31","title":"Deep Models for Multi-View 3D Object Recognition: A Review","abstract":"Human decision-making often relies on visual information from multiple perspectives or views. In contrast, machine learning-based object recognition utilizes information from a single image of the object. However, the information conveyed by a single image may not be sufficient for accurate decision-making, particularly in complex recognition problems. The utilization of multi-view 3D representations for object recognition has thus far demonstrated the most promising results for achieving state-of-the-art performance. This review paper comprehensively covers recent progress in multi-view 3D object recognition methods for 3D classification and retrieval tasks. Specifically, we focus on deep learning-based and transformer-based techniques, as they are widely utilized and have achieved state-of-the-art performance. We provide detailed information about existing deep learning-based and transformer-based multi-view 3D object recognition models, including the most commonly used 3D datasets, camera configurations and number of views, view selection strategies, pre-trained CNN architectures, fusion strategies, and recognition performance on 3D classification and 3D retrieval tasks. Additionally, we examine various computer vision applications that use multi-view classification. Finally, we highlight key findings and future directions for developing multi-view 3D object recognition methods to provide readers with a comprehensive understanding of the field.","sentences":["Human decision-making often relies on visual information from multiple perspectives or views.","In contrast, machine learning-based object recognition utilizes information from a single image of the object.","However, the information conveyed by a single image may not be sufficient for accurate decision-making, particularly in complex recognition problems.","The utilization of multi-view 3D representations for object recognition has thus far demonstrated the most promising results for achieving state-of-the-art performance.","This review paper comprehensively covers recent progress in multi-view 3D object recognition methods for 3D classification and retrieval tasks.","Specifically, we focus on deep learning-based and transformer-based techniques, as they are widely utilized and have achieved state-of-the-art performance.","We provide detailed information about existing deep learning-based and transformer-based multi-view 3D object recognition models, including the most commonly used 3D datasets, camera configurations and number of views, view selection strategies, pre-trained CNN architectures, fusion strategies, and recognition performance on 3D classification and 3D retrieval tasks.","Additionally, we examine various computer vision applications that use multi-view classification.","Finally, we highlight key findings and future directions for developing multi-view 3D object recognition methods to provide readers with a comprehensive understanding of the field."],"url":"http://arxiv.org/abs/2404.15224v1"}
{"created":"2024-04-23 16:52:43","title":"The Architecture of Truth","abstract":"The theory of institutions is framed as an indexed/fibered duality, where the indexed aspect specifies the fibered aspect. Tarski represented truth in terms of a satisfaction relation. The theory of institutions encodes satisfaction as its core architecture in the indexed aspect. Logical environments enrich this truth architecture by axiomatizing the truth adjunction in the fibered aspect. The truth architecture is preserved by morphisms of logical environments. (Although not every institution is a logical environment, each institution has an associated logical environment defined via the intent of the structures of the institution, and each institution is represented by an indexed functor into the structure category of the classification logical environment $\\mathtt{Cls}$.)","sentences":["The theory of institutions is framed as an indexed/fibered duality, where the indexed aspect specifies the fibered aspect.","Tarski represented truth in terms of a satisfaction relation.","The theory of institutions encodes satisfaction as its core architecture in the indexed aspect.","Logical environments enrich this truth architecture by axiomatizing the truth adjunction in the fibered aspect.","The truth architecture is preserved by morphisms of logical environments.","(Although not every institution is a logical environment, each institution has an associated logical environment defined via the intent of the structures of the institution, and each institution is represented by an indexed functor into the structure category of the classification logical environment $\\mathtt{Cls}$.)"],"url":"http://arxiv.org/abs/2404.15221v1"}
{"created":"2024-04-23 16:51:26","title":"The Power of the Noisy Channel: Unsupervised End-to-End Task-Oriented Dialogue with LLMs","abstract":"Training task-oriented dialogue systems typically requires turn-level annotations for interacting with their APIs: e.g. a dialogue state and the system actions taken at each step. These annotations can be costly to produce, error-prone, and require both domain and annotation expertise. With advances in LLMs, we hypothesize unlabelled data and a schema definition are sufficient for building a working task-oriented dialogue system, completely unsupervised. Using only (1) a well-defined API schema (2) a set of unlabelled dialogues between a user and agent, we develop a novel approach for inferring turn-level annotations as latent variables using a noisy channel model. We iteratively improve these pseudo-labels with expectation-maximization (EM), and use the inferred labels to train an end-to-end dialogue agent. Evaluating our approach on the MultiWOZ benchmark, our method more than doubles the dialogue success rate of a strong GPT-3.5 baseline.","sentences":["Training task-oriented dialogue systems typically requires turn-level annotations for interacting with their APIs: e.g. a dialogue state and the system actions taken at each step.","These annotations can be costly to produce, error-prone, and require both domain and annotation expertise.","With advances in LLMs, we hypothesize unlabelled data and a schema definition are sufficient for building a working task-oriented dialogue system, completely unsupervised.","Using only (1) a well-defined API schema (2) a set of unlabelled dialogues between a user and agent, we develop a novel approach for inferring turn-level annotations as latent variables using a noisy channel model.","We iteratively improve these pseudo-labels with expectation-maximization (EM), and use the inferred labels to train an end-to-end dialogue agent.","Evaluating our approach on the MultiWOZ benchmark, our method more than doubles the dialogue success rate of a strong GPT-3.5 baseline."],"url":"http://arxiv.org/abs/2404.15219v1"}
{"created":"2024-04-23 16:46:27","title":"Bottoms Up for CHCs: Novel Transformation of Linear Constrained Horn Clauses to Software Verification","abstract":"Constrained Horn Clauses (CHCs) have conventionally been used as a low-level representation in formal verification. Most existing solvers use a diverse set of specialized techniques, including direct state space traversal or under-approximating abstraction, necessitating purpose-built complex algorithms. Other solvers successfully simplified the verification workflow by translating the problem to inputs for other verification tasks, leveraging the strengths of existing algorithms. One such approach transforms the CHC problem into a recursive program roughly emulating a top-down solver for the deduction task; and verifying the reachability of a safety violation specified as a control location. We propose an alternative bottom-up approach for linear CHCs, and evaluate the two options in the open-source model checking framework THETA on both synthetic and industrial examples. We find that there is a more than twofold increase in the number of solved tasks when the novel bottom-up approach is used in the verification workflow, in contrast with the top-down technique.","sentences":["Constrained Horn Clauses (CHCs) have conventionally been used as a low-level representation in formal verification.","Most existing solvers use a diverse set of specialized techniques, including direct state space traversal or under-approximating abstraction, necessitating purpose-built complex algorithms.","Other solvers successfully simplified the verification workflow by translating the problem to inputs for other verification tasks, leveraging the strengths of existing algorithms.","One such approach transforms the CHC problem into a recursive program roughly emulating a top-down solver for the deduction task; and verifying the reachability of a safety violation specified as a control location.","We propose an alternative bottom-up approach for linear CHCs, and evaluate the two options in the open-source model checking framework THETA on both synthetic and industrial examples.","We find that there is a more than twofold increase in the number of solved tasks when the novel bottom-up approach is used in the verification workflow, in contrast with the top-down technique."],"url":"http://arxiv.org/abs/2404.15215v1"}
{"created":"2024-04-23 16:46:10","title":"Embedding Differential Dynamic Logic in PVS","abstract":"Differential dynamic logic (dL) is a formal framework for specifying and reasoning about hybrid systems, i.e., dynamical systems that exhibit both continuous and discrete behaviors. These kinds of systems arise in many safety- and mission-critical applications. This paper presents a formalization of dL in the Prototype Verification System (PVS) that includes the semantics of hybrid programs and dL's proof calculus. The formalization embeds dL into the PVS logic, resulting in a version of dL whose proof calculus is not only formally verified, but is also available for the verification of hybrid programs within PVS itself. This embedding, called Plaidypvs (Properly Assured Implementation of dL for Hybrid Program Verification and Specification), supports standard dL style proofs, but further leverages the capabilities of PVS to allow reasoning about entire classes of hybrid programs. The embedding also allows the user to import the well-established definitions and mathematical theories available in PVS.","sentences":["Differential dynamic logic (dL) is a formal framework for specifying and reasoning about hybrid systems, i.e., dynamical systems that exhibit both continuous and discrete behaviors.","These kinds of systems arise in many safety- and mission-critical applications.","This paper presents a formalization of dL in the Prototype Verification System (PVS) that includes the semantics of hybrid programs and dL's proof calculus.","The formalization embeds dL into the PVS logic, resulting in a version of dL whose proof calculus is not only formally verified, but is also available for the verification of hybrid programs within PVS itself.","This embedding, called Plaidypvs (Properly Assured Implementation of dL for Hybrid Program Verification and Specification), supports standard dL style proofs, but further leverages the capabilities of PVS to allow reasoning about entire classes of hybrid programs.","The embedding also allows the user to import the well-established definitions and mathematical theories available in PVS."],"url":"http://arxiv.org/abs/2404.15214v1"}
{"created":"2024-04-23 16:39:03","title":"Does Instruction Tuning Make LLMs More Consistent?","abstract":"The purpose of instruction tuning is enabling zero-shot performance, but instruction tuning has also been shown to improve chain-of-thought reasoning and value alignment (Si et al., 2023). Here we consider the impact on $\\textit{consistency}$, i.e., the sensitivity of language models to small perturbations in the input. We compare 10 instruction-tuned LLaMA models to the original LLaMA-7b model and show that almost across-the-board they become more consistent, both in terms of their representations and their predictions in zero-shot and downstream tasks. We explain these improvements through mechanistic analyses of factual recall.","sentences":["The purpose of instruction tuning is enabling zero-shot performance, but instruction tuning has also been shown to improve chain-of-thought reasoning and value alignment (Si et al., 2023).","Here we consider the impact on $\\textit{consistency}$, i.e., the sensitivity of language models to small perturbations in the input.","We compare 10 instruction-tuned LLaMA models to the original LLaMA-7b model and show that almost across-the-board they become more consistent, both in terms of their representations and their predictions in zero-shot and downstream tasks.","We explain these improvements through mechanistic analyses of factual recall."],"url":"http://arxiv.org/abs/2404.15206v1"}
{"created":"2024-04-23 16:35:59","title":"CORE-BEHRT: A Carefully Optimized and Rigorously Evaluated BEHRT","abstract":"BERT-based models for Electronic Health Records (EHR) have surged in popularity following the release of BEHRT and Med-BERT. Subsequent models have largely built on these foundations despite the fundamental design choices of these pioneering models remaining underexplored. To address this issue, we introduce CORE-BEHRT, a Carefully Optimized and Rigorously Evaluated BEHRT. Through incremental optimization, we isolate the sources of improvement for key design choices, giving us insights into the effect of data representation and individual technical components on performance. Evaluating this across a set of generic tasks (death, pain treatment, and general infection), we showed that improving data representation can increase the average downstream performance from 0.785 to 0.797 AUROC, primarily when including medication and timestamps. Improving the architecture and training protocol on top of this increased average downstream performance to 0.801 AUROC. We then demonstrated the consistency of our optimization through a rigorous evaluation across 25 diverse clinical prediction tasks. We observed significant performance increases in 17 out of 25 tasks and improvements in 24 tasks, highlighting the generalizability of our findings. Our findings provide a strong foundation for future work and aim to increase the trustworthiness of BERT-based EHR models.","sentences":["BERT-based models for Electronic Health Records (EHR) have surged in popularity following the release of BEHRT and Med-BERT.","Subsequent models have largely built on these foundations despite the fundamental design choices of these pioneering models remaining underexplored.","To address this issue, we introduce CORE-BEHRT, a Carefully Optimized and Rigorously Evaluated BEHRT.","Through incremental optimization, we isolate the sources of improvement for key design choices, giving us insights into the effect of data representation and individual technical components on performance.","Evaluating this across a set of generic tasks (death, pain treatment, and general infection), we showed that improving data representation can increase the average downstream performance from 0.785 to 0.797 AUROC, primarily when including medication and timestamps.","Improving the architecture and training protocol on top of this increased average downstream performance to 0.801 AUROC.","We then demonstrated the consistency of our optimization through a rigorous evaluation across 25 diverse clinical prediction tasks.","We observed significant performance increases in 17 out of 25 tasks and improvements in 24 tasks, highlighting the generalizability of our findings.","Our findings provide a strong foundation for future work and aim to increase the trustworthiness of BERT-based EHR models."],"url":"http://arxiv.org/abs/2404.15201v1"}
{"created":"2024-04-23 16:35:14","title":"Reinforcement Learning with Adaptive Control Regularization for Safe Control of Critical Systems","abstract":"Reinforcement Learning (RL) is a powerful method for controlling dynamic systems, but its learning mechanism can lead to unpredictable actions that undermine the safety of critical systems. Here, we propose RL with Adaptive Control Regularization (RL-ACR) that ensures RL safety by combining the RL policy with a control regularizer that hard-codes safety constraints over forecasted system behaviors. The adaptability is achieved by using a learnable \"focus\" weight trained to maximize the cumulative reward of the policy combination. As the RL policy improves through off-policy learning, the focus weight improves the initial sub-optimum strategy by gradually relying more on the RL policy. We demonstrate the effectiveness of RL-ACR in a critical medical control application and further investigate its performance in four classic control environments.","sentences":["Reinforcement Learning (RL) is a powerful method for controlling dynamic systems, but its learning mechanism can lead to unpredictable actions that undermine the safety of critical systems.","Here, we propose RL with Adaptive Control Regularization (RL-ACR) that ensures RL safety by combining the RL policy with a control regularizer that hard-codes safety constraints over forecasted system behaviors.","The adaptability is achieved by using a learnable \"focus\" weight trained to maximize the cumulative reward of the policy combination.","As the RL policy improves through off-policy learning, the focus weight improves the initial sub-optimum strategy by gradually relying more on the RL policy.","We demonstrate the effectiveness of RL-ACR in a critical medical control application and further investigate its performance in four classic control environments."],"url":"http://arxiv.org/abs/2404.15199v1"}
{"created":"2024-04-23 16:34:34","title":"Setting up the Data Printer with Improved English to Ukrainian Machine Translation","abstract":"To build large language models for Ukrainian we need to expand our corpora with large amounts of new algorithmic tasks expressed in natural language. Examples of task performance expressed in English are abundant, so with a high-quality translation system our community will be enabled to curate datasets faster. To aid this goal, we introduce a recipe to build a translation system using supervised finetuning of a large pretrained language model with a noisy parallel dataset of 3M pairs of Ukrainian and English sentences followed by a second phase of training using 17K examples selected by k-fold perplexity filtering on another dataset of higher quality. Our decoder-only model named Dragoman beats performance of previous state of the art encoder-decoder models on the FLORES devtest set.","sentences":["To build large language models for Ukrainian we need to expand our corpora with large amounts of new algorithmic tasks expressed in natural language.","Examples of task performance expressed in English are abundant, so with a high-quality translation system our community will be enabled to curate datasets faster.","To aid this goal, we introduce a recipe to build a translation system using supervised finetuning of a large pretrained language model with a noisy parallel dataset of 3M pairs of Ukrainian and English sentences followed by a second phase of training using 17K examples selected by k-fold perplexity filtering on another dataset of higher quality.","Our decoder-only model named Dragoman beats performance of previous state of the art encoder-decoder models on the FLORES devtest set."],"url":"http://arxiv.org/abs/2404.15196v1"}
{"created":"2024-04-23 16:33:28","title":"Closed Loop Interactive Embodied Reasoning for Robot Manipulation","abstract":"Embodied reasoning systems integrate robotic hardware and cognitive processes to perform complex tasks typically in response to a natural language query about a specific physical environment. This usually involves changing the belief about the scene or physically interacting and changing the scene (e.g. 'Sort the objects from lightest to heaviest'). In order to facilitate the development of such systems we introduce a new simulating environment that makes use of MuJoCo physics engine and high-quality renderer Blender to provide realistic visual observations that are also accurate to the physical state of the scene. Together with the simulator we propose a new benchmark composed of 10 classes of multi-step reasoning scenarios that require simultaneous visual and physical measurements. Finally, we develop a new modular Closed Loop Interactive Reasoning (CLIER) approach that takes into account the measurements of non-visual object properties, changes in the scene caused by external disturbances as well as uncertain outcomes of robotic actions. We extensively evaluate our reasoning approach in simulation and in the real world manipulation tasks with a success rate above 76% and 64%, respectively.","sentences":["Embodied reasoning systems integrate robotic hardware and cognitive processes to perform complex tasks typically in response to a natural language query about a specific physical environment.","This usually involves changing the belief about the scene or physically interacting and changing the scene (e.g. 'Sort the objects from lightest to heaviest').","In order to facilitate the development of such systems we introduce a new simulating environment that makes use of MuJoCo physics engine and high-quality renderer Blender to provide realistic visual observations that are also accurate to the physical state of the scene.","Together with the simulator we propose a new benchmark composed of 10 classes of multi-step reasoning scenarios that require simultaneous visual and physical measurements.","Finally, we develop a new modular Closed Loop Interactive Reasoning (CLIER) approach that takes into account the measurements of non-visual object properties, changes in the scene caused by external disturbances as well as uncertain outcomes of robotic actions.","We extensively evaluate our reasoning approach in simulation and in the real world manipulation tasks with a success rate above 76% and 64%, respectively."],"url":"http://arxiv.org/abs/2404.15194v1"}
{"created":"2024-04-23 16:28:03","title":"Evaluating Physician-AI Interaction for Cancer Management: Paving the Path towards Precision Oncology","abstract":"We evaluated how clinicians approach clinical decision-making when given findings from both randomized controlled trials (RCTs) and machine learning (ML) models. To do so, we designed a clinical decision support system (CDSS) that displays survival curves and adverse event information from a synthetic RCT and ML model for 12 patients with multiple myeloma. We conducted an interventional study in a simulated setting to evaluate how clinicians synthesized the available data to make treatment decisions. Participants were invited to participate in a follow-up interview to discuss their choices in an open-ended format. When ML model results were concordant with RCT results, physicians had increased confidence in treatment choice compared to when they were given RCT results alone. When ML model results were discordant with RCT results, the majority of physicians followed the ML model recommendation in their treatment selection. Perceived reliability of the ML model was consistently higher after physicians were provided with data on how it was trained and validated. Follow-up interviews revealed four major themes: (1) variability in what variables participants used for decision-making, (2) perceived advantages to an ML model over RCT data, (3) uncertainty around decision-making when the ML model quality was poor, and (4) perception that this type of study is an important thought exercise for clinicians. Overall, ML-based CDSSs have the potential to change treatment decisions in cancer management. However, meticulous development and validation of these systems as well as clinician training are required before deployment.","sentences":["We evaluated how clinicians approach clinical decision-making when given findings from both randomized controlled trials (RCTs) and machine learning (ML) models.","To do so, we designed a clinical decision support system (CDSS) that displays survival curves and adverse event information from a synthetic RCT and ML model for 12 patients with multiple myeloma.","We conducted an interventional study in a simulated setting to evaluate how clinicians synthesized the available data to make treatment decisions.","Participants were invited to participate in a follow-up interview to discuss their choices in an open-ended format.","When ML model results were concordant with RCT results, physicians had increased confidence in treatment choice compared to when they were given RCT results alone.","When ML model results were discordant with RCT results, the majority of physicians followed the ML model recommendation in their treatment selection.","Perceived reliability of the ML model was consistently higher after physicians were provided with data on how it was trained and validated.","Follow-up interviews revealed four major themes: (1) variability in what variables participants used for decision-making, (2) perceived advantages to an ML model over RCT data, (3) uncertainty around decision-making when the ML model quality was poor, and (4) perception that this type of study is an important thought exercise for clinicians.","Overall, ML-based CDSSs have the potential to change treatment decisions in cancer management.","However, meticulous development and validation of these systems as well as clinician training are required before deployment."],"url":"http://arxiv.org/abs/2404.15187v1"}
{"created":"2024-04-23 16:16:37","title":"An Analysis of the Math Requirements of 199 CS BS/BA Degrees at 158 U.S. Universities","abstract":"For at least 40 years, there has been debate and disagreement as to the role of mathematics in the computer science curriculum. This paper presents the results of an analysis of the math requirements of 199 Computer Science BS/BA degrees from 158 U.S. universities, looking not only at which math courses are required, but how they are used as prerequisites (and corequisites) for computer science (CS) courses. Our analysis shows that while there is consensus that discrete math is critical for a CS degree, and further that calculus is almost always required for the BS in CS, there is little consensus as to when a student should have mastered these subjects. Based on our analysis of how math requirements impact access, retention and on-time degree completion for the BS and the BA in CS, we provide several recommendations for CS departments to consider.","sentences":["For at least 40 years, there has been debate and disagreement as to the role of mathematics in the computer science curriculum.","This paper presents the results of an analysis of the math requirements of 199 Computer Science BS/BA degrees from 158 U.S. universities, looking not only at which math courses are required, but how they are used as prerequisites (and corequisites) for computer science (CS) courses.","Our analysis shows that while there is consensus that discrete math is critical for a CS degree, and further that calculus is almost always required for the BS in CS, there is little consensus as to when a student should have mastered these subjects.","Based on our analysis of how math requirements impact access, retention and on-time degree completion for the BS and the BA in CS, we provide several recommendations for CS departments to consider."],"url":"http://arxiv.org/abs/2404.15177v1"}
{"created":"2024-04-23 16:14:20","title":"Fourier-enhanced Implicit Neural Fusion Network for Multispectral and Hyperspectral Image Fusion","abstract":"Recently, implicit neural representations (INR) have made significant strides in various vision-related domains, providing a novel solution for Multispectral and Hyperspectral Image Fusion (MHIF) tasks. However, INR is prone to losing high-frequency information and is confined to the lack of global perceptual capabilities. To address these issues, this paper introduces a Fourier-enhanced Implicit Neural Fusion Network (FeINFN) specifically designed for MHIF task, targeting the following phenomena: The Fourier amplitudes of the HR-HSI latent code and LR-HSI are remarkably similar; however, their phases exhibit different patterns. In FeINFN, we innovatively propose a spatial and frequency implicit fusion function (Spa-Fre IFF), helping INR capture high-frequency information and expanding the receptive field. Besides, a new decoder employing a complex Gabor wavelet activation function, called Spatial-Frequency Interactive Decoder (SFID), is invented to enhance the interaction of INR features. Especially, we further theoretically prove that the Gabor wavelet activation possesses a time-frequency tightness property that favors learning the optimal bandwidths in the decoder. Experiments on two benchmark MHIF datasets verify the state-of-the-art (SOTA) performance of the proposed method, both visually and quantitatively. Also, ablation studies demonstrate the mentioned contributions. The code will be available on Anonymous GitHub (https://anonymous.4open.science/r/FeINFN-15C9/) after possible acceptance.","sentences":["Recently, implicit neural representations (INR) have made significant strides in various vision-related domains, providing a novel solution for Multispectral and Hyperspectral Image Fusion (MHIF) tasks.","However, INR is prone to losing high-frequency information and is confined to the lack of global perceptual capabilities.","To address these issues, this paper introduces a Fourier-enhanced Implicit Neural Fusion Network (FeINFN) specifically designed for MHIF task, targeting the following phenomena: The Fourier amplitudes of the HR-HSI latent code and LR-HSI are remarkably similar; however, their phases exhibit different patterns.","In FeINFN, we innovatively propose a spatial and frequency implicit fusion function (Spa-Fre IFF), helping INR capture high-frequency information and expanding the receptive field.","Besides, a new decoder employing a complex Gabor wavelet activation function, called Spatial-Frequency Interactive Decoder (SFID), is invented to enhance the interaction of INR features.","Especially, we further theoretically prove that the Gabor wavelet activation possesses a time-frequency tightness property that favors learning the optimal bandwidths in the decoder.","Experiments on two benchmark MHIF datasets verify the state-of-the-art (SOTA) performance of the proposed method, both visually and quantitatively.","Also, ablation studies demonstrate the mentioned contributions.","The code will be available on Anonymous GitHub (https://anonymous.4open.science/r/FeINFN-15C9/) after possible acceptance."],"url":"http://arxiv.org/abs/2404.15174v1"}
{"created":"2024-04-23 16:02:33","title":"Adaptive Mixed-Scale Feature Fusion Network for Blind AI-Generated Image Quality Assessment","abstract":"With the increasing maturity of the text-to-image and image-to-image generative models, AI-generated images (AGIs) have shown great application potential in advertisement, entertainment, education, social media, etc. Although remarkable advancements have been achieved in generative models, very few efforts have been paid to design relevant quality assessment models. In this paper, we propose a novel blind image quality assessment (IQA) network, named AMFF-Net, for AGIs. AMFF-Net evaluates AGI quality from three dimensions, i.e., \"visual quality\", \"authenticity\", and \"consistency\". Specifically, inspired by the characteristics of the human visual system and motivated by the observation that \"visual quality\" and \"authenticity\" are characterized by both local and global aspects, AMFF-Net scales the image up and down and takes the scaled images and original-sized image as the inputs to obtain multi-scale features. After that, an Adaptive Feature Fusion (AFF) block is used to adaptively fuse the multi-scale features with learnable weights. In addition, considering the correlation between the image and prompt, AMFF-Net compares the semantic features from text encoder and image encoder to evaluate the text-to-image alignment. We carry out extensive experiments on three AGI quality assessment databases, and the experimental results show that our AMFF-Net obtains better performance than nine state-of-the-art blind IQA methods. The results of ablation experiments further demonstrate the effectiveness of the proposed multi-scale input strategy and AFF block.","sentences":["With the increasing maturity of the text-to-image and image-to-image generative models, AI-generated images (AGIs) have shown great application potential in advertisement, entertainment, education, social media, etc.","Although remarkable advancements have been achieved in generative models, very few efforts have been paid to design relevant quality assessment models.","In this paper, we propose a novel blind image quality assessment (IQA) network, named AMFF-Net, for AGIs.","AMFF-Net evaluates AGI quality from three dimensions, i.e., \"visual quality\", \"authenticity\", and \"consistency\".","Specifically, inspired by the characteristics of the human visual system and motivated by the observation that \"visual quality\" and \"authenticity\" are characterized by both local and global aspects, AMFF-Net scales the image up and down and takes the scaled images and original-sized image as the inputs to obtain multi-scale features.","After that, an Adaptive Feature Fusion (AFF) block is used to adaptively fuse the multi-scale features with learnable weights.","In addition, considering the correlation between the image and prompt, AMFF-Net compares the semantic features from text encoder and image encoder to evaluate the text-to-image alignment.","We carry out extensive experiments on three AGI quality assessment databases, and the experimental results show that our AMFF-Net obtains better performance than nine state-of-the-art blind IQA methods.","The results of ablation experiments further demonstrate the effectiveness of the proposed multi-scale input strategy and AFF block."],"url":"http://arxiv.org/abs/2404.15163v1"}
{"created":"2024-04-23 16:01:33","title":"Combating Missing Modalities in Egocentric Videos at Test Time","abstract":"Understanding videos that contain multiple modalities is crucial, especially in egocentric videos, where combining various sensory inputs significantly improves tasks like action recognition and moment localization. However, real-world applications often face challenges with incomplete modalities due to privacy concerns, efficiency needs, or hardware issues. Current methods, while effective, often necessitate retraining the model entirely to handle missing modalities, making them computationally intensive, particularly with large training datasets. In this study, we propose a novel approach to address this issue at test time without requiring retraining. We frame the problem as a test-time adaptation task, where the model adjusts to the available unlabeled data at test time. Our method, MiDl~(Mutual information with self-Distillation), encourages the model to be insensitive to the specific modality source present during testing by minimizing the mutual information between the prediction and the available modality. Additionally, we incorporate self-distillation to maintain the model's original performance when both modalities are available. MiDl represents the first self-supervised, online solution for handling missing modalities exclusively at test time. Through experiments with various pretrained models and datasets, MiDl demonstrates substantial performance improvement without the need for retraining.","sentences":["Understanding videos that contain multiple modalities is crucial, especially in egocentric videos, where combining various sensory inputs significantly improves tasks like action recognition and moment localization.","However, real-world applications often face challenges with incomplete modalities due to privacy concerns, efficiency needs, or hardware issues.","Current methods, while effective, often necessitate retraining the model entirely to handle missing modalities, making them computationally intensive, particularly with large training datasets.","In this study, we propose a novel approach to address this issue at test time without requiring retraining.","We frame the problem as a test-time adaptation task, where the model adjusts to the available unlabeled data at test time.","Our method, MiDl~(Mutual information with self-Distillation), encourages the model to be insensitive to the specific modality source present during testing by minimizing the mutual information between the prediction and the available modality.","Additionally, we incorporate self-distillation to maintain the model's original performance when both modalities are available.","MiDl represents the first self-supervised, online solution for handling missing modalities exclusively at test time.","Through experiments with various pretrained models and datasets, MiDl demonstrates substantial performance improvement without the need for retraining."],"url":"http://arxiv.org/abs/2404.15161v1"}
{"created":"2024-04-23 15:57:55","title":"Regressive Side Effects of Training Language Models to Mimic Student Misconceptions","abstract":"This paper presents a novel exploration into the regressive side effects of training Large Language Models (LLMs) to mimic student misconceptions for personalized education. We highlight the problem that as LLMs are trained to more accurately mimic student misconceptions, there is a compromise in the factual integrity and reasoning ability of the models. Our work involved training an LLM on a student-tutor dialogue dataset to predict student responses. The results demonstrated a decrease in the model's performance across multiple benchmark datasets, including the ARC reasoning challenge and TruthfulQA, which evaluates the truthfulness of model's generated responses. Furthermore, the HaluEval Dial dataset, used for hallucination detection, and MemoTrap, a memory-based task dataset, also reported a decline in the model accuracy. To combat these side effects, we introduced a \"hallucination token\" technique. This token, appended at the beginning of each student response during training, instructs the model to switch between mimicking student misconceptions and providing factually accurate responses. Despite the significant improvement across all datasets, the technique does not completely restore the LLM's baseline performance, indicating the need for further research in this area. This paper contributes to the ongoing discussion on the use of LLMs for student modeling, emphasizing the need for a balance between personalized education and factual accuracy.","sentences":["This paper presents a novel exploration into the regressive side effects of training Large Language Models (LLMs) to mimic student misconceptions for personalized education.","We highlight the problem that as LLMs are trained to more accurately mimic student misconceptions, there is a compromise in the factual integrity and reasoning ability of the models.","Our work involved training an LLM on a student-tutor dialogue dataset to predict student responses.","The results demonstrated a decrease in the model's performance across multiple benchmark datasets, including the ARC reasoning challenge and TruthfulQA, which evaluates the truthfulness of model's generated responses.","Furthermore, the HaluEval Dial dataset, used for hallucination detection, and MemoTrap, a memory-based task dataset, also reported a decline in the model accuracy.","To combat these side effects, we introduced a \"hallucination token\" technique.","This token, appended at the beginning of each student response during training, instructs the model to switch between mimicking student misconceptions and providing factually accurate responses.","Despite the significant improvement across all datasets, the technique does not completely restore the LLM's baseline performance, indicating the need for further research in this area.","This paper contributes to the ongoing discussion on the use of LLMs for student modeling, emphasizing the need for a balance between personalized education and factual accuracy."],"url":"http://arxiv.org/abs/2404.15156v1"}
{"created":"2024-04-23 15:52:53","title":"Lost in Magnitudes: Exploring the Design Space for Visualizing Data with Large Value Ranges","abstract":"We explore the design space for the static visualization of datasets with quantitative attributes that vary over multiple orders of magnitude-we call these attributes Orders of Magnitude Values (OMVs)-and provide design guidelines and recommendations on effective visual encodings for OMVs. Current charts rely on linear or logarithmic scales to visualize values, leading to limitations in performing simple tasks for OMVs. In particular, linear scales prevent the reading of smaller magnitudes and their comparisons, while logarithmic scales are challenging for the general public to understand. Our design space leverages the approach of dividing OMVs into two different parts: mantissa and exponent, in a way similar to scientific notation. This separation allows for a visual encoding of both parts. For our exploration, we use four datasets, each with two attributes: an OMV, divided into mantissa and exponent, and a second attribute that is nominal, ordinal, time, or quantitative. We start from the original design space described by the Grammar of Graphics and systematically generate all possible visualizations for these datasets, employing different marks and visual channels. We refine this design space by enforcing integrity constraints from visualization and graphical perception literature. Through a qualitative assessment of all viable combinations, we discuss the most effective visualizations for OMVs, focusing on channel and task effectiveness. The article's main contributions are 1) the presentation of the design space of OMVs, 2) the generation of a large number of OMV visualizations, among which some are novel and effective, 3) the refined definition of a scale that we call E+M for OMVs, and 4) guidelines and recommendations for designing effective OMV visualizations. These efforts aim to enrich visualization systems to better support data with OMVs and guide future research.","sentences":["We explore the design space for the static visualization of datasets with quantitative attributes that vary over multiple orders of magnitude-we call these attributes Orders of Magnitude Values (OMVs)-and provide design guidelines and recommendations on effective visual encodings for OMVs.","Current charts rely on linear or logarithmic scales to visualize values, leading to limitations in performing simple tasks for OMVs.","In particular, linear scales prevent the reading of smaller magnitudes and their comparisons, while logarithmic scales are challenging for the general public to understand.","Our design space leverages the approach of dividing OMVs into two different parts: mantissa and exponent, in a way similar to scientific notation.","This separation allows for a visual encoding of both parts.","For our exploration, we use four datasets, each with two attributes: an OMV, divided into mantissa and exponent, and a second attribute that is nominal, ordinal, time, or quantitative.","We start from the original design space described by the Grammar of Graphics and systematically generate all possible visualizations for these datasets, employing different marks and visual channels.","We refine this design space by enforcing integrity constraints from visualization and graphical perception literature.","Through a qualitative assessment of all viable combinations, we discuss the most effective visualizations for OMVs, focusing on channel and task effectiveness.","The article's main contributions are 1) the presentation of the design space of OMVs, 2) the generation of a large number of OMV visualizations, among which some are novel and effective, 3) the refined definition of a scale that we call E+M for OMVs, and 4) guidelines and recommendations for designing effective OMV visualizations.","These efforts aim to enrich visualization systems to better support data with OMVs and guide future research."],"url":"http://arxiv.org/abs/2404.15150v1"}
{"created":"2024-04-23 15:52:52","title":"Bias patterns in the application of LLMs for clinical decision support: A comprehensive study","abstract":"Large Language Models (LLMs) have emerged as powerful candidates to inform clinical decision-making processes. While these models play an increasingly prominent role in shaping the digital landscape, two growing concerns emerge in healthcare applications: 1) to what extent do LLMs exhibit social bias based on patients' protected attributes (like race), and 2) how do design choices (like architecture design and prompting strategies) influence the observed biases? To answer these questions rigorously, we evaluated eight popular LLMs across three question-answering (QA) datasets using clinical vignettes (patient descriptions) standardized for bias evaluations. We employ red-teaming strategies to analyze how demographics affect LLM outputs, comparing both general-purpose and clinically-trained models. Our extensive experiments reveal various disparities (some significant) across protected groups. We also observe several counter-intuitive patterns such as larger models not being necessarily less biased and fined-tuned models on medical data not being necessarily better than the general-purpose models. Furthermore, our study demonstrates the impact of prompt design on bias patterns and shows that specific phrasing can influence bias patterns and reflection-type approaches (like Chain of Thought) can reduce biased outcomes effectively. Consistent with prior studies, we call on additional evaluations, scrutiny, and enhancement of LLMs used in clinical decision support applications.","sentences":["Large Language Models (LLMs) have emerged as powerful candidates to inform clinical decision-making processes.","While these models play an increasingly prominent role in shaping the digital landscape, two growing concerns emerge in healthcare applications: 1) to what extent do LLMs exhibit social bias based on patients' protected attributes (like race), and 2) how do design choices (like architecture design and prompting strategies) influence the observed biases?","To answer these questions rigorously, we evaluated eight popular LLMs across three question-answering (QA) datasets using clinical vignettes (patient descriptions) standardized for bias evaluations.","We employ red-teaming strategies to analyze how demographics affect LLM outputs, comparing both general-purpose and clinically-trained models.","Our extensive experiments reveal various disparities (some significant) across protected groups.","We also observe several counter-intuitive patterns such as larger models not being necessarily less biased and fined-tuned models on medical data not being necessarily better than the general-purpose models.","Furthermore, our study demonstrates the impact of prompt design on bias patterns and shows that specific phrasing can influence bias patterns and reflection-type approaches (like Chain of Thought) can reduce biased outcomes effectively.","Consistent with prior studies, we call on additional evaluations, scrutiny, and enhancement of LLMs used in clinical decision support applications."],"url":"http://arxiv.org/abs/2404.15149v1"}
{"created":"2024-04-23 15:49:37","title":"Rethinking LLM Memorization through the Lens of Adversarial Compression","abstract":"Large language models (LLMs) trained on web-scale datasets raise substantial concerns regarding permissible data usage. One major question is whether these models \"memorize\" all their training data or they integrate many data sources in some way more akin to how a human would learn and synthesize information. The answer hinges, to a large degree, on $\\textit{how we define memorization}$. In this work, we propose the Adversarial Compression Ratio (ACR) as a metric for assessing memorization in LLMs -- a given string from the training data is considered memorized if it can be elicited by a prompt shorter than the string itself. In other words, these strings can be \"compressed\" with the model by computing adversarial prompts of fewer tokens. We outline the limitations of existing notions of memorization and show how the ACR overcomes these challenges by (i) offering an adversarial view to measuring memorization, especially for monitoring unlearning and compliance; and (ii) allowing for the flexibility to measure memorization for arbitrary strings at a reasonably low compute. Our definition serves as a valuable and practical tool for determining when model owners may be violating terms around data usage, providing a potential legal tool and a critical lens through which to address such scenarios. Project page: https://locuslab.github.io/acr-memorization.","sentences":["Large language models (LLMs) trained on web-scale datasets raise substantial concerns regarding permissible data usage.","One major question is whether these models \"memorize\" all their training data or they integrate many data sources in some way more akin to how a human would learn and synthesize information.","The answer hinges, to a large degree, on $\\textit{how we define memorization}$. In this work, we propose the Adversarial Compression Ratio (ACR) as a metric for assessing memorization in LLMs -- a given string from the training data is considered memorized if it can be elicited by a prompt shorter than the string itself.","In other words, these strings can be \"compressed\" with the model by computing adversarial prompts of fewer tokens.","We outline the limitations of existing notions of memorization and show how the ACR overcomes these challenges by (i) offering an adversarial view to measuring memorization, especially for monitoring unlearning and compliance; and (ii) allowing for the flexibility to measure memorization for arbitrary strings at a reasonably low compute.","Our definition serves as a valuable and practical tool for determining when model owners may be violating terms around data usage, providing a potential legal tool and a critical lens through which to address such scenarios.","Project page: https://locuslab.github.io/acr-memorization."],"url":"http://arxiv.org/abs/2404.15146v1"}
{"created":"2024-04-23 15:48:51","title":"Every Breath You Don't Take: Deepfake Speech Detection Using Breath","abstract":"Deepfake speech represents a real and growing threat to systems and society. Many detectors have been created to aid in defense against speech deepfakes. While these detectors implement myriad methodologies, many rely on low-level fragments of the speech generation process. We hypothesize that breath, a higher-level part of speech, is a key component of natural speech and thus improper generation in deepfake speech is a performant discriminator. To evaluate this, we create a breath detector and leverage this against a custom dataset of online news article audio to discriminate between real/deepfake speech. Additionally, we make this custom dataset publicly available to facilitate comparison for future work. Applying our simple breath detector as a deepfake speech discriminator on in-the-wild samples allows for accurate classification (perfect 1.0 AUPRC and 0.0 EER on test data) across 33.6 hours of audio. We compare our model with the state-of-the-art SSL-wav2vec model and show that this complex deep learning model completely fails to classify the same in-the-wild samples (0.72 AUPRC and 0.99 EER).","sentences":["Deepfake speech represents a real and growing threat to systems and society.","Many detectors have been created to aid in defense against speech deepfakes.","While these detectors implement myriad methodologies, many rely on low-level fragments of the speech generation process.","We hypothesize that breath, a higher-level part of speech, is a key component of natural speech and thus improper generation in deepfake speech is a performant discriminator.","To evaluate this, we create a breath detector and leverage this against a custom dataset of online news article audio to discriminate between real/deepfake speech.","Additionally, we make this custom dataset publicly available to facilitate comparison for future work.","Applying our simple breath detector as a deepfake speech discriminator on in-the-wild samples allows for accurate classification (perfect 1.0 AUPRC and 0.0 EER on test data) across 33.6 hours of audio.","We compare our model with the state-of-the-art SSL-wav2vec model and show that this complex deep learning model completely fails to classify the same in-the-wild samples (0.72 AUPRC and 0.99 EER)."],"url":"http://arxiv.org/abs/2404.15143v1"}
{"created":"2024-04-23 15:47:58","title":"CutDiffusion: A Simple, Fast, Cheap, and Strong Diffusion Extrapolation Method","abstract":"Transforming large pre-trained low-resolution diffusion models to cater to higher-resolution demands, i.e., diffusion extrapolation, significantly improves diffusion adaptability. We propose tuning-free CutDiffusion, aimed at simplifying and accelerating the diffusion extrapolation process, making it more affordable and improving performance. CutDiffusion abides by the existing patch-wise extrapolation but cuts a standard patch diffusion process into an initial phase focused on comprehensive structure denoising and a subsequent phase dedicated to specific detail refinement. Comprehensive experiments highlight the numerous almighty advantages of CutDiffusion: (1) simple method construction that enables a concise higher-resolution diffusion process without third-party engagement; (2) fast inference speed achieved through a single-step higher-resolution diffusion process, and fewer inference patches required; (3) cheap GPU cost resulting from patch-wise inference and fewer patches during the comprehensive structure denoising; (4) strong generation performance, stemming from the emphasis on specific detail refinement.","sentences":["Transforming large pre-trained low-resolution diffusion models to cater to higher-resolution demands, i.e., diffusion extrapolation, significantly improves diffusion adaptability.","We propose tuning-free CutDiffusion, aimed at simplifying and accelerating the diffusion extrapolation process, making it more affordable and improving performance.","CutDiffusion abides by the existing patch-wise extrapolation but cuts a standard patch diffusion process into an initial phase focused on comprehensive structure denoising and a subsequent phase dedicated to specific detail refinement.","Comprehensive experiments highlight the numerous almighty advantages of CutDiffusion: (1) simple method construction that enables a concise higher-resolution diffusion process without third-party engagement; (2) fast inference speed achieved through a single-step higher-resolution diffusion process, and fewer inference patches required; (3) cheap GPU cost resulting from patch-wise inference and fewer patches during the comprehensive structure denoising; (4) strong generation performance, stemming from the emphasis on specific detail refinement."],"url":"http://arxiv.org/abs/2404.15141v1"}
{"created":"2024-04-23 15:42:31","title":"From Space-Time to Space-Order: Directly Planning a Temporal Planning Graph by Redefining CBS","abstract":"The majority of multi-agent path finding (MAPF) methods compute collision-free space-time paths which require agents to be at a specific location at a specific discretized timestep. However, executing these space-time paths directly on robotic systems is infeasible due to real-time execution differences (e.g. delays) which can lead to collisions. To combat this, current methods translate the space-time paths into a temporal plan graph (TPG) that only requires that agents observe the order in which they navigate through locations where their paths cross. However, planning space-time paths and then post-processing them into a TPG does not reduce the required agent-to-agent coordination, which is fixed once the space-time paths are computed. To that end, we propose a novel algorithm Space-Order CBS that can directly plan a TPG and explicitly minimize coordination. Our main theoretical insight is our novel perspective on viewing a TPG as a set of space-visitation order paths where agents visit locations in relative orders (e.g. 1st vs 2nd) as opposed to specific timesteps. We redefine unique conflicts and constraints for adapting CBS for space-order planning. We experimentally validate how Space-Order CBS can return TPGs which significantly reduce coordination, thus subsequently reducing the amount of agent-agent communication and leading to more robustness to delays during execution.","sentences":["The majority of multi-agent path finding (MAPF) methods compute collision-free space-time paths which require agents to be at a specific location at a specific discretized timestep.","However, executing these space-time paths directly on robotic systems is infeasible due to real-time execution differences (e.g. delays) which can lead to collisions.","To combat this, current methods translate the space-time paths into a temporal plan graph (TPG) that only requires that agents observe the order in which they navigate through locations where their paths cross.","However, planning space-time paths and then post-processing them into a TPG does not reduce the required agent-to-agent coordination, which is fixed once the space-time paths are computed.","To that end, we propose a novel algorithm Space-Order CBS that can directly plan a TPG and explicitly minimize coordination.","Our main theoretical insight is our novel perspective on viewing a TPG as a set of space-visitation order paths where agents visit locations in relative orders (e.g. 1st vs 2nd) as opposed to specific timesteps.","We redefine unique conflicts and constraints for adapting CBS for space-order planning.","We experimentally validate how Space-Order CBS can return TPGs which significantly reduce coordination, thus subsequently reducing the amount of agent-agent communication and leading to more robustness to delays during execution."],"url":"http://arxiv.org/abs/2404.15137v1"}
{"created":"2024-04-23 15:36:29","title":"Linear-Function Correcting Codes","abstract":"In this paper, we study linear-function correcting codes, a class of codes designed to protect linear function evaluations of a message against errors. The work \"Function-Correcting Codes\" by Lenz et al. 2023 provides a graphical representation for the problem of constructing function-correcting codes. We use this graph to get a lower bound the on redundancy required for function correction. By considering the function to be a bijection, such an approach also provides a lower bound on the redundancy required for classical systematic error correcting codes. For linear-function correction, we characterise the spectrum of the adjacency matrix of this graph, which gives rise to lower bounds on redundancy. The work \"Function-Correcting Codes\" gives an equivalence between function-correcting codes and irregular-distance codes. We identify a structure imposed by linearity on the distance requirement of the equivalent irregular-distance code which provides a simplified Plotkin-like bound. We propose a version of the sphere packing bound for linear-function correcting codes. We identify a class of linear functions for which an upper bound proposed by Lenz et al., is tight. We also identify a class of functions for which coset-wise coding is equivalent to a lower dimensional classical error correction problem.","sentences":["In this paper, we study linear-function correcting codes, a class of codes designed to protect linear function evaluations of a message against errors.","The work \"Function-Correcting Codes\" by Lenz et al. 2023 provides a graphical representation for the problem of constructing function-correcting codes.","We use this graph to get a lower bound the on redundancy required for function correction.","By considering the function to be a bijection, such an approach also provides a lower bound on the redundancy required for classical systematic error correcting codes.","For linear-function correction, we characterise the spectrum of the adjacency matrix of this graph, which gives rise to lower bounds on redundancy.","The work \"Function-Correcting Codes\" gives an equivalence between function-correcting codes and irregular-distance codes.","We identify a structure imposed by linearity on the distance requirement of the equivalent irregular-distance code which provides a simplified Plotkin-like bound.","We propose a version of the sphere packing bound for linear-function correcting codes.","We identify a class of linear functions for which an upper bound proposed by Lenz et al., is tight.","We also identify a class of functions for which coset-wise coding is equivalent to a lower dimensional classical error correction problem."],"url":"http://arxiv.org/abs/2404.15135v1"}
{"created":"2024-04-23 15:32:03","title":"Black Hole Search by a Set of Scattered Agents in Dynamic Rings","abstract":"In this paper we investigate the problem of searching for a black hole in a dynamic graph by a set of scattered agents (i.e., the agents start from arbitrary locations of the graph). The black hole is a node that silently destroys any agent visiting it. This kind of malicious node nicely models network failures such as a crashed host or a virus that erases the visiting agents. The black hole search problem is solved when at least one agent survives, and it has the entire map of the graph with the location of the black hole. We consider the case in which the underlining graph is a dynamic 1-interval connected ring: a ring graph in which at each round at most one edge can be missing. We first show that the problem cannot be solved if the agents can only communicate by using a face-to-face mechanism: this holds for any set of agents of constant size, with respect to the size $n$ of the ring.   To circumvent this impossibility we consider agents equipped with movable pebbles that can be left on nodes as a form of communication with other agents. When pebbles are available, three agents can localize the black hole in $O(n^2)$ moves. We show that such a number of agents is optimal.   We also show that the complexity is tight, that is $\\Omega(n^2)$ moves are required for any algorithm solving the problem with three agents, even with stronger communication mechanisms (e.g., a whiteboard on each node on which agents can write messages of unlimited size). To the best of our knowledge this is the first paper examining the problem of searching a black hole in a dynamic environment with scattered agents.","sentences":["In this paper we investigate the problem of searching for a black hole in a dynamic graph by a set of scattered agents (i.e., the agents start from arbitrary locations of the graph).","The black hole is a node that silently destroys any agent visiting it.","This kind of malicious node nicely models network failures such as a crashed host or a virus that erases the visiting agents.","The black hole search problem is solved when at least one agent survives, and it has the entire map of the graph with the location of the black hole.","We consider the case in which the underlining graph is a dynamic 1-interval connected ring: a ring graph in which at each round at most one edge can be missing.","We first show that the problem cannot be solved if the agents can only communicate by using a face-to-face mechanism: this holds for any set of agents of constant size, with respect to the size $n$ of the ring.   ","To circumvent this impossibility we consider agents equipped with movable pebbles that can be left on nodes as a form of communication with other agents.","When pebbles are available, three agents can localize the black hole in $O(n^2)$ moves.","We show that such a number of agents is optimal.   ","We also show that the complexity is tight, that is $\\Omega(n^2)$ moves are required for any algorithm solving the problem with three agents, even with stronger communication mechanisms (e.g., a whiteboard on each node on which agents can write messages of unlimited size).","To the best of our knowledge this is the first paper examining the problem of searching a black hole in a dynamic environment with scattered agents."],"url":"http://arxiv.org/abs/2404.15132v1"}
{"created":"2024-04-23 15:31:46","title":"Optimizing Multi-Touch Textile and Tactile Skin Sensing Through Circuit Parameter Estimation","abstract":"Tactile and textile skin technologies have become increasingly important for enhancing human-robot interaction and allowing robots to adapt to different environments. Despite notable advancements, there are ongoing challenges in skin signal processing, particularly in achieving both accuracy and speed in dynamic touch sensing. This paper introduces a new framework that poses the touch sensing problem as an estimation problem of resistive sensory arrays. Utilizing a Regularized Least Squares objective function which estimates the resistance distribution of the skin. We enhance the touch sensing accuracy and mitigate the ghosting effects, where false or misleading touches may be registered. Furthermore, our study presents a streamlined skin design that simplifies manufacturing processes without sacrificing performance. Experimental outcomes substantiate the effectiveness of our method, showing 26.9% improvement in multi-touch force-sensing accuracy for the tactile skin.","sentences":["Tactile and textile skin technologies have become increasingly important for enhancing human-robot interaction and allowing robots to adapt to different environments.","Despite notable advancements, there are ongoing challenges in skin signal processing, particularly in achieving both accuracy and speed in dynamic touch sensing.","This paper introduces a new framework that poses the touch sensing problem as an estimation problem of resistive sensory arrays.","Utilizing a Regularized Least Squares objective function which estimates the resistance distribution of the skin.","We enhance the touch sensing accuracy and mitigate the ghosting effects, where false or misleading touches may be registered.","Furthermore, our study presents a streamlined skin design that simplifies manufacturing processes without sacrificing performance.","Experimental outcomes substantiate the effectiveness of our method, showing 26.9% improvement in multi-touch force-sensing accuracy for the tactile skin."],"url":"http://arxiv.org/abs/2404.15131v1"}
{"created":"2024-04-23 15:29:02","title":"Gallbladder Cancer Detection in Ultrasound Images based on YOLO and Faster R-CNN","abstract":"Medical image analysis is a significant application of artificial intelligence for disease diagnosis. A crucial step in this process is the identification of regions of interest within the images. This task can be automated using object detection algorithms. YOLO and Faster R-CNN are renowned for such algorithms, each with its own strengths and weaknesses. This study aims to explore the advantages of both techniques to select more accurate bounding boxes for gallbladder detection from ultrasound images, thereby enhancing gallbladder cancer classification. A fusion method that leverages the benefits of both techniques is presented in this study. The proposed method demonstrated superior classification performance, with an accuracy of 92.62%, compared to the individual use of Faster R-CNN and YOLOv8, which yielded accuracies of 90.16% and 82.79%, respectively.","sentences":["Medical image analysis is a significant application of artificial intelligence for disease diagnosis.","A crucial step in this process is the identification of regions of interest within the images.","This task can be automated using object detection algorithms.","YOLO and Faster R-CNN are renowned for such algorithms, each with its own strengths and weaknesses.","This study aims to explore the advantages of both techniques to select more accurate bounding boxes for gallbladder detection from ultrasound images, thereby enhancing gallbladder cancer classification.","A fusion method that leverages the benefits of both techniques is presented in this study.","The proposed method demonstrated superior classification performance, with an accuracy of 92.62%, compared to the individual use of Faster R-CNN and YOLOv8, which yielded accuracies of 90.16% and 82.79%, respectively."],"url":"http://arxiv.org/abs/2404.15129v1"}
{"created":"2024-04-23 15:27:19","title":"MedDr: Diagnosis-Guided Bootstrapping for Large-Scale Medical Vision-Language Learning","abstract":"The rapid advancement of large-scale vision-language models has showcased remarkable capabilities across various tasks. However, the lack of extensive and high-quality image-text data in medicine has greatly hindered the development of large-scale medical vision-language models. In this work, we present a diagnosis-guided bootstrapping strategy that exploits both image and label information to construct vision-language datasets. Based on the constructed dataset, we developed MedDr, a generalist foundation model for healthcare capable of handling diverse medical data modalities, including radiology, pathology, dermatology, retinography, and endoscopy. Moreover, during inference, we propose a simple but effective retrieval-augmented medical diagnosis strategy, which enhances the model's generalization ability. Extensive experiments on visual question answering, medical report generation, and medical image diagnosis demonstrate the superiority of our method.","sentences":["The rapid advancement of large-scale vision-language models has showcased remarkable capabilities across various tasks.","However, the lack of extensive and high-quality image-text data in medicine has greatly hindered the development of large-scale medical vision-language models.","In this work, we present a diagnosis-guided bootstrapping strategy that exploits both image and label information to construct vision-language datasets.","Based on the constructed dataset, we developed MedDr, a generalist foundation model for healthcare capable of handling diverse medical data modalities, including radiology, pathology, dermatology, retinography, and endoscopy.","Moreover, during inference, we propose a simple but effective retrieval-augmented medical diagnosis strategy, which enhances the model's generalization ability.","Extensive experiments on visual question answering, medical report generation, and medical image diagnosis demonstrate the superiority of our method."],"url":"http://arxiv.org/abs/2404.15127v1"}
{"created":"2024-04-23 15:20:17","title":"Taming Diffusion Probabilistic Models for Character Control","abstract":"We present a novel character control framework that effectively utilizes motion diffusion probabilistic models to generate high-quality and diverse character animations, responding in real-time to a variety of dynamic user-supplied control signals. At the heart of our method lies a transformer-based Conditional Autoregressive Motion Diffusion Model (CAMDM), which takes as input the character's historical motion and can generate a range of diverse potential future motions conditioned on high-level, coarse user control. To meet the demands for diversity, controllability, and computational efficiency required by a real-time controller, we incorporate several key algorithmic designs. These include separate condition tokenization, classifier-free guidance on past motion, and heuristic future trajectory extension, all designed to address the challenges associated with taming motion diffusion probabilistic models for character control. As a result, our work represents the first model that enables real-time generation of high-quality, diverse character animations based on user interactive control, supporting animating the character in multiple styles with a single unified model. We evaluate our method on a diverse set of locomotion skills, demonstrating the merits of our method over existing character controllers. Project page and source codes: https://aiganimation.github.io/CAMDM/","sentences":["We present a novel character control framework that effectively utilizes motion diffusion probabilistic models to generate high-quality and diverse character animations, responding in real-time to a variety of dynamic user-supplied control signals.","At the heart of our method lies a transformer-based Conditional Autoregressive Motion Diffusion Model (CAMDM), which takes as input the character's historical motion and can generate a range of diverse potential future motions conditioned on high-level, coarse user control.","To meet the demands for diversity, controllability, and computational efficiency required by a real-time controller, we incorporate several key algorithmic designs.","These include separate condition tokenization, classifier-free guidance on past motion, and heuristic future trajectory extension, all designed to address the challenges associated with taming motion diffusion probabilistic models for character control.","As a result, our work represents the first model that enables real-time generation of high-quality, diverse character animations based on user interactive control, supporting animating the character in multiple styles with a single unified model.","We evaluate our method on a diverse set of locomotion skills, demonstrating the merits of our method over existing character controllers.","Project page and source codes: https://aiganimation.github.io/CAMDM/"],"url":"http://arxiv.org/abs/2404.15121v1"}
{"created":"2024-04-23 15:16:49","title":"Identifying phase transitions in physical systems with neural networks: a neural architecture search perspective","abstract":"The use of machine learning algorithms to investigate phase transitions in physical systems is a valuable way to better understand the characteristics of these systems. Neural networks have been used to extract information of phases and phase transitions directly from many-body configurations. However, one limitation of neural networks is that they require the definition of the model architecture and parameters previous to their application, and such determination is itself a difficult problem. In this paper, we investigate for the first time the relationship between the accuracy of neural networks for information of phases and the network configuration (that comprises the architecture and hyperparameters). We formulate the phase analysis as a regression task, address the question of generating data that reflects the different states of the physical system, and evaluate the performance of neural architecture search for this task. After obtaining the optimized architectures, we further implement smart data processing and analytics by means of neuron coverage metrics, assessing the capability of these metrics to estimate phase transitions. Our results identify the neuron coverage metric as promising for detecting phase transitions in physical systems.","sentences":["The use of machine learning algorithms to investigate phase transitions in physical systems is a valuable way to better understand the characteristics of these systems.","Neural networks have been used to extract information of phases and phase transitions directly from many-body configurations.","However, one limitation of neural networks is that they require the definition of the model architecture and parameters previous to their application, and such determination is itself a difficult problem.","In this paper, we investigate for the first time the relationship between the accuracy of neural networks for information of phases and the network configuration (that comprises the architecture and hyperparameters).","We formulate the phase analysis as a regression task, address the question of generating data that reflects the different states of the physical system, and evaluate the performance of neural architecture search for this task.","After obtaining the optimized architectures, we further implement smart data processing and analytics by means of neuron coverage metrics, assessing the capability of these metrics to estimate phase transitions.","Our results identify the neuron coverage metric as promising for detecting phase transitions in physical systems."],"url":"http://arxiv.org/abs/2404.15118v1"}
{"created":"2024-04-23 15:03:37","title":"Compete and Compose: Learning Independent Mechanisms for Modular World Models","abstract":"We present COmpetitive Mechanisms for Efficient Transfer (COMET), a modular world model which leverages reusable, independent mechanisms across different environments. COMET is trained on multiple environments with varying dynamics via a two-step process: competition and composition. This enables the model to recognise and learn transferable mechanisms. Specifically, in the competition phase, COMET is trained with a winner-takes-all gradient allocation, encouraging the emergence of independent mechanisms. These are then re-used in the composition phase, where COMET learns to re-compose learnt mechanisms in ways that capture the dynamics of intervened environments. In so doing, COMET explicitly reuses prior knowledge, enabling efficient and interpretable adaptation. We evaluate COMET on environments with image-based observations. In contrast to competitive baselines, we demonstrate that COMET captures recognisable mechanisms without supervision. Moreover, we show that COMET is able to adapt to new environments with varying numbers of objects with improved sample efficiency compared to more conventional finetuning approaches.","sentences":["We present COmpetitive Mechanisms for Efficient Transfer (COMET), a modular world model which leverages reusable, independent mechanisms across different environments.","COMET is trained on multiple environments with varying dynamics via a two-step process: competition and composition.","This enables the model to recognise and learn transferable mechanisms.","Specifically, in the competition phase, COMET is trained with a winner-takes-all gradient allocation, encouraging the emergence of independent mechanisms.","These are then re-used in the composition phase, where COMET learns to re-compose learnt mechanisms in ways that capture the dynamics of intervened environments.","In so doing, COMET explicitly reuses prior knowledge, enabling efficient and interpretable adaptation.","We evaluate COMET on environments with image-based observations.","In contrast to competitive baselines, we demonstrate that COMET captures recognisable mechanisms without supervision.","Moreover, we show that COMET is able to adapt to new environments with varying numbers of objects with improved sample efficiency compared to more conventional finetuning approaches."],"url":"http://arxiv.org/abs/2404.15109v1"}
{"created":"2024-04-23 15:02:31","title":"Virtual Takeovers in the Metaverse: Interrogating Power in Our Past and Future(s) with Multi-Layered Narratives","abstract":"Mariah is an augmented reality (AR) mobile application that exposes power structures (e.g., capitalism, patriarchy, white supremacy) through storytelling and celebrates acts of resistance against them. People can use Mariah to \"legally trespass\" the metaverse as a form of protest. Mariah provides historical context to the user's physical surroundings by superimposing images and playing stories about people who have experienced, and resisted, injustice. We share two implementations of Mariah that raise questions about free speech and property rights in the metaverse: (1) a protest against museums accepting \"dirty money\" from the opioid epidemic; and (2) a commemoration of sites where people have resisted power structures. Mariah is a case study for how experimenting with a technology in non-sanctioned ways (i.e., \"hacking\") can expose ways that it might interact with, and potentially amplify, existing power structures.","sentences":["Mariah is an augmented reality (AR) mobile application that exposes power structures (e.g., capitalism, patriarchy, white supremacy) through storytelling and celebrates acts of resistance against them.","People can use Mariah to \"legally trespass\" the metaverse as a form of protest.","Mariah provides historical context to the user's physical surroundings by superimposing images and playing stories about people who have experienced, and resisted, injustice.","We share two implementations of Mariah that raise questions about free speech and property rights in the metaverse: (1) a protest against museums accepting \"dirty money\" from the opioid epidemic; and (2) a commemoration of sites where people have resisted power structures.","Mariah is a case study for how experimenting with a technology in non-sanctioned ways (i.e., \"hacking\") can expose ways that it might interact with, and potentially amplify, existing power structures."],"url":"http://arxiv.org/abs/2404.15108v1"}
{"created":"2024-04-23 15:01:36","title":"MIMOSA: Human-AI Co-Creation of Computational Spatial Audio Effects on Videos","abstract":"Spatial audio offers more immersive video consumption experiences to viewers; however, creating and editing spatial audio often expensive and requires specialized equipment and skills, posing a high barrier for amateur video creators. We present MIMOSA, a human-AI co-creation tool that enables amateur users to computationally generate and manipulate spatial audio effects. For a video with only monaural or stereo audio, MIMOSA automatically grounds each sound source to the corresponding sounding object in the visual scene and enables users to further validate and fix the errors in the locations of sounding objects. Users can also augment the spatial audio effect by flexibly manipulating the sounding source positions and creatively customizing the audio effect. The design of MIMOSA exemplifies a human-AI collaboration approach that, instead of utilizing state-of art end-to-end \"black-box\" ML models, uses a multistep pipeline that aligns its interpretable intermediate results with the user's workflow. A lab user study with 15 participants demonstrates MIMOSA's usability, usefulness, expressiveness, and capability in creating immersive spatial audio effects in collaboration with users.","sentences":["Spatial audio offers more immersive video consumption experiences to viewers; however, creating and editing spatial audio often expensive and requires specialized equipment and skills, posing a high barrier for amateur video creators.","We present MIMOSA, a human-AI co-creation tool that enables amateur users to computationally generate and manipulate spatial audio effects.","For a video with only monaural or stereo audio, MIMOSA automatically grounds each sound source to the corresponding sounding object in the visual scene and enables users to further validate and fix the errors in the locations of sounding objects.","Users can also augment the spatial audio effect by flexibly manipulating the sounding source positions and creatively customizing the audio effect.","The design of MIMOSA exemplifies a human-AI collaboration approach that, instead of utilizing state-of art end-to-end \"black-box\" ML models, uses a multistep pipeline that aligns its interpretable intermediate results with the user's workflow.","A lab user study with 15 participants demonstrates MIMOSA's usability, usefulness, expressiveness, and capability in creating immersive spatial audio effects in collaboration with users."],"url":"http://arxiv.org/abs/2404.15107v1"}
{"created":"2024-04-23 14:56:15","title":"Identifying Fairness Issues in Automatically Generated Testing Content","abstract":"Natural language generation tools are powerful and effective for generating content. However, language models are known to display bias and fairness issues, making them impractical to deploy for many use cases. We here focus on how fairness issues impact automatically generated test content, which can have stringent requirements to ensure the test measures only what it was intended to measure. Specifically, we identify test content that is focused on particular domains and experiences that only reflect a certain demographic or that are potentially emotionally upsetting; both of which could inadvertently impact a test-taker's score. This kind of content doesn't reflect typical biases out of context, making it challenging even for modern models that contain safeguards. We build a dataset of 621 generated texts annotated for fairness and explore a variety of methods for classification: fine-tuning, topic-based classification, and prompting, including few-shot and self-correcting prompts. We find that combining prompt self-correction and few-shot learning performs best, yielding an F1 score of .791 on our held-out test set, while much smaller BERT- and topic-based models have competitive performance on out-of-domain data.","sentences":["Natural language generation tools are powerful and effective for generating content.","However, language models are known to display bias and fairness issues, making them impractical to deploy for many use cases.","We here focus on how fairness issues impact automatically generated test content, which can have stringent requirements to ensure the test measures only what it was intended to measure.","Specifically, we identify test content that is focused on particular domains and experiences that only reflect a certain demographic or that are potentially emotionally upsetting; both of which could inadvertently impact a test-taker's score.","This kind of content doesn't reflect typical biases out of context, making it challenging even for modern models that contain safeguards.","We build a dataset of 621 generated texts annotated for fairness and explore a variety of methods for classification: fine-tuning, topic-based classification, and prompting, including few-shot and self-correcting prompts.","We find that combining prompt self-correction and few-shot learning performs best, yielding an F1 score of .791 on our held-out test set, while much smaller BERT- and topic-based models have competitive performance on out-of-domain data."],"url":"http://arxiv.org/abs/2404.15104v1"}
{"created":"2024-04-23 14:55:32","title":"Multi-view Content-aware Indexing for Long Document Retrieval","abstract":"Long document question answering (DocQA) aims to answer questions from long documents over 10k words. They usually contain content structures such as sections, sub-sections, and paragraph demarcations. However, the indexing methods of long documents remain under-explored, while existing systems generally employ fixed-length chunking. As they do not consider content structures, the resultant chunks can exclude vital information or include irrelevant content. Motivated by this, we propose the Multi-view Content-aware indexing (MC-indexing) for more effective long DocQA via (i) segment structured document into content chunks, and (ii) represent each content chunk in raw-text, keywords, and summary views. We highlight that MC-indexing requires neither training nor fine-tuning. Having plug-and-play capability, it can be seamlessly integrated with any retrievers to boost their performance. Besides, we propose a long DocQA dataset that includes not only question-answer pair, but also document structure and answer scope. When compared to state-of-art chunking schemes, MC-indexing has significantly increased the recall by 42.8%, 30.0%, 23.9%, and 16.3% via top k= 1.5, 3, 5, and 10 respectively. These improved scores are the average of 8 widely used retrievers (2 sparse and 6 dense) via extensive experiments.","sentences":["Long document question answering (DocQA) aims to answer questions from long documents over 10k words.","They usually contain content structures such as sections, sub-sections, and paragraph demarcations.","However, the indexing methods of long documents remain under-explored, while existing systems generally employ fixed-length chunking.","As they do not consider content structures, the resultant chunks can exclude vital information or include irrelevant content.","Motivated by this, we propose the Multi-view Content-aware indexing (MC-indexing) for more effective long DocQA via (i) segment structured document into content chunks, and (ii) represent each content chunk in raw-text, keywords, and summary views.","We highlight that MC-indexing requires neither training nor fine-tuning.","Having plug-and-play capability, it can be seamlessly integrated with any retrievers to boost their performance.","Besides, we propose a long DocQA dataset that includes not only question-answer pair, but also document structure and answer scope.","When compared to state-of-art chunking schemes, MC-indexing has significantly increased the recall by 42.8%, 30.0%, 23.9%, and 16.3% via top k= 1.5, 3, 5, and 10 respectively.","These improved scores are the average of 8 widely used retrievers (2 sparse and 6 dense) via extensive experiments."],"url":"http://arxiv.org/abs/2404.15103v1"}
{"created":"2024-04-23 14:53:15","title":"Multimodal Large Language Model is a Human-Aligned Annotator for Text-to-Image Generation","abstract":"Recent studies have demonstrated the exceptional potentials of leveraging human preference datasets to refine text-to-image generative models, enhancing the alignment between generated images and textual prompts. Despite these advances, current human preference datasets are either prohibitively expensive to construct or suffer from a lack of diversity in preference dimensions, resulting in limited applicability for instruction tuning in open-source text-to-image generative models and hinder further exploration. To address these challenges and promote the alignment of generative models through instruction tuning, we leverage multimodal large language models to create VisionPrefer, a high-quality and fine-grained preference dataset that captures multiple preference aspects. We aggregate feedback from AI annotators across four aspects: prompt-following, aesthetic, fidelity, and harmlessness to construct VisionPrefer. To validate the effectiveness of VisionPrefer, we train a reward model VP-Score over VisionPrefer to guide the training of text-to-image generative models and the preference prediction accuracy of VP-Score is comparable to human annotators. Furthermore, we use two reinforcement learning methods to supervised fine-tune generative models to evaluate the performance of VisionPrefer, and extensive experimental results demonstrate that VisionPrefer significantly improves text-image alignment in compositional image generation across diverse aspects, e.g., aesthetic, and generalizes better than previous human-preference metrics across various image distributions. Moreover, VisionPrefer indicates that the integration of AI-generated synthetic data as a supervisory signal is a promising avenue for achieving improved alignment with human preferences in vision generative models.","sentences":["Recent studies have demonstrated the exceptional potentials of leveraging human preference datasets to refine text-to-image generative models, enhancing the alignment between generated images and textual prompts.","Despite these advances, current human preference datasets are either prohibitively expensive to construct or suffer from a lack of diversity in preference dimensions, resulting in limited applicability for instruction tuning in open-source text-to-image generative models and hinder further exploration.","To address these challenges and promote the alignment of generative models through instruction tuning, we leverage multimodal large language models to create VisionPrefer, a high-quality and fine-grained preference dataset that captures multiple preference aspects.","We aggregate feedback from AI annotators across four aspects: prompt-following, aesthetic, fidelity, and harmlessness to construct VisionPrefer.","To validate the effectiveness of VisionPrefer, we train a reward model VP-Score over VisionPrefer to guide the training of text-to-image generative models and the preference prediction accuracy of VP-Score is comparable to human annotators.","Furthermore, we use two reinforcement learning methods to supervised fine-tune generative models to evaluate the performance of VisionPrefer, and extensive experimental results demonstrate that VisionPrefer significantly improves text-image alignment in compositional image generation across diverse aspects, e.g., aesthetic, and generalizes better than previous human-preference metrics across various image distributions.","Moreover, VisionPrefer indicates that the integration of AI-generated synthetic data as a supervisory signal is a promising avenue for achieving improved alignment with human preferences in vision generative models."],"url":"http://arxiv.org/abs/2404.15100v1"}
{"created":"2024-04-23 14:52:09","title":"Impedance Matching: Enabling an RL-Based Running Jump in a Quadruped Robot","abstract":"Replicating the remarkable athleticism seen in animals has long been a challenge in robotics control. Although Reinforcement Learning (RL) has demonstrated significant progress in dynamic legged locomotion control, the substantial sim-to-real gap often hinders the real-world demonstration of truly dynamic movements. We propose a new framework to mitigate this gap through frequency-domain analysis-based impedance matching between simulated and real robots. Our framework offers a structured guideline for parameter selection and the range for dynamics randomization in simulation, thus facilitating a safe sim-to-real transfer. The learned policy using our framework enabled jumps across distances of 55 cm and heights of 38 cm. The results are, to the best of our knowledge, one of the highest and longest running jumps demonstrated by an RL-based control policy in a real quadruped robot. Note that the achieved jumping height is approximately 85% of that obtained from a state-of-the-art trajectory optimization method, which can be seen as the physical limit for the given robot hardware. In addition, our control policy accomplished stable walking at speeds up to 2 m/s in the forward and backward directions, and 1 m/s in the sideway direction.","sentences":["Replicating the remarkable athleticism seen in animals has long been a challenge in robotics control.","Although Reinforcement Learning (RL) has demonstrated significant progress in dynamic legged locomotion control, the substantial sim-to-real gap often hinders the real-world demonstration of truly dynamic movements.","We propose a new framework to mitigate this gap through frequency-domain analysis-based impedance matching between simulated and real robots.","Our framework offers a structured guideline for parameter selection and the range for dynamics randomization in simulation, thus facilitating a safe sim-to-real transfer.","The learned policy using our framework enabled jumps across distances of 55 cm and heights of 38 cm.","The results are, to the best of our knowledge, one of the highest and longest running jumps demonstrated by an RL-based control policy in a real quadruped robot.","Note that the achieved jumping height is approximately 85% of that obtained from a state-of-the-art trajectory optimization method, which can be seen as the physical limit for the given robot hardware.","In addition, our control policy accomplished stable walking at speeds up to 2 m/s in the forward and backward directions, and 1 m/s in the sideway direction."],"url":"http://arxiv.org/abs/2404.15096v1"}
{"created":"2024-04-23 14:49:55","title":"Using ARIMA to Predict the Expansion of Subscriber Data Consumption","abstract":"This study discusses how insights retrieved from subscriber data can impact decision-making in telecommunications, focusing on predictive modeling using machine learning techniques such as the ARIMA model. The study explores time series forecasting to predict subscriber usage trends, evaluating the ARIMA model's performance using various metrics. It also compares ARIMA with Convolutional Neural Network (CNN) models, highlighting ARIMA's superiority in accuracy and execution speed. The study suggests future directions for research, including exploring additional forecasting models and considering other factors affecting subscriber data usage.","sentences":["This study discusses how insights retrieved from subscriber data can impact decision-making in telecommunications, focusing on predictive modeling using machine learning techniques such as the ARIMA model.","The study explores time series forecasting to predict subscriber usage trends, evaluating the ARIMA model's performance using various metrics.","It also compares ARIMA with Convolutional Neural Network (CNN) models, highlighting ARIMA's superiority in accuracy and execution speed.","The study suggests future directions for research, including exploring additional forecasting models and considering other factors affecting subscriber data usage."],"url":"http://arxiv.org/abs/2404.15095v1"}
{"created":"2024-04-23 14:48:11","title":"Who's actually being Studied? A Call for Population Analysis in Software Engineering Research","abstract":"Population analysis is crucial for ensuring that empirical software engineering (ESE) research is representative and its findings are valid. Yet, there is a persistent gap between sampling processes and the holistic examination of populations, which this position paper addresses. We explore the challenges ranging from analysing populations of individual software engineers to organizations and projects. We discuss the interplay between generalizability and transferability and advocate for appropriate population frames. We also present a compelling case for improved population analysis aiming to enhance the empirical rigor and external validity of ESE research.","sentences":["Population analysis is crucial for ensuring that empirical software engineering (ESE) research is representative and its findings are valid.","Yet, there is a persistent gap between sampling processes and the holistic examination of populations, which this position paper addresses.","We explore the challenges ranging from analysing populations of individual software engineers to organizations and projects.","We discuss the interplay between generalizability and transferability and advocate for appropriate population frames.","We also present a compelling case for improved population analysis aiming to enhance the empirical rigor and external validity of ESE research."],"url":"http://arxiv.org/abs/2404.15093v1"}
{"created":"2024-04-23 14:44:36","title":"Predictive Intent Maintenance with Intent Drift Detection in Next Generation Network","abstract":"Intent-Based Networking (IBN) is a known concept for enabling the autonomous configuration and self-adaptation of networks. One of the major issues in IBN is maintaining the applied intent due the effects of drifts over time, which is the gradual degradation in the fulfillment of the intents, before they fail. Despite its critical role to intent assurance and maintenance, intent drift detection was largely overlooked in the literature. To fill this gap, we propose an intent drift detection algorithm for predictive maintenance of intents which can use various unsupervised learning techniques (Affinity Propagation, DBSCAN, Gaussian Mixture Models, Hierarchical clustering, K-Means clustering, OPTICS, One-Class SVM), here applied and comparatively analyzed due to their simplicity, yet efficiency in detecting drifts. The results show that DBSCAN is the best model for detecting the intent drifts. The worst performance is exhibited by the Affinity Propagation model, reflected in its poorest accuracy and latency values.","sentences":["Intent-Based Networking (IBN) is a known concept for enabling the autonomous configuration and self-adaptation of networks.","One of the major issues in IBN is maintaining the applied intent due the effects of drifts over time, which is the gradual degradation in the fulfillment of the intents, before they fail.","Despite its critical role to intent assurance and maintenance, intent drift detection was largely overlooked in the literature.","To fill this gap, we propose an intent drift detection algorithm for predictive maintenance of intents which can use various unsupervised learning techniques (Affinity Propagation, DBSCAN, Gaussian Mixture Models, Hierarchical clustering, K-Means clustering, OPTICS, One-Class SVM), here applied and comparatively analyzed due to their simplicity, yet efficiency in detecting drifts.","The results show that DBSCAN is the best model for detecting the intent drifts.","The worst performance is exhibited by the Affinity Propagation model, reflected in its poorest accuracy and latency values."],"url":"http://arxiv.org/abs/2404.15091v1"}
{"created":"2024-04-23 14:34:16","title":"Hyperparameter Optimization Can Even be Harmful in Off-Policy Learning and How to Deal with It","abstract":"There has been a growing interest in off-policy evaluation in the literature such as recommender systems and personalized medicine. We have so far seen significant progress in developing estimators aimed at accurately estimating the effectiveness of counterfactual policies based on biased logged data. However, there are many cases where those estimators are used not only to evaluate the value of decision making policies but also to search for the best hyperparameters from a large candidate space. This work explores the latter hyperparameter optimization (HPO) task for off-policy learning. We empirically show that naively applying an unbiased estimator of the generalization performance as a surrogate objective in HPO can cause an unexpected failure, merely pursuing hyperparameters whose generalization performance is greatly overestimated. We then propose simple and computationally efficient corrections to the typical HPO procedure to deal with the aforementioned issues simultaneously. Empirical investigations demonstrate the effectiveness of our proposed HPO algorithm in situations where the typical procedure fails severely.","sentences":["There has been a growing interest in off-policy evaluation in the literature such as recommender systems and personalized medicine.","We have so far seen significant progress in developing estimators aimed at accurately estimating the effectiveness of counterfactual policies based on biased logged data.","However, there are many cases where those estimators are used not only to evaluate the value of decision making policies but also to search for the best hyperparameters from a large candidate space.","This work explores the latter hyperparameter optimization (HPO) task for off-policy learning.","We empirically show that naively applying an unbiased estimator of the generalization performance as a surrogate objective in HPO can cause an unexpected failure, merely pursuing hyperparameters whose generalization performance is greatly overestimated.","We then propose simple and computationally efficient corrections to the typical HPO procedure to deal with the aforementioned issues simultaneously.","Empirical investigations demonstrate the effectiveness of our proposed HPO algorithm in situations where the typical procedure fails severely."],"url":"http://arxiv.org/abs/2404.15084v1"}
{"created":"2024-04-23 14:33:23","title":"Between Flat-Earthers and Fitness Coaches: Who is Citing Scientific Publications in YouTube Video Descriptions?","abstract":"In this study, we undertake an extensive analysis of YouTube channels that reference research publications in their video descriptions, offering a unique insight into the intersection of digital media and academia. Our investigation focuses on three principal aspects: the background of YouTube channel owners, their thematic focus, and the nature of their operational dynamics, specifically addressing whether they work individually or in groups. Our results highlight a strong emphasis on content related to science and engineering, as well as health, particularly in channels managed by individual researchers and academic institutions. However, there is a notable variation in the popularity of these channels, with professional YouTubers and commercial media entities often outperforming in terms of viewer engagement metrics like likes, comments, and views. This underscores the challenge academic channels face in attracting a wider audience. Further, we explore the role of academic actors on YouTube, scrutinizing their impact in disseminating research and the types of publications they reference. Despite a general inclination towards professional academic topics, these channels displayed a varied effectiveness in spotlighting highly cited research. Often, they referenced a wide array of publications, indicating a diverse but not necessarily impact-focused approach to content selection.","sentences":["In this study, we undertake an extensive analysis of YouTube channels that reference research publications in their video descriptions, offering a unique insight into the intersection of digital media and academia.","Our investigation focuses on three principal aspects: the background of YouTube channel owners, their thematic focus, and the nature of their operational dynamics, specifically addressing whether they work individually or in groups.","Our results highlight a strong emphasis on content related to science and engineering, as well as health, particularly in channels managed by individual researchers and academic institutions.","However, there is a notable variation in the popularity of these channels, with professional YouTubers and commercial media entities often outperforming in terms of viewer engagement metrics like likes, comments, and views.","This underscores the challenge academic channels face in attracting a wider audience.","Further, we explore the role of academic actors on YouTube, scrutinizing their impact in disseminating research and the types of publications they reference.","Despite a general inclination towards professional academic topics, these channels displayed a varied effectiveness in spotlighting highly cited research.","Often, they referenced a wide array of publications, indicating a diverse but not necessarily impact-focused approach to content selection."],"url":"http://arxiv.org/abs/2404.15083v1"}
{"created":"2024-04-23 14:31:15","title":"Perturbing Attention Gives You More Bang for the Buck: Subtle Imaging Perturbations That Efficiently Fool Customized Diffusion Models","abstract":"Diffusion models (DMs) embark a new era of generative modeling and offer more opportunities for efficient generating high-quality and realistic data samples. However, their widespread use has also brought forth new challenges in model security, which motivates the creation of more effective adversarial attackers on DMs to understand its vulnerability. We propose CAAT, a simple but generic and efficient approach that does not require costly training to effectively fool latent diffusion models (LDMs). The approach is based on the observation that cross-attention layers exhibits higher sensitivity to gradient change, allowing for leveraging subtle perturbations on published images to significantly corrupt the generated images. We show that a subtle perturbation on an image can significantly impact the cross-attention layers, thus changing the mapping between text and image during the fine-tuning of customized diffusion models. Extensive experiments demonstrate that CAAT is compatible with diverse diffusion models and outperforms baseline attack methods in a more effective (more noise) and efficient (twice as fast as Anti-DreamBooth and Mist) manner.","sentences":["Diffusion models (DMs) embark a new era of generative modeling and offer more opportunities for efficient generating high-quality and realistic data samples.","However, their widespread use has also brought forth new challenges in model security, which motivates the creation of more effective adversarial attackers on DMs to understand its vulnerability.","We propose CAAT, a simple but generic and efficient approach that does not require costly training to effectively fool latent diffusion models (LDMs).","The approach is based on the observation that cross-attention layers exhibits higher sensitivity to gradient change, allowing for leveraging subtle perturbations on published images to significantly corrupt the generated images.","We show that a subtle perturbation on an image can significantly impact the cross-attention layers, thus changing the mapping between text and image during the fine-tuning of customized diffusion models.","Extensive experiments demonstrate that CAAT is compatible with diverse diffusion models and outperforms baseline attack methods in a more effective (more noise) and efficient (twice as fast as Anti-DreamBooth and Mist) manner."],"url":"http://arxiv.org/abs/2404.15081v1"}
{"created":"2024-04-23 14:31:02","title":"Flexible Field Sizes in Secure Distributed Matrix Multiplication via Efficient Interference Cancellation","abstract":"In this paper, we propose a new secure distributed matrix multiplication (SDMM) scheme using the inner product partitioning. We construct a scheme with a minimal number of workers and no redundancy, and another scheme with redundancy against stragglers. Unlike previous constructions in the literature, we do not utilize algebraic methods such as locally repairable codes or algebraic geometry codes. Our construction, which is based on generalized Reed-Solomon codes, improves the flexibility of the field size as it does not assume any divisibility constraints among the different parameters. We achieve a minimal number of workers by efficiently canceling all interference terms with a suitable orthogonal decoding vector. Finally, we discuss how the MDS conjecture impacts the smallest achievable field size for SDMM schemes and show that our construction almost achieves the bound given by the conjecture.","sentences":["In this paper, we propose a new secure distributed matrix multiplication (SDMM) scheme using the inner product partitioning.","We construct a scheme with a minimal number of workers and no redundancy, and another scheme with redundancy against stragglers.","Unlike previous constructions in the literature, we do not utilize algebraic methods such as locally repairable codes or algebraic geometry codes.","Our construction, which is based on generalized Reed-Solomon codes, improves the flexibility of the field size as it does not assume any divisibility constraints among the different parameters.","We achieve a minimal number of workers by efficiently canceling all interference terms with a suitable orthogonal decoding vector.","Finally, we discuss how the MDS conjecture impacts the smallest achievable field size for SDMM schemes and show that our construction almost achieves the bound given by the conjecture."],"url":"http://arxiv.org/abs/2404.15080v1"}
{"created":"2024-04-23 14:24:31","title":"Outage Probability Analysis of Wireless Paths with Faulty Reconfigurable Intelligent Surfaces","abstract":"We consider a next generation wireless network incorporating a base station a set of typically low-cost and faulty Reconfigurable Intelligent Surfaces (RISs). The base station needs to select the path including the RIS to provide the maximum signal-to-noise ratio (SNR) to the user. We study the effect of the number of elements, distance and RIS hardware failure on the path outage probability, and based on the known signal propagation model at high frequencies, derive the closed-form expression for the said probability of outage. Numerical results show the path outage likelihood as function of the probability of hardware failure of RIS elements, the number of elements, and the distance between mobile users and the RIS.","sentences":["We consider a next generation wireless network incorporating a base station a set of typically low-cost and faulty Reconfigurable Intelligent Surfaces (RISs).","The base station needs to select the path including the RIS to provide the maximum signal-to-noise ratio (SNR) to the user.","We study the effect of the number of elements, distance and RIS hardware failure on the path outage probability, and based on the known signal propagation model at high frequencies, derive the closed-form expression for the said probability of outage.","Numerical results show the path outage likelihood as function of the probability of hardware failure of RIS elements, the number of elements, and the distance between mobile users and the RIS."],"url":"http://arxiv.org/abs/2404.15074v1"}
{"created":"2024-04-23 14:19:13","title":"Dynamicity-aware Social Bot Detection with Dynamic Graph Transformers","abstract":"Detecting social bots has evolved into a pivotal yet intricate task, aimed at combating the dissemination of misinformation and preserving the authenticity of online interactions. While earlier graph-based approaches, which leverage topological structure of social networks, yielded notable outcomes, they overlooked the inherent dynamicity of social networks -- In reality, they largely depicted the social network as a static graph and solely relied on its most recent state. Due to the absence of dynamicity modeling, such approaches are vulnerable to evasion, particularly when advanced social bots interact with other users to camouflage identities and escape detection. To tackle these challenges, we propose BotDGT, a novel framework that not only considers the topological structure, but also effectively incorporates dynamic nature of social network. Specifically, we characterize a social network as a dynamic graph. A structural module is employed to acquire topological information from each historical snapshot. Additionally, a temporal module is proposed to integrate historical context and model the evolving behavior patterns exhibited by social bots and legitimate users. Experimental results demonstrate the superiority of BotDGT against the leading methods that neglected the dynamic nature of social networks in terms of accuracy, recall, and F1-score.","sentences":["Detecting social bots has evolved into a pivotal yet intricate task, aimed at combating the dissemination of misinformation and preserving the authenticity of online interactions.","While earlier graph-based approaches, which leverage topological structure of social networks, yielded notable outcomes, they overlooked the inherent dynamicity of social networks -- In reality, they largely depicted the social network as a static graph and solely relied on its most recent state.","Due to the absence of dynamicity modeling, such approaches are vulnerable to evasion, particularly when advanced social bots interact with other users to camouflage identities and escape detection.","To tackle these challenges, we propose BotDGT, a novel framework that not only considers the topological structure, but also effectively incorporates dynamic nature of social network.","Specifically, we characterize a social network as a dynamic graph.","A structural module is employed to acquire topological information from each historical snapshot.","Additionally, a temporal module is proposed to integrate historical context and model the evolving behavior patterns exhibited by social bots and legitimate users.","Experimental results demonstrate the superiority of BotDGT against the leading methods that neglected the dynamic nature of social networks in terms of accuracy, recall, and F1-score."],"url":"http://arxiv.org/abs/2404.15070v1"}
{"created":"2024-04-23 14:16:47","title":"Understanding IoT Domain Names: Analysis and Classification Using Machine Learning","abstract":"In this paper, we investigate the domain names of servers on the Internet that are accessed by IoT devices performing machine-to-machine communications. Using machine learning, we classify between them and domain names of servers contacted by other types of devices. By surveying past studies that used testbeds with real-world devices and using lists of top visited websites, we construct lists of domain names of both types of servers. We study the statistical properties of the domain name lists and train six machine learning models to perform the classification. The word embedding technique we use to get the real-value representation of the domain names is Word2vec. Among the models we train, Random Forest achieves the highest performance in classifying the domain names, yielding the highest accuracy, precision, recall, and F1 score. Our work offers novel insights to IoT, potentially informing protocol design and aiding in network security and performance monitoring.","sentences":["In this paper, we investigate the domain names of servers on the Internet that are accessed by IoT devices performing machine-to-machine communications.","Using machine learning, we classify between them and domain names of servers contacted by other types of devices.","By surveying past studies that used testbeds with real-world devices and using lists of top visited websites, we construct lists of domain names of both types of servers.","We study the statistical properties of the domain name lists and train six machine learning models to perform the classification.","The word embedding technique we use to get the real-value representation of the domain names is Word2vec.","Among the models we train, Random Forest achieves the highest performance in classifying the domain names, yielding the highest accuracy, precision, recall, and F1 score.","Our work offers novel insights to IoT, potentially informing protocol design and aiding in network security and performance monitoring."],"url":"http://arxiv.org/abs/2404.15068v1"}
{"created":"2024-04-23 14:13:53","title":"Enhancing Textual Personality Detection toward Social Media: Integrating Long-term and Short-term Perspectives","abstract":"Textual personality detection aims to identify personality characteristics by analyzing user-generated content toward social media platforms. Numerous psychological literature highlighted that personality encompasses both long-term stable traits and short-term dynamic states. However, existing studies often concentrate only on either long-term or short-term personality representations, without effectively combining both aspects. This limitation hinders a comprehensive understanding of individuals' personalities, as both stable traits and dynamic states are vital. To bridge this gap, we propose a Dual Enhanced Network(DEN) to jointly model users' long-term and short-term personality for textual personality detection. In DEN, a Long-term Personality Encoding is devised to effectively model long-term stable personality traits. Short-term Personality Encoding is presented to capture short-term dynamic personality states. The Bi-directional Interaction component facilitates the integration of both personality aspects, allowing for a comprehensive representation of the user's personality. Experimental results on two personality detection datasets demonstrate the effectiveness of the DEN model and the benefits of considering both the dynamic and stable nature of personality characteristics for textual personality detection.","sentences":["Textual personality detection aims to identify personality characteristics by analyzing user-generated content toward social media platforms.","Numerous psychological literature highlighted that personality encompasses both long-term stable traits and short-term dynamic states.","However, existing studies often concentrate only on either long-term or short-term personality representations, without effectively combining both aspects.","This limitation hinders a comprehensive understanding of individuals' personalities, as both stable traits and dynamic states are vital.","To bridge this gap, we propose a Dual Enhanced Network(DEN) to jointly model users' long-term and short-term personality for textual personality detection.","In DEN, a Long-term Personality Encoding is devised to effectively model long-term stable personality traits.","Short-term Personality Encoding is presented to capture short-term dynamic personality states.","The Bi-directional Interaction component facilitates the integration of both personality aspects, allowing for a comprehensive representation of the user's personality.","Experimental results on two personality detection datasets demonstrate the effectiveness of the DEN model and the benefits of considering both the dynamic and stable nature of personality characteristics for textual personality detection."],"url":"http://arxiv.org/abs/2404.15067v1"}
{"created":"2024-04-23 14:12:48","title":"Formal Verification of Graph Convolutional Networks with Uncertain Node Features and Uncertain Graph Structure","abstract":"Graph neural networks are becoming increasingly popular in the field of machine learning due to their unique ability to process data structured in graphs. They have also been applied in safety-critical environments where perturbations inherently occur. However, these perturbations require us to formally verify neural networks before their deployment in safety-critical environments as neural networks are prone to adversarial attacks. While there exists research on the formal verification of neural networks, there is no work verifying the robustness of generic graph convolutional network architectures with uncertainty in the node features and in the graph structure over multiple message-passing steps. This work addresses this research gap by explicitly preserving the non-convex dependencies of all elements in the underlying computations through reachability analysis with (matrix) polynomial zonotopes. We demonstrate our approach on three popular benchmark datasets.","sentences":["Graph neural networks are becoming increasingly popular in the field of machine learning due to their unique ability to process data structured in graphs.","They have also been applied in safety-critical environments where perturbations inherently occur.","However, these perturbations require us to formally verify neural networks before their deployment in safety-critical environments as neural networks are prone to adversarial attacks.","While there exists research on the formal verification of neural networks, there is no work verifying the robustness of generic graph convolutional network architectures with uncertainty in the node features and in the graph structure over multiple message-passing steps.","This work addresses this research gap by explicitly preserving the non-convex dependencies of all elements in the underlying computations through reachability analysis with (matrix) polynomial zonotopes.","We demonstrate our approach on three popular benchmark datasets."],"url":"http://arxiv.org/abs/2404.15065v1"}
{"created":"2024-04-23 14:10:00","title":"Neural Slicer for Multi-Axis 3D Printing","abstract":"We introduce a novel neural network-based computational pipeline as a representation-agnostic slicer for multi-axis 3D printing. This advanced slicer can work on models with diverse representations and intricate topology. The approach involves employing neural networks to establish a deformation mapping, defining a scalar field in the space surrounding an input model. Isosurfaces are subsequently extracted from this field to generate curved layers for 3D printing. Creating a differentiable pipeline enables us to optimize the mapping through loss functions directly defined on the field gradients as the local printing directions. New loss functions have been introduced to meet the manufacturing objectives of support-free and strength reinforcement. Our new computation pipeline relies less on the initial values of the field and can generate slicing results with significantly improved performance.","sentences":["We introduce a novel neural network-based computational pipeline as a representation-agnostic slicer for multi-axis 3D printing.","This advanced slicer can work on models with diverse representations and intricate topology.","The approach involves employing neural networks to establish a deformation mapping, defining a scalar field in the space surrounding an input model.","Isosurfaces are subsequently extracted from this field to generate curved layers for 3D printing.","Creating a differentiable pipeline enables us to optimize the mapping through loss functions directly defined on the field gradients as the local printing directions.","New loss functions have been introduced to meet the manufacturing objectives of support-free and strength reinforcement.","Our new computation pipeline relies less on the initial values of the field and can generate slicing results with significantly improved performance."],"url":"http://arxiv.org/abs/2404.15061v1"}
{"created":"2024-04-23 14:07:39","title":"Using deep reinforcement learning to promote sustainable human behaviour on a common pool resource problem","abstract":"A canonical social dilemma arises when finite resources are allocated to a group of people, who can choose to either reciprocate with interest, or keep the proceeds for themselves. What resource allocation mechanisms will encourage levels of reciprocation that sustain the commons? Here, in an iterated multiplayer trust game, we use deep reinforcement learning (RL) to design an allocation mechanism that endogenously promotes sustainable contributions from human participants to a common pool resource. We first trained neural networks to behave like human players, creating a stimulated economy that allowed us to study how different mechanisms influenced the dynamics of receipt and reciprocation. We then used RL to train a social planner to maximise aggregate return to players. The social planner discovered a redistributive policy that led to a large surplus and an inclusive economy, in which players made roughly equal gains. The RL agent increased human surplus over baseline mechanisms based on unrestricted welfare or conditional cooperation, by conditioning its generosity on available resources and temporarily sanctioning defectors by allocating fewer resources to them. Examining the AI policy allowed us to develop an explainable mechanism that performed similarly and was more popular among players. Deep reinforcement learning can be used to discover mechanisms that promote sustainable human behaviour.","sentences":["A canonical social dilemma arises when finite resources are allocated to a group of people, who can choose to either reciprocate with interest, or keep the proceeds for themselves.","What resource allocation mechanisms will encourage levels of reciprocation that sustain the commons?","Here, in an iterated multiplayer trust game, we use deep reinforcement learning (RL) to design an allocation mechanism that endogenously promotes sustainable contributions from human participants to a common pool resource.","We first trained neural networks to behave like human players, creating a stimulated economy that allowed us to study how different mechanisms influenced the dynamics of receipt and reciprocation.","We then used RL to train a social planner to maximise aggregate return to players.","The social planner discovered a redistributive policy that led to a large surplus and an inclusive economy, in which players made roughly equal gains.","The RL agent increased human surplus over baseline mechanisms based on unrestricted welfare or conditional cooperation, by conditioning its generosity on available resources and temporarily sanctioning defectors by allocating fewer resources to them.","Examining the AI policy allowed us to develop an explainable mechanism that performed similarly and was more popular among players.","Deep reinforcement learning can be used to discover mechanisms that promote sustainable human behaviour."],"url":"http://arxiv.org/abs/2404.15059v1"}
{"created":"2024-04-23 14:07:20","title":"A Mechanism-Based Approach to Mitigating Harms from Persuasive Generative AI","abstract":"Recent generative AI systems have demonstrated more advanced persuasive capabilities and are increasingly permeating areas of life where they can influence decision-making. Generative AI presents a new risk profile of persuasion due the opportunity for reciprocal exchange and prolonged interactions. This has led to growing concerns about harms from AI persuasion and how they can be mitigated, highlighting the need for a systematic study of AI persuasion. The current definitions of AI persuasion are unclear and related harms are insufficiently studied. Existing harm mitigation approaches prioritise harms from the outcome of persuasion over harms from the process of persuasion. In this paper, we lay the groundwork for the systematic study of AI persuasion. We first put forward definitions of persuasive generative AI. We distinguish between rationally persuasive generative AI, which relies on providing relevant facts, sound reasoning, or other forms of trustworthy evidence, and manipulative generative AI, which relies on taking advantage of cognitive biases and heuristics or misrepresenting information. We also put forward a map of harms from AI persuasion, including definitions and examples of economic, physical, environmental, psychological, sociocultural, political, privacy, and autonomy harm. We then introduce a map of mechanisms that contribute to harmful persuasion. Lastly, we provide an overview of approaches that can be used to mitigate against process harms of persuasion, including prompt engineering for manipulation classification and red teaming. Future work will operationalise these mitigations and study the interaction between different types of mechanisms of persuasion.","sentences":["Recent generative AI systems have demonstrated more advanced persuasive capabilities and are increasingly permeating areas of life where they can influence decision-making.","Generative AI presents a new risk profile of persuasion due the opportunity for reciprocal exchange and prolonged interactions.","This has led to growing concerns about harms from AI persuasion and how they can be mitigated, highlighting the need for a systematic study of AI persuasion.","The current definitions of AI persuasion are unclear and related harms are insufficiently studied.","Existing harm mitigation approaches prioritise harms from the outcome of persuasion over harms from the process of persuasion.","In this paper, we lay the groundwork for the systematic study of AI persuasion.","We first put forward definitions of persuasive generative AI.","We distinguish between rationally persuasive generative AI, which relies on providing relevant facts, sound reasoning, or other forms of trustworthy evidence, and manipulative generative AI, which relies on taking advantage of cognitive biases and heuristics or misrepresenting information.","We also put forward a map of harms from AI persuasion, including definitions and examples of economic, physical, environmental, psychological, sociocultural, political, privacy, and autonomy harm.","We then introduce a map of mechanisms that contribute to harmful persuasion.","Lastly, we provide an overview of approaches that can be used to mitigate against process harms of persuasion, including prompt engineering for manipulation classification and red teaming.","Future work will operationalise these mitigations and study the interaction between different types of mechanisms of persuasion."],"url":"http://arxiv.org/abs/2404.15058v1"}
{"created":"2024-04-23 13:51:20","title":"Finite Automata for Efficient Graph Recognition","abstract":"Engelfriet and Vereijken have shown that linear graph grammars based on hyperedge replacement generate graph languages that can be considered as interpretations of regular string languages over typed symbols. In this paper we show that finite automata can be lifted from strings to graphs within the same framework. For the efficient recognition of graphs with these automata, we make them deterministic by a modified powerset construction, and state sufficient conditions under which deterministic finite graph automata recognize graphs without the need to use backtracking.","sentences":["Engelfriet and Vereijken have shown that linear graph grammars based on hyperedge replacement generate graph languages that can be considered as interpretations of regular string languages over typed symbols.","In this paper we show that finite automata can be lifted from strings to graphs within the same framework.","For the efficient recognition of graphs with these automata, we make them deterministic by a modified powerset construction, and state sufficient conditions under which deterministic finite graph automata recognize graphs without the need to use backtracking."],"url":"http://arxiv.org/abs/2404.15052v1"}
{"created":"2024-04-23 13:47:09","title":"Multi-Head Mixture-of-Experts","abstract":"Sparse Mixtures of Experts (SMoE) scales model capacity without significant increases in training and inference costs, but exhibits the following two issues: (1) Low expert activation, where only a small subset of experts are activated for optimization. (2) Lacking fine-grained analytical capabilities for multiple semantic concepts within individual tokens. We propose Multi-Head Mixture-of-Experts (MH-MoE), which employs a multi-head mechanism to split each token into multiple sub-tokens. These sub-tokens are then assigned to and processed by a diverse set of experts in parallel, and seamlessly reintegrated into the original token form. The multi-head mechanism enables the model to collectively attend to information from various representation spaces within different experts, while significantly enhances expert activation, thus deepens context understanding and alleviate overfitting. Moreover, our MH-MoE is straightforward to implement and decouples from other SMoE optimization methods, making it easy to integrate with other SMoE models for enhanced performance. Extensive experimental results across three tasks: English-focused language modeling, Multi-lingual language modeling and Masked multi-modality modeling tasks, demonstrate the effectiveness of MH-MoE.","sentences":["Sparse Mixtures of Experts (SMoE) scales model capacity without significant increases in training and inference costs, but exhibits the following two issues: (1) Low expert activation, where only a small subset of experts are activated for optimization.","(2) Lacking fine-grained analytical capabilities for multiple semantic concepts within individual tokens.","We propose Multi-Head Mixture-of-Experts (MH-MoE), which employs a multi-head mechanism to split each token into multiple sub-tokens.","These sub-tokens are then assigned to and processed by a diverse set of experts in parallel, and seamlessly reintegrated into the original token form.","The multi-head mechanism enables the model to collectively attend to information from various representation spaces within different experts, while significantly enhances expert activation, thus deepens context understanding and alleviate overfitting.","Moreover, our MH-MoE is straightforward to implement and decouples from other SMoE optimization methods, making it easy to integrate with other SMoE models for enhanced performance.","Extensive experimental results across three tasks: English-focused language modeling, Multi-lingual language modeling and Masked multi-modality modeling tasks, demonstrate the effectiveness of MH-MoE."],"url":"http://arxiv.org/abs/2404.15045v1"}
{"created":"2024-04-23 13:45:03","title":"Mapping Parallel Matrix Multiplication in GotoBLAS2 to the AMD Versal ACAP for Deep Learning","abstract":"This paper investigates the design of parallel general matrix multiplication (GEMM) for a Versal Adaptive Compute Accelerated Platform (ACAP) equipped with a VC1902 system-on-chip and multiple Artificial Intelligence Engines (AIEs). Our efforts aim to port standard optimization techniques applied in the high-performance realization of GEMM on CPUs to the Versal ACAP. In particular, 1) we address the flexible exploitation of the Versal ACA multi-level memory hierarchy; 2) we delve into the efficient use of the vector units in the AIE tiles, proposing an architecture-specific micro-kernel for mixed precision arithmetic to address the strong demand for adaptive-precision inference in deep learning; and 3) we introduce a parallel design for GEMM that spans multiple AIE tiles, enhancing the computational throughput. We conduct experimental profiling, with up to 32 AI Engines, that demonstrates the high parallel scalability of the solution.","sentences":["This paper investigates the design of parallel general matrix multiplication (GEMM) for a Versal Adaptive Compute Accelerated Platform (ACAP) equipped with a VC1902 system-on-chip and multiple Artificial Intelligence Engines (AIEs).","Our efforts aim to port standard optimization techniques applied in the high-performance realization of GEMM on CPUs to the Versal ACAP.","In particular, 1) we address the flexible exploitation of the Versal ACA multi-level memory hierarchy; 2) we delve into the efficient use of the vector units in the AIE tiles, proposing an architecture-specific micro-kernel for mixed precision arithmetic to address the strong demand for adaptive-precision inference in deep learning; and 3) we introduce a parallel design for GEMM that spans multiple AIE tiles, enhancing the computational throughput.","We conduct experimental profiling, with up to 32 AI Engines, that demonstrates the high parallel scalability of the solution."],"url":"http://arxiv.org/abs/2404.15043v1"}
