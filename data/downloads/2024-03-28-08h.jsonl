{"created":"2024-03-27 17:59:56","title":"Real Acoustic Fields: An Audio-Visual Room Acoustics Dataset and Benchmark","abstract":"We present a new dataset called Real Acoustic Fields (RAF) that captures real acoustic room data from multiple modalities. The dataset includes high-quality and densely captured room impulse response data paired with multi-view images, and precise 6DoF pose tracking data for sound emitters and listeners in the rooms. We used this dataset to evaluate existing methods for novel-view acoustic synthesis and impulse response generation which previously relied on synthetic data. In our evaluation, we thoroughly assessed existing audio and audio-visual models against multiple criteria and proposed settings to enhance their performance on real-world data. We also conducted experiments to investigate the impact of incorporating visual data (i.e., images and depth) into neural acoustic field models. Additionally, we demonstrated the effectiveness of a simple sim2real approach, where a model is pre-trained with simulated data and fine-tuned with sparse real-world data, resulting in significant improvements in the few-shot learning approach. RAF is the first dataset to provide densely captured room acoustic data, making it an ideal resource for researchers working on audio and audio-visual neural acoustic field modeling techniques. Demos and datasets are available on our project page: https://facebookresearch.github.io/real-acoustic-fields/","sentences":["We present a new dataset called Real Acoustic Fields (RAF) that captures real acoustic room data from multiple modalities.","The dataset includes high-quality and densely captured room impulse response data paired with multi-view images, and precise 6DoF pose tracking data for sound emitters and listeners in the rooms.","We used this dataset to evaluate existing methods for novel-view acoustic synthesis and impulse response generation which previously relied on synthetic data.","In our evaluation, we thoroughly assessed existing audio and audio-visual models against multiple criteria and proposed settings to enhance their performance on real-world data.","We also conducted experiments to investigate the impact of incorporating visual data (i.e., images and depth) into neural acoustic field models.","Additionally, we demonstrated the effectiveness of a simple sim2real approach, where a model is pre-trained with simulated data and fine-tuned with sparse real-world data, resulting in significant improvements in the few-shot learning approach.","RAF is the first dataset to provide densely captured room acoustic data, making it an ideal resource for researchers working on audio and audio-visual neural acoustic field modeling techniques.","Demos and datasets are available on our project page: https://facebookresearch.github.io/real-acoustic-fields/"],"url":"http://arxiv.org/abs/2403.18821v1"}
{"created":"2024-03-27 17:59:54","title":"MetaCap: Meta-learning Priors from Multi-View Imagery for Sparse-view Human Performance Capture and Rendering","abstract":"Faithful human performance capture and free-view rendering from sparse RGB observations is a long-standing problem in Vision and Graphics. The main challenges are the lack of observations and the inherent ambiguities of the setting, e.g. occlusions and depth ambiguity. As a result, radiance fields, which have shown great promise in capturing high-frequency appearance and geometry details in dense setups, perform poorly when na\\\"ively supervising them on sparse camera views, as the field simply overfits to the sparse-view inputs. To address this, we propose MetaCap, a method for efficient and high-quality geometry recovery and novel view synthesis given very sparse or even a single view of the human. Our key idea is to meta-learn the radiance field weights solely from potentially sparse multi-view videos, which can serve as a prior when fine-tuning them on sparse imagery depicting the human. This prior provides a good network weight initialization, thereby effectively addressing ambiguities in sparse-view capture. Due to the articulated structure of the human body and motion-induced surface deformations, learning such a prior is non-trivial. Therefore, we propose to meta-learn the field weights in a pose-canonicalized space, which reduces the spatial feature range and makes feature learning more effective. Consequently, one can fine-tune our field parameters to quickly generalize to unseen poses, novel illumination conditions as well as novel and sparse (even monocular) camera views. For evaluating our method under different scenarios, we collect a new dataset, WildDynaCap, which contains subjects captured in, both, a dense camera dome and in-the-wild sparse camera rigs, and demonstrate superior results compared to recent state-of-the-art methods on both public and WildDynaCap dataset.","sentences":["Faithful human performance capture and free-view rendering from sparse RGB observations is a long-standing problem in Vision and Graphics.","The main challenges are the lack of observations and the inherent ambiguities of the setting, e.g. occlusions and depth ambiguity.","As a result, radiance fields, which have shown great promise in capturing high-frequency appearance and geometry details in dense setups, perform poorly when na\\\"ively supervising them on sparse camera views, as the field simply overfits to the sparse-view inputs.","To address this, we propose MetaCap, a method for efficient and high-quality geometry recovery and novel view synthesis given very sparse or even a single view of the human.","Our key idea is to meta-learn the radiance field weights solely from potentially sparse multi-view videos, which can serve as a prior when fine-tuning them on sparse imagery depicting the human.","This prior provides a good network weight initialization, thereby effectively addressing ambiguities in sparse-view capture.","Due to the articulated structure of the human body and motion-induced surface deformations, learning such a prior is non-trivial.","Therefore, we propose to meta-learn the field weights in a pose-canonicalized space, which reduces the spatial feature range and makes feature learning more effective.","Consequently, one can fine-tune our field parameters to quickly generalize to unseen poses, novel illumination conditions as well as novel and sparse (even monocular) camera views.","For evaluating our method under different scenarios, we collect a new dataset, WildDynaCap, which contains subjects captured in, both, a dense camera dome and in-the-wild sparse camera rigs, and demonstrate superior results compared to recent state-of-the-art methods on both public and WildDynaCap dataset."],"url":"http://arxiv.org/abs/2403.18820v1"}
{"created":"2024-03-27 17:59:53","title":"Benchmarking Object Detectors with COCO: A New Path Forward","abstract":"The Common Objects in Context (COCO) dataset has been instrumental in benchmarking object detectors over the past decade. Like every dataset, COCO contains subtle errors and imperfections stemming from its annotation procedure. With the advent of high-performing models, we ask whether these errors of COCO are hindering its utility in reliably benchmarking further progress. In search for an answer, we inspect thousands of masks from COCO (2017 version) and uncover different types of errors such as imprecise mask boundaries, non-exhaustively annotated instances, and mislabeled masks. Due to the prevalence of COCO, we choose to correct these errors to maintain continuity with prior research. We develop COCO-ReM (Refined Masks), a cleaner set of annotations with visibly better mask quality than COCO-2017. We evaluate fifty object detectors and find that models that predict visually sharper masks score higher on COCO-ReM, affirming that they were being incorrectly penalized due to errors in COCO-2017. Moreover, our models trained using COCO-ReM converge faster and score higher than their larger variants trained using COCO-2017, highlighting the importance of data quality in improving object detectors. With these findings, we advocate using COCO-ReM for future object detection research. Our dataset is available at https://cocorem.xyz","sentences":["The Common Objects in Context (COCO) dataset has been instrumental in benchmarking object detectors over the past decade.","Like every dataset, COCO contains subtle errors and imperfections stemming from its annotation procedure.","With the advent of high-performing models, we ask whether these errors of COCO are hindering its utility in reliably benchmarking further progress.","In search for an answer, we inspect thousands of masks from COCO (2017 version) and uncover different types of errors such as imprecise mask boundaries, non-exhaustively annotated instances, and mislabeled masks.","Due to the prevalence of COCO, we choose to correct these errors to maintain continuity with prior research.","We develop COCO-ReM (Refined Masks), a cleaner set of annotations with visibly better mask quality than COCO-2017.","We evaluate fifty object detectors and find that models that predict visually sharper masks score higher on COCO-ReM, affirming that they were being incorrectly penalized due to errors in COCO-2017.","Moreover, our models trained using COCO-ReM converge faster and score higher than their larger variants trained using COCO-2017, highlighting the importance of data quality in improving object detectors.","With these findings, we advocate using COCO-ReM for future object detection research.","Our dataset is available at https://cocorem.xyz"],"url":"http://arxiv.org/abs/2403.18819v1"}
{"created":"2024-03-27 17:59:52","title":"ObjectDrop: Bootstrapping Counterfactuals for Photorealistic Object Removal and Insertion","abstract":"Diffusion models have revolutionized image editing but often generate images that violate physical laws, particularly the effects of objects on the scene, e.g., occlusions, shadows, and reflections. By analyzing the limitations of self-supervised approaches, we propose a practical solution centered on a \\q{counterfactual} dataset. Our method involves capturing a scene before and after removing a single object, while minimizing other changes. By fine-tuning a diffusion model on this dataset, we are able to not only remove objects but also their effects on the scene. However, we find that applying this approach for photorealistic object insertion requires an impractically large dataset. To tackle this challenge, we propose bootstrap supervision; leveraging our object removal model trained on a small counterfactual dataset, we synthetically expand this dataset considerably. Our approach significantly outperforms prior methods in photorealistic object removal and insertion, particularly at modeling the effects of objects on the scene.","sentences":["Diffusion models have revolutionized image editing but often generate images that violate physical laws, particularly the effects of objects on the scene, e.g., occlusions, shadows, and reflections.","By analyzing the limitations of self-supervised approaches, we propose a practical solution centered on a \\q{counterfactual} dataset.","Our method involves capturing a scene before and after removing a single object, while minimizing other changes.","By fine-tuning a diffusion model on this dataset, we are able to not only remove objects but also their effects on the scene.","However, we find that applying this approach for photorealistic object insertion requires an impractically large dataset.","To tackle this challenge, we propose bootstrap supervision; leveraging our object removal model trained on a small counterfactual dataset, we synthetically expand this dataset considerably.","Our approach significantly outperforms prior methods in photorealistic object removal and insertion, particularly at modeling the effects of objects on the scene."],"url":"http://arxiv.org/abs/2403.18818v1"}
{"created":"2024-03-27 17:59:33","title":"Garment3DGen: 3D Garment Stylization and Texture Generation","abstract":"We introduce Garment3DGen a new method to synthesize 3D garment assets from a base mesh given a single input image as guidance. Our proposed approach allows users to generate 3D textured clothes based on both real and synthetic images, such as those generated by text prompts. The generated assets can be directly draped and simulated on human bodies. First, we leverage the recent progress of image to 3D diffusion methods to generate 3D garment geometries. However, since these geometries cannot be utilized directly for downstream tasks, we propose to use them as pseudo ground-truth and set up a mesh deformation optimization procedure that deforms a base template mesh to match the generated 3D target. Second, we introduce carefully designed losses that allow the input base mesh to freely deform towards the desired target, yet preserve mesh quality and topology such that they can be simulated. Finally, a texture estimation module generates high-fidelity texture maps that are globally and locally consistent and faithfully capture the input guidance, allowing us to render the generated 3D assets. With Garment3DGen users can generate the textured 3D garment of their choice without the need of artist intervention. One can provide a textual prompt describing the garment they desire to generate a simulation-ready 3D asset. We present a plethora of quantitative and qualitative comparisons on various assets both real and generated and provide use-cases of how one can generate simulation-ready 3D garments.","sentences":["We introduce Garment3DGen a new method to synthesize 3D garment assets from a base mesh given a single input image as guidance.","Our proposed approach allows users to generate 3D textured clothes based on both real and synthetic images, such as those generated by text prompts.","The generated assets can be directly draped and simulated on human bodies.","First, we leverage the recent progress of image to 3D diffusion methods to generate 3D garment geometries.","However, since these geometries cannot be utilized directly for downstream tasks, we propose to use them as pseudo ground-truth and set up a mesh deformation optimization procedure that deforms a base template mesh to match the generated 3D target.","Second, we introduce carefully designed losses that allow the input base mesh to freely deform towards the desired target, yet preserve mesh quality and topology such that they can be simulated.","Finally, a texture estimation module generates high-fidelity texture maps that are globally and locally consistent and faithfully capture the input guidance, allowing us to render the generated 3D assets.","With Garment3DGen users can generate the textured 3D garment of their choice without the need of artist intervention.","One can provide a textual prompt describing the garment they desire to generate a simulation-ready 3D asset.","We present a plethora of quantitative and qualitative comparisons on various assets both real and generated and provide use-cases of how one can generate simulation-ready 3D garments."],"url":"http://arxiv.org/abs/2403.18816v1"}
{"created":"2024-03-27 17:59:04","title":"Mini-Gemini: Mining the Potential of Multi-modality Vision Language Models","abstract":"In this work, we introduce Mini-Gemini, a simple and effective framework enhancing multi-modality Vision Language Models (VLMs). Despite the advancements in VLMs facilitating basic visual dialog and reasoning, a performance gap persists compared to advanced models like GPT-4 and Gemini. We try to narrow the gap by mining the potential of VLMs for better performance and any-to-any workflow from three aspects, i.e., high-resolution visual tokens, high-quality data, and VLM-guided generation. To enhance visual tokens, we propose to utilize an additional visual encoder for high-resolution refinement without increasing the visual token count. We further construct a high-quality dataset that promotes precise image comprehension and reasoning-based generation, expanding the operational scope of current VLMs. In general, Mini-Gemini further mines the potential of VLMs and empowers current frameworks with image understanding, reasoning, and generation simultaneously. Mini-Gemini supports a series of dense and MoE Large Language Models (LLMs) from 2B to 34B. It is demonstrated to achieve leading performance in several zero-shot benchmarks and even surpasses the developed private models. Code and models are available at https://github.com/dvlab-research/MiniGemini.","sentences":["In this work, we introduce Mini-Gemini, a simple and effective framework enhancing multi-modality Vision Language Models (VLMs).","Despite the advancements in VLMs facilitating basic visual dialog and reasoning, a performance gap persists compared to advanced models like GPT-4 and Gemini.","We try to narrow the gap by mining the potential of VLMs for better performance and any-to-any workflow from three aspects, i.e., high-resolution visual tokens, high-quality data, and VLM-guided generation.","To enhance visual tokens, we propose to utilize an additional visual encoder for high-resolution refinement without increasing the visual token count.","We further construct a high-quality dataset that promotes precise image comprehension and reasoning-based generation, expanding the operational scope of current VLMs.","In general, Mini-Gemini further mines the potential of VLMs and empowers current frameworks with image understanding, reasoning, and generation simultaneously.","Mini-Gemini supports a series of dense and MoE Large Language Models (LLMs) from 2B to 34B. It is demonstrated to achieve leading performance in several zero-shot benchmarks and even surpasses the developed private models.","Code and models are available at https://github.com/dvlab-research/MiniGemini."],"url":"http://arxiv.org/abs/2403.18814v1"}
{"created":"2024-03-27 17:57:16","title":"On the Communication Complexity of Approximate Pattern Matching","abstract":"The decades-old Pattern Matching with Edits problem, given a length-$n$ string $T$ (the text), a length-$m$ string $P$ (the pattern), and a positive integer $k$ (the threshold), asks to list all fragments of $T$ that are at edit distance at most $k$ from $P$. The one-way communication complexity of this problem is the minimum amount of space needed to encode the answer so that it can be retrieved without accessing the input strings $P$ and $T$.   The closely related Pattern Matching with Mismatches problem (defined in terms of the Hamming distance instead of the edit distance) is already well understood from the communication complexity perspective: Clifford, Kociumaka, and Porat [SODA 2019] proved that $\\Omega(n/m \\cdot k \\log(m/k))$ bits are necessary and $O(n/m \\cdot k\\log (m|\\Sigma|/k))$ bits are sufficient; the upper bound allows encoding not only the occurrences of $P$ in $T$ with at most $k$ mismatches but also the substitutions needed to make each $k$-mismatch occurrence exact.   Despite recent improvements in the running time [Charalampopoulos, Kociumaka, and Wellnitz; FOCS 2020 and 2022], the communication complexity of Pattern Matching with Edits remained unexplored, with a lower bound of $\\Omega(n/m \\cdot k\\log(m/k))$ bits and an upper bound of $O(n/m \\cdot k^3\\log m)$ bits stemming from previous research. In this work, we prove an upper bound of $O(n/m \\cdot k \\log^2 m)$ bits, thus establishing the optimal communication complexity up to logarithmic factors. We also show that $O(n/m \\cdot k \\log m \\log (m|\\Sigma|))$ bits allow encoding, for each $k$-error occurrence of $P$ in $T$, the shortest sequence of edits needed to make the occurrence exact.   We leverage the techniques behind our new result on the communication complexity to obtain quantum algorithms for Pattern Matching with Edits.","sentences":["The decades-old Pattern Matching with Edits problem, given a length-$n$ string $T$ (the text), a length-$m$ string $P$ (the pattern), and a positive integer $k$ (the threshold), asks to list all fragments of $T$ that are at edit distance at most $k$ from $P$. The one-way communication complexity of this problem is the minimum amount of space needed to encode the answer so that it can be retrieved without accessing the input strings $P$ and $T$.   The closely related Pattern Matching with Mismatches problem (defined in terms of the Hamming distance instead of the edit distance) is already well understood from the communication complexity perspective: Clifford, Kociumaka, and","Porat [SODA 2019] proved that $\\Omega(n/m \\cdot k \\log(m/k))$ bits are necessary and $O(n/m \\cdot k\\log (m|\\Sigma|/k))$ bits are sufficient; the upper bound allows encoding not only the occurrences of $P$ in $T$ with at most $k$ mismatches but also the substitutions needed to make each $k$-mismatch occurrence exact.   ","Despite recent improvements in the running time [Charalampopoulos, Kociumaka, and Wellnitz; FOCS 2020 and 2022], the communication complexity of Pattern Matching with Edits remained unexplored, with a lower bound of $\\Omega(n/m \\cdot k\\log(m/k))$ bits and an upper bound of $O(n/m \\cdot k^3\\log m)$ bits stemming from previous research.","In this work, we prove an upper bound of $O(n/m \\cdot k \\log^2 m)$ bits, thus establishing the optimal communication complexity up to logarithmic factors.","We also show that $O(n/m \\cdot k \\log m \\log (m|\\Sigma|))$ bits allow encoding, for each $k$-error occurrence of $P$ in $T$, the shortest sequence of edits needed to make the occurrence exact.   ","We leverage the techniques behind our new result on the communication complexity to obtain quantum algorithms for Pattern Matching with Edits."],"url":"http://arxiv.org/abs/2403.18812v1"}
{"created":"2024-03-27 17:57:02","title":"Duolando: Follower GPT with Off-Policy Reinforcement Learning for Dance Accompaniment","abstract":"We introduce a novel task within the field of 3D dance generation, termed dance accompaniment, which necessitates the generation of responsive movements from a dance partner, the \"follower\", synchronized with the lead dancer's movements and the underlying musical rhythm. Unlike existing solo or group dance generation tasks, a duet dance scenario entails a heightened degree of interaction between the two participants, requiring delicate coordination in both pose and position. To support this task, we first build a large-scale and diverse duet interactive dance dataset, DD100, by recording about 117 minutes of professional dancers' performances. To address the challenges inherent in this task, we propose a GPT-based model, Duolando, which autoregressively predicts the subsequent tokenized motion conditioned on the coordinated information of the music, the leader's and the follower's movements. To further enhance the GPT's capabilities of generating stable results on unseen conditions (music and leader motions), we devise an off-policy reinforcement learning strategy that allows the model to explore viable trajectories from out-of-distribution samplings, guided by human-defined rewards. Based on the collected dataset and proposed method, we establish a benchmark with several carefully designed metrics.","sentences":["We introduce a novel task within the field of 3D dance generation, termed dance accompaniment, which necessitates the generation of responsive movements from a dance partner, the \"follower\", synchronized with the lead dancer's movements and the underlying musical rhythm.","Unlike existing solo or group dance generation tasks, a duet dance scenario entails a heightened degree of interaction between the two participants, requiring delicate coordination in both pose and position.","To support this task, we first build a large-scale and diverse duet interactive dance dataset, DD100, by recording about 117 minutes of professional dancers' performances.","To address the challenges inherent in this task, we propose a GPT-based model, Duolando, which autoregressively predicts the subsequent tokenized motion conditioned on the coordinated information of the music, the leader's and the follower's movements.","To further enhance the GPT's capabilities of generating stable results on unseen conditions (music and leader motions), we devise an off-policy reinforcement learning strategy that allows the model to explore viable trajectories from out-of-distribution samplings, guided by human-defined rewards.","Based on the collected dataset and proposed method, we establish a benchmark with several carefully designed metrics."],"url":"http://arxiv.org/abs/2403.18811v1"}
{"created":"2024-03-27 17:53:30","title":"ECoDepth: Effective Conditioning of Diffusion Models for Monocular Depth Estimation","abstract":"In the absence of parallax cues, a learning-based single image depth estimation (SIDE) model relies heavily on shading and contextual cues in the image. While this simplicity is attractive, it is necessary to train such models on large and varied datasets, which are difficult to capture. It has been shown that using embeddings from pre-trained foundational models, such as CLIP, improves zero shot transfer in several applications. Taking inspiration from this, in our paper we explore the use of global image priors generated from a pre-trained ViT model to provide more detailed contextual information. We argue that the embedding vector from a ViT model, pre-trained on a large dataset, captures greater relevant information for SIDE than the usual route of generating pseudo image captions, followed by CLIP based text embeddings. Based on this idea, we propose a new SIDE model using a diffusion backbone which is conditioned on ViT embeddings. Our proposed design establishes a new state-of-the-art (SOTA) for SIDE on NYUv2 dataset, achieving Abs Rel error of 0.059(14% improvement) compared to 0.069 by the current SOTA (VPD). And on KITTI dataset, achieving Sq Rel error of 0.139 (2% improvement) compared to 0.142 by the current SOTA (GEDepth). For zero-shot transfer with a model trained on NYUv2, we report mean relative improvement of (20%, 23%, 81%, 25%) over NeWCRFs on (Sun-RGBD, iBims1, DIODE, HyperSim) datasets, compared to (16%, 18%, 45%, 9%) by ZoeDepth. The code is available at https://github.com/Aradhye2002/EcoDepth.","sentences":["In the absence of parallax cues, a learning-based single image depth estimation (SIDE) model relies heavily on shading and contextual cues in the image.","While this simplicity is attractive, it is necessary to train such models on large and varied datasets, which are difficult to capture.","It has been shown that using embeddings from pre-trained foundational models, such as CLIP, improves zero shot transfer in several applications.","Taking inspiration from this, in our paper we explore the use of global image priors generated from a pre-trained ViT model to provide more detailed contextual information.","We argue that the embedding vector from a ViT model, pre-trained on a large dataset, captures greater relevant information for SIDE than the usual route of generating pseudo image captions, followed by CLIP based text embeddings.","Based on this idea, we propose a new SIDE model using a diffusion backbone which is conditioned on ViT embeddings.","Our proposed design establishes a new state-of-the-art (SOTA) for SIDE on NYUv2 dataset, achieving Abs Rel error of 0.059(14% improvement) compared to 0.069 by the current SOTA (VPD).","And on KITTI dataset, achieving Sq Rel error of 0.139 (2% improvement) compared to 0.142 by the current SOTA (GEDepth).","For zero-shot transfer with a model trained on NYUv2, we report mean relative improvement of (20%, 23%, 81%, 25%) over NeWCRFs on (Sun-RGBD, iBims1, DIODE, HyperSim) datasets, compared to (16%, 18%, 45%, 9%) by ZoeDepth.","The code is available at https://github.com/Aradhye2002/EcoDepth."],"url":"http://arxiv.org/abs/2403.18807v1"}
{"created":"2024-03-27 17:50:00","title":"Is Modularity Transferable? A Case Study through the Lens of Knowledge Distillation","abstract":"The rise of Modular Deep Learning showcases its potential in various Natural Language Processing applications. Parameter-efficient fine-tuning (PEFT) modularity has been shown to work for various use cases, from domain adaptation to multilingual setups. However, all this work covers the case where the modular components are trained and deployed within one single Pre-trained Language Model (PLM). This model-specific setup is a substantial limitation on the very modularity that modular architectures are trying to achieve. We ask whether current modular approaches are transferable between models and whether we can transfer the modules from more robust and larger PLMs to smaller ones. In this work, we aim to fill this gap via a lens of Knowledge Distillation, commonly used for model compression, and present an extremely straightforward approach to transferring pre-trained, task-specific PEFT modules between same-family PLMs. Moreover, we propose a method that allows the transfer of modules between incompatible PLMs without any change in the inference complexity. The experiments on Named Entity Recognition, Natural Language Inference, and Paraphrase Identification tasks over multiple languages and PEFT methods showcase the initial potential of transferable modularity.","sentences":["The rise of Modular Deep Learning showcases its potential in various Natural Language Processing applications.","Parameter-efficient fine-tuning (PEFT) modularity has been shown to work for various use cases, from domain adaptation to multilingual setups.","However, all this work covers the case where the modular components are trained and deployed within one single Pre-trained Language Model (PLM).","This model-specific setup is a substantial limitation on the very modularity that modular architectures are trying to achieve.","We ask whether current modular approaches are transferable between models and whether we can transfer the modules from more robust and larger PLMs to smaller ones.","In this work, we aim to fill this gap via a lens of Knowledge Distillation, commonly used for model compression, and present an extremely straightforward approach to transferring pre-trained, task-specific PEFT modules between same-family PLMs.","Moreover, we propose a method that allows the transfer of modules between incompatible PLMs without any change in the inference complexity.","The experiments on Named Entity Recognition, Natural Language Inference, and Paraphrase Identification tasks over multiple languages and PEFT methods showcase the initial potential of transferable modularity."],"url":"http://arxiv.org/abs/2403.18804v1"}
{"created":"2024-03-27 17:49:31","title":"Projective Methods for Mitigating Gender Bias in Pre-trained Language Models","abstract":"Mitigation of gender bias in NLP has a long history tied to debiasing static word embeddings. More recently, attention has shifted to debiasing pre-trained language models. We study to what extent the simplest projective debiasing methods, developed for word embeddings, can help when applied to BERT's internal representations. Projective methods are fast to implement, use a small number of saved parameters, and make no updates to the existing model parameters. We evaluate the efficacy of the methods in reducing both intrinsic bias, as measured by BERT's next sentence prediction task, and in mitigating observed bias in a downstream setting when fine-tuned. To this end, we also provide a critical analysis of a popular gender-bias assessment test for quantifying intrinsic bias, resulting in an enhanced test set and new bias measures. We find that projective methods can be effective at both intrinsic bias and downstream bias mitigation, but that the two outcomes are not necessarily correlated. This finding serves as a warning that intrinsic bias test sets, based either on language modeling tasks or next sentence prediction, should not be the only benchmark in developing a debiased language model.","sentences":["Mitigation of gender bias in NLP has a long history tied to debiasing static word embeddings.","More recently, attention has shifted to debiasing pre-trained language models.","We study to what extent the simplest projective debiasing methods, developed for word embeddings, can help when applied to BERT's internal representations.","Projective methods are fast to implement, use a small number of saved parameters, and make no updates to the existing model parameters.","We evaluate the efficacy of the methods in reducing both intrinsic bias, as measured by BERT's next sentence prediction task, and in mitigating observed bias in a downstream setting when fine-tuned.","To this end, we also provide a critical analysis of a popular gender-bias assessment test for quantifying intrinsic bias, resulting in an enhanced test set and new bias measures.","We find that projective methods can be effective at both intrinsic bias and downstream bias mitigation, but that the two outcomes are not necessarily correlated.","This finding serves as a warning that intrinsic bias test sets, based either on language modeling tasks or next sentence prediction, should not be the only benchmark in developing a debiased language model."],"url":"http://arxiv.org/abs/2403.18803v1"}
{"created":"2024-03-27 17:48:55","title":"Long-form factuality in large language models","abstract":"Large language models (LLMs) often generate content that contains factual errors when responding to fact-seeking prompts on open-ended topics. To benchmark a model's long-form factuality in open domains, we first use GPT-4 to generate LongFact, a prompt set comprising thousands of questions spanning 38 topics. We then propose that LLM agents can be used as automated evaluators for long-form factuality through a method which we call Search-Augmented Factuality Evaluator (SAFE). SAFE utilizes an LLM to break down a long-form response into a set of individual facts and to evaluate the accuracy of each fact using a multi-step reasoning process comprising sending search queries to Google Search and determining whether a fact is supported by the search results. Furthermore, we propose extending F1 score as an aggregated metric for long-form factuality. To do so, we balance the percentage of supported facts in a response (precision) with the percentage of provided facts relative to a hyperparameter representing a user's preferred response length (recall).   Empirically, we demonstrate that LLM agents can achieve superhuman rating performance - on a set of ~16k individual facts, SAFE agrees with crowdsourced human annotators 72% of the time, and on a random subset of 100 disagreement cases, SAFE wins 76% of the time. At the same time, SAFE is more than 20 times cheaper than human annotators. We also benchmark thirteen language models on LongFact across four model families (Gemini, GPT, Claude, and PaLM-2), finding that larger language models generally achieve better long-form factuality. LongFact, SAFE, and all experimental code are available at https://github.com/google-deepmind/long-form-factuality.","sentences":["Large language models (LLMs) often generate content that contains factual errors when responding to fact-seeking prompts on open-ended topics.","To benchmark a model's long-form factuality in open domains, we first use GPT-4 to generate LongFact, a prompt set comprising thousands of questions spanning 38 topics.","We then propose that LLM agents can be used as automated evaluators for long-form factuality through a method which we call Search-Augmented Factuality Evaluator (SAFE).","SAFE utilizes an LLM to break down a long-form response into a set of individual facts and to evaluate the accuracy of each fact using a multi-step reasoning process comprising sending search queries to Google Search and determining whether a fact is supported by the search results.","Furthermore, we propose extending F1 score as an aggregated metric for long-form factuality.","To do so, we balance the percentage of supported facts in a response (precision) with the percentage of provided facts relative to a hyperparameter representing a user's preferred response length (recall).   ","Empirically, we demonstrate that LLM agents can achieve superhuman rating performance - on a set of ~16k individual facts, SAFE agrees with crowdsourced human annotators 72% of the time, and on a random subset of 100 disagreement cases, SAFE wins 76% of the time.","At the same time, SAFE is more than 20 times cheaper than human annotators.","We also benchmark thirteen language models on LongFact across four model families (Gemini, GPT, Claude, and PaLM-2), finding that larger language models generally achieve better long-form factuality.","LongFact, SAFE, and all experimental code are available at https://github.com/google-deepmind/long-form-factuality."],"url":"http://arxiv.org/abs/2403.18802v1"}
{"created":"2024-03-27 17:44:29","title":"SolderlessPCB: Reusing Electronic Components in PCB Prototyping through Detachable 3D Printed Housings","abstract":"The iterative prototyping process for printed circuit boards (PCBs) frequently employs surface-mounted device (SMD) components, which are often discarded rather than reused due to the challenges associated with desoldering, leading to unnecessary electronic waste. This paper introduces SolderlessPCB, a collection of techniques for solder-free PCB prototyping, specifically designed to promote the recycling and reuse of electronic components. Central to this approach are custom 3D-printable housings that allow SMD components to be mounted onto PCBs without soldering. We detail the design of SolderlessPCB and the experiments conducted to evaluate its design parameters, electrical performance, and durability. To illustrate the potential for reusing SMD components with SolderlessPCB, we discuss two scenarios: the reuse of components from earlier design iterations and from obsolete prototypes. We also provide examples demonstrating that SolderlessPCB can handle high-current applications and is suitable for high-speed data transmission. The paper concludes by discussing the limitations of our approach and suggesting future directions to overcome these challenges.","sentences":["The iterative prototyping process for printed circuit boards (PCBs) frequently employs surface-mounted device (SMD) components, which are often discarded rather than reused due to the challenges associated with desoldering, leading to unnecessary electronic waste.","This paper introduces SolderlessPCB, a collection of techniques for solder-free PCB prototyping, specifically designed to promote the recycling and reuse of electronic components.","Central to this approach are custom 3D-printable housings that allow SMD components to be mounted onto PCBs without soldering.","We detail the design of SolderlessPCB and the experiments conducted to evaluate its design parameters, electrical performance, and durability.","To illustrate the potential for reusing SMD components with SolderlessPCB, we discuss two scenarios: the reuse of components from earlier design iterations and from obsolete prototypes.","We also provide examples demonstrating that SolderlessPCB can handle high-current applications and is suitable for high-speed data transmission.","The paper concludes by discussing the limitations of our approach and suggesting future directions to overcome these challenges."],"url":"http://arxiv.org/abs/2403.18797v1"}
{"created":"2024-03-27 17:40:14","title":"Gamba: Marry Gaussian Splatting with Mamba for single view 3D reconstruction","abstract":"We tackle the challenge of efficiently reconstructing a 3D asset from a single image with growing demands for automated 3D content creation pipelines. Previous methods primarily rely on Score Distillation Sampling (SDS) and Neural Radiance Fields (NeRF). Despite their significant success, these approaches encounter practical limitations due to lengthy optimization and considerable memory usage. In this report, we introduce Gamba, an end-to-end amortized 3D reconstruction model from single-view images, emphasizing two main insights: (1) 3D representation: leveraging a large number of 3D Gaussians for an efficient 3D Gaussian splatting process; (2) Backbone design: introducing a Mamba-based sequential network that facilitates context-dependent reasoning and linear scalability with the sequence (token) length, accommodating a substantial number of Gaussians. Gamba incorporates significant advancements in data preprocessing, regularization design, and training methodologies. We assessed Gamba against existing optimization-based and feed-forward 3D generation approaches using the real-world scanned OmniObject3D dataset. Here, Gamba demonstrates competitive generation capabilities, both qualitatively and quantitatively, while achieving remarkable speed, approximately 0.6 second on a single NVIDIA A100 GPU.","sentences":["We tackle the challenge of efficiently reconstructing a 3D asset from a single image with growing demands for automated 3D content creation pipelines.","Previous methods primarily rely on Score Distillation Sampling (SDS) and Neural Radiance Fields (NeRF).","Despite their significant success, these approaches encounter practical limitations due to lengthy optimization and considerable memory usage.","In this report, we introduce Gamba, an end-to-end amortized 3D reconstruction model from single-view images, emphasizing two main insights: (1) 3D representation: leveraging a large number of 3D Gaussians for an efficient 3D Gaussian splatting process; (2) Backbone design: introducing a Mamba-based sequential network that facilitates context-dependent reasoning and linear scalability with the sequence (token) length, accommodating a substantial number of Gaussians.","Gamba incorporates significant advancements in data preprocessing, regularization design, and training methodologies.","We assessed Gamba against existing optimization-based and feed-forward 3D generation approaches using the real-world scanned OmniObject3D dataset.","Here, Gamba demonstrates competitive generation capabilities, both qualitatively and quantitatively, while achieving remarkable speed, approximately 0.6 second on a single NVIDIA A100 GPU."],"url":"http://arxiv.org/abs/2403.18795v1"}
{"created":"2024-03-27 17:35:24","title":"Object Pose Estimation via the Aggregation of Diffusion Features","abstract":"Estimating the pose of objects from images is a crucial task of 3D scene understanding, and recent approaches have shown promising results on very large benchmarks. However, these methods experience a significant performance drop when dealing with unseen objects. We believe that it results from the limited generalizability of image features. To address this problem, we have an in-depth analysis on the features of diffusion models, e.g. Stable Diffusion, which hold substantial potential for modeling unseen objects. Based on this analysis, we then innovatively introduce these diffusion features for object pose estimation. To achieve this, we propose three distinct architectures that can effectively capture and aggregate diffusion features of different granularity, greatly improving the generalizability of object pose estimation. Our approach outperforms the state-of-the-art methods by a considerable margin on three popular benchmark datasets, LM, O-LM, and T-LESS. In particular, our method achieves higher accuracy than the previous best arts on unseen objects: 98.2% vs. 93.5% on Unseen LM, 85.9% vs. 76.3% on Unseen O-LM, showing the strong generalizability of our method. Our code is released at https://github.com/Tianfu18/diff-feats-pose.","sentences":["Estimating the pose of objects from images is a crucial task of 3D scene understanding, and recent approaches have shown promising results on very large benchmarks.","However, these methods experience a significant performance drop when dealing with unseen objects.","We believe that it results from the limited generalizability of image features.","To address this problem, we have an in-depth analysis on the features of diffusion models, e.g. Stable Diffusion, which hold substantial potential for modeling unseen objects.","Based on this analysis, we then innovatively introduce these diffusion features for object pose estimation.","To achieve this, we propose three distinct architectures that can effectively capture and aggregate diffusion features of different granularity, greatly improving the generalizability of object pose estimation.","Our approach outperforms the state-of-the-art methods by a considerable margin on three popular benchmark datasets, LM, O-LM, and T-LESS.","In particular, our method achieves higher accuracy than the previous best arts on unseen objects: 98.2% vs. 93.5% on Unseen LM, 85.9% vs. 76.3% on Unseen O-LM, showing the strong generalizability of our method.","Our code is released at https://github.com/Tianfu18/diff-feats-pose."],"url":"http://arxiv.org/abs/2403.18791v1"}
{"created":"2024-03-27 17:33:55","title":"Peregrine: ML-based Malicious Traffic Detection for Terabit Networks","abstract":"Malicious traffic detectors leveraging machine learning (ML), namely those incorporating deep learning techniques, exhibit impressive detection capabilities across multiple attacks. However, their effectiveness becomes compromised when deployed in networks handling Terabit-speed traffic. In practice, these systems require substantial traffic sampling to reconcile the high data plane packet rates with the comparatively slower processing speeds of ML detection. As sampling significantly reduces traffic observability, it fundamentally undermines their detection capability.   We present Peregrine, an ML-based malicious traffic detector for Terabit networks. The key idea is to run the detection process partially in the network data plane. Specifically, we offload the detector's ML feature computation to a commodity switch. The Peregrine switch processes a diversity of features per-packet, at Tbps line rates - three orders of magnitude higher than the fastest detector - to feed the ML-based component in the control plane. Our offloading approach presents a distinct advantage. While, in practice, current systems sample raw traffic, in Peregrine sampling occurs after feature computation. This essential trait enables computing features over all traffic, significantly enhancing detection performance. The Peregrine detector is not only effective for Terabit networks, but it is also energy- and cost-efficient. Further, by shifting a compute-heavy component to the switch, it saves precious CPU cycles and improves detection throughput.","sentences":["Malicious traffic detectors leveraging machine learning (ML), namely those incorporating deep learning techniques, exhibit impressive detection capabilities across multiple attacks.","However, their effectiveness becomes compromised when deployed in networks handling Terabit-speed traffic.","In practice, these systems require substantial traffic sampling to reconcile the high data plane packet rates with the comparatively slower processing speeds of ML detection.","As sampling significantly reduces traffic observability, it fundamentally undermines their detection capability.   ","We present Peregrine, an ML-based malicious traffic detector for Terabit networks.","The key idea is to run the detection process partially in the network data plane.","Specifically, we offload the detector's ML feature computation to a commodity switch.","The Peregrine switch processes a diversity of features per-packet, at Tbps line rates - three orders of magnitude higher than the fastest detector - to feed the ML-based component in the control plane.","Our offloading approach presents a distinct advantage.","While, in practice, current systems sample raw traffic, in Peregrine sampling occurs after feature computation.","This essential trait enables computing features over all traffic, significantly enhancing detection performance.","The Peregrine detector is not only effective for Terabit networks, but it is also energy- and cost-efficient.","Further, by shifting a compute-heavy component to the switch, it saves precious CPU cycles and improves detection throughput."],"url":"http://arxiv.org/abs/2403.18788v1"}
{"created":"2024-03-27 17:32:04","title":"SplatFace: Gaussian Splat Face Reconstruction Leveraging an Optimizable Surface","abstract":"We present SplatFace, a novel Gaussian splatting framework designed for 3D human face reconstruction without reliance on accurate pre-determined geometry. Our method is designed to simultaneously deliver both high-quality novel view rendering and accurate 3D mesh reconstructions. We incorporate a generic 3D Morphable Model (3DMM) to provide a surface geometric structure, making it possible to reconstruct faces with a limited set of input images. We introduce a joint optimization strategy that refines both the Gaussians and the morphable surface through a synergistic non-rigid alignment process. A novel distance metric, splat-to-surface, is proposed to improve alignment by considering both the Gaussian position and covariance. The surface information is also utilized to incorporate a world-space densification process, resulting in superior reconstruction quality. Our experimental analysis demonstrates that the proposed method is competitive with both other Gaussian splatting techniques in novel view synthesis and other 3D reconstruction methods in producing 3D face meshes with high geometric precision.","sentences":["We present SplatFace, a novel Gaussian splatting framework designed for 3D human face reconstruction without reliance on accurate pre-determined geometry.","Our method is designed to simultaneously deliver both high-quality novel view rendering and accurate 3D mesh reconstructions.","We incorporate a generic 3D Morphable Model (3DMM) to provide a surface geometric structure, making it possible to reconstruct faces with a limited set of input images.","We introduce a joint optimization strategy that refines both the Gaussians and the morphable surface through a synergistic non-rigid alignment process.","A novel distance metric, splat-to-surface, is proposed to improve alignment by considering both the Gaussian position and covariance.","The surface information is also utilized to incorporate a world-space densification process, resulting in superior reconstruction quality.","Our experimental analysis demonstrates that the proposed method is competitive with both other Gaussian splatting techniques in novel view synthesis and other 3D reconstruction methods in producing 3D face meshes with high geometric precision."],"url":"http://arxiv.org/abs/2403.18784v1"}
{"created":"2024-03-27 17:31:39","title":"Towards a World-English Language Model for On-Device Virtual Assistants","abstract":"Neural Network Language Models (NNLMs) for Virtual Assistants (VAs) are generally language-, region-, and in some cases, device-dependent, which increases the effort to scale and maintain them. Combining NNLMs for one or more of the categories is one way to improve scalability. In this work, we combine regional variants of English to build a ``World English'' NNLM for on-device VAs. In particular, we investigate the application of adapter bottlenecks to model dialect-specific characteristics in our existing production NNLMs {and enhance the multi-dialect baselines}. We find that adapter modules are more effective in modeling dialects than specializing entire sub-networks. Based on this insight and leveraging the design of our production models, we introduce a new architecture for World English NNLM that meets the accuracy, latency, and memory constraints of our single-dialect models.","sentences":["Neural Network Language Models (NNLMs) for Virtual Assistants (VAs) are generally language-, region-, and in some cases, device-dependent, which increases the effort to scale and maintain them.","Combining NNLMs for one or more of the categories is one way to improve scalability.","In this work, we combine regional variants of English to build a ``World English'' NNLM for on-device VAs.","In particular, we investigate the application of adapter bottlenecks to model dialect-specific characteristics in our existing production NNLMs {and enhance the multi-dialect baselines}.","We find that adapter modules are more effective in modeling dialects than specializing entire sub-networks.","Based on this insight and leveraging the design of our production models, we introduce a new architecture for World English NNLM that meets the accuracy, latency, and memory constraints of our single-dialect models."],"url":"http://arxiv.org/abs/2403.18783v1"}
{"created":"2024-03-27 17:29:30","title":"Hypergraph Unreliability in Quasi-Polynomial Time","abstract":"The hypergraph unreliability problem asks for the probability that a hypergraph gets disconnected when every hyperedge fails independently with a given probability. For graphs, the unreliability problem has been studied over many decades, and multiple fully polynomial-time approximation schemes are known starting with the work of Karger (STOC 1995). In contrast, prior to this work, no non-trivial result was known for hypergraphs (of arbitrary rank).   In this paper, we give quasi-polynomial time approximation schemes for the hypergraph unreliability problem. For any fixed $\\varepsilon \\in (0, 1)$, we first give a $(1+\\varepsilon)$-approximation algorithm that runs in $m^{O(\\log n)}$ time on an $m$-hyperedge, $n$-vertex hypergraph. Then, we improve the running time to $m\\cdot n^{O(\\log^2 n)}$ with an additional exponentially small additive term in the approximation.","sentences":["The hypergraph unreliability problem asks for the probability that a hypergraph gets disconnected when every hyperedge fails independently with a given probability.","For graphs, the unreliability problem has been studied over many decades, and multiple fully polynomial-time approximation schemes are known starting with the work of Karger (STOC 1995).","In contrast, prior to this work, no non-trivial result was known for hypergraphs (of arbitrary rank).   ","In this paper, we give quasi-polynomial time approximation schemes for the hypergraph unreliability problem.","For any fixed $\\varepsilon \\in (0, 1)$, we first give a $(1+\\varepsilon)$-approximation algorithm that runs in $m^{O(\\log n)}$ time on an $m$-hyperedge, $n$-vertex hypergraph.","Then, we improve the running time to $m\\cdot n^{O(\\log^2 n)}$ with an additional exponentially small additive term in the approximation."],"url":"http://arxiv.org/abs/2403.18781v1"}
{"created":"2024-03-27 17:26:42","title":"3P-LLM: Probabilistic Path Planning using Large Language Model for Autonomous Robot Navigation","abstract":"Much worldly semantic knowledge can be encoded in large language models (LLMs). Such information could be of great use to robots that want to carry out high-level, temporally extended commands stated in natural language. However, the lack of real-world experience that language models have is a key limitation that makes it challenging to use them for decision-making inside a particular embodiment. This research assesses the feasibility of using LLM (GPT-3.5-turbo chatbot by OpenAI) for robotic path planning. The shortcomings of conventional approaches to managing complex environments and developing trustworthy plans for shifting environmental conditions serve as the driving force behind the research. Due to the sophisticated natural language processing abilities of LLM, the capacity to provide effective and adaptive path-planning algorithms in real-time, great accuracy, and few-shot learning capabilities, GPT-3.5-turbo is well suited for path planning in robotics. In numerous simulated scenarios, the research compares the performance of GPT-3.5-turbo with that of state-of-the-art path planners like Rapidly Exploring Random Tree (RRT) and A*. We observed that GPT-3.5-turbo is able to provide real-time path planning feedback to the robot and outperforms its counterparts. This paper establishes the foundation for LLM-powered path planning for robotic systems.","sentences":["Much worldly semantic knowledge can be encoded in large language models (LLMs).","Such information could be of great use to robots that want to carry out high-level, temporally extended commands stated in natural language.","However, the lack of real-world experience that language models have is a key limitation that makes it challenging to use them for decision-making inside a particular embodiment.","This research assesses the feasibility of using LLM (GPT-3.5-turbo chatbot by OpenAI) for robotic path planning.","The shortcomings of conventional approaches to managing complex environments and developing trustworthy plans for shifting environmental conditions serve as the driving force behind the research.","Due to the sophisticated natural language processing abilities of LLM, the capacity to provide effective and adaptive path-planning algorithms in real-time, great accuracy, and few-shot learning capabilities, GPT-3.5-turbo is well suited for path planning in robotics.","In numerous simulated scenarios, the research compares the performance of GPT-3.5-turbo with that of state-of-the-art path planners like Rapidly Exploring Random Tree (RRT) and A*.","We observed that GPT-3.5-turbo is able to provide real-time path planning feedback to the robot and outperforms its counterparts.","This paper establishes the foundation for LLM-powered path planning for robotic systems."],"url":"http://arxiv.org/abs/2403.18778v1"}
{"created":"2024-03-27 17:23:52","title":"New Graph and Hypergraph Container Lemmas with Applications in Property Testing","abstract":"The graph and hypergraph container methods are powerful tools with a wide range of applications across combinatorics. Recently, Blais and Seth (FOCS 2023) showed that the graph container method is particularly well-suited for the analysis of the natural canonical tester for two fundamental graph properties: having a large independent set and $k$-colorability. In this work, we show that the connection between the container method and property testing extends further along two different directions.   First, we show that the container method can be used to analyze the canonical tester for many other properties of graphs and hypergraphs. We introduce a new hypergraph container lemma and use it to give an upper bound of $\\widetilde{O}(kq^3/\\epsilon)$ on the sample complexity of $\\epsilon$-testing satisfiability, where $q$ is the number of variables per constraint and $k$ is the size of the alphabet. This is the first upper bound for the problem that is polynomial in all of $k$, $q$ and $1/\\epsilon$. As a corollary, we get new upper bounds on the sample complexity of the canonical testers for hypergraph colorability and for every semi-homogeneous graph partition property.   Second, we show that the container method can also be used to study the query complexity of (non-canonical) graph property testers. This result is obtained by introducing a new container lemma for the class of all independent set stars, a strict superset of the class of all independent sets. We use this container lemma to give a new upper bound of $\\widetilde{O}(\\rho^5/\\epsilon^{7/2})$ on the query complexity of $\\epsilon$-testing the $\\rho$-independent set property. This establishes for the first time the non-optimality of the canonical tester for a non-homogeneous graph partition property.","sentences":["The graph and hypergraph container methods are powerful tools with a wide range of applications across combinatorics.","Recently, Blais and Seth (FOCS 2023) showed that the graph container method is particularly well-suited for the analysis of the natural canonical tester for two fundamental graph properties: having a large independent set and $k$-colorability.","In this work, we show that the connection between the container method and property testing extends further along two different directions.   ","First, we show that the container method can be used to analyze the canonical tester for many other properties of graphs and hypergraphs.","We introduce a new hypergraph container lemma and use it to give an upper bound of $\\widetilde{O}(kq^3/\\epsilon)$ on the sample complexity of $\\epsilon$-testing satisfiability, where $q$ is the number of variables per constraint and $k$ is the size of the alphabet.","This is the first upper bound for the problem that is polynomial in all of $k$, $q$ and $1/\\epsilon$. As a corollary, we get new upper bounds on the sample complexity of the canonical testers for hypergraph colorability and for every semi-homogeneous graph partition property.   ","Second, we show that the container method can also be used to study the query complexity of (non-canonical) graph property testers.","This result is obtained by introducing a new container lemma for the class of all independent set stars, a strict superset of the class of all independent sets.","We use this container lemma to give a new upper bound of $\\widetilde{O}(\\rho^5/\\epsilon^{7/2})$ on the query complexity of $\\epsilon$-testing the $\\rho$-independent set property.","This establishes for the first time the non-optimality of the canonical tester for a non-homogeneous graph partition property."],"url":"http://arxiv.org/abs/2403.18777v1"}
{"created":"2024-03-27 17:23:39","title":"ImageNet-D: Benchmarking Neural Network Robustness on Diffusion Synthetic Object","abstract":"We establish rigorous benchmarks for visual perception robustness. Synthetic images such as ImageNet-C, ImageNet-9, and Stylized ImageNet provide specific type of evaluation over synthetic corruptions, backgrounds, and textures, yet those robustness benchmarks are restricted in specified variations and have low synthetic quality. In this work, we introduce generative model as a data source for synthesizing hard images that benchmark deep models' robustness. Leveraging diffusion models, we are able to generate images with more diversified backgrounds, textures, and materials than any prior work, where we term this benchmark as ImageNet-D. Experimental results show that ImageNet-D results in a significant accuracy drop to a range of vision models, from the standard ResNet visual classifier to the latest foundation models like CLIP and MiniGPT-4, significantly reducing their accuracy by up to 60\\%. Our work suggests that diffusion models can be an effective source to test vision models. The code and dataset are available at https://github.com/chenshuang-zhang/imagenet_d.","sentences":["We establish rigorous benchmarks for visual perception robustness.","Synthetic images such as ImageNet-C, ImageNet-9, and Stylized ImageNet provide specific type of evaluation over synthetic corruptions, backgrounds, and textures, yet those robustness benchmarks are restricted in specified variations and have low synthetic quality.","In this work, we introduce generative model as a data source for synthesizing hard images that benchmark deep models' robustness.","Leveraging diffusion models, we are able to generate images with more diversified backgrounds, textures, and materials than any prior work, where we term this benchmark as ImageNet-D. Experimental results show that ImageNet-D results in a significant accuracy drop to a range of vision models, from the standard ResNet visual classifier to the latest foundation models like CLIP and MiniGPT-4, significantly reducing their accuracy by up to 60\\%.","Our work suggests that diffusion models can be an effective source to test vision models.","The code and dataset are available at https://github.com/chenshuang-zhang/imagenet_d."],"url":"http://arxiv.org/abs/2403.18775v1"}
{"created":"2024-03-27 17:20:39","title":"CheckEval: Robust Evaluation Framework using Large Language Model via Checklist","abstract":"We introduce CheckEval, a novel evaluation framework using Large Language Models, addressing the challenges of ambiguity and inconsistency in current evaluation methods. CheckEval addresses these challenges by dividing evaluation criteria into detailed sub-aspects and constructing a checklist of Boolean questions for each, simplifying the evaluation. This approach not only renders the process more interpretable but also significantly enhances the robustness and reliability of results by focusing on specific evaluation dimensions. Validated through a focused case study using the SummEval benchmark, CheckEval indicates a strong correlation with human judgments. Furthermore, it demonstrates a highly consistent Inter-Annotator Agreement. These findings highlight the effectiveness of CheckEval for objective, flexible, and precise evaluations. By offering a customizable and interactive framework, CheckEval sets a new standard for the use of LLMs in evaluation, responding to the evolving needs of the field and establishing a clear method for future LLM-based evaluation.","sentences":["We introduce CheckEval, a novel evaluation framework using Large Language Models, addressing the challenges of ambiguity and inconsistency in current evaluation methods.","CheckEval addresses these challenges by dividing evaluation criteria into detailed sub-aspects and constructing a checklist of Boolean questions for each, simplifying the evaluation.","This approach not only renders the process more interpretable but also significantly enhances the robustness and reliability of results by focusing on specific evaluation dimensions.","Validated through a focused case study using the SummEval benchmark, CheckEval indicates a strong correlation with human judgments.","Furthermore, it demonstrates a highly consistent Inter-Annotator Agreement.","These findings highlight the effectiveness of CheckEval for objective, flexible, and precise evaluations.","By offering a customizable and interactive framework, CheckEval sets a new standard for the use of LLMs in evaluation, responding to the evolving needs of the field and establishing a clear method for future LLM-based evaluation."],"url":"http://arxiv.org/abs/2403.18771v1"}
{"created":"2024-03-27 17:13:38","title":"Improved Neural Protoform Reconstruction via Reflex Prediction","abstract":"Protolanguage reconstruction is central to historical linguistics. The comparative method, one of the most influential theoretical and methodological frameworks in the history of the language sciences, allows linguists to infer protoforms (reconstructed ancestral words) from their reflexes (related modern words) based on the assumption of regular sound change. Not surprisingly, numerous computational linguists have attempted to operationalize comparative reconstruction through various computational models, the most successful of which have been supervised encoder-decoder models, which treat the problem of predicting protoforms given sets of reflexes as a sequence-to-sequence problem. We argue that this framework ignores one of the most important aspects of the comparative method: not only should protoforms be inferable from cognate sets (sets of related reflexes) but the reflexes should also be inferable from the protoforms. Leveraging another line of research -- reflex prediction -- we propose a system in which candidate protoforms from a reconstruction model are reranked by a reflex prediction model. We show that this more complete implementation of the comparative method allows us to surpass state-of-the-art protoform reconstruction methods on three of four Chinese and Romance datasets.","sentences":["Protolanguage reconstruction is central to historical linguistics.","The comparative method, one of the most influential theoretical and methodological frameworks in the history of the language sciences, allows linguists to infer protoforms (reconstructed ancestral words) from their reflexes (related modern words) based on the assumption of regular sound change.","Not surprisingly, numerous computational linguists have attempted to operationalize comparative reconstruction through various computational models, the most successful of which have been supervised encoder-decoder models, which treat the problem of predicting protoforms given sets of reflexes as a sequence-to-sequence problem.","We argue that this framework ignores one of the most important aspects of the comparative method: not only should protoforms be inferable from cognate sets (sets of related reflexes) but the reflexes should also be inferable from the protoforms.","Leveraging another line of research -- reflex prediction -- we propose a system in which candidate protoforms from a reconstruction model are reranked by a reflex prediction model.","We show that this more complete implementation of the comparative method allows us to surpass state-of-the-art protoform reconstruction methods on three of four Chinese and Romance datasets."],"url":"http://arxiv.org/abs/2403.18769v1"}
{"created":"2024-03-27 17:05:03","title":"Superior Parallel Big Data Clustering through Competitive Stochastic Sample Size Optimization in Big-means","abstract":"This paper introduces a novel K-means clustering algorithm, an advancement on the conventional Big-means methodology. The proposed method efficiently integrates parallel processing, stochastic sampling, and competitive optimization to create a scalable variant designed for big data applications. It addresses scalability and computation time challenges typically faced with traditional techniques. The algorithm adjusts sample sizes dynamically for each worker during execution, optimizing performance. Data from these sample sizes are continually analyzed, facilitating the identification of the most efficient configuration. By incorporating a competitive element among workers using different sample sizes, efficiency within the Big-means algorithm is further stimulated. In essence, the algorithm balances computational time and clustering quality by employing a stochastic, competitive sampling strategy in a parallel computing setting.","sentences":["This paper introduces a novel K-means clustering algorithm, an advancement on the conventional Big-means methodology.","The proposed method efficiently integrates parallel processing, stochastic sampling, and competitive optimization to create a scalable variant designed for big data applications.","It addresses scalability and computation time challenges typically faced with traditional techniques.","The algorithm adjusts sample sizes dynamically for each worker during execution, optimizing performance.","Data from these sample sizes are continually analyzed, facilitating the identification of the most efficient configuration.","By incorporating a competitive element among workers using different sample sizes, efficiency within the Big-means algorithm is further stimulated.","In essence, the algorithm balances computational time and clustering quality by employing a stochastic, competitive sampling strategy in a parallel computing setting."],"url":"http://arxiv.org/abs/2403.18766v1"}
{"created":"2024-03-27 17:03:31","title":"CaT: Constraints as Terminations for Legged Locomotion Reinforcement Learning","abstract":"Deep Reinforcement Learning (RL) has demonstrated impressive results in solving complex robotic tasks such as quadruped locomotion. Yet, current solvers fail to produce efficient policies respecting hard constraints. In this work, we advocate for integrating constraints into robot learning and present Constraints as Terminations (CaT), a novel constrained RL algorithm. Departing from classical constrained RL formulations, we reformulate constraints through stochastic terminations during policy learning: any violation of a constraint triggers a probability of terminating potential future rewards the RL agent could attain. We propose an algorithmic approach to this formulation, by minimally modifying widely used off-the-shelf RL algorithms in robot learning (such as Proximal Policy Optimization). Our approach leads to excellent constraint adherence without introducing undue complexity and computational overhead, thus mitigating barriers to broader adoption. Through empirical evaluation on the real quadruped robot Solo crossing challenging obstacles, we demonstrate that CaT provides a compelling solution for incorporating constraints into RL frameworks. Videos and code are available at https://constraints-as-terminations.github.io.","sentences":["Deep Reinforcement Learning (RL) has demonstrated impressive results in solving complex robotic tasks such as quadruped locomotion.","Yet, current solvers fail to produce efficient policies respecting hard constraints.","In this work, we advocate for integrating constraints into robot learning and present Constraints as Terminations (CaT), a novel constrained RL algorithm.","Departing from classical constrained RL formulations, we reformulate constraints through stochastic terminations during policy learning: any violation of a constraint triggers a probability of terminating potential future rewards the RL agent could attain.","We propose an algorithmic approach to this formulation, by minimally modifying widely used off-the-shelf RL algorithms in robot learning (such as Proximal Policy Optimization).","Our approach leads to excellent constraint adherence without introducing undue complexity and computational overhead, thus mitigating barriers to broader adoption.","Through empirical evaluation on the real quadruped robot Solo crossing challenging obstacles, we demonstrate that CaT provides a compelling solution for incorporating constraints into RL frameworks.","Videos and code are available at https://constraints-as-terminations.github.io."],"url":"http://arxiv.org/abs/2403.18765v1"}
{"created":"2024-03-27 17:02:21","title":"Temporal Logic Formalisation of ISO 34502 Critical Scenarios: Modular Construction with the RSS Safety Distance","abstract":"As the development of autonomous vehicles progresses, efficient safety assurance methods become increasingly necessary. Safety assurance methods such as monitoring and scenario-based testing call for formalisation of driving scenarios. In this paper, we develop a temporal-logic formalisation of an important class of critical scenarios in the ISO standard 34502. We use signal temporal logic (STL) as a logical formalism. Our formalisation has two main features: 1) modular composition of logical formulas for systematic and comprehensive formalisation (following the compositional methodology of ISO 34502); 2) use of the RSS distance for defining danger. We find our formalisation comes with few parameters to tune thanks to the RSS distance. We experimentally evaluated our formalisation; using its results, we discuss the validity of our formalisation and its stability with respect to the choice of some parameter values.","sentences":["As the development of autonomous vehicles progresses, efficient safety assurance methods become increasingly necessary.","Safety assurance methods such as monitoring and scenario-based testing call for formalisation of driving scenarios.","In this paper, we develop a temporal-logic formalisation of an important class of critical scenarios in the ISO standard 34502.","We use signal temporal logic (STL) as a logical formalism.","Our formalisation has two main features: 1) modular composition of logical formulas for systematic and comprehensive formalisation (following the compositional methodology of ISO 34502); 2) use of the RSS distance for defining danger.","We find our formalisation comes with few parameters to tune thanks to the RSS distance.","We experimentally evaluated our formalisation; using its results, we discuss the validity of our formalisation and its stability with respect to the choice of some parameter values."],"url":"http://arxiv.org/abs/2403.18764v1"}
{"created":"2024-03-27 17:01:10","title":"ModaLink: Unifying Modalities for Efficient Image-to-PointCloud Place Recognition","abstract":"Place recognition is an important task for robots and autonomous cars to localize themselves and close loops in pre-built maps. While single-modal sensor-based methods have shown satisfactory performance, cross-modal place recognition that retrieving images from a point-cloud database remains a challenging problem. Current cross-modal methods transform images into 3D points using depth estimation for modality conversion, which are usually computationally intensive and need expensive labeled data for depth supervision. In this work, we introduce a fast and lightweight framework to encode images and point clouds into place-distinctive descriptors. We propose an effective Field of View (FoV) transformation module to convert point clouds into an analogous modality as images. This module eliminates the necessity for depth estimation and helps subsequent modules achieve real-time performance. We further design a non-negative factorization-based encoder to extract mutually consistent semantic features between point clouds and images. This encoder yields more distinctive global descriptors for retrieval. Experimental results on the KITTI dataset show that our proposed methods achieve state-of-the-art performance while running in real time. Additional evaluation on the HAOMO dataset covering a 17 km trajectory further shows the practical generalization capabilities. We have released the implementation of our methods as open source at: https://github.com/haomo-ai/ModaLink.git.","sentences":["Place recognition is an important task for robots and autonomous cars to localize themselves and close loops in pre-built maps.","While single-modal sensor-based methods have shown satisfactory performance, cross-modal place recognition that retrieving images from a point-cloud database remains a challenging problem.","Current cross-modal methods transform images into 3D points using depth estimation for modality conversion, which are usually computationally intensive and need expensive labeled data for depth supervision.","In this work, we introduce a fast and lightweight framework to encode images and point clouds into place-distinctive descriptors.","We propose an effective Field of View (FoV) transformation module to convert point clouds into an analogous modality as images.","This module eliminates the necessity for depth estimation and helps subsequent modules achieve real-time performance.","We further design a non-negative factorization-based encoder to extract mutually consistent semantic features between point clouds and images.","This encoder yields more distinctive global descriptors for retrieval.","Experimental results on the KITTI dataset show that our proposed methods achieve state-of-the-art performance while running in real time.","Additional evaluation on the HAOMO dataset covering a 17 km trajectory further shows the practical generalization capabilities.","We have released the implementation of our methods as open source at: https://github.com/haomo-ai/ModaLink.git."],"url":"http://arxiv.org/abs/2403.18762v1"}
{"created":"2024-03-27 16:59:21","title":"MATTopo: Topology-preserving Medial Axis Transform with Restricted Power Diagram","abstract":"We present a novel volumetric RPD (restricted power diagram) based framework for approximating the medial axes of 3D CAD shapes adaptively, while preserving topological equivalence, medial features, and geometric convergence. To solve the topology preservation problem, we propose a volumetric RPD based strategy, which discretizes the input volume into sub-regions given a set of medial spheres. With this intermediate structure, we convert the homotopy equivalence between the generated medial mesh and the input 3D shape into a localized problem between each primitive of the medial mesh (vertex, edge, face) and its dual restricted elements (power cell, power face, power edge), by checking their connected components and Euler characteristics. We further proposed a fractional Euler characteristic strategy for efficient GPU-based computation of Euler characteristic for each restricted element on the fly while computing the volumetric RPD. Compared with existing voxel-based or sampling-based methods, our method is the first that can adaptively and directly revise the medial mesh without modifying the dependent structure globally, such as voxel size or sampling density. Compared with the feature preservation method MATFP, our method offers geometrically comparable results with fewer number of spheres, while more robustly captures the topology of the input shape.","sentences":["We present a novel volumetric RPD (restricted power diagram) based framework for approximating the medial axes of 3D CAD shapes adaptively, while preserving topological equivalence, medial features, and geometric convergence.","To solve the topology preservation problem, we propose a volumetric RPD based strategy, which discretizes the input volume into sub-regions given a set of medial spheres.","With this intermediate structure, we convert the homotopy equivalence between the generated medial mesh and the input 3D shape into a localized problem between each primitive of the medial mesh (vertex, edge, face) and its dual restricted elements (power cell, power face, power edge), by checking their connected components and Euler characteristics.","We further proposed a fractional Euler characteristic strategy for efficient GPU-based computation of Euler characteristic for each restricted element on the fly while computing the volumetric RPD.","Compared with existing voxel-based or sampling-based methods, our method is the first that can adaptively and directly revise the medial mesh without modifying the dependent structure globally, such as voxel size or sampling density.","Compared with the feature preservation method MATFP, our method offers geometrically comparable results with fewer number of spheres, while more robustly captures the topology of the input shape."],"url":"http://arxiv.org/abs/2403.18761v1"}
{"created":"2024-03-27 16:58:20","title":"MLDT: Multi-Level Decomposition for Complex Long-Horizon Robotic Task Planning with Open-Source Large Language Model","abstract":"In the realm of data-driven AI technology, the application of open-source large language models (LLMs) in robotic task planning represents a significant milestone. Recent robotic task planning methods based on open-source LLMs typically leverage vast task planning datasets to enhance models' planning abilities. While these methods show promise, they struggle with complex long-horizon tasks, which require comprehending more context and generating longer action sequences. This paper addresses this limitation by proposing MLDT, theMulti-Level Decomposition Task planning method. This method innovatively decomposes tasks at the goal-level, task-level, and action-level to mitigate the challenge of complex long-horizon tasks. In order to enhance open-source LLMs' planning abilities, we introduce a goal-sensitive corpus generation method to create high-quality training data and conduct instruction tuning on the generated corpus. Since the complexity of the existing datasets is not high enough, we construct a more challenging dataset, LongTasks, to specifically evaluate planning ability on complex long-horizon tasks. We evaluate our method using various LLMs on four datasets in VirtualHome. Our results demonstrate a significant performance enhancement in robotic task planning, showcasing MLDT's effectiveness in overcoming the limitations of existing methods based on open-source LLMs as well as its practicality in complex, real-world scenarios.","sentences":["In the realm of data-driven AI technology, the application of open-source large language models (LLMs) in robotic task planning represents a significant milestone.","Recent robotic task planning methods based on open-source LLMs typically leverage vast task planning datasets to enhance models' planning abilities.","While these methods show promise, they struggle with complex long-horizon tasks, which require comprehending more context and generating longer action sequences.","This paper addresses this limitation by proposing MLDT, theMulti-Level Decomposition Task planning method.","This method innovatively decomposes tasks at the goal-level, task-level, and action-level to mitigate the challenge of complex long-horizon tasks.","In order to enhance open-source LLMs' planning abilities, we introduce a goal-sensitive corpus generation method to create high-quality training data and conduct instruction tuning on the generated corpus.","Since the complexity of the existing datasets is not high enough, we construct a more challenging dataset, LongTasks, to specifically evaluate planning ability on complex long-horizon tasks.","We evaluate our method using various LLMs on four datasets in VirtualHome.","Our results demonstrate a significant performance enhancement in robotic task planning, showcasing MLDT's effectiveness in overcoming the limitations of existing methods based on open-source LLMs as well as its practicality in complex, real-world scenarios."],"url":"http://arxiv.org/abs/2403.18760v1"}
{"created":"2024-03-27 16:56:14","title":"Detection of subclinical atherosclerosis by image-based deep learning on chest x-ray","abstract":"Aims. To develop a deep-learning based system for recognition of subclinical atherosclerosis on a plain frontal chest x-ray. Methods and Results. A deep-learning algorithm to predict coronary artery calcium (CAC) score (the AI-CAC model) was developed on 460 chest x-ray (80% training cohort, 20% internal validation cohort) of primary prevention patients (58.4% male, median age 63 [51-74] years) with available paired chest x-ray and chest computed tomography (CT) indicated for any clinical reason and performed within 3 months. The CAC score calculated on chest CT was used as ground truth. The model was validated on an temporally-independent cohort of 90 patients from the same institution (external validation). The diagnostic accuracy of the AI-CAC model assessed by the area under the curve (AUC) was the primary outcome. Overall, median AI-CAC score was 35 (0-388) and 28.9% patients had no AI-CAC. AUC of the AI-CAC model to identify a CAC>0 was 0.90 in the internal validation cohort and 0.77 in the external validation cohort. Sensitivity was consistently above 92% in both cohorts. In the overall cohort (n=540), among patients with AI-CAC=0, a single ASCVD event occurred, after 4.3 years. Patients with AI-CAC>0 had significantly higher Kaplan Meier estimates for ASCVD events (13.5% vs. 3.4%, log-rank=0.013). Conclusion. The AI-CAC model seems to accurately detect subclinical atherosclerosis on chest x-ray with elevated sensitivity, and to predict ASCVD events with elevated negative predictive value. Adoption of the AI-CAC model to refine CV risk stratification or as an opportunistic screening tool requires prospective evaluation.","sentences":["Aims.","To develop a deep-learning based system for recognition of subclinical atherosclerosis on a plain frontal chest x-ray.","Methods and Results.","A deep-learning algorithm to predict coronary artery calcium (CAC) score (the AI-CAC model) was developed on 460 chest x-ray (80% training cohort, 20% internal validation cohort) of primary prevention patients (58.4% male, median age 63","[51-74] years) with available paired chest x-ray and chest computed tomography (CT) indicated for any clinical reason and performed within 3 months.","The CAC score calculated on chest CT was used as ground truth.","The model was validated on an temporally-independent cohort of 90 patients from the same institution (external validation).","The diagnostic accuracy of the AI-CAC model assessed by the area under the curve (AUC) was the primary outcome.","Overall, median AI-CAC score was 35 (0-388) and 28.9% patients had no AI-CAC.","AUC of the AI-CAC model to identify a CAC>0 was 0.90 in the internal validation cohort and 0.77 in the external validation cohort.","Sensitivity was consistently above 92% in both cohorts.","In the overall cohort (n=540), among patients with AI-CAC=0, a single ASCVD event occurred, after 4.3 years.","Patients with AI-CAC>0 had significantly higher Kaplan Meier estimates for ASCVD events (13.5% vs. 3.4%, log-rank=0.013).","Conclusion.","The AI-CAC model seems to accurately detect subclinical atherosclerosis on chest x-ray with elevated sensitivity, and to predict ASCVD events with elevated negative predictive value.","Adoption of the AI-CAC model to refine CV risk stratification or as an opportunistic screening tool requires prospective evaluation."],"url":"http://arxiv.org/abs/2403.18756v1"}
{"created":"2024-03-27 16:54:45","title":"Many-Objective Evolutionary Influence Maximization: Balancing Spread, Budget, Fairness, and Time","abstract":"The Influence Maximization (IM) problem seeks to discover the set of nodes in a graph that can spread the information propagation at most. This problem is known to be NP-hard, and it is usually studied by maximizing the influence (spread) and, optionally, optimizing a second objective, such as minimizing the seed set size or maximizing the influence fairness. However, in many practical scenarios multiple aspects of the IM problem must be optimized at the same time. In this work, we propose a first case study where several IM-specific objective functions, namely budget, fairness, communities, and time, are optimized on top of the maximization of influence and minimization of the seed set size. To this aim, we introduce MOEIM (Many-Objective Evolutionary Algorithm for Influence Maximization) a Multi-Objective Evolutionary Algorithm (MOEA) based on NSGA-II incorporating graph-aware operators and a smart initialization. We compare MOEIM in two experimental settings, including a total of nine graph datasets, two heuristic methods, a related MOEA, and a state-of-the-art Deep Learning approach. The experiments show that MOEIM overall outperforms the competitors in most of the tested many-objective settings. To conclude, we also investigate the correlation between the objectives, leading to novel insights into the topic. The codebase is available at https://github.com/eliacunegatti/MOEIM.","sentences":["The Influence Maximization (IM) problem seeks to discover the set of nodes in a graph that can spread the information propagation at most.","This problem is known to be NP-hard, and it is usually studied by maximizing the influence (spread) and, optionally, optimizing a second objective, such as minimizing the seed set size or maximizing the influence fairness.","However, in many practical scenarios multiple aspects of the IM problem must be optimized at the same time.","In this work, we propose a first case study where several IM-specific objective functions, namely budget, fairness, communities, and time, are optimized on top of the maximization of influence and minimization of the seed set size.","To this aim, we introduce MOEIM (Many-Objective Evolutionary Algorithm for Influence Maximization) a Multi-Objective Evolutionary Algorithm (MOEA) based on NSGA-II incorporating graph-aware operators and a smart initialization.","We compare MOEIM in two experimental settings, including a total of nine graph datasets, two heuristic methods, a related MOEA, and a state-of-the-art Deep Learning approach.","The experiments show that MOEIM overall outperforms the competitors in most of the tested many-objective settings.","To conclude, we also investigate the correlation between the objectives, leading to novel insights into the topic.","The codebase is available at https://github.com/eliacunegatti/MOEIM."],"url":"http://arxiv.org/abs/2403.18755v1"}
{"created":"2024-03-27 16:45:02","title":"CYCLE: Learning to Self-Refine the Code Generation","abstract":"Pre-trained code language models have achieved promising performance in code generation and improved the programming efficiency of human developers. However, their self-refinement capability is typically overlooked by the existing evaluations of code LMs, which focus only on the accuracy of the one-time prediction. For the cases when code LMs fail to implement the correct program, developers actually find it hard to debug and fix the faulty prediction since it is not written by the developers themselves. Unfortunately, our study reveals that code LMs cannot efficiently self-refine their faulty generations as well.   In this paper, we propose CYCLE framework, learning to self-refine the faulty generation according to the available feedback, such as the execution results reported by the test suites. We evaluate CYCLE on three popular code generation benchmarks, HumanEval, MBPP, and APPS. The results reveal that CYCLE successfully maintains, sometimes improves, the quality of one-time code generation, while significantly improving the self-refinement capability of code LMs. We implement four variants of CYCLE with varied numbers of parameters across 350M, 1B, 2B, and 3B, and the experiments show that CYCLE consistently boosts the code generation performance, by up to 63.5%, across benchmarks and varied model sizes. We also notice that CYCLE outperforms code LMs that have 3$\\times$ more parameters in self-refinement.","sentences":["Pre-trained code language models have achieved promising performance in code generation and improved the programming efficiency of human developers.","However, their self-refinement capability is typically overlooked by the existing evaluations of code LMs, which focus only on the accuracy of the one-time prediction.","For the cases when code LMs fail to implement the correct program, developers actually find it hard to debug and fix the faulty prediction since it is not written by the developers themselves.","Unfortunately, our study reveals that code LMs cannot efficiently self-refine their faulty generations as well.   ","In this paper, we propose CYCLE framework, learning to self-refine the faulty generation according to the available feedback, such as the execution results reported by the test suites.","We evaluate CYCLE on three popular code generation benchmarks, HumanEval, MBPP, and APPS.","The results reveal that CYCLE successfully maintains, sometimes improves, the quality of one-time code generation, while significantly improving the self-refinement capability of code LMs.","We implement four variants of CYCLE with varied numbers of parameters across 350M, 1B, 2B, and 3B, and the experiments show that CYCLE consistently boosts the code generation performance, by up to 63.5%, across benchmarks and varied model sizes.","We also notice that CYCLE outperforms code LMs that have 3$\\times$ more parameters in self-refinement."],"url":"http://arxiv.org/abs/2403.18746v1"}
{"created":"2024-03-27 16:44:39","title":"Fast Decision Algorithms for Efficient Access Point Assignment in SDN-Controlled Wireless Access Networks","abstract":"Global optimization of access point (AP) assignment to user terminals requires efficient monitoring of user behavior, fast decision algorithms, efficient control signaling, and fast AP reassignment mechanisms. In this scenario, software defined networking (SDN) technology may be suitable for network monitoring, signaling, and control. We recently proposed embedding virtual switches in user terminals for direct management by an SDN controller, further contributing to SDN-oriented access network optimization. However, since users may restrict terminal-side traffic monitoring for privacy reasons (a common assumption by previous authors), we infer user traffic classes at the APs. On the other hand, since handovers will be more frequent in dense small-cell networks (e.g., mmWave-based 5G deployments will require dense network topologies with inter-site distances of ~150-200 m), the delay to take assignment decisions should be minimal. To this end, we propose taking fast decisions based exclusively on extremely simple network-side application flow-type predictions based on past user behavior. Using real data we show that a centralized allocation algorithm based on those predictions achieves network utilization levels that approximate those of optimal allocations. We also test a distributed version of this algorithm. Finally, we quantify the elapsed time since a user traffic event takes place until its terminal is assigned an AP, when needed.","sentences":["Global optimization of access point (AP) assignment to user terminals requires efficient monitoring of user behavior, fast decision algorithms, efficient control signaling, and fast AP reassignment mechanisms.","In this scenario, software defined networking (SDN) technology may be suitable for network monitoring, signaling, and control.","We recently proposed embedding virtual switches in user terminals for direct management by an SDN controller, further contributing to SDN-oriented access network optimization.","However, since users may restrict terminal-side traffic monitoring for privacy reasons (a common assumption by previous authors), we infer user traffic classes at the APs.","On the other hand, since handovers will be more frequent in dense small-cell networks (e.g., mmWave-based 5G deployments will require dense network topologies with inter-site distances of ~150-200 m), the delay to take assignment decisions should be minimal.","To this end, we propose taking fast decisions based exclusively on extremely simple network-side application flow-type predictions based on past user behavior.","Using real data we show that a centralized allocation algorithm based on those predictions achieves network utilization levels that approximate those of optimal allocations.","We also test a distributed version of this algorithm.","Finally, we quantify the elapsed time since a user traffic event takes place until its terminal is assigned an AP, when needed."],"url":"http://arxiv.org/abs/2403.18745v1"}
{"created":"2024-03-27 16:39:28","title":"Understanding the Learning Dynamics of Alignment with Human Feedback","abstract":"Aligning large language models (LLMs) with human intentions has become a critical task for safely deploying models in real-world systems. While existing alignment approaches have seen empirical success, theoretically understanding how these methods affect model behavior remains an open question. Our work provides an initial attempt to theoretically analyze the learning dynamics of human preference alignment. We formally show how the distribution of preference datasets influences the rate of model updates and provide rigorous guarantees on the training accuracy. Our theory also reveals an intricate phenomenon where the optimization is prone to prioritizing certain behaviors with higher preference distinguishability. We empirically validate our findings on contemporary LLMs and alignment tasks, reinforcing our theoretical insights and shedding light on considerations for future alignment approaches. Disclaimer: This paper contains potentially offensive text; reader discretion is advised.","sentences":["Aligning large language models (LLMs) with human intentions has become a critical task for safely deploying models in real-world systems.","While existing alignment approaches have seen empirical success, theoretically understanding how these methods affect model behavior remains an open question.","Our work provides an initial attempt to theoretically analyze the learning dynamics of human preference alignment.","We formally show how the distribution of preference datasets influences the rate of model updates and provide rigorous guarantees on the training accuracy.","Our theory also reveals an intricate phenomenon where the optimization is prone to prioritizing certain behaviors with higher preference distinguishability.","We empirically validate our findings on contemporary LLMs and alignment tasks, reinforcing our theoretical insights and shedding light on considerations for future alignment approaches.","Disclaimer:","This paper contains potentially offensive text; reader discretion is advised."],"url":"http://arxiv.org/abs/2403.18742v1"}
{"created":"2024-03-27 16:32:32","title":"Usage-Specific Survival Modeling Based on Operational Data and Neural Networks","abstract":"Accurate predictions of when a component will fail are crucial when planning maintenance, and by modeling the distribution of these failure times, survival models have shown to be particularly useful in this context. The presented methodology is based on conventional neural network-based survival models that are trained using data that is continuously gathered and stored at specific times, called snapshots. An important property of this type of training data is that it can contain more than one snapshot from a specific individual which results in that standard maximum likelihood training can not be directly applied since the data is not independent. However, the papers show that if the data is in a specific format where all snapshot times are the same for all individuals, called homogeneously sampled, maximum likelihood training can be applied and produce desirable results. In many cases, the data is not homogeneously sampled and in this case, it is proposed to resample the data to make it homogeneously sampled. How densely the dataset is sampled turns out to be an important parameter; it should be chosen large enough to produce good results, but this also increases the size of the dataset which makes training slow. To reduce the number of samples needed during training, the paper also proposes a technique to, instead of resampling the dataset once before the training starts, randomly resample the dataset at the start of each epoch during the training. The proposed methodology is evaluated on both a simulated dataset and an experimental dataset of starter battery failures. The results show that if the data is homogeneously sampled the methodology works as intended and produces accurate survival models. The results also show that randomly resampling the dataset on each epoch is an effective way to reduce the size of the training data.","sentences":["Accurate predictions of when a component will fail are crucial when planning maintenance, and by modeling the distribution of these failure times, survival models have shown to be particularly useful in this context.","The presented methodology is based on conventional neural network-based survival models that are trained using data that is continuously gathered and stored at specific times, called snapshots.","An important property of this type of training data is that it can contain more than one snapshot from a specific individual which results in that standard maximum likelihood training can not be directly applied since the data is not independent.","However, the papers show that if the data is in a specific format where all snapshot times are the same for all individuals, called homogeneously sampled, maximum likelihood training can be applied and produce desirable results.","In many cases, the data is not homogeneously sampled and in this case, it is proposed to resample the data to make it homogeneously sampled.","How densely the dataset is sampled turns out to be an important parameter; it should be chosen large enough to produce good results, but this also increases the size of the dataset which makes training slow.","To reduce the number of samples needed during training, the paper also proposes a technique to, instead of resampling the dataset once before the training starts, randomly resample the dataset at the start of each epoch during the training.","The proposed methodology is evaluated on both a simulated dataset and an experimental dataset of starter battery failures.","The results show that if the data is homogeneously sampled the methodology works as intended and produces accurate survival models.","The results also show that randomly resampling the dataset on each epoch is an effective way to reduce the size of the training data."],"url":"http://arxiv.org/abs/2403.18739v1"}
{"created":"2024-03-27 16:24:26","title":"Nonlinear model reduction for operator learning","abstract":"Operator learning provides methods to approximate mappings between infinite-dimensional function spaces. Deep operator networks (DeepONets) are a notable architecture in this field. Recently, an extension of DeepONet based on model reduction and neural networks, proper orthogonal decomposition (POD)-DeepONet, has been able to outperform other architectures in terms of accuracy for several benchmark tests. We extend this idea towards nonlinear model order reduction by proposing an efficient framework that combines neural networks with kernel principal component analysis (KPCA) for operator learning. Our results demonstrate the superior performance of KPCA-DeepONet over POD-DeepONet.","sentences":["Operator learning provides methods to approximate mappings between infinite-dimensional function spaces.","Deep operator networks (DeepONets) are a notable architecture in this field.","Recently, an extension of DeepONet based on model reduction and neural networks, proper orthogonal decomposition (POD)-DeepONet, has been able to outperform other architectures in terms of accuracy for several benchmark tests.","We extend this idea towards nonlinear model order reduction by proposing an efficient framework that combines neural networks with kernel principal component analysis (KPCA) for operator learning.","Our results demonstrate the superior performance of KPCA-DeepONet over POD-DeepONet."],"url":"http://arxiv.org/abs/2403.18735v1"}
{"created":"2024-03-27 16:21:24","title":"Enhancing Manufacturing Quality Prediction Models through the Integration of Explainability Methods","abstract":"This research presents a method that utilizes explainability techniques to amplify the performance of machine learning (ML) models in forecasting the quality of milling processes, as demonstrated in this paper through a manufacturing use case. The methodology entails the initial training of ML models, followed by a fine-tuning phase where irrelevant features identified through explainability methods are eliminated. This procedural refinement results in performance enhancements, paving the way for potential reductions in manufacturing costs and a better understanding of the trained ML models. This study highlights the usefulness of explainability techniques in both explaining and optimizing predictive models in the manufacturing realm.","sentences":["This research presents a method that utilizes explainability techniques to amplify the performance of machine learning (ML) models in forecasting the quality of milling processes, as demonstrated in this paper through a manufacturing use case.","The methodology entails the initial training of ML models, followed by a fine-tuning phase where irrelevant features identified through explainability methods are eliminated.","This procedural refinement results in performance enhancements, paving the way for potential reductions in manufacturing costs and a better understanding of the trained ML models.","This study highlights the usefulness of explainability techniques in both explaining and optimizing predictive models in the manufacturing realm."],"url":"http://arxiv.org/abs/2403.18731v1"}
{"created":"2024-03-27 16:20:55","title":"Towards Image Ambient Lighting Normalization","abstract":"Lighting normalization is a crucial but underexplored restoration task with broad applications. However, existing works often simplify this task within the context of shadow removal, limiting the light sources to one and oversimplifying the scene, thus excluding complex self-shadows and restricting surface classes to smooth ones. Although promising, such simplifications hinder generalizability to more realistic settings encountered in daily use. In this paper, we propose a new challenging task termed Ambient Lighting Normalization (ALN), which enables the study of interactions between shadows, unifying image restoration and shadow removal in a broader context. To address the lack of appropriate datasets for ALN, we introduce the large-scale high-resolution dataset Ambient6K, comprising samples obtained from multiple light sources and including self-shadows resulting from complex geometries, which is the first of its kind. For benchmarking, we select various mainstream methods and rigorously evaluate them on Ambient6K. Additionally, we propose IFBlend, a novel strong baseline that maximizes Image-Frequency joint entropy to selectively restore local areas under different lighting conditions, without relying on shadow localization priors. Experiments show that IFBlend achieves SOTA scores on Ambient6K and exhibits competitive performance on conventional shadow removal benchmarks compared to shadow-specific models with mask priors. The dataset, benchmark, and code are available at https://github.com/fvasluianu97/IFBlend.","sentences":["Lighting normalization is a crucial but underexplored restoration task with broad applications.","However, existing works often simplify this task within the context of shadow removal, limiting the light sources to one and oversimplifying the scene, thus excluding complex self-shadows and restricting surface classes to smooth ones.","Although promising, such simplifications hinder generalizability to more realistic settings encountered in daily use.","In this paper, we propose a new challenging task termed Ambient Lighting Normalization (ALN), which enables the study of interactions between shadows, unifying image restoration and shadow removal in a broader context.","To address the lack of appropriate datasets for ALN, we introduce the large-scale high-resolution dataset Ambient6K, comprising samples obtained from multiple light sources and including self-shadows resulting from complex geometries, which is the first of its kind.","For benchmarking, we select various mainstream methods and rigorously evaluate them on Ambient6K. Additionally, we propose IFBlend, a novel strong baseline that maximizes Image-Frequency joint entropy to selectively restore local areas under different lighting conditions, without relying on shadow localization priors.","Experiments show that IFBlend achieves SOTA scores on Ambient6K and exhibits competitive performance on conventional shadow removal benchmarks compared to shadow-specific models with mask priors.","The dataset, benchmark, and code are available at https://github.com/fvasluianu97/IFBlend."],"url":"http://arxiv.org/abs/2403.18730v1"}
{"created":"2024-03-27 16:20:08","title":"ConstraintFlow: A DSL for Specification and Verification of Neural Network Analyses","abstract":"The uninterpretability of DNNs hinders their deployment to safety-critical applications. Recent works have shown that Abstract-Interpretation-based formal certification techniques provide promising avenues for building trust in DNNs to some extent. The intricate mathematical background of Abstract Interpretation poses two challenges: (i) easily designing the algorithms that capture the intricate DNN behavior by balancing cost vs. precision tradeoff, and (ii) maintaining the over-approximation-based soundness of these certifiers.   General-purpose programming languages like C++ provide extensive functionality, however, verifying the soundness of the algorithms written in them can be impractical. The most commonly used DNN certification libraries like auto_LiRPA and ERAN prove the correctness of their analyses. However, they consist of only a few hard-coded abstract domains and abstract transformers (or transfer functions) and do not allow the user to define new analyses. Further, these libraries can handle only specific DNN architectures.   To address these issues, we develop a declarative DSL -- ConstraintFlow -- that can be used to specify Abstract Interpretation-based DNN certifiers. In ConstraintFlow, programmers can easily define various existing and new abstract domains and transformers, all within just a few 10s of Lines of Code as opposed to 1000s of LOCs of existing libraries. We also provide lightweight automatic verification, which can be used to ensure the over-approximation-based soundness of the certifier code written in ConstraintFlow for arbitrary (but bounded) DNN architectures. Using this automated verification procedure, for the first time, we can verify the soundness of state-of-the-art DNN certifiers for arbitrary DNN architectures, all within a few minutes. We prove the soundness of our verification procedure and the completeness of a subset of ConstraintFlow.","sentences":["The uninterpretability of DNNs hinders their deployment to safety-critical applications.","Recent works have shown that Abstract-Interpretation-based formal certification techniques provide promising avenues for building trust in DNNs to some extent.","The intricate mathematical background of Abstract Interpretation poses two challenges: (i) easily designing the algorithms that capture the intricate DNN behavior by balancing cost vs. precision tradeoff, and (ii) maintaining the over-approximation-based soundness of these certifiers.   ","General-purpose programming languages like C++ provide extensive functionality, however, verifying the soundness of the algorithms written in them can be impractical.","The most commonly used DNN certification libraries like auto_LiRPA and ERAN prove the correctness of their analyses.","However, they consist of only a few hard-coded abstract domains and abstract transformers (or transfer functions) and do not allow the user to define new analyses.","Further, these libraries can handle only specific DNN architectures.   ","To address these issues, we develop a declarative DSL -- ConstraintFlow -- that can be used to specify Abstract Interpretation-based DNN certifiers.","In ConstraintFlow, programmers can easily define various existing and new abstract domains and transformers, all within just a few 10s of Lines of Code as opposed to 1000s of LOCs of existing libraries.","We also provide lightweight automatic verification, which can be used to ensure the over-approximation-based soundness of the certifier code written in ConstraintFlow for arbitrary (but bounded) DNN architectures.","Using this automated verification procedure, for the first time, we can verify the soundness of state-of-the-art DNN certifiers for arbitrary DNN architectures, all within a few minutes.","We prove the soundness of our verification procedure and the completeness of a subset of ConstraintFlow."],"url":"http://arxiv.org/abs/2403.18729v1"}
{"created":"2024-03-27 16:15:21","title":"Probabilistic Model Checking of Stochastic Reinforcement Learning Policies","abstract":"We introduce a method to verify stochastic reinforcement learning (RL) policies. This approach is compatible with any RL algorithm as long as the algorithm and its corresponding environment collectively adhere to the Markov property. In this setting, the future state of the environment should depend solely on its current state and the action executed, independent of any previous states or actions. Our method integrates a verification technique, referred to as model checking, with RL, leveraging a Markov decision process, a trained RL policy, and a probabilistic computation tree logic (PCTL) formula to build a formal model that can be subsequently verified via the model checker Storm. We demonstrate our method's applicability across multiple benchmarks, comparing it to baseline methods called deterministic safety estimates and naive monolithic model checking. Our results show that our method is suited to verify stochastic RL policies.","sentences":["We introduce a method to verify stochastic reinforcement learning (RL) policies.","This approach is compatible with any RL algorithm as long as the algorithm and its corresponding environment collectively adhere to the Markov property.","In this setting, the future state of the environment should depend solely on its current state and the action executed, independent of any previous states or actions.","Our method integrates a verification technique, referred to as model checking, with RL, leveraging a Markov decision process, a trained RL policy, and a probabilistic computation tree logic (PCTL) formula to build a formal model that can be subsequently verified via the model checker Storm.","We demonstrate our method's applicability across multiple benchmarks, comparing it to baseline methods called deterministic safety estimates and naive monolithic model checking.","Our results show that our method is suited to verify stochastic RL policies."],"url":"http://arxiv.org/abs/2403.18725v1"}
{"created":"2024-03-27 16:13:41","title":"Four Formal Models of IEEE 1394 Link Layer","abstract":"We revisit the IEEE 1394 high-performance serial bus (\"FireWire\"), which became a success story in formal methods after three PhD students, by using process algebra and model checking, detected a deadlock error in this IEEE standard. We present four formal models for the asynchronous mode of the Link Layer of IEEE 1394: the original model in muCRL, a simplified model in mCRL2, a revised model in LOTOS, and a novel model in LNT.","sentences":["We revisit the IEEE 1394 high-performance serial bus (\"FireWire\"), which became a success story in formal methods after three PhD students, by using process algebra and model checking, detected a deadlock error in this IEEE standard.","We present four formal models for the asynchronous mode of the Link Layer of IEEE 1394: the original model in muCRL, a simplified model in mCRL2, a revised model in LOTOS, and a novel model in LNT."],"url":"http://arxiv.org/abs/2403.18723v1"}
{"created":"2024-03-27 16:13:12","title":"Formally Modelling the Rijkswaterstaat Tunnel Control Systems in a Constrained Industrial Environment","abstract":"Rijkswaterstaat, the National Dutch body responsible for infrastructure, recognised the importance of formal modelling and set up a program to model the control of road tunnels. This is done to improve the standardisation of tunnel control and make communication with suppliers smoother. A subset of SysML is used to formulate the models, which are substantial. In an earlier paper we have shown that these models can be used to prove behavioural properties by manually translating the models to mCRL2. In this paper we report on an automatic translation to mCRL2. As the results of the translation became unwieldy, we also investigated modelling tunnel control in the specification language Dezyne which has built-in verification capabilities and compared the results.","sentences":["Rijkswaterstaat, the National Dutch body responsible for infrastructure, recognised the importance of formal modelling and set up a program to model the control of road tunnels.","This is done to improve the standardisation of tunnel control and make communication with suppliers smoother.","A subset of SysML is used to formulate the models, which are substantial.","In an earlier paper we have shown that these models can be used to prove behavioural properties by manually translating the models to mCRL2.","In this paper we report on an automatic translation to mCRL2.","As the results of the translation became unwieldy, we also investigated modelling tunnel control in the specification language Dezyne which has built-in verification capabilities and compared the results."],"url":"http://arxiv.org/abs/2403.18722v1"}
{"created":"2024-03-27 16:11:49","title":"PhysicsAssistant: An LLM-Powered Interactive Learning Robot for Physics Lab Investigations","abstract":"Robot systems in education can leverage Large language models' (LLMs) natural language understanding capabilities to provide assistance and facilitate learning. This paper proposes a multimodal interactive robot (PhysicsAssistant) built on YOLOv8 object detection, cameras, speech recognition, and chatbot using LLM to provide assistance to students' physics labs. We conduct a user study on ten 8th-grade students to empirically evaluate the performance of PhysicsAssistant with a human expert. The Expert rates the assistants' responses to student queries on a 0-4 scale based on Bloom's taxonomy to provide educational support. We have compared the performance of PhysicsAssistant (YOLOv8+GPT-3.5-turbo) with GPT-4 and found that the human expert rating of both systems for factual understanding is the same. However, the rating of GPT-4 for conceptual and procedural knowledge (3 and 3.2 vs 2.2 and 2.6, respectively) is significantly higher than PhysicsAssistant (p < 0.05). However, the response time of GPT-4 is significantly higher than PhysicsAssistant (3.54 vs 1.64 sec, p < 0.05). Hence, despite the relatively lower response quality of PhysicsAssistant than GPT-4, it has shown potential for being used as a real-time lab assistant to provide timely responses and can offload teachers' labor to assist with repetitive tasks. To the best of our knowledge, this is the first attempt to build such an interactive multimodal robotic assistant for K-12 science (physics) education.","sentences":["Robot systems in education can leverage Large language models' (LLMs) natural language understanding capabilities to provide assistance and facilitate learning.","This paper proposes a multimodal interactive robot (PhysicsAssistant) built on YOLOv8 object detection, cameras, speech recognition, and chatbot using LLM to provide assistance to students' physics labs.","We conduct a user study on ten 8th-grade students to empirically evaluate the performance of PhysicsAssistant with a human expert.","The Expert rates the assistants' responses to student queries on a 0-4 scale based on Bloom's taxonomy to provide educational support.","We have compared the performance of PhysicsAssistant (YOLOv8+GPT-3.5-turbo) with GPT-4 and found that the human expert rating of both systems for factual understanding is the same.","However, the rating of GPT-4 for conceptual and procedural knowledge (3 and 3.2 vs 2.2 and 2.6, respectively) is significantly higher than PhysicsAssistant (p < 0.05).","However, the response time of GPT-4 is significantly higher than PhysicsAssistant (3.54 vs 1.64 sec, p < 0.05).","Hence, despite the relatively lower response quality of PhysicsAssistant than GPT-4, it has shown potential for being used as a real-time lab assistant to provide timely responses and can offload teachers' labor to assist with repetitive tasks.","To the best of our knowledge, this is the first attempt to build such an interactive multimodal robotic assistant for K-12 science (physics) education."],"url":"http://arxiv.org/abs/2403.18721v1"}
{"created":"2024-03-27 16:11:23","title":"Testing Resource Isolation for System-on-Chip Architectures","abstract":"Ensuring resource isolation at the hardware level is a crucial step towards more security inside the Internet of Things. Even though there is still no generally accepted technique to generate appropriate tests, it became clear that tests should be generated at the system level. In this paper, we illustrate the modeling aspects in test generation for resource isolation, namely modeling the behavior and expressing the intended test scenario. We present both aspects using the industrial standard PSS and an academic approach based on conformance testing.","sentences":["Ensuring resource isolation at the hardware level is a crucial step towards more security inside the Internet of Things.","Even though there is still no generally accepted technique to generate appropriate tests, it became clear that tests should be generated at the system level.","In this paper, we illustrate the modeling aspects in test generation for resource isolation, namely modeling the behavior and expressing the intended test scenario.","We present both aspects using the industrial standard PSS and an academic approach based on conformance testing."],"url":"http://arxiv.org/abs/2403.18720v1"}
{"created":"2024-03-27 16:06:37","title":"Semi-Supervised Learning for Deep Causal Generative Models","abstract":"Developing models that can answer questions of the form \"How would $x$ change if $y$ had been $z$?\" is fundamental for advancing medical image analysis. Training causal generative models that address such counterfactual questions, though, currently requires that all relevant variables have been observed and that corresponding labels are available in training data. However, clinical data may not have complete records for all patients and state of the art causal generative models are unable to take full advantage of this. We thus develop, for the first time, a semi-supervised deep causal generative model that exploits the causal relationships between variables to maximise the use of all available data. We explore this in the setting where each sample is either fully labelled or fully unlabelled, as well as the more clinically realistic case of having different labels missing for each sample. We leverage techniques from causal inference to infer missing values and subsequently generate realistic counterfactuals, even for samples with incomplete labels.","sentences":["Developing models that can answer questions of the form \"How would $x$ change if $y$ had been $z$?\" is fundamental for advancing medical image analysis.","Training causal generative models that address such counterfactual questions, though, currently requires that all relevant variables have been observed and that corresponding labels are available in training data.","However, clinical data may not have complete records for all patients and state of the art causal generative models are unable to take full advantage of this.","We thus develop, for the first time, a semi-supervised deep causal generative model that exploits the causal relationships between variables to maximise the use of all available data.","We explore this in the setting where each sample is either fully labelled or fully unlabelled, as well as the more clinically realistic case of having different labels missing for each sample.","We leverage techniques from causal inference to infer missing values and subsequently generate realistic counterfactuals, even for samples with incomplete labels."],"url":"http://arxiv.org/abs/2403.18717v1"}
{"created":"2024-03-27 16:05:02","title":"Statistical testing of random number generators and their improvement using randomness extraction","abstract":"Random number generators (RNGs) are notoriously hard to build and test, especially in a cryptographic setting. Although one cannot conclusively determine the quality of an RNG by testing the statistical properties of its output alone, running numerical tests is both a powerful verification tool and the only universally applicable method. In this work, we present and make available a comprehensive statistical testing environment (STE) that is based on existing statistical test suites. The STE can be parameterised to run lightweight (i.e. fast) all the way to intensive testing, which goes far beyond what is required by certification bodies. With it, we benchmark the statistical properties of several RNGs, comparing them against each other. We then present and implement a variety of post-processing methods, in the form of randomness extractors, which improve the RNG's output quality under different sets of assumptions and analyse their impact through numerical testing with the STE.","sentences":["Random number generators (RNGs) are notoriously hard to build and test, especially in a cryptographic setting.","Although one cannot conclusively determine the quality of an RNG by testing the statistical properties of its output alone, running numerical tests is both a powerful verification tool and the only universally applicable method.","In this work, we present and make available a comprehensive statistical testing environment (STE) that is based on existing statistical test suites.","The STE can be parameterised to run lightweight (i.e. fast) all the way to intensive testing, which goes far beyond what is required by certification bodies.","With it, we benchmark the statistical properties of several RNGs, comparing them against each other.","We then present and implement a variety of post-processing methods, in the form of randomness extractors, which improve the RNG's output quality under different sets of assumptions and analyse their impact through numerical testing with the STE."],"url":"http://arxiv.org/abs/2403.18716v1"}
{"created":"2024-03-27 16:04:47","title":"Mitigating Hallucinations in Large Vision-Language Models with Instruction Contrastive Decoding","abstract":"Large Vision-Language Models (LVLMs) are increasingly adept at generating contextually detailed and coherent responses from visual inputs. However, their application in multimodal decision-making and open-ended generation is hindered by a notable rate of hallucinations, where generated text inaccurately represents the visual contents. To address this issue, this paper introduces the Instruction Contrastive Decoding (ICD) method, a novel approach designed to reduce hallucinations during LVLM inference. Our method is inspired by our observation that what we call disturbance instructions significantly exacerbate hallucinations in multimodal fusion modules. ICD contrasts distributions from standard and instruction disturbance, thereby increasing alignment uncertainty and effectively subtracting hallucinated concepts from the original distribution. Through comprehensive experiments on discriminative benchmarks (POPE and MME) and a generative benchmark (LLaVa-Bench), we demonstrate that ICD significantly mitigates both object-level and attribute-level hallucinations. Moreover, our method not only addresses hallucinations but also significantly enhances the general perception and recognition capabilities of LVLMs.","sentences":["Large Vision-Language Models (LVLMs) are increasingly adept at generating contextually detailed and coherent responses from visual inputs.","However, their application in multimodal decision-making and open-ended generation is hindered by a notable rate of hallucinations, where generated text inaccurately represents the visual contents.","To address this issue, this paper introduces the Instruction Contrastive Decoding (ICD) method, a novel approach designed to reduce hallucinations during LVLM inference.","Our method is inspired by our observation that what we call disturbance instructions significantly exacerbate hallucinations in multimodal fusion modules.","ICD contrasts distributions from standard and instruction disturbance, thereby increasing alignment uncertainty and effectively subtracting hallucinated concepts from the original distribution.","Through comprehensive experiments on discriminative benchmarks (POPE and MME) and a generative benchmark (LLaVa-Bench), we demonstrate that ICD significantly mitigates both object-level and attribute-level hallucinations.","Moreover, our method not only addresses hallucinations but also significantly enhances the general perception and recognition capabilities of LVLMs."],"url":"http://arxiv.org/abs/2403.18715v1"}
{"created":"2024-03-27 16:02:00","title":"Bringing Textual Prompt to AI-Generated Image Quality Assessment","abstract":"AI-Generated Images (AGIs) have inherent multimodal nature. Unlike traditional image quality assessment (IQA) on natural scenarios, AGIs quality assessment (AGIQA) takes the correspondence of image and its textual prompt into consideration. This is coupled in the ground truth score, which confuses the unimodal IQA methods. To solve this problem, we introduce IP-IQA (AGIs Quality Assessment via Image and Prompt), a multimodal framework for AGIQA via corresponding image and prompt incorporation. Specifically, we propose a novel incremental pretraining task named Image2Prompt for better understanding of AGIs and their corresponding textual prompts. An effective and efficient image-prompt fusion module, along with a novel special [QA] token, are also applied. Both are plug-and-play and beneficial for the cooperation of image and its corresponding prompt. Experiments demonstrate that our IP-IQA achieves the state-of-the-art on AGIQA-1k and AGIQA-3k datasets. Code will be available.","sentences":["AI-Generated Images (AGIs) have inherent multimodal nature.","Unlike traditional image quality assessment (IQA) on natural scenarios, AGIs quality assessment (AGIQA) takes the correspondence of image and its textual prompt into consideration.","This is coupled in the ground truth score, which confuses the unimodal IQA methods.","To solve this problem, we introduce IP-IQA (AGIs Quality Assessment via Image and Prompt), a multimodal framework for AGIQA via corresponding image and prompt incorporation.","Specifically, we propose a novel incremental pretraining task named Image2Prompt for better understanding of AGIs and their corresponding textual prompts.","An effective and efficient image-prompt fusion module, along with a novel special [QA] token, are also applied.","Both are plug-and-play and beneficial for the cooperation of image and its corresponding prompt.","Experiments demonstrate that our IP-IQA achieves the state-of-the-art on AGIQA-1k and AGIQA-3k datasets.","Code will be available."],"url":"http://arxiv.org/abs/2403.18714v1"}
{"created":"2024-03-27 16:00:48","title":"Characterization of Spatial-Temporal Channel Statistics from Indoor Measurement Data at D Band","abstract":"Millimeter-wave (mmWave) and D Band (110--170~GHz) frequencies are poised to play a pivotal role in the advancement of sixth-generation (6G) systems and beyond, owing to their ability to enhance performance metrics such as capacity, ultra-low latency, and spectral efficiency. This paper concentrates on deriving statistical insights into power, delay, and the number of paths based on measurements conducted across four distinct locations at a center frequency of 143.1 GHz. The findings underscore the suitability of various distributions in characterizing power behavior in line-of-sight (LOS) scenarios, including lognormal, Nakagami, gamma, and beta distributions, whereas the loglogistic distribution gives the optimal fit for power distribution in non-line-of-sight (NLOS) scenarios. Moreover, the exponential distribution shows to be the most appropriate model for the delay distribution in both LOS and NLOS scenarios. In terms of the number of paths, observations indicate a tendency for the highest concentration within the 10 m to 30 m distance range between the transmitter (Tx) and receiver (Rx). These insights shed light on the statistical nature of D band propagation characteristics, which are vital for informing the design and optimization of future 6G communication systems","sentences":["Millimeter-wave (mmWave) and D Band (110--170~GHz) frequencies are poised to play a pivotal role in the advancement of sixth-generation (6G) systems and beyond, owing to their ability to enhance performance metrics such as capacity, ultra-low latency, and spectral efficiency.","This paper concentrates on deriving statistical insights into power, delay, and the number of paths based on measurements conducted across four distinct locations at a center frequency of 143.1 GHz.","The findings underscore the suitability of various distributions in characterizing power behavior in line-of-sight (LOS) scenarios, including lognormal, Nakagami, gamma, and beta distributions, whereas the loglogistic distribution gives the optimal fit for power distribution in non-line-of-sight (NLOS) scenarios.","Moreover, the exponential distribution shows to be the most appropriate model for the delay distribution in both LOS and NLOS scenarios.","In terms of the number of paths, observations indicate a tendency for the highest concentration within the 10 m to 30 m distance range between the transmitter (Tx) and receiver (Rx).","These insights shed light on the statistical nature of D band propagation characteristics, which are vital for informing the design and optimization of future 6G communication systems"],"url":"http://arxiv.org/abs/2403.18713v1"}
{"created":"2024-03-27 15:58:25","title":"SAT-NGP : Unleashing Neural Graphics Primitives for Fast Relightable Transient-Free 3D reconstruction from Satellite Imagery","abstract":"Current stereo-vision pipelines produce high accuracy 3D reconstruction when using multiple pairs or triplets of satellite images. However, these pipelines are sensitive to the changes between images that can occur as a result of multi-date acquisitions. Such variations are mainly due to variable shadows, reflexions and transient objects (cars, vegetation). To take such changes into account, Neural Radiance Fields (NeRF) have recently been applied to multi-date satellite imagery. However, Neural methods are very compute-intensive, taking dozens of hours to learn, compared with minutes for standard stereo-vision pipelines. Following the ideas of Instant Neural Graphics Primitives we propose to use an efficient sampling strategy and multi-resolution hash encoding to accelerate the learning. Our model, Satellite Neural Graphics Primitives (SAT-NGP) decreases the learning time to 15 minutes while maintaining the quality of the 3D reconstruction.","sentences":["Current stereo-vision pipelines produce high accuracy 3D reconstruction when using multiple pairs or triplets of satellite images.","However, these pipelines are sensitive to the changes between images that can occur as a result of multi-date acquisitions.","Such variations are mainly due to variable shadows, reflexions and transient objects (cars, vegetation).","To take such changes into account, Neural Radiance Fields (NeRF) have recently been applied to multi-date satellite imagery.","However, Neural methods are very compute-intensive, taking dozens of hours to learn, compared with minutes for standard stereo-vision pipelines.","Following the ideas of Instant Neural Graphics Primitives we propose to use an efficient sampling strategy and multi-resolution hash encoding to accelerate the learning.","Our model, Satellite Neural Graphics Primitives (SAT-NGP) decreases the learning time to 15 minutes while maintaining the quality of the 3D reconstruction."],"url":"http://arxiv.org/abs/2403.18711v1"}
{"created":"2024-03-27 15:57:42","title":"Deep Learning for Traffic Flow Prediction using Cellular Automata-based Model and CNN-LSTM architecture","abstract":"Recent works have attempted to use deep learning to predict future states of traffic flow, but have met with mixed results. These approaches face two key challenges. First, training deep learning neural networks requires large amounts of training data which are not yet easily available for traffic flow systems. Second, even when data is available, the neural networks require access to historical data that covers most possible traffic flow dynamics to successfully predict future traffic states. Specifically, these deep learning approaches do not fully leverage domain-knowledge about traffic flow dynamics, despite a significant existing knowledge-base. In this work, we propose to solve both issues using a Convolutional Neural Network (CNNs) with Long Short Term Memory (LSTM) deep learning architecture to successfully predict traffic flow, while leveraging a cellular automata-based statistical mechanics model of traffic flow to generate training and test data. Another major contribution of this paper is the insight that training data for a large traffic system can actually be sampled from the simulations of a much smaller traffic system. This is achieved through observing that the normalized energy distribution of the statistical mechanics model is scale invariant, which significantly eases the burden of data generation for large scale traffic systems. The resulting simulations indicate good agreement between the predicted and the true traffic flow dynamics.","sentences":["Recent works have attempted to use deep learning to predict future states of traffic flow, but have met with mixed results.","These approaches face two key challenges.","First, training deep learning neural networks requires large amounts of training data which are not yet easily available for traffic flow systems.","Second, even when data is available, the neural networks require access to historical data that covers most possible traffic flow dynamics to successfully predict future traffic states.","Specifically, these deep learning approaches do not fully leverage domain-knowledge about traffic flow dynamics, despite a significant existing knowledge-base.","In this work, we propose to solve both issues using a Convolutional Neural Network (CNNs) with Long Short Term Memory (LSTM) deep learning architecture to successfully predict traffic flow, while leveraging a cellular automata-based statistical mechanics model of traffic flow to generate training and test data.","Another major contribution of this paper is the insight that training data for a large traffic system can actually be sampled from the simulations of a much smaller traffic system.","This is achieved through observing that the normalized energy distribution of the statistical mechanics model is scale invariant, which significantly eases the burden of data generation for large scale traffic systems.","The resulting simulations indicate good agreement between the predicted and the true traffic flow dynamics."],"url":"http://arxiv.org/abs/2403.18710v1"}
{"created":"2024-03-27 15:56:42","title":"Dense Vision Transformer Compression with Few Samples","abstract":"Few-shot model compression aims to compress a large model into a more compact one with only a tiny training set (even without labels). Block-level pruning has recently emerged as a leading technique in achieving high accuracy and low latency in few-shot CNN compression. But, few-shot compression for Vision Transformers (ViT) remains largely unexplored, which presents a new challenge. In particular, the issue of sparse compression exists in traditional CNN few-shot methods, which can only produce very few compressed models of different model sizes. This paper proposes a novel framework for few-shot ViT compression named DC-ViT. Instead of dropping the entire block, DC-ViT selectively eliminates the attention module while retaining and reusing portions of the MLP module. DC-ViT enables dense compression, which outputs numerous compressed models that densely populate the range of model complexity. DC-ViT outperforms state-of-the-art few-shot compression methods by a significant margin of 10 percentage points, along with lower latency in the compression of ViT and its variants.","sentences":["Few-shot model compression aims to compress a large model into a more compact one with only a tiny training set (even without labels).","Block-level pruning has recently emerged as a leading technique in achieving high accuracy and low latency in few-shot CNN compression.","But, few-shot compression for Vision Transformers (ViT) remains largely unexplored, which presents a new challenge.","In particular, the issue of sparse compression exists in traditional CNN few-shot methods, which can only produce very few compressed models of different model sizes.","This paper proposes a novel framework for few-shot ViT compression named DC-ViT. Instead of dropping the entire block, DC-ViT selectively eliminates the attention module while retaining and reusing portions of the MLP module.","DC-ViT enables dense compression, which outputs numerous compressed models that densely populate the range of model complexity.","DC-ViT outperforms state-of-the-art few-shot compression methods by a significant margin of 10 percentage points, along with lower latency in the compression of ViT and its variants."],"url":"http://arxiv.org/abs/2403.18708v1"}
{"created":"2024-03-27 15:54:55","title":"Conditional Wasserstein Distances with Applications in Bayesian OT Flow Matching","abstract":"In inverse problems, many conditional generative models approximate the posterior measure by minimizing a distance between the joint measure and its learned approximation. While this approach also controls the distance between the posterior measures in the case of the Kullback--Leibler divergence, this is in general not hold true for the Wasserstein distance. In this paper, we introduce a conditional Wasserstein distance via a set of restricted couplings that equals the expected Wasserstein distance of the posteriors. Interestingly, the dual formulation of the conditional Wasserstein-1 flow resembles losses in the conditional Wasserstein GAN literature in a quite natural way. We derive theoretical properties of the conditional Wasserstein distance, characterize the corresponding geodesics and velocity fields as well as the flow ODEs. Subsequently, we propose to approximate the velocity fields by relaxing the conditional Wasserstein distance. Based on this, we propose an extension of OT Flow Matching for solving Bayesian inverse problems and demonstrate its numerical advantages on an inverse problem and class-conditional image generation.","sentences":["In inverse problems, many conditional generative models approximate the posterior measure by minimizing a distance between the joint measure and its learned approximation.","While this approach also controls the distance between the posterior measures in the case of the Kullback--Leibler divergence, this is in general not hold true for the Wasserstein distance.","In this paper, we introduce a conditional Wasserstein distance via a set of restricted couplings that equals the expected Wasserstein distance of the posteriors.","Interestingly, the dual formulation of the conditional Wasserstein-1 flow resembles losses in the conditional Wasserstein GAN literature in a quite natural way.","We derive theoretical properties of the conditional Wasserstein distance, characterize the corresponding geodesics and velocity fields as well as the flow ODEs.","Subsequently, we propose to approximate the velocity fields by relaxing the conditional Wasserstein distance.","Based on this, we propose an extension of OT Flow Matching for solving Bayesian inverse problems and demonstrate its numerical advantages on an inverse problem and class-conditional image generation."],"url":"http://arxiv.org/abs/2403.18705v1"}
{"created":"2024-03-27 15:50:24","title":"Toward CXL-Native Memory Tiering via Device-Side Profiling","abstract":"The Compute Express Link (CXL) interconnect has provided the ability to integrate diverse memory types into servers via byte-addressable SerDes links. Harnessing the full potential of such heterogeneous memory systems requires efficient memory tiering. However, existing research in this domain has been constrained by low-resolution and high-overhead memory access profiling techniques. To address this critical challenge, we propose to enhance existing memory tiering systems with a novel NeoMem solution. NeoMem offloads memory profiling functions to device-side controllers, integrating a dedicated hardware unit called NeoProf. NeoProf readily tracks memory access and provides the operating system with crucial page hotness statistics and other useful system state information. On the OS kernel side, we introduce a revamped memory-tiering strategy, enabling accurate and timely hot page promotion based on NeoProf statistics. We implement NeoMem on a real CXL-enabled FPGA platform and Linux kernel v6.3. Comprehensive evaluations demonstrate that NeoMem achieves 32% to 67% geomean speedup over several existing memory tiering solutions.","sentences":["The Compute Express Link (CXL) interconnect has provided the ability to integrate diverse memory types into servers via byte-addressable SerDes links.","Harnessing the full potential of such heterogeneous memory systems requires efficient memory tiering.","However, existing research in this domain has been constrained by low-resolution and high-overhead memory access profiling techniques.","To address this critical challenge, we propose to enhance existing memory tiering systems with a novel NeoMem solution.","NeoMem offloads memory profiling functions to device-side controllers, integrating a dedicated hardware unit called NeoProf.","NeoProf readily tracks memory access and provides the operating system with crucial page hotness statistics and other useful system state information.","On the OS kernel side, we introduce a revamped memory-tiering strategy, enabling accurate and timely hot page promotion based on NeoProf statistics.","We implement NeoMem on a real CXL-enabled FPGA platform and Linux kernel v6.3.","Comprehensive evaluations demonstrate that NeoMem achieves 32% to 67% geomean speedup over several existing memory tiering solutions."],"url":"http://arxiv.org/abs/2403.18702v1"}
{"created":"2024-03-27 15:48:16","title":"Contrastive Learning with Orthonormal Anchors (CLOA)","abstract":"This study focuses on addressing the instability issues prevalent in contrastive learning, specifically examining the InfoNCE loss function and its derivatives. We reveal a critical observation that these loss functions exhibit a restrictive behavior, leading to a convergence phenomenon where embeddings tend to merge into a singular point. This \"over-fusion\" effect detrimentally affects classification accuracy in subsequent supervised-learning tasks. Through theoretical analysis, we demonstrate that embeddings, when equalized or confined to a rank-1 linear subspace, represent a local minimum for InfoNCE. In response to this challenge, our research introduces an innovative strategy that leverages the same or fewer labeled data than typically used in the fine-tuning phase. The loss we proposed, Orthonormal Anchor Regression Loss, is designed to disentangle embedding clusters, significantly enhancing the distinctiveness of each embedding while simultaneously ensuring their aggregation into dense, well-defined clusters. Our method demonstrates remarkable improvements with just a fraction of the conventional label requirements, as evidenced by our results on CIFAR10 and CIFAR100 datasets.","sentences":["This study focuses on addressing the instability issues prevalent in contrastive learning, specifically examining the InfoNCE loss function and its derivatives.","We reveal a critical observation that these loss functions exhibit a restrictive behavior, leading to a convergence phenomenon where embeddings tend to merge into a singular point.","This \"over-fusion\" effect detrimentally affects classification accuracy in subsequent supervised-learning tasks.","Through theoretical analysis, we demonstrate that embeddings, when equalized or confined to a rank-1 linear subspace, represent a local minimum for InfoNCE.","In response to this challenge, our research introduces an innovative strategy that leverages the same or fewer labeled data than typically used in the fine-tuning phase.","The loss we proposed, Orthonormal Anchor Regression Loss, is designed to disentangle embedding clusters, significantly enhancing the distinctiveness of each embedding while simultaneously ensuring their aggregation into dense, well-defined clusters.","Our method demonstrates remarkable improvements with just a fraction of the conventional label requirements, as evidenced by our results on CIFAR10 and CIFAR100 datasets."],"url":"http://arxiv.org/abs/2403.18699v1"}
{"created":"2024-03-27 15:46:25","title":"The Invalsi Benchmark: measuring Language Models Mathematical and Language understanding in Italian","abstract":"While Italian is by all metrics a high resource language, currently, there are isn't a Language Model pre-trained exclusively in this language. This results in a lower number of available benchmarks to evaluate the performance of language models in Italian.   This work presents two new benchmarks to evaluate the models performance on mathematical understanding and language understanding in Italian. These benchmarks are based on real tests that are undertaken by students of age between 11 and 18 within the Italian school system and have therefore been validated by several experts in didactics and pedagogy.   To validate this dataset we evaluate the performance of 9 language models that are the best performing when writing in Italian, including our own fine-tuned models. We show that this is a challenging benchmark where current language models are bound by 60\\% accuracy.   We believe that the release of this dataset paves the way for improving future models mathematical and language understanding in Italian.","sentences":["While Italian is by all metrics a high resource language, currently, there are isn't a Language Model pre-trained exclusively in this language.","This results in a lower number of available benchmarks to evaluate the performance of language models in Italian.   ","This work presents two new benchmarks to evaluate the models performance on mathematical understanding and language understanding in Italian.","These benchmarks are based on real tests that are undertaken by students of age between 11 and 18 within the Italian school system and have therefore been validated by several experts in didactics and pedagogy.   ","To validate this dataset we evaluate the performance of 9 language models that are the best performing when writing in Italian, including our own fine-tuned models.","We show that this is a challenging benchmark where current language models are bound by 60\\% accuracy.   ","We believe that the release of this dataset paves the way for improving future models mathematical and language understanding in Italian."],"url":"http://arxiv.org/abs/2403.18697v1"}
{"created":"2024-03-27 15:42:01","title":"Teaching Introductory HRI: UChicago Course \"Human-Robot Interaction: Research and Practice\"","abstract":"In 2020, I designed the course CMSC 20630/30630 Human-Robot Interaction: Research and Practice as a hands-on introduction to human-robot interaction (HRI) research for both undergraduate and graduate students at the University of Chicago. Since 2020, I have taught and refined this course each academic year. Human-Robot Interaction: Research and Practice focuses on the core concepts and cutting-edge research in the field of human-robot interaction (HRI), covering topics that include: nonverbal robot behavior, verbal robot behavior, social dynamics, norms & ethics, collaboration & learning, group interactions, applications, and future challenges of HRI. Course meetings involve students in the class leading discussions about cutting-edge peer-reviewed research HRI publications. Students also participate in a quarter-long collaborative research project, where they pursue an HRI research question that often involves conducing their own human-subjects research study where they recruit human subjects to interact with a robot. In this paper, I detail the structure of the course and its learning goals as well as my reflections and student feedback on the course.","sentences":["In 2020, I designed the course CMSC 20630/30630 Human-Robot Interaction: Research and Practice as a hands-on introduction to human-robot interaction (HRI) research for both undergraduate and graduate students at the University of Chicago.","Since 2020, I have taught and refined this course each academic year.","Human-Robot Interaction:","Research and Practice focuses on the core concepts and cutting-edge research in the field of human-robot interaction (HRI), covering topics that include: nonverbal robot behavior, verbal robot behavior, social dynamics, norms & ethics, collaboration & learning, group interactions, applications, and future challenges of HRI.","Course meetings involve students in the class leading discussions about cutting-edge peer-reviewed research HRI publications.","Students also participate in a quarter-long collaborative research project, where they pursue an HRI research question that often involves conducing their own human-subjects research study where they recruit human subjects to interact with a robot.","In this paper, I detail the structure of the course and its learning goals as well as my reflections and student feedback on the course."],"url":"http://arxiv.org/abs/2403.18692v1"}
{"created":"2024-03-27 15:41:23","title":"Annolid: Annotate, Segment, and Track Anything You Need","abstract":"Annolid is a deep learning-based software package designed for the segmentation, labeling, and tracking of research targets within video files, focusing primarily on animal behavior analysis. Based on state-of-the-art instance segmentation methods, Annolid now harnesses the Cutie video object segmentation model to achieve resilient, markerless tracking of multiple animals from single annotated frames, even in environments in which they may be partially or entirely concealed by environmental features or by one another. Our integration of Segment Anything and Grounding-DINO strategies additionally enables the automatic masking and segmentation of recognizable animals and objects by text command, removing the need for manual annotation. Annolid's comprehensive approach to object segmentation flexibly accommodates a broad spectrum of behavior analysis applications, enabling the classification of diverse behavioral states such as freezing, digging, pup huddling, and social interactions in addition to the tracking of animals and their body parts.","sentences":["Annolid is a deep learning-based software package designed for the segmentation, labeling, and tracking of research targets within video files, focusing primarily on animal behavior analysis.","Based on state-of-the-art instance segmentation methods, Annolid now harnesses the Cutie video object segmentation model to achieve resilient, markerless tracking of multiple animals from single annotated frames, even in environments in which they may be partially or entirely concealed by environmental features or by one another.","Our integration of Segment Anything and Grounding-DINO strategies additionally enables the automatic masking and segmentation of recognizable animals and objects by text command, removing the need for manual annotation.","Annolid's comprehensive approach to object segmentation flexibly accommodates a broad spectrum of behavior analysis applications, enabling the classification of diverse behavioral states such as freezing, digging, pup huddling, and social interactions in addition to the tracking of animals and their body parts."],"url":"http://arxiv.org/abs/2403.18690v1"}
{"created":"2024-03-27 15:29:08","title":"Representatividad Muestral en la Incertidumbre Sim\u00e9trica Multivariada para la Selecci\u00f3n de Atributos","abstract":"In this work, we analyze the behavior of the multivariate symmetric uncertainty (MSU) measure through the use of statistical simulation techniques under various mixes of informative and non-informative randomly generated features. Experiments show how the number of attributes, their cardinalities, and the sample size affect the MSU. In this thesis, through observation of results, it is proposed an heuristic condition that preserves good quality in the MSU under different combinations of these three factors, providing a new useful criterion to help drive the process of dimension reduction.   --   En el presente trabajo hemos analizado el comportamiento de una versi\\'on multivariada de la incertidumbre sim\\'etrica a trav\\'es de t\\'ecnicas de simulaci\\'on estad\\'isticas sobre varias combinaciones de atributos informativos y no-informativos generados de forma aleatoria. Los experimentos muestran como el n\\'umero de atributos, sus cardinalidades y el tama\\~no muestral afectan al MSU como medida. En esta tesis, mediante la observaci\\'on de resultados hemos propuesto una condici\\'on que preserva una buena calidad en el MSU bajo diferentes combinaciones de los tres factores mencionados, lo cual provee un nuevo y valioso criterio para llevar a cabo el proceso de reducci\\'on de dimensionalidad.","sentences":["In this work, we analyze the behavior of the multivariate symmetric uncertainty (MSU) measure through the use of statistical simulation techniques under various mixes of informative and non-informative randomly generated features.","Experiments show how the number of attributes, their cardinalities, and the sample size affect the MSU.","In this thesis, through observation of results, it is proposed an heuristic condition that preserves good quality in the MSU under different combinations of these three factors, providing a new useful criterion to help drive the process of dimension reduction.   ","--   En el presente trabajo hemos analizado el comportamiento de una versi\\'on multivariada de la incertidumbre sim\\'etrica a trav\\'es de t\\'ecnicas de simulaci\\'on estad\\'isticas sobre varias combinaciones de atributos informativos y no-informativos generados de forma aleatoria.","Los experimentos muestran como el n\\'umero de atributos, sus cardinalidades y el tama\\~no muestral afectan al MSU como medida.","En esta tesis, mediante la observaci\\'on de resultados hemos propuesto una condici\\'on que preserva una buena calidad en el MSU bajo diferentes combinaciones de los tres factores mencionados, lo cual provee un nuevo y valioso criterio para llevar a cabo el proceso de reducci\\'on de dimensionalidad."],"url":"http://arxiv.org/abs/2403.18685v1"}
{"created":"2024-03-27 15:27:36","title":"Scaling Laws For Dense Retrieval","abstract":"Scaling up neural models has yielded significant advancements in a wide array of tasks, particularly in language generation. Previous studies have found that the performance of neural models frequently adheres to predictable scaling laws, correlated with factors such as training set size and model size. This insight is invaluable, especially as large-scale experiments grow increasingly resource-intensive. Yet, such scaling law has not been fully explored in dense retrieval due to the discrete nature of retrieval metrics and complex relationships between training data and model sizes in retrieval tasks. In this study, we investigate whether the performance of dense retrieval models follows the scaling law as other neural models. We propose to use contrastive log-likelihood as the evaluation metric and conduct extensive experiments with dense retrieval models implemented with different numbers of parameters and trained with different amounts of annotated data. Results indicate that, under our settings, the performance of dense retrieval models follows a precise power-law scaling related to the model size and the number of annotations. Additionally, we examine scaling with prevalent data augmentation methods to assess the impact of annotation quality, and apply the scaling law to find the best resource allocation strategy under a budget constraint. We believe that these insights will significantly contribute to understanding the scaling effect of dense retrieval models and offer meaningful guidance for future research endeavors.","sentences":["Scaling up neural models has yielded significant advancements in a wide array of tasks, particularly in language generation.","Previous studies have found that the performance of neural models frequently adheres to predictable scaling laws, correlated with factors such as training set size and model size.","This insight is invaluable, especially as large-scale experiments grow increasingly resource-intensive.","Yet, such scaling law has not been fully explored in dense retrieval due to the discrete nature of retrieval metrics and complex relationships between training data and model sizes in retrieval tasks.","In this study, we investigate whether the performance of dense retrieval models follows the scaling law as other neural models.","We propose to use contrastive log-likelihood as the evaluation metric and conduct extensive experiments with dense retrieval models implemented with different numbers of parameters and trained with different amounts of annotated data.","Results indicate that, under our settings, the performance of dense retrieval models follows a precise power-law scaling related to the model size and the number of annotations.","Additionally, we examine scaling with prevalent data augmentation methods to assess the impact of annotation quality, and apply the scaling law to find the best resource allocation strategy under a budget constraint.","We believe that these insights will significantly contribute to understanding the scaling effect of dense retrieval models and offer meaningful guidance for future research endeavors."],"url":"http://arxiv.org/abs/2403.18684v1"}
{"created":"2024-03-27 15:26:09","title":"JumpBackHash: Say Goodbye to the Modulo Operation to Distribute Keys Uniformly to Buckets","abstract":"The distribution of keys to a given number of buckets is a fundamental task in distributed data processing and storage. A simple, fast, and therefore popular approach is to map the hash values of keys to buckets based on the remainder after dividing by the number of buckets. Unfortunately, these mappings are not stable when the number of buckets changes, which can lead to severe spikes in system resource utilization, such as network or database requests. Consistent hash algorithms can minimize remappings, but are either significantly slower than the modulo-based approach, require floating-point arithmetic, or are based on a family of hash functions rarely available in standard libraries. This paper introduces JumpBackHash, which uses only integer arithmetic and a standard pseudorandom generator. Due to its speed and simple implementation, it can safely replace the modulo-based approach to improve assignment and system stability. A production-ready Java implementation of JumpBackHash has been released as part of the Hash4j open source library.","sentences":["The distribution of keys to a given number of buckets is a fundamental task in distributed data processing and storage.","A simple, fast, and therefore popular approach is to map the hash values of keys to buckets based on the remainder after dividing by the number of buckets.","Unfortunately, these mappings are not stable when the number of buckets changes, which can lead to severe spikes in system resource utilization, such as network or database requests.","Consistent hash algorithms can minimize remappings, but are either significantly slower than the modulo-based approach, require floating-point arithmetic, or are based on a family of hash functions rarely available in standard libraries.","This paper introduces JumpBackHash, which uses only integer arithmetic and a standard pseudorandom generator.","Due to its speed and simple implementation, it can safely replace the modulo-based approach to improve assignment and system stability.","A production-ready Java implementation of JumpBackHash has been released as part of the Hash4j open source library."],"url":"http://arxiv.org/abs/2403.18682v1"}
{"created":"2024-03-27 15:24:54","title":"TransFusion: Contrastive Learning with Transformers","abstract":"This paper proposes a novel framework, TransFusion, designed to make the process of contrastive learning more analytical and explainable. TransFusion consists of attention blocks whose softmax being replaced by ReLU, and its final block's weighted-sum operation is truncated to leave the adjacency matrix as the output. The model is trained by minimizing the Jensen-Shannon Divergence between its output and the target affinity matrix, which indicates whether each pair of samples belongs to the same or different classes. The main contribution of TransFusion lies in defining a theoretical limit for answering two fundamental questions in the field: the maximum level of data augmentation and the minimum batch size required for effective contrastive learning. Furthermore, experimental results indicate that TransFusion successfully extracts features that isolate clusters from complex real-world data, leading to improved classification accuracy in downstream tasks.","sentences":["This paper proposes a novel framework, TransFusion, designed to make the process of contrastive learning more analytical and explainable.","TransFusion consists of attention blocks whose softmax being replaced by ReLU, and its final block's weighted-sum operation is truncated to leave the adjacency matrix as the output.","The model is trained by minimizing the Jensen-Shannon Divergence between its output and the target affinity matrix, which indicates whether each pair of samples belongs to the same or different classes.","The main contribution of TransFusion lies in defining a theoretical limit for answering two fundamental questions in the field: the maximum level of data augmentation and the minimum batch size required for effective contrastive learning.","Furthermore, experimental results indicate that TransFusion successfully extracts features that isolate clusters from complex real-world data, leading to improved classification accuracy in downstream tasks."],"url":"http://arxiv.org/abs/2403.18681v1"}
{"created":"2024-03-27 15:22:16","title":"NL-ITI: Optimizing Probing and Intervention for Improvement of ITI Method","abstract":"Large Language Models (LLM) are prone to returning false information. It constitutes one of major challenges in the AI field. In our work, we explore paradigm introduced by Inference-Time-Intervention (ITI). In first stage, it identifies attention heads, which contain the highest amount of desired type of knowledge (e.g., truthful). Afterwards, during inference, LLM activations are shifted for chosen subset of attention heads. We further improved the ITI framework by introducing a nonlinear probing and multi-token intervention - Non-Linear ITI (NL-ITI). NL-ITI is tested on diverse multiple-choice benchmarks, including TruthfulQA, on which we report around 14% MC1 metric improvement with respect to the baseline ITI results. NL-ITI achieves also encouraging results on other testsets - on Business Ethics subdomain of MMLU, around 18% MC1 improvement over baseline LLaMA2-7B. Additionally, NL-ITI performs better while being less invasive in the behavior of LLM at the same time (as measured by Kullback-Leibler divergence).","sentences":["Large Language Models (LLM) are prone to returning false information.","It constitutes one of major challenges in the AI field.","In our work, we explore paradigm introduced by Inference-Time-Intervention (ITI).","In first stage, it identifies attention heads, which contain the highest amount of desired type of knowledge (e.g., truthful).","Afterwards, during inference, LLM activations are shifted for chosen subset of attention heads.","We further improved the ITI framework by introducing a nonlinear probing and multi-token intervention - Non-Linear ITI (NL-ITI).","NL-ITI is tested on diverse multiple-choice benchmarks, including TruthfulQA, on which we report around 14% MC1 metric improvement with respect to the baseline ITI results.","NL-ITI achieves also encouraging results on other testsets - on Business Ethics subdomain of MMLU, around 18% MC1 improvement over baseline LLaMA2-7B.","Additionally, NL-ITI performs better while being less invasive in the behavior of LLM at the same time (as measured by Kullback-Leibler divergence)."],"url":"http://arxiv.org/abs/2403.18680v1"}
{"created":"2024-03-27 15:21:58","title":"An Exploratory Study on Upper-Level Computing Students' Use of Large Language Models as Tools in a Semester-Long Project","abstract":"Background: Large Language Models (LLMs) such as ChatGPT and CoPilot are influencing software engineering practice. Software engineering educators must teach future software engineers how to use such tools well. As of yet, there have been few studies that report on the use of LLMs in the classroom. It is, therefore, important to evaluate students' perception of LLMs and possible ways of adapting the computing curriculum to these shifting paradigms.   Purpose: The purpose of this study is to explore computing students' experiences and approaches to using LLMs during a semester-long software engineering project.   Design/Method: We collected data from a senior-level software engineering course at Purdue University. This course uses a project-based learning (PBL) design. The students used LLMs such as ChatGPT and Copilot in their projects. A sample of these student teams were interviewed to understand (1) how they used LLMs in their projects; and (2) whether and how their perspectives on LLMs changed over the course of the semester. We analyzed the data to identify themes related to students' usage patterns and learning outcomes.   Results/Discussion: When computing students utilize LLMs within a project, their use cases cover both technical and professional applications. In addition, these students perceive LLMs to be efficient tools in obtaining information and completion of tasks. However, there were concerns about the responsible use of LLMs without being detrimental to their own learning outcomes. Based on our findings, we recommend future research to investigate the usage of LLM's in lower-level computer engineering courses to understand whether and how LLMs can be integrated as a learning aid without hurting the learning outcomes.","sentences":["Background: Large Language Models (LLMs) such as ChatGPT and CoPilot are influencing software engineering practice.","Software engineering educators must teach future software engineers how to use such tools well.","As of yet, there have been few studies that report on the use of LLMs in the classroom.","It is, therefore, important to evaluate students' perception of LLMs and possible ways of adapting the computing curriculum to these shifting paradigms.   ","Purpose: The purpose of this study is to explore computing students' experiences and approaches to using LLMs during a semester-long software engineering project.   ","Design/Method: We collected data from a senior-level software engineering course at Purdue University.","This course uses a project-based learning (PBL) design.","The students used LLMs such as ChatGPT and Copilot in their projects.","A sample of these student teams were interviewed to understand (1) how they used LLMs in their projects; and (2) whether and how their perspectives on LLMs changed over the course of the semester.","We analyzed the data to identify themes related to students' usage patterns and learning outcomes.   ","Results/Discussion: When computing students utilize LLMs within a project, their use cases cover both technical and professional applications.","In addition, these students perceive LLMs to be efficient tools in obtaining information and completion of tasks.","However, there were concerns about the responsible use of LLMs without being detrimental to their own learning outcomes.","Based on our findings, we recommend future research to investigate the usage of LLM's in lower-level computer engineering courses to understand whether and how LLMs can be integrated as a learning aid without hurting the learning outcomes."],"url":"http://arxiv.org/abs/2403.18679v1"}
{"created":"2024-03-27 15:17:10","title":"Deep Learning for Robust and Explainable Models in Computer Vision","abstract":"Recent breakthroughs in machine and deep learning (ML and DL) research have provided excellent tools for leveraging enormous amounts of data and optimizing huge models with millions of parameters to obtain accurate networks for image processing. These developments open up tremendous opportunities for using artificial intelligence (AI) in the automation and human assisted AI industry. However, as more and more models are deployed and used in practice, many challenges have emerged. This thesis presents various approaches that address robustness and explainability challenges for using ML and DL in practice.   Robustness and reliability are the critical components of any model before certification and deployment in practice. Deep convolutional neural networks (CNNs) exhibit vulnerability to transformations of their inputs, such as rotation and scaling, or intentional manipulations as described in the adversarial attack literature. In addition, building trust in AI-based models requires a better understanding of current models and developing methods that are more explainable and interpretable a priori.   This thesis presents developments in computer vision models' robustness and explainability. Furthermore, this thesis offers an example of using vision models' feature response visualization (models' interpretations) to improve robustness despite interpretability and robustness being seemingly unrelated in the related research. Besides methodological developments for robust and explainable vision models, a key message of this thesis is introducing model interpretation techniques as a tool for understanding vision models and improving their design and robustness. In addition to the theoretical developments, this thesis demonstrates several applications of ML and DL in different contexts, such as medical imaging and affective computing.","sentences":["Recent breakthroughs in machine and deep learning (ML and DL) research have provided excellent tools for leveraging enormous amounts of data and optimizing huge models with millions of parameters to obtain accurate networks for image processing.","These developments open up tremendous opportunities for using artificial intelligence (AI) in the automation and human assisted AI industry.","However, as more and more models are deployed and used in practice, many challenges have emerged.","This thesis presents various approaches that address robustness and explainability challenges for using ML and DL in practice.   ","Robustness and reliability are the critical components of any model before certification and deployment in practice.","Deep convolutional neural networks (CNNs) exhibit vulnerability to transformations of their inputs, such as rotation and scaling, or intentional manipulations as described in the adversarial attack literature.","In addition, building trust in AI-based models requires a better understanding of current models and developing methods that are more explainable and interpretable a priori.   ","This thesis presents developments in computer vision models' robustness and explainability.","Furthermore, this thesis offers an example of using vision models' feature response visualization (models' interpretations) to improve robustness despite interpretability and robustness being seemingly unrelated in the related research.","Besides methodological developments for robust and explainable vision models, a key message of this thesis is introducing model interpretation techniques as a tool for understanding vision models and improving their design and robustness.","In addition to the theoretical developments, this thesis demonstrates several applications of ML and DL in different contexts, such as medical imaging and affective computing."],"url":"http://arxiv.org/abs/2403.18674v1"}
{"created":"2024-03-27 15:15:14","title":"Fact Checking Beyond Training Set","abstract":"Evaluating the veracity of everyday claims is time consuming and in some cases requires domain expertise. We empirically demonstrate that the commonly used fact checking pipeline, known as the retriever-reader, suffers from performance deterioration when it is trained on the labeled data from one domain and used in another domain. Afterwards, we delve into each component of the pipeline and propose novel algorithms to address this problem. We propose an adversarial algorithm to make the retriever component robust against distribution shift. Our core idea is to initially train a bi-encoder on the labeled source data, and then, to adversarially train two separate document and claim encoders using unlabeled target data. We then focus on the reader component and propose to train it such that it is insensitive towards the order of claims and evidence documents. Our empirical evaluations support the hypothesis that such a reader shows a higher robustness against distribution shift. To our knowledge, there is no publicly available multi-topic fact checking dataset. Thus, we propose a simple automatic method to re-purpose two well-known fact checking datasets. We then construct eight fact checking scenarios from these datasets, and compare our model to a set of strong baseline models, including recent domain adaptation models that use GPT4 for generating synthetic data.","sentences":["Evaluating the veracity of everyday claims is time consuming and in some cases requires domain expertise.","We empirically demonstrate that the commonly used fact checking pipeline, known as the retriever-reader, suffers from performance deterioration when it is trained on the labeled data from one domain and used in another domain.","Afterwards, we delve into each component of the pipeline and propose novel algorithms to address this problem.","We propose an adversarial algorithm to make the retriever component robust against distribution shift.","Our core idea is to initially train a bi-encoder on the labeled source data, and then, to adversarially train two separate document and claim encoders using unlabeled target data.","We then focus on the reader component and propose to train it such that it is insensitive towards the order of claims and evidence documents.","Our empirical evaluations support the hypothesis that such a reader shows a higher robustness against distribution shift.","To our knowledge, there is no publicly available multi-topic fact checking dataset.","Thus, we propose a simple automatic method to re-purpose two well-known fact checking datasets.","We then construct eight fact checking scenarios from these datasets, and compare our model to a set of strong baseline models, including recent domain adaptation models that use GPT4 for generating synthetic data."],"url":"http://arxiv.org/abs/2403.18671v1"}
{"created":"2024-03-27 15:11:07","title":"Aiming for Relevance","abstract":"Vital signs are crucial in intensive care units (ICUs). They are used to track the patient's state and to identify clinically significant changes. Predicting vital sign trajectories is valuable for early detection of adverse events. However, conventional machine learning metrics like RMSE often fail to capture the true clinical relevance of such predictions. We introduce novel vital sign prediction performance metrics that align with clinical contexts, focusing on deviations from clinical norms, overall trends, and trend deviations. These metrics are derived from empirical utility curves obtained in a previous study through interviews with ICU clinicians. We validate the metrics' usefulness using simulated and real clinical datasets (MIMIC and eICU). Furthermore, we employ these metrics as loss functions for neural networks, resulting in models that excel in predicting clinically significant events. This research paves the way for clinically relevant machine learning model evaluation and optimization, promising to improve ICU patient care. 10 pages, 9 figures.","sentences":["Vital signs are crucial in intensive care units (ICUs).","They are used to track the patient's state and to identify clinically significant changes.","Predicting vital sign trajectories is valuable for early detection of adverse events.","However, conventional machine learning metrics like RMSE often fail to capture the true clinical relevance of such predictions.","We introduce novel vital sign prediction performance metrics that align with clinical contexts, focusing on deviations from clinical norms, overall trends, and trend deviations.","These metrics are derived from empirical utility curves obtained in a previous study through interviews with ICU clinicians.","We validate the metrics' usefulness using simulated and real clinical datasets (MIMIC and eICU).","Furthermore, we employ these metrics as loss functions for neural networks, resulting in models that excel in predicting clinically significant events.","This research paves the way for clinically relevant machine learning model evaluation and optimization, promising to improve ICU patient care.","10 pages, 9 figures."],"url":"http://arxiv.org/abs/2403.18668v1"}
{"created":"2024-03-27 15:11:00","title":"Improving Content Recommendation: Knowledge Graph-Based Semantic Contrastive Learning for Diversity and Cold-Start Users","abstract":"Addressing the challenges related to data sparsity, cold-start problems, and diversity in recommendation systems is both crucial and demanding. Many current solutions leverage knowledge graphs to tackle these issues by combining both item-based and user-item collaborative signals. A common trend in these approaches focuses on improving ranking performance at the cost of escalating model complexity, reducing diversity, and complicating the task. It is essential to provide recommendations that are both personalized and diverse, rather than solely relying on achieving high rank-based performance, such as Click-through Rate, Recall, etc. In this paper, we propose a hybrid multi-task learning approach, training on user-item and item-item interactions. We apply item-based contrastive learning on descriptive text, sampling positive and negative pairs based on item metadata. Our approach allows the model to better understand the relationships between entities within the knowledge graph by utilizing semantic information from text. It leads to more accurate, relevant, and diverse user recommendations and a benefit that extends even to cold-start users who have few interactions with items. We perform extensive experiments on two widely used datasets to validate the effectiveness of our approach. Our findings demonstrate that jointly training user-item interactions and item-based signals using synopsis text is highly effective. Furthermore, our results provide evidence that item-based contrastive learning enhances the quality of entity embeddings, as indicated by metrics such as uniformity and alignment.","sentences":["Addressing the challenges related to data sparsity, cold-start problems, and diversity in recommendation systems is both crucial and demanding.","Many current solutions leverage knowledge graphs to tackle these issues by combining both item-based and user-item collaborative signals.","A common trend in these approaches focuses on improving ranking performance at the cost of escalating model complexity, reducing diversity, and complicating the task.","It is essential to provide recommendations that are both personalized and diverse, rather than solely relying on achieving high rank-based performance, such as Click-through Rate, Recall, etc.","In this paper, we propose a hybrid multi-task learning approach, training on user-item and item-item interactions.","We apply item-based contrastive learning on descriptive text, sampling positive and negative pairs based on item metadata.","Our approach allows the model to better understand the relationships between entities within the knowledge graph by utilizing semantic information from text.","It leads to more accurate, relevant, and diverse user recommendations and a benefit that extends even to cold-start users who have few interactions with items.","We perform extensive experiments on two widely used datasets to validate the effectiveness of our approach.","Our findings demonstrate that jointly training user-item interactions and item-based signals using synopsis text is highly effective.","Furthermore, our results provide evidence that item-based contrastive learning enhances the quality of entity embeddings, as indicated by metrics such as uniformity and alignment."],"url":"http://arxiv.org/abs/2403.18667v1"}
{"created":"2024-03-27 15:03:38","title":"InstructBrush: Learning Attention-based Instruction Optimization for Image Editing","abstract":"In recent years, instruction-based image editing methods have garnered significant attention in image editing. However, despite encompassing a wide range of editing priors, these methods are helpless when handling editing tasks that are challenging to accurately describe through language. We propose InstructBrush, an inversion method for instruction-based image editing methods to bridge this gap. It extracts editing effects from exemplar image pairs as editing instructions, which are further applied for image editing. Two key techniques are introduced into InstructBrush, Attention-based Instruction Optimization and Transformation-oriented Instruction Initialization, to address the limitations of the previous method in terms of inversion effects and instruction generalization. To explore the ability of instruction inversion methods to guide image editing in open scenarios, we establish a TransformationOriented Paired Benchmark (TOP-Bench), which contains a rich set of scenes and editing types. The creation of this benchmark paves the way for further exploration of instruction inversion. Quantitatively and qualitatively, our approach achieves superior performance in editing and is more semantically consistent with the target editing effects.","sentences":["In recent years, instruction-based image editing methods have garnered significant attention in image editing.","However, despite encompassing a wide range of editing priors, these methods are helpless when handling editing tasks that are challenging to accurately describe through language.","We propose InstructBrush, an inversion method for instruction-based image editing methods to bridge this gap.","It extracts editing effects from exemplar image pairs as editing instructions, which are further applied for image editing.","Two key techniques are introduced into InstructBrush, Attention-based Instruction Optimization and Transformation-oriented Instruction Initialization, to address the limitations of the previous method in terms of inversion effects and instruction generalization.","To explore the ability of instruction inversion methods to guide image editing in open scenarios, we establish a TransformationOriented Paired Benchmark (TOP-Bench), which contains a rich set of scenes and editing types.","The creation of this benchmark paves the way for further exploration of instruction inversion.","Quantitatively and qualitatively, our approach achieves superior performance in editing and is more semantically consistent with the target editing effects."],"url":"http://arxiv.org/abs/2403.18660v1"}
{"created":"2024-03-27 15:03:33","title":"INEXA: Interactive and Explainable Process Model Abstraction Through Object-Centric Process Mining","abstract":"Process events are recorded by multiple information systems at different granularity levels. Based on the resulting event logs, process models are discovered at different granularity levels, as well. Events stored at a fine-grained granularity level, for example, may hinder the discovered process model to be displayed due the high number of resulting model elements. The discovered process model of a real-world manufacturing process, for example, consists of 1,489 model elements and over 2,000 arcs. Existing process model abstraction techniques could help reducing the size of the model, but would disconnect it from the underlying event log. Existing event abstraction techniques do neither support the analysis of mixed granularity levels, nor interactive exploration of a suitable granularity level. To enable the exploration of discovered process models at different granularity levels, we propose INEXA, an interactive, explainable process model abstraction method that keeps the link to the event log. As a starting point, INEXA aggregates large process models to a \"displayable\" size, e.g., for the manufacturing use case to a process model with 58 model elements. Then, the process analyst can explore granularity levels interactively, while applied abstractions are automatically traced in the event log for explainability.","sentences":["Process events are recorded by multiple information systems at different granularity levels.","Based on the resulting event logs, process models are discovered at different granularity levels, as well.","Events stored at a fine-grained granularity level, for example, may hinder the discovered process model to be displayed due the high number of resulting model elements.","The discovered process model of a real-world manufacturing process, for example, consists of 1,489 model elements and over 2,000 arcs.","Existing process model abstraction techniques could help reducing the size of the model, but would disconnect it from the underlying event log.","Existing event abstraction techniques do neither support the analysis of mixed granularity levels, nor interactive exploration of a suitable granularity level.","To enable the exploration of discovered process models at different granularity levels, we propose INEXA, an interactive, explainable process model abstraction method that keeps the link to the event log.","As a starting point, INEXA aggregates large process models to a \"displayable\" size, e.g., for the manufacturing use case to a process model with 58 model elements.","Then, the process analyst can explore granularity levels interactively, while applied abstractions are automatically traced in the event log for explainability."],"url":"http://arxiv.org/abs/2403.18659v1"}
{"created":"2024-03-27 14:58:11","title":"MPC-CBF with Adaptive Safety Margins for Safety-critical Teleoperation over Imperfect Network Connections","abstract":"The paper focuses on the design of a control strategy for safety-critical remote teleoperation. The main goal is to make the controlled system track the desired velocity specified by an operator while avoiding obstacles despite communication delays. Control Barrier Functions (CBFs) are used to define the safety constraints that the system has to respect to avoid obstacles, while Model Predictive Control (MPC) provides the framework for adjusting the desired input, taking the constraints into account. The resulting input is sent to the remote system, where appropriate low-level velocity controllers translate it into system-specific commands. The main novelty of the paper is a method to make the CBFs robust against the uncertainties caused by the network delays affecting the system's state and do so in a less conservative manner. The results show how the proposed method successfully solves the safety-critical teleoperation problem, making the controlled systems avoid obstacles with different types of network delay. The controller has also been tested in simulation and on a real manipulator, demonstrating its general applicability when reliable low-level velocity controllers are available.","sentences":["The paper focuses on the design of a control strategy for safety-critical remote teleoperation.","The main goal is to make the controlled system track the desired velocity specified by an operator while avoiding obstacles despite communication delays.","Control Barrier Functions (CBFs) are used to define the safety constraints that the system has to respect to avoid obstacles, while Model Predictive Control (MPC) provides the framework for adjusting the desired input, taking the constraints into account.","The resulting input is sent to the remote system, where appropriate low-level velocity controllers translate it into system-specific commands.","The main novelty of the paper is a method to make the CBFs robust against the uncertainties caused by the network delays affecting the system's state and do so in a less conservative manner.","The results show how the proposed method successfully solves the safety-critical teleoperation problem, making the controlled systems avoid obstacles with different types of network delay.","The controller has also been tested in simulation and on a real manipulator, demonstrating its general applicability when reliable low-level velocity controllers are available."],"url":"http://arxiv.org/abs/2403.18650v1"}
{"created":"2024-03-27 14:56:44","title":"Addressing Data Annotation Challenges in Multiple Sensors: A Solution for Scania Collected Datasets","abstract":"Data annotation in autonomous vehicles is a critical step in the development of Deep Neural Network (DNN) based models or the performance evaluation of the perception system. This often takes the form of adding 3D bounding boxes on time-sequential and registered series of point-sets captured from active sensors like Light Detection and Ranging (LiDAR) and Radio Detection and Ranging (RADAR). When annotating multiple active sensors, there is a need to motion compensate and translate the points to a consistent coordinate frame and timestamp respectively. However, highly dynamic objects pose a unique challenge, as they can appear at different timestamps in each sensor's data. Without knowing the speed of the objects, their position appears to be different in different sensor outputs. Thus, even after motion compensation, highly dynamic objects are not matched from multiple sensors in the same frame, and human annotators struggle to add unique bounding boxes that capture all objects. This article focuses on addressing this challenge, primarily within the context of Scania collected datasets. The proposed solution takes a track of an annotated object as input and uses the Moving Horizon Estimation (MHE) to robustly estimate its speed. The estimated speed profile is utilized to correct the position of the annotated box and add boxes to object clusters missed by the original annotation.","sentences":["Data annotation in autonomous vehicles is a critical step in the development of Deep Neural Network (DNN) based models or the performance evaluation of the perception system.","This often takes the form of adding 3D bounding boxes on time-sequential and registered series of point-sets captured from active sensors like Light Detection and Ranging (LiDAR) and Radio Detection and Ranging (RADAR).","When annotating multiple active sensors, there is a need to motion compensate and translate the points to a consistent coordinate frame and timestamp respectively.","However, highly dynamic objects pose a unique challenge, as they can appear at different timestamps in each sensor's data.","Without knowing the speed of the objects, their position appears to be different in different sensor outputs.","Thus, even after motion compensation, highly dynamic objects are not matched from multiple sensors in the same frame, and human annotators struggle to add unique bounding boxes that capture all objects.","This article focuses on addressing this challenge, primarily within the context of Scania collected datasets.","The proposed solution takes a track of an annotated object as input and uses the Moving Horizon Estimation (MHE) to robustly estimate its speed.","The estimated speed profile is utilized to correct the position of the annotated box and add boxes to object clusters missed by the original annotation."],"url":"http://arxiv.org/abs/2403.18649v1"}
{"created":"2024-03-27 14:54:27","title":"SDSAT: Accelerating LLM Inference through Speculative Decoding with Semantic Adaptive Tokens","abstract":"We propose an acceleration scheme for large language models (LLMs) through Speculative Decoding with Semantic Adaptive Tokens (SDSAT). The primary objective of this design is to enhance the LLM model's ability to generate draft tokens more accurately without compromising the model's accuracy. The core strategies involve: 1) Fine-tune the model by incorporating semantic adaptive tokens that possess flexible decoding capabilities without changing its structure, allowing them to generate high-quality draft tokens. 2) By employing a training method that does not affect the standard tokens, the model can acquire parallel decoding abilities atop its original framework with minimal training overhead. 3) We have designed the \"two-step-draft-then-verify\" generation strategies using both greedy search and nucleus sampling. Experiments conducted on the CodeLlama-13B and 7B models have yielded speed increases of over 3.5X and 3.0X, respectively. Please refer to https://github.com/hasuoshenyun/SDSAT.","sentences":["We propose an acceleration scheme for large language models (LLMs) through Speculative Decoding with Semantic Adaptive Tokens (SDSAT).","The primary objective of this design is to enhance the LLM model's ability to generate draft tokens more accurately without compromising the model's accuracy.","The core strategies involve: 1) Fine-tune the model by incorporating semantic adaptive tokens that possess flexible decoding capabilities without changing its structure, allowing them to generate high-quality draft tokens.","2) By employing a training method that does not affect the standard tokens, the model can acquire parallel decoding abilities atop its original framework with minimal training overhead.","3) We have designed the \"two-step-draft-then-verify\" generation strategies using both greedy search and nucleus sampling.","Experiments conducted on the CodeLlama-13B and 7B models have yielded speed increases of over 3.5X and 3.0X, respectively.","Please refer to https://github.com/hasuoshenyun/SDSAT."],"url":"http://arxiv.org/abs/2403.18647v1"}
{"created":"2024-03-27 14:46:54","title":"Sampling-Based Motion Planning with Online Racing Line Generation for Autonomous Driving on Three-Dimensional Race Tracks","abstract":"Existing approaches to trajectory planning for autonomous racing employ sampling-based methods, generating numerous jerk-optimal trajectories and selecting the most favorable feasible trajectory based on a cost function penalizing deviations from an offline-calculated racing line. While successful on oval tracks, these methods face limitations on complex circuits due to the simplistic geometry of jerk-optimal edges failing to capture the complexity of the racing line. Additionally, they only consider two-dimensional tracks, potentially neglecting or surpassing the actual dynamic potential. In this paper, we present a sampling-based local trajectory planning approach for autonomous racing that can maintain the lap time of the racing line even on complex race tracks and consider the race track's three-dimensional effects. In simulative experiments, we demonstrate that our approach achieves lower lap times and improved utilization of dynamic limits compared to existing approaches. We also investigate the impact of online racing line generation, in which the time-optimal solution is planned from the current vehicle state for a limited spatial horizon, in contrast to a closed racing line calculated offline. We show that combining the sampling-based planner with the online racing line generation can significantly reduce lap times in multi-vehicle scenarios.","sentences":["Existing approaches to trajectory planning for autonomous racing employ sampling-based methods, generating numerous jerk-optimal trajectories and selecting the most favorable feasible trajectory based on a cost function penalizing deviations from an offline-calculated racing line.","While successful on oval tracks, these methods face limitations on complex circuits due to the simplistic geometry of jerk-optimal edges failing to capture the complexity of the racing line.","Additionally, they only consider two-dimensional tracks, potentially neglecting or surpassing the actual dynamic potential.","In this paper, we present a sampling-based local trajectory planning approach for autonomous racing that can maintain the lap time of the racing line even on complex race tracks and consider the race track's three-dimensional effects.","In simulative experiments, we demonstrate that our approach achieves lower lap times and improved utilization of dynamic limits compared to existing approaches.","We also investigate the impact of online racing line generation, in which the time-optimal solution is planned from the current vehicle state for a limited spatial horizon, in contrast to a closed racing line calculated offline.","We show that combining the sampling-based planner with the online racing line generation can significantly reduce lap times in multi-vehicle scenarios."],"url":"http://arxiv.org/abs/2403.18643v1"}
{"created":"2024-03-27 14:45:43","title":"Collective schedules: axioms and algorithms","abstract":"The collective schedules problem consists in computing a schedule of tasks shared between individuals. Tasks may have different duration, and individuals have preferences over the order of the shared tasks. This problem has numerous applications since tasks may model public infrastructure projects, events taking place in a shared room, or work done by co-workers. Our aim is, given the preferred schedules of individuals (voters), to return a consensus schedule. We propose an axiomatic study of the collective schedule problem, by using classic axioms in computational social choice and new axioms that take into account the duration of the tasks. We show that some axioms are incompatible, and we study the axioms fulfilled by three rules: one which has been studied in the seminal paper on collective schedules (Pascual et al. 2018), one which generalizes the Kemeny rule, and one which generalizes Spearman's footrule. From an algorithmic point of view, we show that these rules solve NP-hard problems, but that it is possible to solve optimally these problems for small but realistic size instances, and we give an efficient heuristic for large instances. We conclude this paper with experiments.","sentences":["The collective schedules problem consists in computing a schedule of tasks shared between individuals.","Tasks may have different duration, and individuals have preferences over the order of the shared tasks.","This problem has numerous applications since tasks may model public infrastructure projects, events taking place in a shared room, or work done by co-workers.","Our aim is, given the preferred schedules of individuals (voters), to return a consensus schedule.","We propose an axiomatic study of the collective schedule problem, by using classic axioms in computational social choice and new axioms that take into account the duration of the tasks.","We show that some axioms are incompatible, and we study the axioms fulfilled by three rules: one which has been studied in the seminal paper on collective schedules (Pascual et al. 2018), one which generalizes the Kemeny rule, and one which generalizes Spearman's footrule.","From an algorithmic point of view, we show that these rules solve NP-hard problems, but that it is possible to solve optimally these problems for small but realistic size instances, and we give an efficient heuristic for large instances.","We conclude this paper with experiments."],"url":"http://arxiv.org/abs/2403.18642v1"}
{"created":"2024-03-27 14:40:25","title":"Fusion approaches for emotion recognition from speech using acoustic and text-based features","abstract":"In this paper, we study different approaches for classifying emotions from speech using acoustic and text-based features. We propose to obtain contextualized word embeddings with BERT to represent the information contained in speech transcriptions and show that this results in better performance than using Glove embeddings. We also propose and compare different strategies to combine the audio and text modalities, evaluating them on IEMOCAP and MSP-PODCAST datasets. We find that fusing acoustic and text-based systems is beneficial on both datasets, though only subtle differences are observed across the evaluated fusion approaches. Finally, for IEMOCAP, we show the large effect that the criteria used to define the cross-validation folds have on results. In particular, the standard way of creating folds for this dataset results in a highly optimistic estimation of performance for the text-based system, suggesting that some previous works may overestimate the advantage of incorporating transcriptions.","sentences":["In this paper, we study different approaches for classifying emotions from speech using acoustic and text-based features.","We propose to obtain contextualized word embeddings with BERT to represent the information contained in speech transcriptions and show that this results in better performance than using Glove embeddings.","We also propose and compare different strategies to combine the audio and text modalities, evaluating them on IEMOCAP and MSP-PODCAST datasets.","We find that fusing acoustic and text-based systems is beneficial on both datasets, though only subtle differences are observed across the evaluated fusion approaches.","Finally, for IEMOCAP, we show the large effect that the criteria used to define the cross-validation folds have on results.","In particular, the standard way of creating folds for this dataset results in a highly optimistic estimation of performance for the text-based system, suggesting that some previous works may overestimate the advantage of incorporating transcriptions."],"url":"http://arxiv.org/abs/2403.18635v1"}
{"created":"2024-03-27 14:38:02","title":"First Experiences with the Identification of People at Risk for Diabetes in Argentina using Machine Learning Techniques","abstract":"Detecting Type 2 Diabetes (T2D) and Prediabetes (PD) is a real challenge for medicine due to the absence of pathogenic symptoms and the lack of known associated risk factors. Even though some proposals for machine learning models enable the identification of people at risk, the nature of the condition makes it so that a model suitable for one population may not necessarily be suitable for another. In this article, the development and assessment of predictive models to identify people at risk for T2D and PD specifically in Argentina are discussed. First, the database was thoroughly preprocessed and three specific datasets were generated considering a compromise between the number of records and the amount of available variables. After applying 5 different classification models, the results obtained show that a very good performance was observed for two datasets with some of these models. In particular, RF, DT, and ANN demonstrated great classification power, with good values for the metrics under consideration. Given the lack of this type of tool in Argentina, this work represents the first step towards the development of more sophisticated models.","sentences":["Detecting Type 2 Diabetes (T2D) and Prediabetes (PD) is a real challenge for medicine due to the absence of pathogenic symptoms and the lack of known associated risk factors.","Even though some proposals for machine learning models enable the identification of people at risk, the nature of the condition makes it so that a model suitable for one population may not necessarily be suitable for another.","In this article, the development and assessment of predictive models to identify people at risk for T2D and PD specifically in Argentina are discussed.","First, the database was thoroughly preprocessed and three specific datasets were generated considering a compromise between the number of records and the amount of available variables.","After applying 5 different classification models, the results obtained show that a very good performance was observed for two datasets with some of these models.","In particular, RF, DT, and ANN demonstrated great classification power, with good values for the metrics under consideration.","Given the lack of this type of tool in Argentina, this work represents the first step towards the development of more sophisticated models."],"url":"http://arxiv.org/abs/2403.18631v1"}
{"created":"2024-03-27 14:37:01","title":"To Recommend or Not: Recommendability Identification in Conversations with Pre-trained Language Models","abstract":"Most current recommender systems primarily focus on what to recommend, assuming users always require personalized recommendations. However, with the widely spread of ChatGPT and other chatbots, a more crucial problem in the context of conversational systems is how to minimize user disruption when we provide recommendation services for users. While previous research has extensively explored different user intents in dialogue systems, fewer efforts are made to investigate whether recommendations should be provided. In this paper, we formally define the recommendability identification problem, which aims to determine whether recommendations are necessary in a specific scenario. First, we propose and define the recommendability identification task, which investigates the need for recommendations in the current conversational context. A new dataset is constructed. Subsequently, we discuss and evaluate the feasibility of leveraging pre-trained language models (PLMs) for recommendability identification. Finally, through comparative experiments, we demonstrate that directly employing PLMs with zero-shot results falls short of meeting the task requirements. Besides, fine-tuning or utilizing soft prompt techniques yields comparable results to traditional classification methods. Our work is the first to study recommendability before recommendation and provides preliminary ways to make it a fundamental component of the future recommendation system.","sentences":["Most current recommender systems primarily focus on what to recommend, assuming users always require personalized recommendations.","However, with the widely spread of ChatGPT and other chatbots, a more crucial problem in the context of conversational systems is how to minimize user disruption when we provide recommendation services for users.","While previous research has extensively explored different user intents in dialogue systems, fewer efforts are made to investigate whether recommendations should be provided.","In this paper, we formally define the recommendability identification problem, which aims to determine whether recommendations are necessary in a specific scenario.","First, we propose and define the recommendability identification task, which investigates the need for recommendations in the current conversational context.","A new dataset is constructed.","Subsequently, we discuss and evaluate the feasibility of leveraging pre-trained language models (PLMs) for recommendability identification.","Finally, through comparative experiments, we demonstrate that directly employing PLMs with zero-shot results falls short of meeting the task requirements.","Besides, fine-tuning or utilizing soft prompt techniques yields comparable results to traditional classification methods.","Our work is the first to study recommendability before recommendation and provides preliminary ways to make it a fundamental component of the future recommendation system."],"url":"http://arxiv.org/abs/2403.18628v1"}
{"created":"2024-03-27 14:34:29","title":"Vulnerability Detection with Code Language Models: How Far Are We?","abstract":"In the context of the rising interest in code language models (code LMs) and vulnerability detection, we study the effectiveness of code LMs for detecting vulnerabilities. Our analysis reveals significant shortcomings in existing vulnerability datasets, including poor data quality, low label accuracy, and high duplication rates, leading to unreliable model performance in realistic vulnerability detection scenarios. Additionally, the evaluation methods used with these datasets are not representative of real-world vulnerability detection.   To address these challenges, we introduce PrimeVul, a new dataset for training and evaluating code LMs for vulnerability detection. PrimeVul incorporates a novel set of data labeling techniques that achieve comparable label accuracy to human-verified benchmarks while significantly expanding the dataset. It also implements a rigorous data de-duplication and chronological data splitting strategy to mitigate data leakage issues, alongside introducing more realistic evaluation metrics and settings. This comprehensive approach aims to provide a more accurate assessment of code LMs' performance in real-world conditions.   Evaluating code LMs on PrimeVul reveals that existing benchmarks significantly overestimate the performance of these models. For instance, a state-of-the-art 7B model scored 68.26% F1 on BigVul but only 3.09% F1 on PrimeVul. Attempts to improve performance through advanced training techniques and larger models like GPT-3.5 and GPT-4 were unsuccessful, with results akin to random guessing in the most stringent settings. These findings underscore the considerable gap between current capabilities and the practical requirements for deploying code LMs in security roles, highlighting the need for more innovative research in this domain.","sentences":["In the context of the rising interest in code language models (code LMs) and vulnerability detection, we study the effectiveness of code LMs for detecting vulnerabilities.","Our analysis reveals significant shortcomings in existing vulnerability datasets, including poor data quality, low label accuracy, and high duplication rates, leading to unreliable model performance in realistic vulnerability detection scenarios.","Additionally, the evaluation methods used with these datasets are not representative of real-world vulnerability detection.   ","To address these challenges, we introduce PrimeVul, a new dataset for training and evaluating code LMs for vulnerability detection.","PrimeVul incorporates a novel set of data labeling techniques that achieve comparable label accuracy to human-verified benchmarks while significantly expanding the dataset.","It also implements a rigorous data de-duplication and chronological data splitting strategy to mitigate data leakage issues, alongside introducing more realistic evaluation metrics and settings.","This comprehensive approach aims to provide a more accurate assessment of code LMs' performance in real-world conditions.   ","Evaluating code LMs on PrimeVul reveals that existing benchmarks significantly overestimate the performance of these models.","For instance, a state-of-the-art 7B model scored 68.26% F1 on BigVul but only 3.09% F1 on PrimeVul.","Attempts to improve performance through advanced training techniques and larger models like GPT-3.5 and GPT-4 were unsuccessful, with results akin to random guessing in the most stringent settings.","These findings underscore the considerable gap between current capabilities and the practical requirements for deploying code LMs in security roles, highlighting the need for more innovative research in this domain."],"url":"http://arxiv.org/abs/2403.18624v1"}
{"created":"2024-03-27 14:34:22","title":"Antitrust, Amazon, and Algorithmic Auditing","abstract":"In digital markets, antitrust law and special regulations aim to ensure that markets remain competitive despite the dominating role that digital platforms play today in everyone's life. Unlike traditional markets, market participant behavior is easily observable in these markets. We present a series of empirical investigations into the extent to which Amazon engages in practices that are typically described as self-preferencing. We discuss how the computer science tools used in this paper can be used in a regulatory environment that is based on algorithmic auditing and requires regulating digital markets at scale.","sentences":["In digital markets, antitrust law and special regulations aim to ensure that markets remain competitive despite the dominating role that digital platforms play today in everyone's life.","Unlike traditional markets, market participant behavior is easily observable in these markets.","We present a series of empirical investigations into the extent to which Amazon engages in practices that are typically described as self-preferencing.","We discuss how the computer science tools used in this paper can be used in a regulatory environment that is based on algorithmic auditing and requires regulating digital markets at scale."],"url":"http://arxiv.org/abs/2403.18623v1"}
{"created":"2024-03-27 14:33:58","title":"qIoV: A Quantum-Driven Internet-of-Vehicles-Based Approach for Environmental Monitoring and Rapid Response Systems","abstract":"This research addresses the critical necessity for advanced rapid response operations in managing a spectrum of environmental hazards. We propose a novel framework, qIoV that integrates quantum computing with the Internet-of-Vehicles (IoV) to leverage the computational efficiency, parallelism, and entanglement properties of quantum mechanics. Our approach involves the use of environmental sensors mounted on vehicles for precise air quality assessment. These sensors are designed to be highly sensitive and accurate, leveraging the principles of quantum mechanics to detect and measure environmental parameters. A salient feature of our proposal is the Quantum Mesh Network Fabric (QMF), a system designed to dynamically adjust the quantum network topology in accordance with vehicular movements. This capability is critical to maintaining the integrity of quantum states against environmental and vehicular disturbances, thereby ensuring reliable data transmission and processing. Moreover, our methodology is further augmented by the incorporation of a variational quantum classifier (VQC) with advanced quantum entanglement techniques. This integration offers a significant reduction in latency for hazard alert transmission, thus enabling expedited communication of crucial data to emergency response teams and the public. Our study on the IBM OpenQSAM 3 platform, utilizing a 127 Qubit system, revealed significant advancements in pair plot analysis, achieving over 90% in precision, recall, and F1-Score metrics and an 83% increase in the speed of toxic gas detection compared to conventional methods.Additionally, theoretical analyses validate the efficiency of quantum rotation, teleportation protocols, and the fidelity of quantum entanglement, further underscoring the potential of quantum computing in enhancing analytical performance.","sentences":["This research addresses the critical necessity for advanced rapid response operations in managing a spectrum of environmental hazards.","We propose a novel framework, qIoV that integrates quantum computing with the Internet-of-Vehicles (IoV) to leverage the computational efficiency, parallelism, and entanglement properties of quantum mechanics.","Our approach involves the use of environmental sensors mounted on vehicles for precise air quality assessment.","These sensors are designed to be highly sensitive and accurate, leveraging the principles of quantum mechanics to detect and measure environmental parameters.","A salient feature of our proposal is the Quantum Mesh Network Fabric (QMF), a system designed to dynamically adjust the quantum network topology in accordance with vehicular movements.","This capability is critical to maintaining the integrity of quantum states against environmental and vehicular disturbances, thereby ensuring reliable data transmission and processing.","Moreover, our methodology is further augmented by the incorporation of a variational quantum classifier (VQC) with advanced quantum entanglement techniques.","This integration offers a significant reduction in latency for hazard alert transmission, thus enabling expedited communication of crucial data to emergency response teams and the public.","Our study on the IBM OpenQSAM 3 platform, utilizing a 127 Qubit system, revealed significant advancements in pair plot analysis, achieving over 90% in precision, recall, and F1-Score metrics and an 83% increase in the speed of toxic gas detection compared to conventional methods.","Additionally, theoretical analyses validate the efficiency of quantum rotation, teleportation protocols, and the fidelity of quantum entanglement, further underscoring the potential of quantum computing in enhancing analytical performance."],"url":"http://arxiv.org/abs/2403.18622v1"}
{"created":"2024-03-27 14:32:56","title":"Enhanced OpenMP Algorithm to Compute All-Pairs Shortest Path on x86 Architectures","abstract":"Graphs have become a key tool when modeling and solving problems in different areas. The Floyd-Warshall (FW) algorithm computes the shortest path between all pairs of vertices in a graph and is employed in areas like communication networking, traffic routing, bioinformatics, among others. However, FW is computationally and spatially expensive since it requires O(n^3) operations and O(n^2) memory space. As the graph gets larger, parallel computing becomes necessary to provide a solution in an acceptable time range. In this paper, we studied a FW code developed for Xeon Phi KNL processors and adapted it to run on any Intel x86 processors, losing the specificity of the former. To do so, we verified one by one the optimizations proposed by the original code, making adjustments to the base code where necessary, and analyzing its performance on two Intel servers under different test scenarios. In addition, a new optimization was proposed to increase the concurrency degree of the parallel algorithm, which was implemented using two different synchronization mechanisms. The experimental results show that all optimizations were beneficial on the two x86 platforms selected. Last, the new optimization proposal improved performance by up to 23%.","sentences":["Graphs have become a key tool when modeling and solving problems in different areas.","The Floyd-Warshall (FW) algorithm computes the shortest path between all pairs of vertices in a graph and is employed in areas like communication networking, traffic routing, bioinformatics, among others.","However, FW is computationally and spatially expensive since it requires O(n^3) operations and O(n^2) memory space.","As the graph gets larger, parallel computing becomes necessary to provide a solution in an acceptable time range.","In this paper, we studied a FW code developed for Xeon Phi KNL processors and adapted it to run on any Intel x86 processors, losing the specificity of the former.","To do so, we verified one by one the optimizations proposed by the original code, making adjustments to the base code where necessary, and analyzing its performance on two Intel servers under different test scenarios.","In addition, a new optimization was proposed to increase the concurrency degree of the parallel algorithm, which was implemented using two different synchronization mechanisms.","The experimental results show that all optimizations were beneficial on the two x86 platforms selected.","Last, the new optimization proposal improved performance by up to 23%."],"url":"http://arxiv.org/abs/2403.18619v1"}
{"created":"2024-03-27 14:30:56","title":"Will You Participate? Exploring the Potential of Robotics Competitions on Human-centric Topics","abstract":"This paper presents findings from an exploratory needfinding study investigating the research current status and potential participation of the competitions on the robotics community towards four human-centric topics: safety, privacy, explainability, and federated learning. We conducted a survey with 34 participants across three distinguished European robotics consortia, nearly 60% of whom possessed over five years of research experience in robotics. Our qualitative and quantitative analysis revealed that current mainstream robotic researchers prioritize safety and explainability, expressing a greater willingness to invest in further research in these areas. Conversely, our results indicate that privacy and federated learning garner less attention and are perceived to have lower potential. Additionally, the study suggests a lack of enthusiasm within the robotics community for participating in competitions related to these topics. Based on these findings, we recommend targeting other communities, such as the machine learning community, for future competitions related to these four human-centric topics.","sentences":["This paper presents findings from an exploratory needfinding study investigating the research current status and potential participation of the competitions on the robotics community towards four human-centric topics: safety, privacy, explainability, and federated learning.","We conducted a survey with 34 participants across three distinguished European robotics consortia, nearly 60% of whom possessed over five years of research experience in robotics.","Our qualitative and quantitative analysis revealed that current mainstream robotic researchers prioritize safety and explainability, expressing a greater willingness to invest in further research in these areas.","Conversely, our results indicate that privacy and federated learning garner less attention and are perceived to have lower potential.","Additionally, the study suggests a lack of enthusiasm within the robotics community for participating in competitions related to these topics.","Based on these findings, we recommend targeting other communities, such as the machine learning community, for future competitions related to these four human-centric topics."],"url":"http://arxiv.org/abs/2403.18616v1"}
