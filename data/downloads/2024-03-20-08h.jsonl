{"created":"2024-03-19 17:59:56","title":"LLMLingua-2: Data Distillation for Efficient and Faithful Task-Agnostic Prompt Compression","abstract":"This paper focuses on task-agnostic prompt compression for better generalizability and efficiency. Considering the redundancy in natural language, existing approaches compress prompts by removing tokens or lexical units according to their information entropy obtained from a causal language model such as LLaMa-7B. The challenge is that information entropy may be a suboptimal compression metric: (i) it only leverages unidirectional context and may fail to capture all essential information needed for prompt compression; (ii) it is not aligned with the prompt compression objective.   To address these issues, we propose a data distillation procedure to derive knowledge from an LLM to compress prompts without losing crucial information, and meantime, introduce an extractive text compression dataset. We formulate prompt compression as a token classification problem to guarantee the faithfulness of the compressed prompt to the original one, and use a Transformer encoder as the base architecture to capture all essential information for prompt compression from the full bidirectional context. Our approach leads to lower latency by explicitly learning the compression objective with smaller models such as XLM-RoBERTa-large and mBERT.   We evaluate our method on both in-domain and out-of-domain datasets, including MeetingBank, LongBench, ZeroScrolls, GSM8K, and BBH. Despite its small size, our model shows significant performance gains over strong baselines and demonstrates robust generalization ability across different LLMs. Additionally, our model is 3x-6x faster than existing prompt compression methods, while accelerating the end-to-end latency by 1.6x-2.9x with compression ratios of 2x-5x.","sentences":["This paper focuses on task-agnostic prompt compression for better generalizability and efficiency.","Considering the redundancy in natural language, existing approaches compress prompts by removing tokens or lexical units according to their information entropy obtained from a causal language model such as LLaMa-7B. The challenge is that information entropy may be a suboptimal compression metric: (i) it only leverages unidirectional context and may fail to capture all essential information needed for prompt compression; (ii) it is not aligned with the prompt compression objective.   ","To address these issues, we propose a data distillation procedure to derive knowledge from an LLM to compress prompts without losing crucial information, and meantime, introduce an extractive text compression dataset.","We formulate prompt compression as a token classification problem to guarantee the faithfulness of the compressed prompt to the original one, and use a Transformer encoder as the base architecture to capture all essential information for prompt compression from the full bidirectional context.","Our approach leads to lower latency by explicitly learning the compression objective with smaller models such as XLM-RoBERTa-large and mBERT.   ","We evaluate our method on both in-domain and out-of-domain datasets, including MeetingBank, LongBench, ZeroScrolls, GSM8K, and BBH.","Despite its small size, our model shows significant performance gains over strong baselines and demonstrates robust generalization ability across different LLMs.","Additionally, our model is 3x-6x faster than existing prompt compression methods, while accelerating the end-to-end latency by 1.6x-2.9x with compression ratios of 2x-5x."],"url":"http://arxiv.org/abs/2403.12968v1"}
{"created":"2024-03-19 17:59:52","title":"Wear-Any-Way: Manipulable Virtual Try-on via Sparse Correspondence Alignment","abstract":"This paper introduces a novel framework for virtual try-on, termed Wear-Any-Way. Different from previous methods, Wear-Any-Way is a customizable solution. Besides generating high-fidelity results, our method supports users to precisely manipulate the wearing style. To achieve this goal, we first construct a strong pipeline for standard virtual try-on, supporting single/multiple garment try-on and model-to-model settings in complicated scenarios. To make it manipulable, we propose sparse correspondence alignment which involves point-based control to guide the generation for specific locations. With this design, Wear-Any-Way gets state-of-the-art performance for the standard setting and provides a novel interaction form for customizing the wearing style. For instance, it supports users to drag the sleeve to make it rolled up, drag the coat to make it open, and utilize clicks to control the style of tuck, etc. Wear-Any-Way enables more liberated and flexible expressions of the attires, holding profound implications in the fashion industry.","sentences":["This paper introduces a novel framework for virtual try-on, termed Wear-Any-Way.","Different from previous methods, Wear-Any-Way is a customizable solution.","Besides generating high-fidelity results, our method supports users to precisely manipulate the wearing style.","To achieve this goal, we first construct a strong pipeline for standard virtual try-on, supporting single/multiple garment try-on and model-to-model settings in complicated scenarios.","To make it manipulable, we propose sparse correspondence alignment which involves point-based control to guide the generation for specific locations.","With this design, Wear-Any-Way gets state-of-the-art performance for the standard setting and provides a novel interaction form for customizing the wearing style.","For instance, it supports users to drag the sleeve to make it rolled up, drag the coat to make it open, and utilize clicks to control the style of tuck, etc.","Wear-Any-Way enables more liberated and flexible expressions of the attires, holding profound implications in the fashion industry."],"url":"http://arxiv.org/abs/2403.12965v1"}
{"created":"2024-03-19 17:59:52","title":"Chain-of-Spot: Interactive Reasoning Improves Large Vision-Language Models","abstract":"In the realm of vision-language understanding, the proficiency of models in interpreting and reasoning over visual content has become a cornerstone for numerous applications. However, it is challenging for the visual encoder in Large Vision-Language Models (LVLMs) to extract useful features tailored to questions that aid the language model's response. Furthermore, a common practice among existing LVLMs is to utilize lower-resolution images, which restricts the ability for visual recognition. Our work introduces the Chain-of-Spot (CoS) method, which we describe as Interactive Reasoning, a novel approach that enhances feature extraction by focusing on key regions of interest (ROI) within the image, corresponding to the posed questions or instructions. This technique allows LVLMs to access more detailed visual information without altering the original image resolution, thereby offering multi-granularity image features. By integrating Chain-of-Spot with instruct-following LLaVA-1.5 models, the process of image reasoning consistently improves performance across a wide range of multimodal datasets and benchmarks without bells and whistles and achieves new state-of-the-art results. Our empirical findings demonstrate a significant improvement in LVLMs' ability to understand and reason about visual content, paving the way for more sophisticated visual instruction-following applications. Code and models are available at https://github.com/dongyh20/Chain-of-Spot","sentences":["In the realm of vision-language understanding, the proficiency of models in interpreting and reasoning over visual content has become a cornerstone for numerous applications.","However, it is challenging for the visual encoder in Large Vision-Language Models (LVLMs) to extract useful features tailored to questions that aid the language model's response.","Furthermore, a common practice among existing LVLMs is to utilize lower-resolution images, which restricts the ability for visual recognition.","Our work introduces the Chain-of-Spot (CoS) method, which we describe as Interactive Reasoning, a novel approach that enhances feature extraction by focusing on key regions of interest (ROI) within the image, corresponding to the posed questions or instructions.","This technique allows LVLMs to access more detailed visual information without altering the original image resolution, thereby offering multi-granularity image features.","By integrating Chain-of-Spot with instruct-following LLaVA-1.5 models, the process of image reasoning consistently improves performance across a wide range of multimodal datasets and benchmarks without bells and whistles and achieves new state-of-the-art results.","Our empirical findings demonstrate a significant improvement in LVLMs' ability to understand and reason about visual content, paving the way for more sophisticated visual instruction-following applications.","Code and models are available at https://github.com/dongyh20/Chain-of-Spot"],"url":"http://arxiv.org/abs/2403.12966v1"}
{"created":"2024-03-19 17:59:39","title":"Negative Yields Positive: Unified Dual-Path Adapter for Vision-Language Models","abstract":"Recently, large-scale pre-trained Vision-Language Models (VLMs) have demonstrated great potential in learning open-world visual representations, and exhibit remarkable performance across a wide range of downstream tasks through efficient fine-tuning. In this work, we innovatively introduce the concept of dual learning into fine-tuning VLMs, i.e., we not only learn what an image is, but also what an image isn't. Building on this concept, we introduce a novel DualAdapter approach to enable dual-path adaptation of VLMs from both positive and negative perspectives with only limited annotated samples. In the inference stage, our DualAdapter performs unified predictions by simultaneously conducting complementary positive selection and negative exclusion across target classes, thereby enhancing the overall recognition accuracy of VLMs in downstream tasks. Our extensive experimental results across 15 datasets validate that the proposed DualAdapter outperforms existing state-of-the-art methods on both few-shot learning and domain generalization tasks while achieving competitive computational efficiency. Code is available at https://github.com/zhangce01/DualAdapter.","sentences":["Recently, large-scale pre-trained Vision-Language Models (VLMs) have demonstrated great potential in learning open-world visual representations, and exhibit remarkable performance across a wide range of downstream tasks through efficient fine-tuning.","In this work, we innovatively introduce the concept of dual learning into fine-tuning VLMs, i.e., we not only learn what an image is, but also what an image isn't.","Building on this concept, we introduce a novel DualAdapter approach to enable dual-path adaptation of VLMs from both positive and negative perspectives with only limited annotated samples.","In the inference stage, our DualAdapter performs unified predictions by simultaneously conducting complementary positive selection and negative exclusion across target classes, thereby enhancing the overall recognition accuracy of VLMs in downstream tasks.","Our extensive experimental results across 15 datasets validate that the proposed DualAdapter outperforms existing state-of-the-art methods on both few-shot learning and domain generalization tasks while achieving competitive computational efficiency.","Code is available at https://github.com/zhangce01/DualAdapter."],"url":"http://arxiv.org/abs/2403.12964v1"}
{"created":"2024-03-19 17:59:33","title":"FouriScale: A Frequency Perspective on Training-Free High-Resolution Image Synthesis","abstract":"In this study, we delve into the generation of high-resolution images from pre-trained diffusion models, addressing persistent challenges, such as repetitive patterns and structural distortions, that emerge when models are applied beyond their trained resolutions. To address this issue, we introduce an innovative, training-free approach FouriScale from the perspective of frequency domain analysis. We replace the original convolutional layers in pre-trained diffusion models by incorporating a dilation technique along with a low-pass operation, intending to achieve structural consistency and scale consistency across resolutions, respectively. Further enhanced by a padding-then-crop strategy, our method can flexibly handle text-to-image generation of various aspect ratios. By using the FouriScale as guidance, our method successfully balances the structural integrity and fidelity of generated images, achieving an astonishing capacity of arbitrary-size, high-resolution, and high-quality generation. With its simplicity and compatibility, our method can provide valuable insights for future explorations into the synthesis of ultra-high-resolution images. The code will be released at https://github.com/LeonHLJ/FouriScale.","sentences":["In this study, we delve into the generation of high-resolution images from pre-trained diffusion models, addressing persistent challenges, such as repetitive patterns and structural distortions, that emerge when models are applied beyond their trained resolutions.","To address this issue, we introduce an innovative, training-free approach FouriScale from the perspective of frequency domain analysis.","We replace the original convolutional layers in pre-trained diffusion models by incorporating a dilation technique along with a low-pass operation, intending to achieve structural consistency and scale consistency across resolutions, respectively.","Further enhanced by a padding-then-crop strategy, our method can flexibly handle text-to-image generation of various aspect ratios.","By using the FouriScale as guidance, our method successfully balances the structural integrity and fidelity of generated images, achieving an astonishing capacity of arbitrary-size, high-resolution, and high-quality generation.","With its simplicity and compatibility, our method can provide valuable insights for future explorations into the synthesis of ultra-high-resolution images.","The code will be released at https://github.com/LeonHLJ/FouriScale."],"url":"http://arxiv.org/abs/2403.12963v1"}
{"created":"2024-03-19 17:59:18","title":"FRESCO: Spatial-Temporal Correspondence for Zero-Shot Video Translation","abstract":"The remarkable efficacy of text-to-image diffusion models has motivated extensive exploration of their potential application in video domains. Zero-shot methods seek to extend image diffusion models to videos without necessitating model training. Recent methods mainly focus on incorporating inter-frame correspondence into attention mechanisms. However, the soft constraint imposed on determining where to attend to valid features can sometimes be insufficient, resulting in temporal inconsistency. In this paper, we introduce FRESCO, intra-frame correspondence alongside inter-frame correspondence to establish a more robust spatial-temporal constraint. This enhancement ensures a more consistent transformation of semantically similar content across frames. Beyond mere attention guidance, our approach involves an explicit update of features to achieve high spatial-temporal consistency with the input video, significantly improving the visual coherence of the resulting translated videos. Extensive experiments demonstrate the effectiveness of our proposed framework in producing high-quality, coherent videos, marking a notable improvement over existing zero-shot methods.","sentences":["The remarkable efficacy of text-to-image diffusion models has motivated extensive exploration of their potential application in video domains.","Zero-shot methods seek to extend image diffusion models to videos without necessitating model training.","Recent methods mainly focus on incorporating inter-frame correspondence into attention mechanisms.","However, the soft constraint imposed on determining where to attend to valid features can sometimes be insufficient, resulting in temporal inconsistency.","In this paper, we introduce FRESCO, intra-frame correspondence alongside inter-frame correspondence to establish a more robust spatial-temporal constraint.","This enhancement ensures a more consistent transformation of semantically similar content across frames.","Beyond mere attention guidance, our approach involves an explicit update of features to achieve high spatial-temporal consistency with the input video, significantly improving the visual coherence of the resulting translated videos.","Extensive experiments demonstrate the effectiveness of our proposed framework in producing high-quality, coherent videos, marking a notable improvement over existing zero-shot methods."],"url":"http://arxiv.org/abs/2403.12962v1"}
{"created":"2024-03-19 17:59:09","title":"TexTile: A Differentiable Metric for Texture Tileability","abstract":"We introduce TexTile, a novel differentiable metric to quantify the degree upon which a texture image can be concatenated with itself without introducing repeating artifacts (i.e., the tileability). Existing methods for tileable texture synthesis focus on general texture quality, but lack explicit analysis of the intrinsic repeatability properties of a texture. In contrast, our TexTile metric effectively evaluates the tileable properties of a texture, opening the door to more informed synthesis and analysis of tileable textures. Under the hood, TexTile is formulated as a binary classifier carefully built from a large dataset of textures of different styles, semantics, regularities, and human annotations.Key to our method is a set of architectural modifications to baseline pre-train image classifiers to overcome their shortcomings at measuring tileability, along with a custom data augmentation and training regime aimed at increasing robustness and accuracy. We demonstrate that TexTile can be plugged into different state-of-the-art texture synthesis methods, including diffusion-based strategies, and generate tileable textures while keeping or even improving the overall texture quality. Furthermore, we show that TexTile can objectively evaluate any tileable texture synthesis method, whereas the current mix of existing metrics produces uncorrelated scores which heavily hinders progress in the field.","sentences":["We introduce TexTile, a novel differentiable metric to quantify the degree upon which a texture image can be concatenated with itself without introducing repeating artifacts (i.e., the tileability).","Existing methods for tileable texture synthesis focus on general texture quality, but lack explicit analysis of the intrinsic repeatability properties of a texture.","In contrast, our TexTile metric effectively evaluates the tileable properties of a texture, opening the door to more informed synthesis and analysis of tileable textures.","Under the hood, TexTile is formulated as a binary classifier carefully built from a large dataset of textures of different styles, semantics, regularities, and human annotations.","Key to our method is a set of architectural modifications to baseline pre-train image classifiers to overcome their shortcomings at measuring tileability, along with a custom data augmentation and training regime aimed at increasing robustness and accuracy.","We demonstrate that TexTile can be plugged into different state-of-the-art texture synthesis methods, including diffusion-based strategies, and generate tileable textures while keeping or even improving the overall texture quality.","Furthermore, we show that TexTile can objectively evaluate any tileable texture synthesis method, whereas the current mix of existing metrics produces uncorrelated scores which heavily hinders progress in the field."],"url":"http://arxiv.org/abs/2403.12961v1"}
{"created":"2024-03-19 17:58:04","title":"FaceXFormer: A Unified Transformer for Facial Analysis","abstract":"In this work, we introduce FaceXformer, an end-to-end unified transformer model for a comprehensive range of facial analysis tasks such as face parsing, landmark detection, head pose estimation, attributes recognition, and estimation of age, gender, race, and landmarks visibility. Conventional methods in face analysis have often relied on task-specific designs and preprocessing techniques, which limit their approach to a unified architecture. Unlike these conventional methods, our FaceXformer leverages a transformer-based encoder-decoder architecture where each task is treated as a learnable token, enabling the integration of multiple tasks within a single framework. Moreover, we propose a parameter-efficient decoder, FaceX, which jointly processes face and task tokens, thereby learning generalized and robust face representations across different tasks. To the best of our knowledge, this is the first work to propose a single model capable of handling all these facial analysis tasks using transformers. We conducted a comprehensive analysis of effective backbones for unified face task processing and evaluated different task queries and the synergy between them. We conduct experiments against state-of-the-art specialized models and previous multi-task models in both intra-dataset and cross-dataset evaluations across multiple benchmarks. Additionally, our model effectively handles images \"in-the-wild,\" demonstrating its robustness and generalizability across eight different tasks, all while maintaining the real-time performance of 37 FPS.","sentences":["In this work, we introduce FaceXformer, an end-to-end unified transformer model for a comprehensive range of facial analysis tasks such as face parsing, landmark detection, head pose estimation, attributes recognition, and estimation of age, gender, race, and landmarks visibility.","Conventional methods in face analysis have often relied on task-specific designs and preprocessing techniques, which limit their approach to a unified architecture.","Unlike these conventional methods, our FaceXformer leverages a transformer-based encoder-decoder architecture where each task is treated as a learnable token, enabling the integration of multiple tasks within a single framework.","Moreover, we propose a parameter-efficient decoder, FaceX, which jointly processes face and task tokens, thereby learning generalized and robust face representations across different tasks.","To the best of our knowledge, this is the first work to propose a single model capable of handling all these facial analysis tasks using transformers.","We conducted a comprehensive analysis of effective backbones for unified face task processing and evaluated different task queries and the synergy between them.","We conduct experiments against state-of-the-art specialized models and previous multi-task models in both intra-dataset and cross-dataset evaluations across multiple benchmarks.","Additionally, our model effectively handles images \"in-the-wild,\" demonstrating its robustness and generalizability across eight different tasks, all while maintaining the real-time performance of 37 FPS."],"url":"http://arxiv.org/abs/2403.12960v1"}
{"created":"2024-03-19 17:58:02","title":"WHAC: World-grounded Humans and Cameras","abstract":"Estimating human and camera trajectories with accurate scale in the world coordinate system from a monocular video is a highly desirable yet challenging and ill-posed problem. In this study, we aim to recover expressive parametric human models (i.e., SMPL-X) and corresponding camera poses jointly, by leveraging the synergy between three critical players: the world, the human, and the camera. Our approach is founded on two key observations. Firstly, camera-frame SMPL-X estimation methods readily recover absolute human depth. Secondly, human motions inherently provide absolute spatial cues. By integrating these insights, we introduce a novel framework, referred to as WHAC, to facilitate world-grounded expressive human pose and shape estimation (EHPS) alongside camera pose estimation, without relying on traditional optimization techniques. Additionally, we present a new synthetic dataset, WHAC-A-Mole, which includes accurately annotated humans and cameras, and features diverse interactive human motions as well as realistic camera trajectories. Extensive experiments on both standard and newly established benchmarks highlight the superiority and efficacy of our framework. We will make the code and dataset publicly available.","sentences":["Estimating human and camera trajectories with accurate scale in the world coordinate system from a monocular video is a highly desirable yet challenging and ill-posed problem.","In this study, we aim to recover expressive parametric human models (i.e., SMPL-X) and corresponding camera poses jointly, by leveraging the synergy between three critical players: the world, the human, and the camera.","Our approach is founded on two key observations.","Firstly, camera-frame SMPL-X estimation methods readily recover absolute human depth.","Secondly, human motions inherently provide absolute spatial cues.","By integrating these insights, we introduce a novel framework, referred to as WHAC, to facilitate world-grounded expressive human pose and shape estimation (EHPS) alongside camera pose estimation, without relying on traditional optimization techniques.","Additionally, we present a new synthetic dataset, WHAC-A-Mole, which includes accurately annotated humans and cameras, and features diverse interactive human motions as well as realistic camera trajectories.","Extensive experiments on both standard and newly established benchmarks highlight the superiority and efficacy of our framework.","We will make the code and dataset publicly available."],"url":"http://arxiv.org/abs/2403.12959v1"}
{"created":"2024-03-19 17:57:58","title":"Dated Data: Tracing Knowledge Cutoffs in Large Language Models","abstract":"Released Large Language Models (LLMs) are often paired with a claimed knowledge cutoff date, or the dates at which training data was gathered. Such information is crucial for applications where the LLM must provide up to date information. However, this statement only scratches the surface: do all resources in the training data share the same knowledge cutoff date? Does the model's demonstrated knowledge for these subsets closely align to their cutoff dates? In this work, we define the notion of an effective cutoff. This is distinct from the LLM designer reported cutoff and applies separately to sub-resources and topics. We propose a simple approach to estimate effective cutoffs on the resource-level temporal alignment of an LLM by probing across versions of the data. Using this analysis, we find that effective cutoffs often differ from reported cutoffs. To understand the root cause of this observation, we conduct a direct large-scale analysis on open pre-training datasets. Our analysis reveals two reasons for these inconsistencies: (1) temporal biases of CommonCrawl data due to non-trivial amounts of old data in new dumps and (2) complications in LLM deduplication schemes involving semantic duplicates and lexical near-duplicates. Overall, our results show that knowledge cutoffs are not as simple as they have seemed and that care must be taken both by LLM dataset curators as well as practitioners who seek to use information from these models.","sentences":["Released Large Language Models (LLMs) are often paired with a claimed knowledge cutoff date, or the dates at which training data was gathered.","Such information is crucial for applications where the LLM must provide up to date information.","However, this statement only scratches the surface: do all resources in the training data share the same knowledge cutoff date?","Does the model's demonstrated knowledge for these subsets closely align to their cutoff dates?","In this work, we define the notion of an effective cutoff.","This is distinct from the LLM designer reported cutoff and applies separately to sub-resources and topics.","We propose a simple approach to estimate effective cutoffs on the resource-level temporal alignment of an LLM by probing across versions of the data.","Using this analysis, we find that effective cutoffs often differ from reported cutoffs.","To understand the root cause of this observation, we conduct a direct large-scale analysis on open pre-training datasets.","Our analysis reveals two reasons for these inconsistencies: (1) temporal biases of CommonCrawl data due to non-trivial amounts of old data in new dumps and (2) complications in LLM deduplication schemes involving semantic duplicates and lexical near-duplicates.","Overall, our results show that knowledge cutoffs are not as simple as they have seemed and that care must be taken both by LLM dataset curators as well as practitioners who seek to use information from these models."],"url":"http://arxiv.org/abs/2403.12958v1"}
{"created":"2024-03-19 17:57:52","title":"GVGEN: Text-to-3D Generation with Volumetric Representation","abstract":"In recent years, 3D Gaussian splatting has emerged as a powerful technique for 3D reconstruction and generation, known for its fast and high-quality rendering capabilities. To address these shortcomings, this paper introduces a novel diffusion-based framework, GVGEN, designed to efficiently generate 3D Gaussian representations from text input. We propose two innovative techniques:(1) Structured Volumetric Representation. We first arrange disorganized 3D Gaussian points as a structured form GaussianVolume. This transformation allows the capture of intricate texture details within a volume composed of a fixed number of Gaussians. To better optimize the representation of these details, we propose a unique pruning and densifying method named the Candidate Pool Strategy, enhancing detail fidelity through selective optimization. (2) Coarse-to-fine Generation Pipeline. To simplify the generation of GaussianVolume and empower the model to generate instances with detailed 3D geometry, we propose a coarse-to-fine pipeline. It initially constructs a basic geometric structure, followed by the prediction of complete Gaussian attributes. Our framework, GVGEN, demonstrates superior performance in qualitative and quantitative assessments compared to existing 3D generation methods. Simultaneously, it maintains a fast generation speed ($\\sim$7 seconds), effectively striking a balance between quality and efficiency.","sentences":["In recent years, 3D Gaussian splatting has emerged as a powerful technique for 3D reconstruction and generation, known for its fast and high-quality rendering capabilities.","To address these shortcomings, this paper introduces a novel diffusion-based framework, GVGEN, designed to efficiently generate 3D Gaussian representations from text input.","We propose two innovative techniques:(1) Structured Volumetric Representation.","We first arrange disorganized 3D Gaussian points as a structured form GaussianVolume.","This transformation allows the capture of intricate texture details within a volume composed of a fixed number of Gaussians.","To better optimize the representation of these details, we propose a unique pruning and densifying method named the Candidate Pool Strategy, enhancing detail fidelity through selective optimization.","(2) Coarse-to-fine Generation Pipeline.","To simplify the generation of GaussianVolume and empower the model to generate instances with detailed 3D geometry, we propose a coarse-to-fine pipeline.","It initially constructs a basic geometric structure, followed by the prediction of complete Gaussian attributes.","Our framework, GVGEN, demonstrates superior performance in qualitative and quantitative assessments compared to existing 3D generation methods.","Simultaneously, it maintains a fast generation speed ($\\sim$7 seconds), effectively striking a balance between quality and efficiency."],"url":"http://arxiv.org/abs/2403.12957v1"}
{"created":"2024-03-19 17:55:22","title":"FutureDepth: Learning to Predict the Future Improves Video Depth Estimation","abstract":"In this paper, we propose a novel video depth estimation approach, FutureDepth, which enables the model to implicitly leverage multi-frame and motion cues to improve depth estimation by making it learn to predict the future at training. More specifically, we propose a future prediction network, F-Net, which takes the features of multiple consecutive frames and is trained to predict multi-frame features one time step ahead iteratively. In this way, F-Net learns the underlying motion and correspondence information, and we incorporate its features into the depth decoding process. Additionally, to enrich the learning of multiframe correspondence cues, we further leverage a reconstruction network, R-Net, which is trained via adaptively masked auto-encoding of multiframe feature volumes. At inference time, both F-Net and R-Net are used to produce queries to work with the depth decoder, as well as a final refinement network. Through extensive experiments on several benchmarks, i.e., NYUDv2, KITTI, DDAD, and Sintel, which cover indoor, driving, and open-domain scenarios, we show that FutureDepth significantly improves upon baseline models, outperforms existing video depth estimation methods, and sets new state-of-the-art (SOTA) accuracy. Furthermore, FutureDepth is more efficient than existing SOTA video depth estimation models and has similar latencies when comparing to monocular models","sentences":["In this paper, we propose a novel video depth estimation approach, FutureDepth, which enables the model to implicitly leverage multi-frame and motion cues to improve depth estimation by making it learn to predict the future at training.","More specifically, we propose a future prediction network, F-Net, which takes the features of multiple consecutive frames and is trained to predict multi-frame features one time step ahead iteratively.","In this way, F-Net learns the underlying motion and correspondence information, and we incorporate its features into the depth decoding process.","Additionally, to enrich the learning of multiframe correspondence cues, we further leverage a reconstruction network, R-Net, which is trained via adaptively masked auto-encoding of multiframe feature volumes.","At inference time, both F-Net and R-Net are used to produce queries to work with the depth decoder, as well as a final refinement network.","Through extensive experiments on several benchmarks, i.e., NYUDv2, KITTI, DDAD, and Sintel, which cover indoor, driving, and open-domain scenarios, we show that FutureDepth significantly improves upon baseline models, outperforms existing video depth estimation methods, and sets new state-of-the-art (SOTA) accuracy.","Furthermore, FutureDepth is more efficient than existing SOTA video depth estimation models and has similar latencies when comparing to monocular models"],"url":"http://arxiv.org/abs/2403.12953v1"}
{"created":"2024-03-19 17:54:34","title":"Just Shift It: Test-Time Prototype Shifting for Zero-Shot Generalization with Vision-Language Models","abstract":"Advancements in vision-language models (VLMs) have propelled the field of computer vision, particularly in the zero-shot learning setting. Despite their promise, the effectiveness of these models often diminishes due to domain shifts in test environments. To address this, we introduce the Test-Time Prototype Shifting (TPS) framework, a pioneering approach designed to adapt VLMs to test datasets using unlabeled test inputs. Our method is based on the notion of modulating per-class prototypes in the shared embedding space. By pre-computing and caching prototypes generated with the pre-trained text encoder, TPS not only facilitates optimization-free prototype reuse for subsequent predictions but also enables seamless integration with current advancements in prompt engineering. At test-time, TPS dynamically learns shift vectors for each prototype based solely on the given test sample, effectively bridging the domain gap and enhancing classification accuracy. A notable aspect of our framework is its significantly reduced memory and computational demands when compared to conventional text-prompt tuning methods. Extensive evaluations across 15 datasets involving natural distribution shifts and cross-dataset generalization demonstrate TPS's superior performance, achieving state-of-the-art results while reducing resource requirements.","sentences":["Advancements in vision-language models (VLMs) have propelled the field of computer vision, particularly in the zero-shot learning setting.","Despite their promise, the effectiveness of these models often diminishes due to domain shifts in test environments.","To address this, we introduce the Test-Time Prototype Shifting (TPS) framework, a pioneering approach designed to adapt VLMs to test datasets using unlabeled test inputs.","Our method is based on the notion of modulating per-class prototypes in the shared embedding space.","By pre-computing and caching prototypes generated with the pre-trained text encoder, TPS not only facilitates optimization-free prototype reuse for subsequent predictions but also enables seamless integration with current advancements in prompt engineering.","At test-time, TPS dynamically learns shift vectors for each prototype based solely on the given test sample, effectively bridging the domain gap and enhancing classification accuracy.","A notable aspect of our framework is its significantly reduced memory and computational demands when compared to conventional text-prompt tuning methods.","Extensive evaluations across 15 datasets involving natural distribution shifts and cross-dataset generalization demonstrate TPS's superior performance, achieving state-of-the-art results while reducing resource requirements."],"url":"http://arxiv.org/abs/2403.12952v1"}
{"created":"2024-03-19 17:50:55","title":"Optimal and Adaptive Non-Stationary Dueling Bandits Under a Generalized Borda Criterion","abstract":"In dueling bandits, the learner receives preference feedback between arms, and the regret of an arm is defined in terms of its suboptimality to a winner arm. The more challenging and practically motivated non-stationary variant of dueling bandits, where preferences change over time, has been the focus of several recent works (Saha and Gupta, 2022; Buening and Saha, 2023; Suk and Agarwal, 2023). The goal is to design algorithms without foreknowledge of the amount of change.   The bulk of known results here studies the Condorcet winner setting, where an arm preferred over any other exists at all times. Yet, such a winner may not exist and, to contrast, the Borda version of this problem (which is always well-defined) has received little attention. In this work, we establish the first optimal and adaptive Borda dynamic regret upper bound, which highlights fundamental differences in the learnability of severe non-stationarity between Condorcet vs. Borda regret objectives in dueling bandits.   Surprisingly, our techniques for non-stationary Borda dueling bandits also yield improved rates within the Condorcet winner setting, and reveal new preference models where tighter notions of non-stationarity are adaptively learnable. This is accomplished through a novel generalized Borda score framework which unites the Borda and Condorcet problems, thus allowing reduction of Condorcet regret to a Borda-like task. Such a generalization was not previously known and is likely to be of independent interest.","sentences":["In dueling bandits, the learner receives preference feedback between arms, and the regret of an arm is defined in terms of its suboptimality to a winner arm.","The more challenging and practically motivated non-stationary variant of dueling bandits, where preferences change over time, has been the focus of several recent works (Saha and Gupta, 2022; Buening and Saha, 2023; Suk and Agarwal, 2023).","The goal is to design algorithms without foreknowledge of the amount of change.   ","The bulk of known results here studies the Condorcet winner setting, where an arm preferred over any other exists at all times.","Yet, such a winner may not exist and, to contrast, the Borda version of this problem (which is always well-defined) has received little attention.","In this work, we establish the first optimal and adaptive Borda dynamic regret upper bound, which highlights fundamental differences in the learnability of severe non-stationarity between Condorcet vs. Borda regret objectives in dueling bandits.   ","Surprisingly, our techniques for non-stationary Borda dueling bandits also yield improved rates within the Condorcet winner setting, and reveal new preference models where tighter notions of non-stationarity are adaptively learnable.","This is accomplished through a novel generalized Borda score framework which unites the Borda and Condorcet problems, thus allowing reduction of Condorcet regret to a Borda-like task.","Such a generalization was not previously known and is likely to be of independent interest."],"url":"http://arxiv.org/abs/2403.12950v1"}
{"created":"2024-03-19 17:50:40","title":"A Novel Energy-Efficient Cross-Layer Design for Scheduling and Routing in 6TiSCH Networks","abstract":"The 6TiSCH protocol stack was proposed to ensure high-performance communications in the Industrial Internet of Things (IIoT). However, the lack of sufficient time slots for nodes outside the 6TiSCH's Destination Oriented Directed Acyclic Graph (DODAG) to transmit their Destination Advertisement Object (DAO) messages and cell reservation requests significantly hinders their integration into the DODAG. This oversight not only prolongs the device's join time but also increases energy consumption during the network formation phase. Moreover, challenges emerge due to the substantial number of control packets employed by both the 6TiSCH Scheduling Function (SF) and routing protocol (RPL), thus draining more energy resources, increasing medium contention, and decreasing spatial reuse. Furthermore, an SF that overlooks previously allocated slots when assigning new ones to the same node may increase jitter, and more complications ensue when it neglects the state of the TSCH queue, thus leading to packet dropping due to queue saturation. Additional complexity arises when the RPL disregards the new parent's schedule saturation during parent switching, which results in inefficient energy and time usage. To address these issues, we introduce in this paper novel mechanisms, strategically situated at the intersection of SF and RPL that are designed to balance the control packet distribution and adaptively manage parent switching. Our proposal, implemented within the 6TiSCH simulator, demonstrates significant improvements across vital performance metrics, such as node's joining time, jitter, latency, energy consumption, and amount of traffic, in comparison to the conventional 6TiSCH benchmark.","sentences":["The 6TiSCH protocol stack was proposed to ensure high-performance communications in the Industrial Internet of Things (IIoT).","However, the lack of sufficient time slots for nodes outside the 6TiSCH's Destination Oriented Directed Acyclic Graph (DODAG) to transmit their Destination Advertisement Object (DAO) messages and cell reservation requests significantly hinders their integration into the DODAG.","This oversight not only prolongs the device's join time but also increases energy consumption during the network formation phase.","Moreover, challenges emerge due to the substantial number of control packets employed by both the 6TiSCH Scheduling Function (SF) and routing protocol (RPL), thus draining more energy resources, increasing medium contention, and decreasing spatial reuse.","Furthermore, an SF that overlooks previously allocated slots when assigning new ones to the same node may increase jitter, and more complications ensue when it neglects the state of the TSCH queue, thus leading to packet dropping due to queue saturation.","Additional complexity arises when the RPL disregards the new parent's schedule saturation during parent switching, which results in inefficient energy and time usage.","To address these issues, we introduce in this paper novel mechanisms, strategically situated at the intersection of SF and RPL that are designed to balance the control packet distribution and adaptively manage parent switching.","Our proposal, implemented within the 6TiSCH simulator, demonstrates significant improvements across vital performance metrics, such as node's joining time, jitter, latency, energy consumption, and amount of traffic, in comparison to the conventional 6TiSCH benchmark."],"url":"http://arxiv.org/abs/2403.12949v1"}
{"created":"2024-03-19 17:50:32","title":"On Safety in Safe Bayesian Optimization","abstract":"Optimizing an unknown function under safety constraints is a central task in robotics, biomedical engineering, and many other disciplines, and increasingly safe Bayesian Optimization (BO) is used for this. Due to the safety critical nature of these applications, it is of utmost importance that theoretical safety guarantees for these algorithms translate into the real world. In this work, we investigate three safety-related issues of the popular class of SafeOpt-type algorithms. First, these algorithms critically rely on frequentist uncertainty bounds for Gaussian Process (GP) regression, but concrete implementations typically utilize heuristics that invalidate all safety guarantees. We provide a detailed analysis of this problem and introduce Real-\\b{eta}-SafeOpt, a variant of the SafeOpt algorithm that leverages recent GP bounds and thus retains all theoretical guarantees. Second, we identify assuming an upper bound on the reproducing kernel Hilbert space (RKHS) norm of the target function, a key technical assumption in SafeOpt-like algorithms, as a central obstacle to real-world usage. To overcome this challenge, we introduce the Lipschitz-only Safe Bayesian Optimization (LoSBO) algorithm, which guarantees safety without an assumption on the RKHS bound, and empirically show that this algorithm is not only safe, but also exhibits superior performance compared to the state-of-the-art on several function classes. Third, SafeOpt and derived algorithms rely on a discrete search space, making them difficult to apply to higher-dimensional problems. To widen the applicability of these algorithms, we introduce Lipschitz-only GP-UCB (LoS-GP-UCB), a variant of LoSBO applicable to moderately high-dimensional problems, while retaining safety.","sentences":["Optimizing an unknown function under safety constraints is a central task in robotics, biomedical engineering, and many other disciplines, and increasingly safe Bayesian Optimization (BO) is used for this.","Due to the safety critical nature of these applications, it is of utmost importance that theoretical safety guarantees for these algorithms translate into the real world.","In this work, we investigate three safety-related issues of the popular class of SafeOpt-type algorithms.","First, these algorithms critically rely on frequentist uncertainty bounds for Gaussian Process (GP) regression, but concrete implementations typically utilize heuristics that invalidate all safety guarantees.","We provide a detailed analysis of this problem and introduce Real-\\b{eta}-SafeOpt, a variant of the SafeOpt algorithm that leverages recent GP bounds and thus retains all theoretical guarantees.","Second, we identify assuming an upper bound on the reproducing kernel Hilbert space (RKHS) norm of the target function, a key technical assumption in SafeOpt-like algorithms, as a central obstacle to real-world usage.","To overcome this challenge, we introduce the Lipschitz-only Safe Bayesian Optimization (LoSBO) algorithm, which guarantees safety without an assumption on the RKHS bound, and empirically show that this algorithm is not only safe, but also exhibits superior performance compared to the state-of-the-art on several function classes.","Third, SafeOpt and derived algorithms rely on a discrete search space, making them difficult to apply to higher-dimensional problems.","To widen the applicability of these algorithms, we introduce Lipschitz-only GP-UCB (LoS-GP-UCB), a variant of LoSBO applicable to moderately high-dimensional problems, while retaining safety."],"url":"http://arxiv.org/abs/2403.12948v1"}
{"created":"2024-03-19 17:48:42","title":"Sample Complexity of Offline Distributionally Robust Linear Markov Decision Processes","abstract":"In offline reinforcement learning (RL), the absence of active exploration calls for attention on the model robustness to tackle the sim-to-real gap, where the discrepancy between the simulated and deployed environments can significantly undermine the performance of the learned policy. To endow the learned policy with robustness in a sample-efficient manner in the presence of high-dimensional state-action space, this paper considers the sample complexity of distributionally robust linear Markov decision processes (MDPs) with an uncertainty set characterized by the total variation distance using offline data. We develop a pessimistic model-based algorithm and establish its sample complexity bound under minimal data coverage assumptions, which outperforms prior art by at least $\\tilde{O}(d)$, where $d$ is the feature dimension. We further improve the performance guarantee of the proposed algorithm by incorporating a carefully-designed variance estimator.","sentences":["In offline reinforcement learning (RL), the absence of active exploration calls for attention on the model robustness to tackle the sim-to-real gap, where the discrepancy between the simulated and deployed environments can significantly undermine the performance of the learned policy.","To endow the learned policy with robustness in a sample-efficient manner in the presence of high-dimensional state-action space, this paper considers the sample complexity of distributionally robust linear Markov decision processes (MDPs) with an uncertainty set characterized by the total variation distance using offline data.","We develop a pessimistic model-based algorithm and establish its sample complexity bound under minimal data coverage assumptions, which outperforms prior art by at least $\\tilde{O}(d)$, where $d$ is the feature dimension.","We further improve the performance guarantee of the proposed algorithm by incorporating a carefully-designed variance estimator."],"url":"http://arxiv.org/abs/2403.12946v1"}
{"created":"2024-03-19 17:48:38","title":"DROID: A Large-Scale In-The-Wild Robot Manipulation Dataset","abstract":"The creation of large, diverse, high-quality robot manipulation datasets is an important stepping stone on the path toward more capable and robust robotic manipulation policies. However, creating such datasets is challenging: collecting robot manipulation data in diverse environments poses logistical and safety challenges and requires substantial investments in hardware and human labour. As a result, even the most general robot manipulation policies today are mostly trained on data collected in a small number of environments with limited scene and task diversity. In this work, we introduce DROID (Distributed Robot Interaction Dataset), a diverse robot manipulation dataset with 76k demonstration trajectories or 350 hours of interaction data, collected across 564 scenes and 84 tasks by 50 data collectors in North America, Asia, and Europe over the course of 12 months. We demonstrate that training with DROID leads to policies with higher performance and improved generalization ability. We open source the full dataset, policy learning code, and a detailed guide for reproducing our robot hardware setup.","sentences":["The creation of large, diverse, high-quality robot manipulation datasets is an important stepping stone on the path toward more capable and robust robotic manipulation policies.","However, creating such datasets is challenging: collecting robot manipulation data in diverse environments poses logistical and safety challenges and requires substantial investments in hardware and human labour.","As a result, even the most general robot manipulation policies today are mostly trained on data collected in a small number of environments with limited scene and task diversity.","In this work, we introduce DROID (Distributed Robot Interaction Dataset), a diverse robot manipulation dataset with 76k demonstration trajectories or 350 hours of interaction data, collected across 564 scenes and 84 tasks by 50 data collectors in North America, Asia, and Europe over the course of 12 months.","We demonstrate that training with DROID leads to policies with higher performance and improved generalization ability.","We open source the full dataset, policy learning code, and a detailed guide for reproducing our robot hardware setup."],"url":"http://arxiv.org/abs/2403.12945v1"}
{"created":"2024-03-19 17:47:37","title":"Vid2Robot: End-to-end Video-conditioned Policy Learning with Cross-Attention Transformers","abstract":"While large-scale robotic systems typically rely on textual instructions for tasks, this work explores a different approach: can robots infer the task directly from observing humans? This shift necessitates the robot's ability to decode human intent and translate it into executable actions within its physical constraints and environment. We introduce Vid2Robot, a novel end-to-end video-based learning framework for robots. Given a video demonstration of a manipulation task and current visual observations, Vid2Robot directly produces robot actions. This is achieved through a unified representation model trained on a large dataset of human video and robot trajectory. The model leverages cross-attention mechanisms to fuse prompt video features to the robot's current state and generate appropriate actions that mimic the observed task. To further improve policy performance, we propose auxiliary contrastive losses that enhance the alignment between human and robot video representations. We evaluate Vid2Robot on real-world robots, demonstrating a 20% improvement in performance compared to other video-conditioned policies when using human demonstration videos. Additionally, our model exhibits emergent capabilities, such as successfully transferring observed motions from one object to another, and long-horizon composition, thus showcasing its potential for real-world applications. Project website: vid2robot.github.io","sentences":["While large-scale robotic systems typically rely on textual instructions for tasks, this work explores a different approach: can robots infer the task directly from observing humans?","This shift necessitates the robot's ability to decode human intent and translate it into executable actions within its physical constraints and environment.","We introduce Vid2Robot, a novel end-to-end video-based learning framework for robots.","Given a video demonstration of a manipulation task and current visual observations, Vid2Robot directly produces robot actions.","This is achieved through a unified representation model trained on a large dataset of human video and robot trajectory.","The model leverages cross-attention mechanisms to fuse prompt video features to the robot's current state and generate appropriate actions that mimic the observed task.","To further improve policy performance, we propose auxiliary contrastive losses that enhance the alignment between human and robot video representations.","We evaluate Vid2Robot on real-world robots, demonstrating a 20% improvement in performance compared to other video-conditioned policies when using human demonstration videos.","Additionally, our model exhibits emergent capabilities, such as successfully transferring observed motions from one object to another, and long-horizon composition, thus showcasing its potential for real-world applications.","Project website: vid2robot.github.io"],"url":"http://arxiv.org/abs/2403.12943v1"}
{"created":"2024-03-19 17:43:57","title":"Neural Differential Algebraic Equations","abstract":"Differential-Algebraic Equations (DAEs) describe the temporal evolution of systems that obey both differential and algebraic constraints. Of particular interest are systems that contain implicit relationships between their components, such as conservation relationships. Here, we present Neural Differential-Algebraic Equations (NDAEs) suitable for data-driven modeling of DAEs. This methodology is built upon the concept of the Universal Differential Equation; that is, a model constructed as a system of Neural Ordinary Differential Equations informed by theory from particular science domains. In this work, we show that the proposed NDAEs abstraction is suitable for relevant system-theoretic data-driven modeling tasks. Presented examples include (i) the inverse problem of tank-manifold dynamics and (ii) discrepancy modeling of a network of pumps, tanks, and pipes. Our experiments demonstrate the proposed method's robustness to noise and extrapolation ability to (i) learn the behaviors of the system components and their interaction physics and (ii) disambiguate between data trends and mechanistic relationships contained in the system.","sentences":["Differential-Algebraic Equations (DAEs) describe the temporal evolution of systems that obey both differential and algebraic constraints.","Of particular interest are systems that contain implicit relationships between their components, such as conservation relationships.","Here, we present Neural Differential-Algebraic Equations (NDAEs) suitable for data-driven modeling of DAEs.","This methodology is built upon the concept of the Universal Differential Equation; that is, a model constructed as a system of Neural Ordinary Differential Equations informed by theory from particular science domains.","In this work, we show that the proposed NDAEs abstraction is suitable for relevant system-theoretic data-driven modeling tasks.","Presented examples include (i) the inverse problem of tank-manifold dynamics and (ii) discrepancy modeling of a network of pumps, tanks, and pipes.","Our experiments demonstrate the proposed method's robustness to noise and extrapolation ability to (i) learn the behaviors of the system components and their interaction physics and (ii) disambiguate between data trends and mechanistic relationships contained in the system."],"url":"http://arxiv.org/abs/2403.12938v1"}
{"created":"2024-03-19 17:43:08","title":"Automatic Information Extraction From Employment Tribunal Judgements Using Large Language Models","abstract":"Court transcripts and judgments are rich repositories of legal knowledge, detailing the intricacies of cases and the rationale behind judicial decisions. The extraction of key information from these documents provides a concise overview of a case, crucial for both legal experts and the public. With the advent of large language models (LLMs), automatic information extraction has become increasingly feasible and efficient. This paper presents a comprehensive study on the application of GPT-4, a large language model, for automatic information extraction from UK Employment Tribunal (UKET) cases. We meticulously evaluated GPT-4's performance in extracting critical information with a manual verification process to ensure the accuracy and relevance of the extracted data. Our research is structured around two primary extraction tasks: the first involves a general extraction of eight key aspects that hold significance for both legal specialists and the general public, including the facts of the case, the claims made, references to legal statutes, references to precedents, general case outcomes and corresponding labels, detailed order and remedies and reasons for the decision. The second task is more focused, aimed at analysing three of those extracted features, namely facts, claims and outcomes, in order to facilitate the development of a tool capable of predicting the outcome of employment law disputes. Through our analysis, we demonstrate that LLMs like GPT-4 can obtain high accuracy in legal information extraction, highlighting the potential of LLMs in revolutionising the way legal information is processed and utilised, offering significant implications for legal research and practice.","sentences":["Court transcripts and judgments are rich repositories of legal knowledge, detailing the intricacies of cases and the rationale behind judicial decisions.","The extraction of key information from these documents provides a concise overview of a case, crucial for both legal experts and the public.","With the advent of large language models (LLMs), automatic information extraction has become increasingly feasible and efficient.","This paper presents a comprehensive study on the application of GPT-4, a large language model, for automatic information extraction from UK Employment Tribunal (UKET) cases.","We meticulously evaluated GPT-4's performance in extracting critical information with a manual verification process to ensure the accuracy and relevance of the extracted data.","Our research is structured around two primary extraction tasks: the first involves a general extraction of eight key aspects that hold significance for both legal specialists and the general public, including the facts of the case, the claims made, references to legal statutes, references to precedents, general case outcomes and corresponding labels, detailed order and remedies and reasons for the decision.","The second task is more focused, aimed at analysing three of those extracted features, namely facts, claims and outcomes, in order to facilitate the development of a tool capable of predicting the outcome of employment law disputes.","Through our analysis, we demonstrate that LLMs like GPT-4 can obtain high accuracy in legal information extraction, highlighting the potential of LLMs in revolutionising the way legal information is processed and utilised, offering significant implications for legal research and practice."],"url":"http://arxiv.org/abs/2403.12936v1"}
{"created":"2024-03-19 17:37:18","title":"Segment Anything for comprehensive analysis of grapevine cluster architecture and berry properties","abstract":"Grape cluster architecture and compactness are complex traits influencing disease susceptibility, fruit quality, and yield. Evaluation methods for these traits include visual scoring, manual methodologies, and computer vision, with the latter being the most scalable approach. Most of the existing computer vision approaches for processing cluster images often rely on conventional segmentation or machine learning with extensive training and limited generalization. The Segment Anything Model (SAM), a novel foundation model trained on a massive image dataset, enables automated object segmentation without additional training. This study demonstrates out-of-the-box SAM's high accuracy in identifying individual berries in 2D cluster images. Using this model, we managed to segment approximately 3,500 cluster images, generating over 150,000 berry masks, each linked with spatial coordinates within their clusters. The correlation between human-identified berries and SAM predictions was very strong (Pearson r2=0.96). Although the visible berry count in images typically underestimates the actual cluster berry count due to visibility issues, we demonstrated that this discrepancy could be adjusted using a linear regression model (adjusted R2=0.87). We emphasized the critical importance of the angle at which the cluster is imaged, noting its substantial effect on berry counts and architecture. We proposed different approaches in which berry location information facilitated the calculation of complex features related to cluster architecture and compactness. Finally, we discussed SAM's potential integration into currently available pipelines for image generation and processing in vineyard conditions.","sentences":["Grape cluster architecture and compactness are complex traits influencing disease susceptibility, fruit quality, and yield.","Evaluation methods for these traits include visual scoring, manual methodologies, and computer vision, with the latter being the most scalable approach.","Most of the existing computer vision approaches for processing cluster images often rely on conventional segmentation or machine learning with extensive training and limited generalization.","The Segment Anything Model (SAM), a novel foundation model trained on a massive image dataset, enables automated object segmentation without additional training.","This study demonstrates out-of-the-box SAM's high accuracy in identifying individual berries in 2D cluster images.","Using this model, we managed to segment approximately 3,500 cluster images, generating over 150,000 berry masks, each linked with spatial coordinates within their clusters.","The correlation between human-identified berries and SAM predictions was very strong (Pearson r2=0.96).","Although the visible berry count in images typically underestimates the actual cluster berry count due to visibility issues, we demonstrated that this discrepancy could be adjusted using a linear regression model (adjusted R2=0.87).","We emphasized the critical importance of the angle at which the cluster is imaged, noting its substantial effect on berry counts and architecture.","We proposed different approaches in which berry location information facilitated the calculation of complex features related to cluster architecture and compactness.","Finally, we discussed SAM's potential integration into currently available pipelines for image generation and processing in vineyard conditions."],"url":"http://arxiv.org/abs/2403.12935v1"}
{"created":"2024-03-19 17:36:28","title":"Zero-Reference Low-Light Enhancement via Physical Quadruple Priors","abstract":"Understanding illumination and reducing the need for supervision pose a significant challenge in low-light enhancement. Current approaches are highly sensitive to data usage during training and illumination-specific hyper-parameters, limiting their ability to handle unseen scenarios. In this paper, we propose a new zero-reference low-light enhancement framework trainable solely with normal light images. To accomplish this, we devise an illumination-invariant prior inspired by the theory of physical light transfer. This prior serves as the bridge between normal and low-light images. Then, we develop a prior-to-image framework trained without low-light data. During testing, this framework is able to restore our illumination-invariant prior back to images, automatically achieving low-light enhancement. Within this framework, we leverage a pretrained generative diffusion model for model ability, introduce a bypass decoder to handle detail distortion, as well as offer a lightweight version for practicality. Extensive experiments demonstrate our framework's superiority in various scenarios as well as good interpretability, robustness, and efficiency. Code is available on our project homepage: http://daooshee.github.io/QuadPrior-Website/","sentences":["Understanding illumination and reducing the need for supervision pose a significant challenge in low-light enhancement.","Current approaches are highly sensitive to data usage during training and illumination-specific hyper-parameters, limiting their ability to handle unseen scenarios.","In this paper, we propose a new zero-reference low-light enhancement framework trainable solely with normal light images.","To accomplish this, we devise an illumination-invariant prior inspired by the theory of physical light transfer.","This prior serves as the bridge between normal and low-light images.","Then, we develop a prior-to-image framework trained without low-light data.","During testing, this framework is able to restore our illumination-invariant prior back to images, automatically achieving low-light enhancement.","Within this framework, we leverage a pretrained generative diffusion model for model ability, introduce a bypass decoder to handle detail distortion, as well as offer a lightweight version for practicality.","Extensive experiments demonstrate our framework's superiority in various scenarios as well as good interpretability, robustness, and efficiency.","Code is available on our project homepage: http://daooshee.github.io/QuadPrior-Website/"],"url":"http://arxiv.org/abs/2403.12933v1"}
{"created":"2024-03-19 17:34:27","title":"You Only Sample Once: Taming One-Step Text-To-Image Synthesis by Self-Cooperative Diffusion GANs","abstract":"We introduce YOSO, a novel generative model designed for rapid, scalable, and high-fidelity one-step image synthesis. This is achieved by integrating the diffusion process with GANs. Specifically, we smooth the distribution by the denoising generator itself, performing self-cooperative learning. We show that our method can serve as a one-step generation model training from scratch with competitive performance. Moreover, we show that our method can be extended to finetune pre-trained text-to-image diffusion for high-quality one-step text-to-image synthesis even with LoRA fine-tuning. In particular, we provide the first diffusion transformer that can generate images in one step trained on 512 resolution, with the capability of adapting to 1024 resolution without explicit training. Our code is provided at https://github.com/Luo-Yihong/YOSO.","sentences":["We introduce YOSO, a novel generative model designed for rapid, scalable, and high-fidelity one-step image synthesis.","This is achieved by integrating the diffusion process with GANs.","Specifically, we smooth the distribution by the denoising generator itself, performing self-cooperative learning.","We show that our method can serve as a one-step generation model training from scratch with competitive performance.","Moreover, we show that our method can be extended to finetune pre-trained text-to-image diffusion for high-quality one-step text-to-image synthesis even with LoRA fine-tuning.","In particular, we provide the first diffusion transformer that can generate images in one step trained on 512 resolution, with the capability of adapting to 1024 resolution without explicit training.","Our code is provided at https://github.com/Luo-Yihong/YOSO."],"url":"http://arxiv.org/abs/2403.12931v1"}
{"created":"2024-03-19 17:32:01","title":"Rapid AIdeation: Generating Ideas With the Self and in Collaboration With Large Language Models","abstract":"Generative artificial intelligence (GenAI) can rapidly produce large and diverse volumes of content. This lends to it a quality of creativity which can be empowering in the early stages of design. In seeking to understand how creative ways to address practical issues can be conceived between humans and GenAI, we conducted a rapid ideation workshop with 21 participants where they used a large language model (LLM) to brainstorm potential solutions and evaluate them. We found that the LLM produced a greater variety of ideas that were of high quality, though not necessarily of higher quality than human-generated ideas. Participants typically prompted in a straightforward manner with concise instructions. We also observed two collaborative dynamics with the LLM fulfilling a consulting role or an assisting role depending on the goals of the users. Notably, we observed an atypical anti-collaboration dynamic where participants used an antagonistic approach to prompt the LLM.","sentences":["Generative artificial intelligence (GenAI) can rapidly produce large and diverse volumes of content.","This lends to it a quality of creativity which can be empowering in the early stages of design.","In seeking to understand how creative ways to address practical issues can be conceived between humans and GenAI, we conducted a rapid ideation workshop with 21 participants where they used a large language model (LLM) to brainstorm potential solutions and evaluate them.","We found that the LLM produced a greater variety of ideas that were of high quality, though not necessarily of higher quality than human-generated ideas.","Participants typically prompted in a straightforward manner with concise instructions.","We also observed two collaborative dynamics with the LLM fulfilling a consulting role or an assisting role depending on the goals of the users.","Notably, we observed an atypical anti-collaboration dynamic where participants used an antagonistic approach to prompt the LLM."],"url":"http://arxiv.org/abs/2403.12928v1"}
{"created":"2024-03-19 17:28:51","title":"Supporting Energy Policy Research with Large Language Models","abstract":"The recent growth in renewable energy development in the United States has been accompanied by a simultaneous surge in renewable energy siting ordinances. These zoning laws play a critical role in dictating the placement of wind and solar resources that are critical for achieving low-carbon energy futures. In this context, efficient access to and management of siting ordinance data becomes imperative. The National Renewable Energy Laboratory (NREL) recently introduced a public wind and solar siting database to fill this need. This paper presents a method for harnessing Large Language Models (LLMs) to automate the extraction of these siting ordinances from legal documents, enabling this database to maintain accurate up-to-date information in the rapidly changing energy policy landscape. A novel contribution of this research is the integration of a decision tree framework with LLMs. Our results show that this approach is 85 to 90% accurate with outputs that can be used directly in downstream quantitative modeling. We discuss opportunities to use this work to support similar large-scale policy research in the energy sector. By unlocking new efficiencies in the extraction and analysis of legal documents using LLMs, this study enables a path forward for automated large-scale energy policy research.","sentences":["The recent growth in renewable energy development in the United States has been accompanied by a simultaneous surge in renewable energy siting ordinances.","These zoning laws play a critical role in dictating the placement of wind and solar resources that are critical for achieving low-carbon energy futures.","In this context, efficient access to and management of siting ordinance data becomes imperative.","The National Renewable Energy Laboratory (NREL) recently introduced a public wind and solar siting database to fill this need.","This paper presents a method for harnessing Large Language Models (LLMs) to automate the extraction of these siting ordinances from legal documents, enabling this database to maintain accurate up-to-date information in the rapidly changing energy policy landscape.","A novel contribution of this research is the integration of a decision tree framework with LLMs.","Our results show that this approach is 85 to 90% accurate with outputs that can be used directly in downstream quantitative modeling.","We discuss opportunities to use this work to support similar large-scale policy research in the energy sector.","By unlocking new efficiencies in the extraction and analysis of legal documents using LLMs, this study enables a path forward for automated large-scale energy policy research."],"url":"http://arxiv.org/abs/2403.12924v1"}
{"created":"2024-03-19 17:27:55","title":"Contextual AD Narration with Interleaved Multimodal Sequence","abstract":"The Audio Description (AD) task aims to generate descriptions of visual elements for visually impaired individuals to help them access long-form video contents, like movie. With video feature, text, character bank and context information as inputs, the generated ADs are able to correspond to the characters by name and provide reasonable, contextual descriptions to help audience understand the storyline of movie. To achieve this goal, we propose to leverage pre-trained foundation models through a simple and unified framework to generate ADs with interleaved multimodal sequence as input, termed as Uni-AD. To enhance the alignment of features across various modalities with finer granularity, we introduce a simple and lightweight module that maps video features into the textual feature space. Moreover, we also propose a character-refinement module to provide more precise information by identifying the main characters who play more significant role in the video context. With these unique designs, we further incorporate contextual information and a contrastive loss into our architecture to generate more smooth and contextual ADs. Experiments on the MAD-eval dataset show that Uni-AD can achieve state-of-the-art performance on AD generation, which demonstrates the effectiveness of our approach. Code will be available at https://github.com/MCG-NJU/Uni-AD.","sentences":["The Audio Description (AD) task aims to generate descriptions of visual elements for visually impaired individuals to help them access long-form video contents, like movie.","With video feature, text, character bank and context information as inputs, the generated ADs are able to correspond to the characters by name and provide reasonable, contextual descriptions to help audience understand the storyline of movie.","To achieve this goal, we propose to leverage pre-trained foundation models through a simple and unified framework to generate ADs with interleaved multimodal sequence as input, termed as Uni-AD.","To enhance the alignment of features across various modalities with finer granularity, we introduce a simple and lightweight module that maps video features into the textual feature space.","Moreover, we also propose a character-refinement module to provide more precise information by identifying the main characters who play more significant role in the video context.","With these unique designs, we further incorporate contextual information and a contrastive loss into our architecture to generate more smooth and contextual ADs.","Experiments on the MAD-eval dataset show that Uni-AD can achieve state-of-the-art performance on AD generation, which demonstrates the effectiveness of our approach.","Code will be available at https://github.com/MCG-NJU/Uni-AD."],"url":"http://arxiv.org/abs/2403.12922v1"}
{"created":"2024-03-19 17:23:44","title":"Semantic Layering in Room Segmentation via LLMs","abstract":"In this paper, we introduce Semantic Layering in Room Segmentation via LLMs (SeLRoS), an advanced method for semantic room segmentation by integrating Large Language Models (LLMs) with traditional 2D map-based segmentation. Unlike previous approaches that solely focus on the geometric segmentation of indoor environments, our work enriches segmented maps with semantic data, including object identification and spatial relationships, to enhance robotic navigation. By leveraging LLMs, we provide a novel framework that interprets and organizes complex information about each segmented area, thereby improving the accuracy and contextual relevance of room segmentation. Furthermore, SeLRoS overcomes the limitations of existing algorithms by using a semantic evaluation method to accurately distinguish true room divisions from those erroneously generated by furniture and segmentation inaccuracies. The effectiveness of SeLRoS is verified through its application across 30 different 3D environments. Source code and experiment videos for this work are available at: https://sites.google.com/view/selros.","sentences":["In this paper, we introduce Semantic Layering in Room Segmentation via LLMs (SeLRoS), an advanced method for semantic room segmentation by integrating Large Language Models (LLMs) with traditional 2D map-based segmentation.","Unlike previous approaches that solely focus on the geometric segmentation of indoor environments, our work enriches segmented maps with semantic data, including object identification and spatial relationships, to enhance robotic navigation.","By leveraging LLMs, we provide a novel framework that interprets and organizes complex information about each segmented area, thereby improving the accuracy and contextual relevance of room segmentation.","Furthermore, SeLRoS overcomes the limitations of existing algorithms by using a semantic evaluation method to accurately distinguish true room divisions from those erroneously generated by furniture and segmentation inaccuracies.","The effectiveness of SeLRoS is verified through its application across 30 different 3D environments.","Source code and experiment videos for this work are available at: https://sites.google.com/view/selros."],"url":"http://arxiv.org/abs/2403.12920v1"}
{"created":"2024-03-19 17:21:29","title":"Generalizable and Stable Finetuning of Pretrained Language Models on Low-Resource Texts","abstract":"Pretrained Language Models (PLMs) have advanced Natural Language Processing (NLP) tasks significantly, but finetuning PLMs on low-resource datasets poses significant challenges such as instability and overfitting. Previous methods tackle these issues by finetuning a strategically chosen subnetwork on a downstream task, while keeping the remaining weights fixed to the pretrained weights. However, they rely on a suboptimal criteria for sub-network selection, leading to suboptimal solutions. To address these limitations, we propose a regularization method based on attention-guided weight mixup for finetuning PLMs. Our approach represents each network weight as a mixup of task-specific weight and pretrained weight, controlled by a learnable attention parameter, providing finer control over sub-network selection. Furthermore, we employ a bi-level optimization (BLO) based framework on two separate splits of the training dataset, improving generalization and combating overfitting. We validate the efficacy of our proposed method through extensive experiments, demonstrating its superiority over previous methods, particularly in the context of finetuning PLMs on low-resource datasets.","sentences":["Pretrained Language Models (PLMs) have advanced Natural Language Processing (NLP) tasks significantly, but finetuning PLMs on low-resource datasets poses significant challenges such as instability and overfitting.","Previous methods tackle these issues by finetuning a strategically chosen subnetwork on a downstream task, while keeping the remaining weights fixed to the pretrained weights.","However, they rely on a suboptimal criteria for sub-network selection, leading to suboptimal solutions.","To address these limitations, we propose a regularization method based on attention-guided weight mixup for finetuning PLMs.","Our approach represents each network weight as a mixup of task-specific weight and pretrained weight, controlled by a learnable attention parameter, providing finer control over sub-network selection.","Furthermore, we employ a bi-level optimization (BLO) based framework on two separate splits of the training dataset, improving generalization and combating overfitting.","We validate the efficacy of our proposed method through extensive experiments, demonstrating its superiority over previous methods, particularly in the context of finetuning PLMs on low-resource datasets."],"url":"http://arxiv.org/abs/2403.12918v1"}
{"created":"2024-03-19 17:16:34","title":"Effects of Automated Misinformation Warning Labels on the Intents to Like, Comment and Share Posts","abstract":"With fact-checking by professionals being difficult to scale on social media, algorithmic techniques have been considered. However, it is uncertain how the public may react to labels by automated fact-checkers. In this study, we investigate the use of automated warning labels derived from misinformation detection literature and investigate their effects on three forms of post engagement. Focusing on political posts, we also consider how partisanship affects engagement. In a two-phases within-subjects experiment with 200 participants, we found that the generic warnings suppressed intents to comment on and share posts, but not on the intent to like them. Furthermore, when different reasons for the labels were provided, their effects on post engagement were inconsistent, suggesting that the reasons could have undesirably motivated engagement instead. Partisanship effects were observed across the labels with higher engagement for politically congruent posts. We discuss the implications on the design and use of automated warning labels.","sentences":["With fact-checking by professionals being difficult to scale on social media, algorithmic techniques have been considered.","However, it is uncertain how the public may react to labels by automated fact-checkers.","In this study, we investigate the use of automated warning labels derived from misinformation detection literature and investigate their effects on three forms of post engagement.","Focusing on political posts, we also consider how partisanship affects engagement.","In a two-phases within-subjects experiment with 200 participants, we found that the generic warnings suppressed intents to comment on and share posts, but not on the intent to like them.","Furthermore, when different reasons for the labels were provided, their effects on post engagement were inconsistent, suggesting that the reasons could have undesirably motivated engagement instead.","Partisanship effects were observed across the labels with higher engagement for politically congruent posts.","We discuss the implications on the design and use of automated warning labels."],"url":"http://arxiv.org/abs/2403.12916v1"}
{"created":"2024-03-19 17:12:58","title":"Ultra-High-Resolution Image Synthesis with Pyramid Diffusion Model","abstract":"We introduce the Pyramid Diffusion Model (PDM), a novel architecture designed for ultra-high-resolution image synthesis. PDM utilizes a pyramid latent representation, providing a broader design space that enables more flexible, structured, and efficient perceptual compression which enable AutoEncoder and Network of Diffusion to equip branches and deeper layers. To enhance PDM's capabilities for generative tasks, we propose the integration of Spatial-Channel Attention and Res-Skip Connection, along with the utilization of Spectral Norm and Decreasing Dropout Strategy for the Diffusion Network and AutoEncoder. In summary, PDM achieves the synthesis of images with a 2K resolution for the first time, demonstrated on two new datasets comprising images of sizes 2048x2048 pixels and 2048x1024 pixels respectively. We believe that this work offers an alternative approach to designing scalable image generative models, while also providing incremental reinforcement for existing frameworks.","sentences":["We introduce the Pyramid Diffusion Model (PDM), a novel architecture designed for ultra-high-resolution image synthesis.","PDM utilizes a pyramid latent representation, providing a broader design space that enables more flexible, structured, and efficient perceptual compression which enable AutoEncoder and Network of Diffusion to equip branches and deeper layers.","To enhance PDM's capabilities for generative tasks, we propose the integration of Spatial-Channel Attention and Res-Skip Connection, along with the utilization of Spectral Norm and Decreasing Dropout Strategy for the Diffusion Network and AutoEncoder.","In summary, PDM achieves the synthesis of images with a 2K resolution for the first time, demonstrated on two new datasets comprising images of sizes 2048x2048 pixels and 2048x1024 pixels respectively.","We believe that this work offers an alternative approach to designing scalable image generative models, while also providing incremental reinforcement for existing frameworks."],"url":"http://arxiv.org/abs/2403.12915v1"}
{"created":"2024-03-19 17:11:25","title":"Fact Checking Chatbot: A Misinformation Intervention for Instant Messaging Apps and an Analysis of Trust in the Fact Checkers","abstract":"In Singapore, there has been a rise in misinformation on mobile instant messaging services (MIMS). MIMS support both small peer-to-peer networks and large groups. Misinformation in the former may spread due to recipients' trust in the sender while in the latter, misinformation can directly reach a wide audience. The encryption of MIMS makes it difficult to address misinformation directly. As such, chatbots have become an alternative solution where users can disclose their chat content directly to fact checking services. To understand how effective fact checking chatbots are as an intervention and how trust in three different fact checkers (i.e., Government, News Outlets, and Artificial Intelligence) may affect this trust, we conducted a within-subjects experiment with 527 Singapore residents. We found mixed results for the fact checkers but support for the chatbot intervention overall. We also found a striking contradiction between participants' trust in the fact checkers and their behaviour towards them. Specifically, those who reported a high level of trust in the government performed worse and tended to follow the fact checking tool less when it was endorsed by the government.","sentences":["In Singapore, there has been a rise in misinformation on mobile instant messaging services (MIMS).","MIMS support both small peer-to-peer networks and large groups.","Misinformation in the former may spread due to recipients' trust in the sender while in the latter, misinformation can directly reach a wide audience.","The encryption of MIMS makes it difficult to address misinformation directly.","As such, chatbots have become an alternative solution where users can disclose their chat content directly to fact checking services.","To understand how effective fact checking chatbots are as an intervention and how trust in three different fact checkers (i.e., Government, News Outlets, and Artificial Intelligence) may affect this trust, we conducted a within-subjects experiment with 527 Singapore residents.","We found mixed results for the fact checkers but support for the chatbot intervention overall.","We also found a striking contradiction between participants' trust in the fact checkers and their behaviour towards them.","Specifically, those who reported a high level of trust in the government performed worse and tended to follow the fact checking tool less when it was endorsed by the government."],"url":"http://arxiv.org/abs/2403.12913v1"}
{"created":"2024-03-19 17:08:24","title":"Yell At Your Robot: Improving On-the-Fly from Language Corrections","abstract":"Hierarchical policies that combine language and low-level control have been shown to perform impressively long-horizon robotic tasks, by leveraging either zero-shot high-level planners like pretrained language and vision-language models (LLMs/VLMs) or models trained on annotated robotic demonstrations. However, for complex and dexterous skills, attaining high success rates on long-horizon tasks still represents a major challenge -- the longer the task is, the more likely it is that some stage will fail. Can humans help the robot to continuously improve its long-horizon task performance through intuitive and natural feedback? In this paper, we make the following observation: high-level policies that index into sufficiently rich and expressive low-level language-conditioned skills can be readily supervised with human feedback in the form of language corrections. We show that even fine-grained corrections, such as small movements (\"move a bit to the left\"), can be effectively incorporated into high-level policies, and that such corrections can be readily obtained from humans observing the robot and making occasional suggestions. This framework enables robots not only to rapidly adapt to real-time language feedback, but also incorporate this feedback into an iterative training scheme that improves the high-level policy's ability to correct errors in both low-level execution and high-level decision-making purely from verbal feedback. Our evaluation on real hardware shows that this leads to significant performance improvement in long-horizon, dexterous manipulation tasks without the need for any additional teleoperation. Videos and code are available at https://yay-robot.github.io/.","sentences":["Hierarchical policies that combine language and low-level control have been shown to perform impressively long-horizon robotic tasks, by leveraging either zero-shot high-level planners like pretrained language and vision-language models (LLMs/VLMs) or models trained on annotated robotic demonstrations.","However, for complex and dexterous skills, attaining high success rates on long-horizon tasks still represents a major challenge -- the longer the task is, the more likely it is that some stage will fail.","Can humans help the robot to continuously improve its long-horizon task performance through intuitive and natural feedback?","In this paper, we make the following observation: high-level policies that index into sufficiently rich and expressive low-level language-conditioned skills can be readily supervised with human feedback in the form of language corrections.","We show that even fine-grained corrections, such as small movements (\"move a bit to the left\"), can be effectively incorporated into high-level policies, and that such corrections can be readily obtained from humans observing the robot and making occasional suggestions.","This framework enables robots not only to rapidly adapt to real-time language feedback, but also incorporate this feedback into an iterative training scheme that improves the high-level policy's ability to correct errors in both low-level execution and high-level decision-making purely from verbal feedback.","Our evaluation on real hardware shows that this leads to significant performance improvement in long-horizon, dexterous manipulation tasks without the need for any additional teleoperation.","Videos and code are available at https://yay-robot.github.io/."],"url":"http://arxiv.org/abs/2403.12910v1"}
{"created":"2024-03-19 17:02:07","title":"TexDreamer: Towards Zero-Shot High-Fidelity 3D Human Texture Generation","abstract":"Texturing 3D humans with semantic UV maps remains a challenge due to the difficulty of acquiring reasonably unfolded UV. Despite recent text-to-3D advancements in supervising multi-view renderings using large text-to-image (T2I) models, issues persist with generation speed, text consistency, and texture quality, resulting in data scarcity among existing datasets. We present TexDreamer, the first zero-shot multimodal high-fidelity 3D human texture generation model. Utilizing an efficient texture adaptation finetuning strategy, we adapt large T2I model to a semantic UV structure while preserving its original generalization capability. Leveraging a novel feature translator module, the trained model is capable of generating high-fidelity 3D human textures from either text or image within seconds. Furthermore, we introduce ArTicuLated humAn textureS (ATLAS), the largest high-resolution (1024 X 1024) 3D human texture dataset which contains 50k high-fidelity textures with text descriptions.","sentences":["Texturing 3D humans with semantic UV maps remains a challenge due to the difficulty of acquiring reasonably unfolded UV.","Despite recent text-to-3D advancements in supervising multi-view renderings using large text-to-image (T2I) models, issues persist with generation speed, text consistency, and texture quality, resulting in data scarcity among existing datasets.","We present TexDreamer, the first zero-shot multimodal high-fidelity 3D human texture generation model.","Utilizing an efficient texture adaptation finetuning strategy, we adapt large T2I model to a semantic UV structure while preserving its original generalization capability.","Leveraging a novel feature translator module, the trained model is capable of generating high-fidelity 3D human textures from either text or image within seconds.","Furthermore, we introduce ArTicuLated humAn textureS (ATLAS), the largest high-resolution (1024 X 1024) 3D human texture dataset which contains 50k high-fidelity textures with text descriptions."],"url":"http://arxiv.org/abs/2403.12906v1"}
{"created":"2024-03-19 16:58:45","title":"Social bots sour activist sentiment without eroding engagement","abstract":"Social media platforms have witnessed a substantial increase in social bot activity, significantly affecting online discourse. Our study explores the dynamic nature of bot engagement related to Extinction Rebellion climate change protests from 18 November 2019 to 10 December 2019. We find that bots exert a greater influence on human behavior than vice versa during heated online periods. To assess the causal impact of human-bot communication, we compared communication histories between human users who directly interacted with bots and matched human users who did not. Our findings demonstrate a consistent negative impact of bot interactions on subsequent human sentiment, with exposed users displaying significantly more negative sentiment than their counterparts. Furthermore, the nature of bot interaction influences human tweeting activity and the sentiment towards protests. Political astroturfing bots increase activity, whereas other bots decrease it. Sentiment changes towards protests depend on the user's original support level, indicating targeted manipulation. However, bot interactions do not change activists' engagement towards protests. Despite the seemingly minor impact of individual bot encounters, the cumulative effect is profound due to the large volume of bot communication. Our findings underscore the importance of unrestricted access to social media data for studying the prevalence and influence of social bots, as with new technological advancements distinguishing between bots and humans becomes nearly impossible.","sentences":["Social media platforms have witnessed a substantial increase in social bot activity, significantly affecting online discourse.","Our study explores the dynamic nature of bot engagement related to Extinction Rebellion climate change protests from 18 November 2019 to 10 December 2019.","We find that bots exert a greater influence on human behavior than vice versa during heated online periods.","To assess the causal impact of human-bot communication, we compared communication histories between human users who directly interacted with bots and matched human users who did not.","Our findings demonstrate a consistent negative impact of bot interactions on subsequent human sentiment, with exposed users displaying significantly more negative sentiment than their counterparts.","Furthermore, the nature of bot interaction influences human tweeting activity and the sentiment towards protests.","Political astroturfing bots increase activity, whereas other bots decrease it.","Sentiment changes towards protests depend on the user's original support level, indicating targeted manipulation.","However, bot interactions do not change activists' engagement towards protests.","Despite the seemingly minor impact of individual bot encounters, the cumulative effect is profound due to the large volume of bot communication.","Our findings underscore the importance of unrestricted access to social media data for studying the prevalence and influence of social bots, as with new technological advancements distinguishing between bots and humans becomes nearly impossible."],"url":"http://arxiv.org/abs/2403.12904v1"}
{"created":"2024-03-19 16:53:53","title":"Toward Sustainable GenAI using Generation Directives for Carbon-Friendly Large Language Model Inference","abstract":"The rapid advancement of Generative Artificial Intelligence (GenAI) across diverse sectors raises significant environmental concerns, notably the carbon emissions from their cloud and high performance computing (HPC) infrastructure. This paper presents Sprout, an innovative framework designed to address these concerns by reducing the carbon footprint of generative Large Language Model (LLM) inference services. Sprout leverages the innovative concept of \"generation directives\" to guide the autoregressive generation process, thereby enhancing carbon efficiency. Our proposed method meticulously balances the need for ecological sustainability with the demand for high-quality generation outcomes. Employing a directive optimizer for the strategic assignment of generation directives to user prompts and an original offline quality evaluator, Sprout demonstrates a significant reduction in carbon emissions by over 40% in real-world evaluations using the Llama2 LLM and global electricity grid data. This research marks a critical step toward aligning AI technology with sustainable practices, highlighting the potential for mitigating environmental impacts in the rapidly expanding domain of generative artificial intelligence.","sentences":["The rapid advancement of Generative Artificial Intelligence (GenAI) across diverse sectors raises significant environmental concerns, notably the carbon emissions from their cloud and high performance computing (HPC) infrastructure.","This paper presents Sprout, an innovative framework designed to address these concerns by reducing the carbon footprint of generative Large Language Model (LLM) inference services.","Sprout leverages the innovative concept of \"generation directives\" to guide the autoregressive generation process, thereby enhancing carbon efficiency.","Our proposed method meticulously balances the need for ecological sustainability with the demand for high-quality generation outcomes.","Employing a directive optimizer for the strategic assignment of generation directives to user prompts and an original offline quality evaluator, Sprout demonstrates a significant reduction in carbon emissions by over 40% in real-world evaluations using the Llama2 LLM and global electricity grid data.","This research marks a critical step toward aligning AI technology with sustainable practices, highlighting the potential for mitigating environmental impacts in the rapidly expanding domain of generative artificial intelligence."],"url":"http://arxiv.org/abs/2403.12900v1"}
{"created":"2024-03-19 16:52:39","title":"Plane Hamiltonian Cycles in Convex Drawings","abstract":"A conjecture by Rafla from 1988 asserts that every simple drawing of the complete graph $K_n$ admits a plane Hamiltonian cycle. It turned out that already the existence of much simpler non-crossing substructures in such drawings is hard to prove. Recent progress was made by Aichholzer et al. and by Suk and Zeng who proved the existence of a plane path of length $\\Omega(\\log n / \\log \\log n)$ and of a plane matching of size $\\Omega(n^{1/2})$ in every simple drawing of $K_{n}$.   Instead of studying simpler substructures, we prove Rafla's conjecture for the subclass of convex drawings, the most general class in the convexity hierarchy introduced by Arroyo et al. Moreover, we show that every convex drawing of $K_n$ contains a plane Hamiltonian path between each pair of vertices (Hamiltonian connectivity) and a plane $k$-cycle for each $3 \\leq k \\leq n$ (pancyclicity), and present further results on maximal plane subdrawings.","sentences":["A conjecture by Rafla from 1988 asserts that every simple drawing of the complete graph $K_n$ admits a plane Hamiltonian cycle.","It turned out that already the existence of much simpler non-crossing substructures in such drawings is hard to prove.","Recent progress was made by Aichholzer et al.","and by Suk and Zeng who proved the existence of a plane path of length $\\Omega(\\log n / \\log \\log n)$ and of a plane matching of size $\\Omega(n^{1/2})$ in every simple drawing of $K_{n}$.   Instead of studying simpler substructures, we prove Rafla's conjecture for the subclass of convex drawings, the most general class in the convexity hierarchy introduced by Arroyo et al.","Moreover, we show that every convex drawing of $K_n$ contains a plane Hamiltonian path between each pair of vertices (Hamiltonian connectivity) and a plane $k$-cycle for each $3 \\leq k \\leq n$ (pancyclicity), and present further results on maximal plane subdrawings."],"url":"http://arxiv.org/abs/2403.12898v1"}
{"created":"2024-03-19 16:48:40","title":"mPLUG-DocOwl 1.5: Unified Structure Learning for OCR-free Document Understanding","abstract":"Structure information is critical for understanding the semantics of text-rich images, such as documents, tables, and charts. Existing Multimodal Large Language Models (MLLMs) for Visual Document Understanding are equipped with text recognition ability but lack general structure understanding abilities for text-rich document images. In this work, we emphasize the importance of structure information in Visual Document Understanding and propose the Unified Structure Learning to boost the performance of MLLMs. Our Unified Structure Learning comprises structure-aware parsing tasks and multi-grained text localization tasks across 5 domains: document, webpage, table, chart, and natural image. To better encode structure information, we design a simple and effective vision-to-text module H-Reducer, which can not only maintain the layout information but also reduce the length of visual features by merging horizontal adjacent patches through convolution, enabling the LLM to understand high-resolution images more efficiently. Furthermore, by constructing structure-aware text sequences and multi-grained pairs of texts and bounding boxes for publicly available text-rich images, we build a comprehensive training set DocStruct4M to support structure learning. Finally, we construct a small but high-quality reasoning tuning dataset DocReason25K to trigger the detailed explanation ability in the document domain. Our model DocOwl 1.5 achieves state-of-the-art performance on 10 visual document understanding benchmarks, improving the SOTA performance of MLLMs with a 7B LLM by more than 10 points in 5/10 benchmarks. Our codes, models, and datasets are publicly available at https://github.com/X-PLUG/mPLUG-DocOwl/tree/main/DocOwl1.5.","sentences":["Structure information is critical for understanding the semantics of text-rich images, such as documents, tables, and charts.","Existing Multimodal Large Language Models (MLLMs) for Visual Document Understanding are equipped with text recognition ability but lack general structure understanding abilities for text-rich document images.","In this work, we emphasize the importance of structure information in Visual Document Understanding and propose the Unified Structure Learning to boost the performance of MLLMs.","Our Unified Structure Learning comprises structure-aware parsing tasks and multi-grained text localization tasks across 5 domains: document, webpage, table, chart, and natural image.","To better encode structure information, we design a simple and effective vision-to-text module H-Reducer, which can not only maintain the layout information but also reduce the length of visual features by merging horizontal adjacent patches through convolution, enabling the LLM to understand high-resolution images more efficiently.","Furthermore, by constructing structure-aware text sequences and multi-grained pairs of texts and bounding boxes for publicly available text-rich images, we build a comprehensive training set DocStruct4M to support structure learning.","Finally, we construct a small but high-quality reasoning tuning dataset DocReason25K to trigger the detailed explanation ability in the document domain.","Our model DocOwl 1.5 achieves state-of-the-art performance on 10 visual document understanding benchmarks, improving the SOTA performance of MLLMs with a 7B LLM by more than 10 points in 5/10 benchmarks.","Our codes, models, and datasets are publicly available at https://github.com/X-PLUG/mPLUG-DocOwl/tree/main/DocOwl1.5."],"url":"http://arxiv.org/abs/2403.12895v1"}
{"created":"2024-03-19 16:46:29","title":"MEDBind: Unifying Language and Multimodal Medical Data Embeddings","abstract":"Medical vision-language pretraining models (VLPM) have achieved remarkable progress in fusing chest X-rays (CXR) with clinical texts, introducing image-text data binding approaches that enable zero-shot learning and downstream clinical tasks. However, the current landscape lacks the holistic integration of additional medical modalities, such as electrocardiograms (ECG). We present MEDBind (Medical Electronic patient recorD), which learns joint embeddings across CXR, ECG, and medical text. Using text data as the central anchor, MEDBind features tri-modality binding, delivering competitive performance in top-K retrieval, zero-shot, and few-shot benchmarks against established VLPM, and the ability for CXR-to-ECG zero-shot classification and retrieval. This seamless integration is achieved through combination of contrastive loss on modality-text pairs with our proposed contrastive loss function, Edge-Modality Contrastive Loss, fostering a cohesive embedding space for CXR, ECG, and text. Finally, we demonstrate that MEDBind can improve downstream tasks by directly integrating CXR and ECG embeddings into a large-language model for multimodal prompt tuning.","sentences":["Medical vision-language pretraining models (VLPM) have achieved remarkable progress in fusing chest X-rays (CXR) with clinical texts, introducing image-text data binding approaches that enable zero-shot learning and downstream clinical tasks.","However, the current landscape lacks the holistic integration of additional medical modalities, such as electrocardiograms (ECG).","We present MEDBind (Medical Electronic patient recorD), which learns joint embeddings across CXR, ECG, and medical text.","Using text data as the central anchor, MEDBind features tri-modality binding, delivering competitive performance in top-K retrieval, zero-shot, and few-shot benchmarks against established VLPM, and the ability for CXR-to-ECG zero-shot classification and retrieval.","This seamless integration is achieved through combination of contrastive loss on modality-text pairs with our proposed contrastive loss function, Edge-Modality Contrastive Loss, fostering a cohesive embedding space for CXR, ECG, and text.","Finally, we demonstrate that MEDBind can improve downstream tasks by directly integrating CXR and ECG embeddings into a large-language model for multimodal prompt tuning."],"url":"http://arxiv.org/abs/2403.12894v1"}
{"created":"2024-03-19 16:45:45","title":"Wideband Modeling and Beamforming for Beyond Diagonal Reconfigurable Intelligent Surfaces","abstract":"This work studies the wideband modeling and beamforming design of beyond diagonal reconfigurable intelligent surface (BD-RIS), which generalizes and goes beyond conventional RIS with diagonal phase shift matrices to achieve enhanced channel gain. Specifically, we investigate the response of BD-RIS in wideband systems by going back to its hardware circuit realizations. We propose a novel wideband model which has simple expressions while capturing the response variations of BD-RIS for signals with different frequencies. With this wideband model, we propose a BD-RIS design algorithm for an orthogonal frequency division multiplexing system to maximize the average rate over all subcarriers. Finally, we provide simulation results to evaluate the performance of the proposed design and show the importance of wideband modeling for BD-RIS.","sentences":["This work studies the wideband modeling and beamforming design of beyond diagonal reconfigurable intelligent surface (BD-RIS), which generalizes and goes beyond conventional RIS with diagonal phase shift matrices to achieve enhanced channel gain.","Specifically, we investigate the response of BD-RIS in wideband systems by going back to its hardware circuit realizations.","We propose a novel wideband model which has simple expressions while capturing the response variations of BD-RIS for signals with different frequencies.","With this wideband model, we propose a BD-RIS design algorithm for an orthogonal frequency division multiplexing system to maximize the average rate over all subcarriers.","Finally, we provide simulation results to evaluate the performance of the proposed design and show the importance of wideband modeling for BD-RIS."],"url":"http://arxiv.org/abs/2403.12893v1"}
{"created":"2024-03-19 16:44:53","title":"Uoc luong kenh truyen trong he thong da robot su dung SDR","abstract":"This study focuses on developing an experimental system for estimating communication channels in a multi-robot mobile system using software-defined radio (SDR) devices. The system consists of two mobile robots programmed for two scenarios: one where the robot remains stationary and another where it follows a predefined trajectory. Communication within the system is conducted through orthogonal frequency-division multiplexing (OFDM) to mitigate the effects of multipath propagation in indoor environments. The system's performance is evaluated using the bit error rate (BER). Connections related to robot motion and communication are implemented using Raspberry Pi 3 and BladeRF x115, respectively. The least squares (LS) technique is employed to estimate the channel with a bit error rate of approximately 10^(-2).","sentences":["This study focuses on developing an experimental system for estimating communication channels in a multi-robot mobile system using software-defined radio (SDR) devices.","The system consists of two mobile robots programmed for two scenarios: one where the robot remains stationary and another where it follows a predefined trajectory.","Communication within the system is conducted through orthogonal frequency-division multiplexing (OFDM) to mitigate the effects of multipath propagation in indoor environments.","The system's performance is evaluated using the bit error rate (BER).","Connections related to robot motion and communication are implemented using Raspberry Pi 3 and BladeRF x115, respectively.","The least squares (LS) technique is employed to estimate the channel with a bit error rate of approximately 10^(-2)."],"url":"http://arxiv.org/abs/2403.12892v1"}
{"created":"2024-03-19 16:40:57","title":"Adaptive Visual Imitation Learning for Robotic Assisted Feeding Across Varied Bowl Configurations and Food Types","abstract":"In this study, we introduce a novel visual imitation network with a spatial attention module for robotic assisted feeding (RAF). The goal is to acquire (i.e., scoop) food items from a bowl. However, achieving robust and adaptive food manipulation is particularly challenging. To deal with this, we propose a framework that integrates visual perception with imitation learning to enable the robot to handle diverse scenarios during scooping. Our approach, named AVIL (adaptive visual imitation learning), exhibits adaptability and robustness across different bowl configurations in terms of material, size, and position, as well as diverse food types including granular, semi-solid, and liquid, even in the presence of distractors. We validate the effectiveness of our approach by conducting experiments on a real robot. We also compare its performance with a baseline. The results demonstrate improvement over the baseline across all scenarios, with an enhancement of up to 2.5 times in terms of a success metric. Notably, our model, trained solely on data from a transparent glass bowl containing granular cereals, showcases generalization ability when tested zero-shot on other bowl configurations with different types of food.","sentences":["In this study, we introduce a novel visual imitation network with a spatial attention module for robotic assisted feeding (RAF).","The goal is to acquire (i.e., scoop) food items from a bowl.","However, achieving robust and adaptive food manipulation is particularly challenging.","To deal with this, we propose a framework that integrates visual perception with imitation learning to enable the robot to handle diverse scenarios during scooping.","Our approach, named AVIL (adaptive visual imitation learning), exhibits adaptability and robustness across different bowl configurations in terms of material, size, and position, as well as diverse food types including granular, semi-solid, and liquid, even in the presence of distractors.","We validate the effectiveness of our approach by conducting experiments on a real robot.","We also compare its performance with a baseline.","The results demonstrate improvement over the baseline across all scenarios, with an enhancement of up to 2.5 times in terms of a success metric.","Notably, our model, trained solely on data from a transparent glass bowl containing granular cereals, showcases generalization ability when tested zero-shot on other bowl configurations with different types of food."],"url":"http://arxiv.org/abs/2403.12891v1"}
{"created":"2024-03-19 16:34:31","title":"Understanding the training of infinitely deep and wide ResNets with Conditional Optimal Transport","abstract":"We study the convergence of gradient flow for the training of deep neural networks. If Residual Neural Networks are a popular example of very deep architectures, their training constitutes a challenging optimization problem due notably to the non-convexity and the non-coercivity of the objective. Yet, in applications, those tasks are successfully solved by simple optimization algorithms such as gradient descent. To better understand this phenomenon, we focus here on a ``mean-field'' model of infinitely deep and arbitrarily wide ResNet, parameterized by probability measures over the product set of layers and parameters and with constant marginal on the set of layers. Indeed, in the case of shallow neural networks, mean field models have proven to benefit from simplified loss-landscapes and good theoretical guarantees when trained with gradient flow for the Wasserstein metric on the set of probability measures. Motivated by this approach, we propose to train our model with gradient flow w.r.t. the conditional Optimal Transport distance: a restriction of the classical Wasserstein distance which enforces our marginal condition. Relying on the theory of gradient flows in metric spaces we first show the well-posedness of the gradient flow equation and its consistency with the training of ResNets at finite width. Performing a local Polyak-\\L{}ojasiewicz analysis, we then show convergence of the gradient flow for well-chosen initializations: if the number of features is finite but sufficiently large and the risk is sufficiently small at initialization, the gradient flow converges towards a global minimizer. This is the first result of this type for infinitely deep and arbitrarily wide ResNets.","sentences":["We study the convergence of gradient flow for the training of deep neural networks.","If Residual Neural Networks are a popular example of very deep architectures, their training constitutes a challenging optimization problem due notably to the non-convexity and the non-coercivity of the objective.","Yet, in applications, those tasks are successfully solved by simple optimization algorithms such as gradient descent.","To better understand this phenomenon, we focus here on a ``mean-field'' model of infinitely deep and arbitrarily wide ResNet, parameterized by probability measures over the product set of layers and parameters and with constant marginal on the set of layers.","Indeed, in the case of shallow neural networks, mean field models have proven to benefit from simplified loss-landscapes and good theoretical guarantees when trained with gradient flow for the Wasserstein metric on the set of probability measures.","Motivated by this approach, we propose to train our model with gradient flow w.r.t.","the conditional Optimal Transport distance: a restriction of the classical Wasserstein distance which enforces our marginal condition.","Relying on the theory of gradient flows in metric spaces we first show the well-posedness of the gradient flow equation and its consistency with the training of ResNets at finite width.","Performing a local Polyak-\\L{}ojasiewicz analysis, we then show convergence of the gradient flow for well-chosen initializations: if the number of features is finite but sufficiently large and the risk is sufficiently small at initialization, the gradient flow converges towards a global minimizer.","This is the first result of this type for infinitely deep and arbitrarily wide ResNets."],"url":"http://arxiv.org/abs/2403.12887v1"}
{"created":"2024-03-19 16:33:26","title":"EmoVOCA: Speech-Driven Emotional 3D Talking Heads","abstract":"The domain of 3D talking head generation has witnessed significant progress in recent years. A notable challenge in this field consists in blending speech-related motions with expression dynamics, which is primarily caused by the lack of comprehensive 3D datasets that combine diversity in spoken sentences with a variety of facial expressions. Whereas literature works attempted to exploit 2D video data and parametric 3D models as a workaround, these still show limitations when jointly modeling the two motions. In this work, we address this problem from a different perspective, and propose an innovative data-driven technique that we used for creating a synthetic dataset, called EmoVOCA, obtained by combining a collection of inexpressive 3D talking heads and a set of 3D expressive sequences. To demonstrate the advantages of this approach, and the quality of the dataset, we then designed and trained an emotional 3D talking head generator that accepts a 3D face, an audio file, an emotion label, and an intensity value as inputs, and learns to animate the audio-synchronized lip movements with expressive traits of the face. Comprehensive experiments, both quantitative and qualitative, using our data and generator evidence superior ability in synthesizing convincing animations, when compared with the best performing methods in the literature. Our code and pre-trained model will be made available.","sentences":["The domain of 3D talking head generation has witnessed significant progress in recent years.","A notable challenge in this field consists in blending speech-related motions with expression dynamics, which is primarily caused by the lack of comprehensive 3D datasets that combine diversity in spoken sentences with a variety of facial expressions.","Whereas literature works attempted to exploit 2D video data and parametric 3D models as a workaround, these still show limitations when jointly modeling the two motions.","In this work, we address this problem from a different perspective, and propose an innovative data-driven technique that we used for creating a synthetic dataset, called EmoVOCA, obtained by combining a collection of inexpressive 3D talking heads and a set of 3D expressive sequences.","To demonstrate the advantages of this approach, and the quality of the dataset, we then designed and trained an emotional 3D talking head generator that accepts a 3D face, an audio file, an emotion label, and an intensity value as inputs, and learns to animate the audio-synchronized lip movements with expressive traits of the face.","Comprehensive experiments, both quantitative and qualitative, using our data and generator evidence superior ability in synthesizing convincing animations, when compared with the best performing methods in the literature.","Our code and pre-trained model will be made available."],"url":"http://arxiv.org/abs/2403.12886v1"}
{"created":"2024-03-19 16:31:30","title":"HYDRA: A Hyper Agent for Dynamic Compositional Visual Reasoning","abstract":"Recent advances in visual reasoning (VR), particularly with the aid of Large Vision-Language Models (VLMs), show promise but require access to large-scale datasets and face challenges such as high computational costs and limited generalization capabilities. Compositional visual reasoning approaches have emerged as effective strategies; however, they heavily rely on the commonsense knowledge encoded in Large Language Models (LLMs) to perform planning, reasoning, or both, without considering the effect of their decisions on the visual reasoning process, which can lead to errors or failed procedures. To address these challenges, we introduce HYDRA, a multi-stage dynamic compositional visual reasoning framework designed for reliable and incrementally progressive general reasoning. HYDRA integrates three essential modules: a planner, a Reinforcement Learning (RL) agent serving as a cognitive controller, and a reasoner. The planner and reasoner modules utilize an LLM to generate instruction samples and executable code from the selected instruction, respectively, while the RL agent dynamically interacts with these modules, making high-level decisions on selection of the best instruction sample given information from the historical state stored through a feedback loop. This adaptable design enables HYDRA to adjust its actions based on previous feedback received during the reasoning process, leading to more reliable reasoning outputs and ultimately enhancing its overall effectiveness. Our framework demonstrates state-of-the-art performance in various VR tasks on four different widely-used datasets.","sentences":["Recent advances in visual reasoning (VR), particularly with the aid of Large Vision-Language Models (VLMs), show promise but require access to large-scale datasets and face challenges such as high computational costs and limited generalization capabilities.","Compositional visual reasoning approaches have emerged as effective strategies; however, they heavily rely on the commonsense knowledge encoded in Large Language Models (LLMs) to perform planning, reasoning, or both, without considering the effect of their decisions on the visual reasoning process, which can lead to errors or failed procedures.","To address these challenges, we introduce HYDRA, a multi-stage dynamic compositional visual reasoning framework designed for reliable and incrementally progressive general reasoning.","HYDRA integrates three essential modules: a planner, a Reinforcement Learning (RL) agent serving as a cognitive controller, and a reasoner.","The planner and reasoner modules utilize an LLM to generate instruction samples and executable code from the selected instruction, respectively, while the RL agent dynamically interacts with these modules, making high-level decisions on selection of the best instruction sample given information from the historical state stored through a feedback loop.","This adaptable design enables HYDRA to adjust its actions based on previous feedback received during the reasoning process, leading to more reliable reasoning outputs and ultimately enhancing its overall effectiveness.","Our framework demonstrates state-of-the-art performance in various VR tasks on four different widely-used datasets."],"url":"http://arxiv.org/abs/2403.12884v1"}
{"created":"2024-03-19 16:29:59","title":"Confusing Pair Correction Based on Category Prototype for Domain Adaptation under Noisy Environments","abstract":"In this paper, we address unsupervised domain adaptation under noisy environments, which is more challenging and practical than traditional domain adaptation. In this scenario, the model is prone to overfitting noisy labels, resulting in a more pronounced domain shift and a notable decline in the overall model performance. Previous methods employed prototype methods for domain adaptation on robust feature spaces. However, these approaches struggle to effectively classify classes with similar features under noisy environments. To address this issue, we propose a new method to detect and correct confusing class pair. We first divide classes into easy and hard classes based on the small loss criterion. We then leverage the top-2 predictions for each sample after aligning the source and target domain to find the confusing pair in the hard classes. We apply label correction to the noisy samples within the confusing pair. With the proposed label correction method, we can train our model with more accurate labels. Extensive experiments confirm the effectiveness of our method and demonstrate its favorable performance compared with existing state-of-the-art methods. Our codes are publicly available at https://github.com/Hehxcf/CPC/.","sentences":["In this paper, we address unsupervised domain adaptation under noisy environments, which is more challenging and practical than traditional domain adaptation.","In this scenario, the model is prone to overfitting noisy labels, resulting in a more pronounced domain shift and a notable decline in the overall model performance.","Previous methods employed prototype methods for domain adaptation on robust feature spaces.","However, these approaches struggle to effectively classify classes with similar features under noisy environments.","To address this issue, we propose a new method to detect and correct confusing class pair.","We first divide classes into easy and hard classes based on the small loss criterion.","We then leverage the top-2 predictions for each sample after aligning the source and target domain to find the confusing pair in the hard classes.","We apply label correction to the noisy samples within the confusing pair.","With the proposed label correction method, we can train our model with more accurate labels.","Extensive experiments confirm the effectiveness of our method and demonstrate its favorable performance compared with existing state-of-the-art methods.","Our codes are publicly available at https://github.com/Hehxcf/CPC/."],"url":"http://arxiv.org/abs/2403.12883v1"}
{"created":"2024-03-19 16:26:10","title":"Agent-FLAN: Designing Data and Methods of Effective Agent Tuning for Large Language Models","abstract":"Open-sourced Large Language Models (LLMs) have achieved great success in various NLP tasks, however, they are still far inferior to API-based models when acting as agents. How to integrate agent ability into general LLMs becomes a crucial and urgent problem. This paper first delivers three key observations: (1) the current agent training corpus is entangled with both formats following and agent reasoning, which significantly shifts from the distribution of its pre-training data; (2) LLMs exhibit different learning speeds on the capabilities required by agent tasks; and (3) current approaches have side-effects when improving agent abilities by introducing hallucinations. Based on the above findings, we propose Agent-FLAN to effectively Fine-tune LANguage models for Agents. Through careful decomposition and redesign of the training corpus, Agent-FLAN enables Llama2-7B to outperform prior best works by 3.5\\% across various agent evaluation datasets. With comprehensively constructed negative samples, Agent-FLAN greatly alleviates the hallucination issues based on our established evaluation benchmark. Besides, it consistently improves the agent capability of LLMs when scaling model sizes while slightly enhancing the general capability of LLMs. The code will be available at https://github.com/InternLM/Agent-FLAN.","sentences":["Open-sourced Large Language Models (LLMs) have achieved great success in various NLP tasks, however, they are still far inferior to API-based models when acting as agents.","How to integrate agent ability into general LLMs becomes a crucial and urgent problem.","This paper first delivers three key observations: (1) the current agent training corpus is entangled with both formats following and agent reasoning, which significantly shifts from the distribution of its pre-training data; (2) LLMs exhibit different learning speeds on the capabilities required by agent tasks; and (3) current approaches have side-effects when improving agent abilities by introducing hallucinations.","Based on the above findings, we propose Agent-FLAN to effectively Fine-tune LANguage models for Agents.","Through careful decomposition and redesign of the training corpus, Agent-FLAN enables Llama2-7B to outperform prior best works by 3.5\\% across various agent evaluation datasets.","With comprehensively constructed negative samples, Agent-FLAN greatly alleviates the hallucination issues based on our established evaluation benchmark.","Besides, it consistently improves the agent capability of LLMs when scaling model sizes while slightly enhancing the general capability of LLMs.","The code will be available at https://github.com/InternLM/Agent-FLAN."],"url":"http://arxiv.org/abs/2403.12881v1"}
{"created":"2024-03-19 16:23:36","title":"Fr\u00e9chet Edit Distance","abstract":"We define and investigate the Fr\\'{e}chet edit distance problem. Given two polygonal curves $\\pi$ and $\\sigma$ and a threshhold value $\\delta>0$, we seek the minimum number of edits to $\\sigma$ such that the Fr\\'{e}chet distance between the edited $\\sigma$ and $\\pi$ is at most $\\delta$. For the edit operations we consider three cases, namely, deletion of vertices, insertion of vertices, or both. For this basic problem we consider a number of variants. Specifically, we provide polynomial time algorithms for both discrete and continuous Fr\\'{e}chet edit distance variants, as well as hardness results for weak Fr\\'{e}chet edit distance variants.","sentences":["We define and investigate the Fr\\'{e}chet edit distance problem.","Given two polygonal curves $\\pi$ and $\\sigma$ and a threshhold value $\\delta>0$, we seek the minimum number of edits to $\\sigma$ such that the Fr\\'{e}chet distance between the edited $\\sigma$ and $\\pi$ is at most $\\delta$.","For the edit operations we consider three cases, namely, deletion of vertices, insertion of vertices, or both.","For this basic problem we consider a number of variants.","Specifically, we provide polynomial time algorithms for both discrete and continuous Fr\\'{e}chet edit distance variants, as well as hardness results for weak Fr\\'{e}chet edit distance variants."],"url":"http://arxiv.org/abs/2403.12878v1"}
{"created":"2024-03-19 16:21:40","title":"LAVA: Long-horizon Visual Action based Food Acquisition","abstract":"Robotic Assisted Feeding (RAF) addresses the fundamental need for individuals with mobility impairments to regain autonomy in feeding themselves. The goal of RAF is to use a robot arm to acquire and transfer food to individuals from the table. Existing RAF methods primarily focus on solid foods, leaving a gap in manipulation strategies for semi-solid and deformable foods. This study introduces Long-horizon Visual Action (LAVA) based food acquisition of liquid, semisolid, and deformable foods. Long-horizon refers to the goal of \"clearing the bowl\" by sequentially acquiring the food from the bowl. LAVA employs a hierarchical policy for long-horizon food acquisition tasks. The framework uses high-level policy to determine primitives by leveraging ScoopNet. At the mid-level, LAVA finds parameters for primitives using vision. To carry out sequential plans in the real world, LAVA delegates action execution which is driven by Low-level policy that uses parameters received from mid-level policy and behavior cloning ensuring precise trajectory execution. We validate our approach on complex real-world acquisition trials involving granular, liquid, semisolid, and deformable food types along with fruit chunks and soup acquisition. Across 46 bowls, LAVA acquires much more efficiently than baselines with a success rate of 89 +/- 4% and generalizes across realistic plate variations such as different positions, varieties, and amount of food in the bowl. Code, datasets, videos, and supplementary materials can be found on our website.","sentences":["Robotic Assisted Feeding (RAF) addresses the fundamental need for individuals with mobility impairments to regain autonomy in feeding themselves.","The goal of RAF is to use a robot arm to acquire and transfer food to individuals from the table.","Existing RAF methods primarily focus on solid foods, leaving a gap in manipulation strategies for semi-solid and deformable foods.","This study introduces Long-horizon Visual Action (LAVA) based food acquisition of liquid, semisolid, and deformable foods.","Long-horizon refers to the goal of \"clearing the bowl\" by sequentially acquiring the food from the bowl.","LAVA employs a hierarchical policy for long-horizon food acquisition tasks.","The framework uses high-level policy to determine primitives by leveraging ScoopNet.","At the mid-level, LAVA finds parameters for primitives using vision.","To carry out sequential plans in the real world, LAVA delegates action execution which is driven by Low-level policy that uses parameters received from mid-level policy and behavior cloning ensuring precise trajectory execution.","We validate our approach on complex real-world acquisition trials involving granular, liquid, semisolid, and deformable food types along with fruit chunks and soup acquisition.","Across 46 bowls, LAVA acquires much more efficiently than baselines with a success rate of 89 +/- 4% and generalizes across realistic plate variations such as different positions, varieties, and amount of food in the bowl.","Code, datasets, videos, and supplementary materials can be found on our website."],"url":"http://arxiv.org/abs/2403.12876v1"}
{"created":"2024-03-19 16:17:21","title":"Short-Term Solar Irradiance Forecasting Under Data Transmission Constraints","abstract":"We report a data-parsimonious machine learning model for short-term forecasting of solar irradiance. The model inputs include sky camera images that are reduced to scalar features to meet data transmission constraints. The output irradiance values are transformed to focus on unknown short-term dynamics. Inspired by control theory, a noise input is used to reflect unmeasured variables and is shown to improve model predictions, often considerably. Five years of data from the NREL Solar Radiation Research Laboratory were used to create three rolling train-validate sets and determine the best representations for time, the optimal span of input measurements, and the most impactful model input data (features). For the chosen test data, the model achieves a mean absolute error of 74.34 $W/m^2$ compared to a baseline 134.35 $W/m^2$ using the persistence of cloudiness model.","sentences":["We report a data-parsimonious machine learning model for short-term forecasting of solar irradiance.","The model inputs include sky camera images that are reduced to scalar features to meet data transmission constraints.","The output irradiance values are transformed to focus on unknown short-term dynamics.","Inspired by control theory, a noise input is used to reflect unmeasured variables and is shown to improve model predictions, often considerably.","Five years of data from the NREL Solar Radiation Research Laboratory were used to create three rolling train-validate sets and determine the best representations for time, the optimal span of input measurements, and the most impactful model input data (features).","For the chosen test data, the model achieves a mean absolute error of 74.34 $W/m^2$ compared to a baseline 134.35 $W/m^2$ using the persistence of cloudiness model."],"url":"http://arxiv.org/abs/2403.12873v1"}
{"created":"2024-03-19 16:15:44","title":"Wildfire danger prediction optimization with transfer learning","abstract":"Convolutional Neural Networks (CNNs) have proven instrumental across various computer science domains, enabling advancements in object detection, classification, and anomaly detection. This paper explores the application of CNNs to analyze geospatial data specifically for identifying wildfire-affected areas. Leveraging transfer learning techniques, we fine-tuned CNN hyperparameters and integrated the Canadian Fire Weather Index (FWI) to assess moisture conditions. The study establishes a methodology for computing wildfire risk levels on a scale of 0 to 5, dynamically linked to weather patterns. Notably, through the integration of transfer learning, the CNN model achieved an impressive accuracy of 95\\% in identifying burnt areas. This research sheds light on the inner workings of CNNs and their practical, real-time utility in predicting and mitigating wildfires. By combining transfer learning and CNNs, this study contributes a robust approach to assess burnt areas, facilitating timely interventions and preventative measures against conflagrations.","sentences":["Convolutional Neural Networks (CNNs) have proven instrumental across various computer science domains, enabling advancements in object detection, classification, and anomaly detection.","This paper explores the application of CNNs to analyze geospatial data specifically for identifying wildfire-affected areas.","Leveraging transfer learning techniques, we fine-tuned CNN hyperparameters and integrated the Canadian Fire Weather Index (FWI) to assess moisture conditions.","The study establishes a methodology for computing wildfire risk levels on a scale of 0 to 5, dynamically linked to weather patterns.","Notably, through the integration of transfer learning, the CNN model achieved an impressive accuracy of 95\\% in identifying burnt areas.","This research sheds light on the inner workings of CNNs and their practical, real-time utility in predicting and mitigating wildfires.","By combining transfer learning and CNNs, this study contributes a robust approach to assess burnt areas, facilitating timely interventions and preventative measures against conflagrations."],"url":"http://arxiv.org/abs/2403.12871v1"}
{"created":"2024-03-19 16:15:08","title":"PoNQ: a Neural QEM-based Mesh Representation","abstract":"Although polygon meshes have been a standard representation in geometry processing, their irregular and combinatorial nature hinders their suitability for learning-based applications. In this work, we introduce a novel learnable mesh representation through a set of local 3D sample Points and their associated Normals and Quadric error metrics (QEM) w.r.t. the underlying shape, which we denote PoNQ. A global mesh is directly derived from PoNQ by efficiently leveraging the knowledge of the local quadric errors. Besides marking the first use of QEM within a neural shape representation, our contribution guarantees both topological and geometrical properties by ensuring that a PoNQ mesh does not self-intersect and is always the boundary of a volume. Notably, our representation does not rely on a regular grid, is supervised directly by the target surface alone, and also handles open surfaces with boundaries and/or sharp features. We demonstrate the efficacy of PoNQ through a learning-based mesh prediction from SDF grids and show that our method surpasses recent state-of-the-art techniques in terms of both surface and edge-based metrics.","sentences":["Although polygon meshes have been a standard representation in geometry processing, their irregular and combinatorial nature hinders their suitability for learning-based applications.","In this work, we introduce a novel learnable mesh representation through a set of local 3D sample Points and their associated Normals and Quadric error metrics (QEM) w.r.t.","the underlying shape, which we denote PoNQ.","A global mesh is directly derived from PoNQ by efficiently leveraging the knowledge of the local quadric errors.","Besides marking the first use of QEM within a neural shape representation, our contribution guarantees both topological and geometrical properties by ensuring that a PoNQ mesh does not self-intersect and is always the boundary of a volume.","Notably, our representation does not rely on a regular grid, is supervised directly by the target surface alone, and also handles open surfaces with boundaries and/or sharp features.","We demonstrate the efficacy of PoNQ through a learning-based mesh prediction from SDF grids and show that our method surpasses recent state-of-the-art techniques in terms of both surface and edge-based metrics."],"url":"http://arxiv.org/abs/2403.12870v1"}
{"created":"2024-03-19 16:12:25","title":"Regularization in Spider-Style Strategy Discovery and Schedule Construction","abstract":"To achieve the best performance, automatic theorem provers often rely on schedules of diverse proving strategies to be tried out (either sequentially or in parallel) on a given problem. In this paper, we report on a large-scale experiment with discovering strategies for the Vampire prover, targeting the FOF fragment of the TPTP library and constructing a schedule for it, based on the ideas of Andrei Voronkov's system Spider. We examine the process from various angles, discuss the difficulty (or ease) of obtaining a strong Vampire schedule for the CASC competition, and establish how well a schedule can be expected to generalize to unseen problems and what factors influence this property.","sentences":["To achieve the best performance, automatic theorem provers often rely on schedules of diverse proving strategies to be tried out (either sequentially or in parallel) on a given problem.","In this paper, we report on a large-scale experiment with discovering strategies for the Vampire prover, targeting the FOF fragment of the TPTP library and constructing a schedule for it, based on the ideas of Andrei Voronkov's system Spider.","We examine the process from various angles, discuss the difficulty (or ease) of obtaining a strong Vampire schedule for the CASC competition, and establish how well a schedule can be expected to generalize to unseen problems and what factors influence this property."],"url":"http://arxiv.org/abs/2403.12869v1"}
{"created":"2024-03-19 16:09:30","title":"PE-Planner: A Performance-Enhanced Quadrotor Motion Planner for Autonomous Flight in Complex and Dynamic Environments","abstract":"The role of a motion planner is pivotal in quadrotor applications, yet existing methods often struggle to adapt to complex environments, limiting their ability to achieve fast, safe, and robust flight. In this letter, we introduce a performance-enhanced quadrotor motion planner designed for autonomous flight in complex environments including dense obstacles, dynamic obstacles, and unknown disturbances. The global planner generates an initial trajectory through kinodynamic path searching and refines it using B-spline trajectory optimization. Subsequently, the local planner takes into account the quadrotor dynamics, estimated disturbance, global reference trajectory, control cost, time cost, and safety constraints to generate real-time control inputs, utilizing the framework of model predictive contouring control. Both simulations and real-world experiments corroborate the heightened robustness, safety, and speed of the proposed motion planner. Additionally, our motion planner achieves flights at more than 6.8 m/s in a challenging and complex racing scenario.","sentences":["The role of a motion planner is pivotal in quadrotor applications, yet existing methods often struggle to adapt to complex environments, limiting their ability to achieve fast, safe, and robust flight.","In this letter, we introduce a performance-enhanced quadrotor motion planner designed for autonomous flight in complex environments including dense obstacles, dynamic obstacles, and unknown disturbances.","The global planner generates an initial trajectory through kinodynamic path searching and refines it using B-spline trajectory optimization.","Subsequently, the local planner takes into account the quadrotor dynamics, estimated disturbance, global reference trajectory, control cost, time cost, and safety constraints to generate real-time control inputs, utilizing the framework of model predictive contouring control.","Both simulations and real-world experiments corroborate the heightened robustness, safety, and speed of the proposed motion planner.","Additionally, our motion planner achieves flights at more than 6.8 m/s in a challenging and complex racing scenario."],"url":"http://arxiv.org/abs/2403.12865v1"}
{"created":"2024-03-19 16:08:27","title":"A Comparison of Deep Learning Architectures for Spacecraft Anomaly Detection","abstract":"Spacecraft operations are highly critical, demanding impeccable reliability and safety. Ensuring the optimal performance of a spacecraft requires the early detection and mitigation of anomalies, which could otherwise result in unit or mission failures. With the advent of deep learning, a surge of interest has been seen in leveraging these sophisticated algorithms for anomaly detection in space operations. This study aims to compare the efficacy of various deep learning architectures in detecting anomalies in spacecraft data. The deep learning models under investigation include Convolutional Neural Networks (CNNs), Recurrent Neural Networks (RNNs), Long Short-Term Memory (LSTM) networks, and Transformer-based architectures. Each of these models was trained and validated using a comprehensive dataset sourced from multiple spacecraft missions, encompassing diverse operational scenarios and anomaly types. Initial results indicate that while CNNs excel in identifying spatial patterns and may be effective for some classes of spacecraft data, LSTMs and RNNs show a marked proficiency in capturing temporal anomalies seen in time-series spacecraft telemetry. The Transformer-based architectures, given their ability to focus on both local and global contexts, have showcased promising results, especially in scenarios where anomalies are subtle and span over longer durations. Additionally, considerations such as computational efficiency, ease of deployment, and real-time processing capabilities were evaluated. While CNNs and LSTMs demonstrated a balance between accuracy and computational demands, Transformer architectures, though highly accurate, require significant computational resources. In conclusion, the choice of deep learning architecture for spacecraft anomaly detection is highly contingent on the nature of the data, the type of anomalies, and operational constraints.","sentences":["Spacecraft operations are highly critical, demanding impeccable reliability and safety.","Ensuring the optimal performance of a spacecraft requires the early detection and mitigation of anomalies, which could otherwise result in unit or mission failures.","With the advent of deep learning, a surge of interest has been seen in leveraging these sophisticated algorithms for anomaly detection in space operations.","This study aims to compare the efficacy of various deep learning architectures in detecting anomalies in spacecraft data.","The deep learning models under investigation include Convolutional Neural Networks (CNNs), Recurrent Neural Networks (RNNs), Long Short-Term Memory (LSTM) networks, and Transformer-based architectures.","Each of these models was trained and validated using a comprehensive dataset sourced from multiple spacecraft missions, encompassing diverse operational scenarios and anomaly types.","Initial results indicate that while CNNs excel in identifying spatial patterns and may be effective for some classes of spacecraft data, LSTMs and RNNs show a marked proficiency in capturing temporal anomalies seen in time-series spacecraft telemetry.","The Transformer-based architectures, given their ability to focus on both local and global contexts, have showcased promising results, especially in scenarios where anomalies are subtle and span over longer durations.","Additionally, considerations such as computational efficiency, ease of deployment, and real-time processing capabilities were evaluated.","While CNNs and LSTMs demonstrated a balance between accuracy and computational demands, Transformer architectures, though highly accurate, require significant computational resources.","In conclusion, the choice of deep learning architecture for spacecraft anomaly detection is highly contingent on the nature of the data, the type of anomalies, and operational constraints."],"url":"http://arxiv.org/abs/2403.12864v1"}
{"created":"2024-03-19 16:06:10","title":"Epistemology of Language Models: Do Language Models Have Holistic Knowledge?","abstract":"This paper investigates the inherent knowledge in language models from the perspective of epistemological holism. The purpose of this paper is to explore whether LLMs exhibit characteristics consistent with epistemological holism. These characteristics suggest that core knowledge, such as general scientific knowledge, each plays a specific role, serving as the foundation of our knowledge system and being difficult to revise. To assess these traits related to holism, we created a scientific reasoning dataset and examined the epistemology of language models through three tasks: Abduction, Revision, and Argument Generation. In the abduction task, the language models explained situations while avoiding revising the core knowledge. However, in other tasks, the language models were revealed not to distinguish between core and peripheral knowledge, showing an incomplete alignment with holistic knowledge principles.","sentences":["This paper investigates the inherent knowledge in language models from the perspective of epistemological holism.","The purpose of this paper is to explore whether LLMs exhibit characteristics consistent with epistemological holism.","These characteristics suggest that core knowledge, such as general scientific knowledge, each plays a specific role, serving as the foundation of our knowledge system and being difficult to revise.","To assess these traits related to holism, we created a scientific reasoning dataset and examined the epistemology of language models through three tasks: Abduction, Revision, and Argument Generation.","In the abduction task, the language models explained situations while avoiding revising the core knowledge.","However, in other tasks, the language models were revealed not to distinguish between core and peripheral knowledge, showing an incomplete alignment with holistic knowledge principles."],"url":"http://arxiv.org/abs/2403.12862v1"}
{"created":"2024-03-19 16:05:51","title":"D-Cubed: Latent Diffusion Trajectory Optimisation for Dexterous Deformable Manipulation","abstract":"Mastering dexterous robotic manipulation of deformable objects is vital for overcoming the limitations of parallel grippers in real-world applications. Current trajectory optimisation approaches often struggle to solve such tasks due to the large search space and the limited task information available from a cost function. In this work, we propose D-Cubed, a novel trajectory optimisation method using a latent diffusion model (LDM) trained from a task-agnostic play dataset to solve dexterous deformable object manipulation tasks. D-Cubed learns a skill-latent space that encodes short-horizon actions in the play dataset using a VAE and trains a LDM to compose the skill latents into a skill trajectory, representing a long-horizon action trajectory in the dataset. To optimise a trajectory for a target task, we introduce a novel gradient-free guided sampling method that employs the Cross-Entropy method within the reverse diffusion process. In particular, D-Cubed samples a small number of noisy skill trajectories using the LDM for exploration and evaluates the trajectories in simulation. Then, D-Cubed selects the trajectory with the lowest cost for the subsequent reverse process. This effectively explores promising solution areas and optimises the sampled trajectories towards a target task throughout the reverse diffusion process. Through empirical evaluation on a public benchmark of dexterous deformable object manipulation tasks, we demonstrate that D-Cubed outperforms traditional trajectory optimisation and competitive baseline approaches by a significant margin. We further demonstrate that trajectories found by D-Cubed readily transfer to a real-world LEAP hand on a folding task.","sentences":["Mastering dexterous robotic manipulation of deformable objects is vital for overcoming the limitations of parallel grippers in real-world applications.","Current trajectory optimisation approaches often struggle to solve such tasks due to the large search space and the limited task information available from a cost function.","In this work, we propose D-Cubed, a novel trajectory optimisation method using a latent diffusion model (LDM) trained from a task-agnostic play dataset to solve dexterous deformable object manipulation tasks.","D-Cubed learns a skill-latent space that encodes short-horizon actions in the play dataset using a VAE and trains a LDM to compose the skill latents into a skill trajectory, representing a long-horizon action trajectory in the dataset.","To optimise a trajectory for a target task, we introduce a novel gradient-free guided sampling method that employs the Cross-Entropy method within the reverse diffusion process.","In particular, D-Cubed samples a small number of noisy skill trajectories using the LDM for exploration and evaluates the trajectories in simulation.","Then, D-Cubed selects the trajectory with the lowest cost for the subsequent reverse process.","This effectively explores promising solution areas and optimises the sampled trajectories towards a target task throughout the reverse diffusion process.","Through empirical evaluation on a public benchmark of dexterous deformable object manipulation tasks, we demonstrate that D-Cubed outperforms traditional trajectory optimisation and competitive baseline approaches by a significant margin.","We further demonstrate that trajectories found by D-Cubed readily transfer to a real-world LEAP hand on a folding task."],"url":"http://arxiv.org/abs/2403.12861v1"}
{"created":"2024-03-19 16:01:25","title":"Equivariant Ensembles and Regularization for Reinforcement Learning in Map-based Path Planning","abstract":"In reinforcement learning (RL), exploiting environmental symmetries can significantly enhance efficiency, robustness, and performance. However, ensuring that the deep RL policy and value networks are respectively equivariant and invariant to exploit these symmetries is a substantial challenge. Related works try to design networks that are equivariant and invariant by construction, limiting them to a very restricted library of components, which in turn hampers the expressiveness of the networks. This paper proposes a method to construct equivariant policies and invariant value functions without specialized neural network components, which we term equivariant ensembles. We further add a regularization term for adding inductive bias during training. In a map-based path planning case study, we show how equivariant ensembles and regularization benefit sample efficiency and performance.","sentences":["In reinforcement learning (RL), exploiting environmental symmetries can significantly enhance efficiency, robustness, and performance.","However, ensuring that the deep RL policy and value networks are respectively equivariant and invariant to exploit these symmetries is a substantial challenge.","Related works try to design networks that are equivariant and invariant by construction, limiting them to a very restricted library of components, which in turn hampers the expressiveness of the networks.","This paper proposes a method to construct equivariant policies and invariant value functions without specialized neural network components, which we term equivariant ensembles.","We further add a regularization term for adding inductive bias during training.","In a map-based path planning case study, we show how equivariant ensembles and regularization benefit sample efficiency and performance."],"url":"http://arxiv.org/abs/2403.12856v1"}
{"created":"2024-03-19 15:57:32","title":"RASP: A Drone-based Reconfigurable Actuation and Sensing Platform Towards Ambient Intelligent Systems","abstract":"Realizing consumer-grade drones that are as useful as robot vacuums throughout our homes or personal smartphones in our daily lives requires drones to sense, actuate, and respond to general scenarios that may arise. Towards this vision, we propose RASP, a modular and reconfigurable sensing and actuation platform that allows drones to autonomously swap onboard sensors and actuators in only 25 seconds, allowing a single drone to quickly adapt to a diverse range of tasks. RASP consists of a mechanical layer to physically swap sensor modules, an electrical layer to maintain power and communication lines to the sensor/actuator, and a software layer to maintain a common interface between the drone and any sensor module in our platform. Leveraging recent advances in large language and visual language models, we further introduce the architecture, implementation, and real-world deployments of a personal assistant system utilizing RASP. We demonstrate that RASP can enable a diverse range of useful tasks in home, office, lab, and other indoor settings.","sentences":["Realizing consumer-grade drones that are as useful as robot vacuums throughout our homes or personal smartphones in our daily lives requires drones to sense, actuate, and respond to general scenarios that may arise.","Towards this vision, we propose RASP, a modular and reconfigurable sensing and actuation platform that allows drones to autonomously swap onboard sensors and actuators in only 25 seconds, allowing a single drone to quickly adapt to a diverse range of tasks.","RASP consists of a mechanical layer to physically swap sensor modules, an electrical layer to maintain power and communication lines to the sensor/actuator, and a software layer to maintain a common interface between the drone and any sensor module in our platform.","Leveraging recent advances in large language and visual language models, we further introduce the architecture, implementation, and real-world deployments of a personal assistant system utilizing RASP.","We demonstrate that RASP can enable a diverse range of useful tasks in home, office, lab, and other indoor settings."],"url":"http://arxiv.org/abs/2403.12853v1"}
{"created":"2024-03-19 15:54:56","title":"Optimizing Service Placement in Edge-to-Cloud AR/VR Systems using a Multi-Objective Genetic Algorithm","abstract":"Augmented Reality (AR) and Virtual Reality (VR) systems involve computationally intensive image processing algorithms that can burden end-devices with limited resources, leading to poor performance in providing low latency services. Edge-to-cloud computing overcomes the limitations of end-devices by offloading their computations to nearby edge devices or remote cloud servers. Although this proves to be sufficient for many applications, optimal placement of latency sensitive AR/VR services in edge-to-cloud infrastructures (to provide desirable service response times and reliability) remain a formidable challenging. To address this challenge, this paper develops a Multi-Objective Genetic Algorithm (MOGA) to optimize the placement of AR/VR-based services in multi-tier edge-to-cloud environments. The primary objective of the proposed MOGA is to minimize the response time of all running services, while maximizing the reliability of the underlying system from both software and hardware perspectives. To evaluate its performance, we mathematically modeled all components and developed a tailor-made simulator to assess its effectiveness on various scales. MOGA was compared with several heuristics to prove that intuitive solutions, which are usually assumed sufficient, are not efficient enough for the stated problem. The experimental results indicated that MOGA can significantly reduce the response time of deployed services by an average of 67\\% on different scales, compared to other heuristic methods. MOGA also ensures reliability of the 97\\% infrastructure (hardware) and 95\\% services (software).","sentences":["Augmented Reality (AR) and Virtual Reality (VR) systems involve computationally intensive image processing algorithms that can burden end-devices with limited resources, leading to poor performance in providing low latency services.","Edge-to-cloud computing overcomes the limitations of end-devices by offloading their computations to nearby edge devices or remote cloud servers.","Although this proves to be sufficient for many applications, optimal placement of latency sensitive AR/VR services in edge-to-cloud infrastructures (to provide desirable service response times and reliability) remain a formidable challenging.","To address this challenge, this paper develops a Multi-Objective Genetic Algorithm (MOGA) to optimize the placement of AR/VR-based services in multi-tier edge-to-cloud environments.","The primary objective of the proposed MOGA is to minimize the response time of all running services, while maximizing the reliability of the underlying system from both software and hardware perspectives.","To evaluate its performance, we mathematically modeled all components and developed a tailor-made simulator to assess its effectiveness on various scales.","MOGA was compared with several heuristics to prove that intuitive solutions, which are usually assumed sufficient, are not efficient enough for the stated problem.","The experimental results indicated that MOGA can significantly reduce the response time of deployed services by an average of 67\\% on different scales, compared to other heuristic methods.","MOGA also ensures reliability of the 97\\% infrastructure (hardware) and 95\\% services (software)."],"url":"http://arxiv.org/abs/2403.12849v1"}
{"created":"2024-03-19 15:54:48","title":"Compositional 3D Scene Synthesis with Scene Graph Guided Layout-Shape Generation","abstract":"Compositional 3D scene synthesis has diverse applications across a spectrum of industries such as robotics, films, and video games, as it closely mirrors the complexity of real-world multi-object environments. Early works typically employ shape retrieval based frameworks which naturally suffer from limited shape diversity. Recent progresses have been made in shape generation with powerful generative models, such as diffusion models, which increases the shape fidelity. However, these approaches separately treat 3D shape generation and layout generation. The synthesized scenes are usually hampered by layout collision, which implies that the scene-level fidelity is still under-explored. In this paper, we aim at generating realistic and reasonable 3D scenes from scene graph. To enrich the representation capability of the given scene graph inputs, large language model is utilized to explicitly aggregate the global graph features with local relationship features. With a unified graph convolution network (GCN), graph features are extracted from scene graphs updated via joint layout-shape distribution. During scene generation, an IoU-based regularization loss is introduced to constrain the predicted 3D layouts. Benchmarked on the SG-FRONT dataset, our method achieves better 3D scene synthesis, especially in terms of scene-level fidelity. The source code will be released after publication.","sentences":["Compositional 3D scene synthesis has diverse applications across a spectrum of industries such as robotics, films, and video games, as it closely mirrors the complexity of real-world multi-object environments.","Early works typically employ shape retrieval based frameworks which naturally suffer from limited shape diversity.","Recent progresses have been made in shape generation with powerful generative models, such as diffusion models, which increases the shape fidelity.","However, these approaches separately treat 3D shape generation and layout generation.","The synthesized scenes are usually hampered by layout collision, which implies that the scene-level fidelity is still under-explored.","In this paper, we aim at generating realistic and reasonable 3D scenes from scene graph.","To enrich the representation capability of the given scene graph inputs, large language model is utilized to explicitly aggregate the global graph features with local relationship features.","With a unified graph convolution network (GCN), graph features are extracted from scene graphs updated via joint layout-shape distribution.","During scene generation, an IoU-based regularization loss is introduced to constrain the predicted 3D layouts.","Benchmarked on the SG-FRONT dataset, our method achieves better 3D scene synthesis, especially in terms of scene-level fidelity.","The source code will be released after publication."],"url":"http://arxiv.org/abs/2403.12848v1"}
{"created":"2024-03-19 15:54:38","title":"Policy Bifurcation in Safe Reinforcement Learning","abstract":"Safe reinforcement learning (RL) offers advanced solutions to constrained optimal control problems. Existing studies in safe RL implicitly assume continuity in policy functions, where policies map states to actions in a smooth, uninterrupted manner; however, our research finds that in some scenarios, the feasible policy should be discontinuous or multi-valued, interpolating between discontinuous local optima can inevitably lead to constraint violations. We are the first to identify the generating mechanism of such a phenomenon, and employ topological analysis to rigorously prove the existence of policy bifurcation in safe RL, which corresponds to the contractibility of the reachable tuple. Our theorem reveals that in scenarios where the obstacle-free state space is non-simply connected, a feasible policy is required to be bifurcated, meaning its output action needs to change abruptly in response to the varying state. To train such a bifurcated policy, we propose a safe RL algorithm called multimodal policy optimization (MUPO), which utilizes a Gaussian mixture distribution as the policy output. The bifurcated behavior can be achieved by selecting the Gaussian component with the highest mixing coefficient. Besides, MUPO also integrates spectral normalization and forward KL divergence to enhance the policy's capability of exploring different modes. Experiments with vehicle control tasks show that our algorithm successfully learns the bifurcated policy and ensures satisfying safety, while a continuous policy suffers from inevitable constraint violations.","sentences":["Safe reinforcement learning (RL) offers advanced solutions to constrained optimal control problems.","Existing studies in safe RL implicitly assume continuity in policy functions, where policies map states to actions in a smooth, uninterrupted manner; however, our research finds that in some scenarios, the feasible policy should be discontinuous or multi-valued, interpolating between discontinuous local optima can inevitably lead to constraint violations.","We are the first to identify the generating mechanism of such a phenomenon, and employ topological analysis to rigorously prove the existence of policy bifurcation in safe RL, which corresponds to the contractibility of the reachable tuple.","Our theorem reveals that in scenarios where the obstacle-free state space is non-simply connected, a feasible policy is required to be bifurcated, meaning its output action needs to change abruptly in response to the varying state.","To train such a bifurcated policy, we propose a safe RL algorithm called multimodal policy optimization (MUPO), which utilizes a Gaussian mixture distribution as the policy output.","The bifurcated behavior can be achieved by selecting the Gaussian component with the highest mixing coefficient.","Besides, MUPO also integrates spectral normalization and forward KL divergence to enhance the policy's capability of exploring different modes.","Experiments with vehicle control tasks show that our algorithm successfully learns the bifurcated policy and ensures satisfying safety, while a continuous policy suffers from inevitable constraint violations."],"url":"http://arxiv.org/abs/2403.12847v1"}
{"created":"2024-03-19 15:51:21","title":"MELTing point: Mobile Evaluation of Language Transformers","abstract":"Transformers have revolutionized the machine learning landscape, gradually making their way into everyday tasks and equipping our computers with ``sparks of intelligence''. However, their runtime requirements have prevented them from being broadly deployed on mobile. As personal devices become increasingly powerful and prompt privacy becomes an ever more pressing issue, we explore the current state of mobile execution of Large Language Models (LLMs). To achieve this, we have created our own automation infrastructure, MELT, which supports the headless execution and benchmarking of LLMs on device, supporting different models, devices and frameworks, including Android, iOS and Nvidia Jetson devices. We evaluate popular instruction fine-tuned LLMs and leverage different frameworks to measure their end-to-end and granular performance, tracing their memory and energy requirements along the way.   Our analysis is the first systematic study of on-device LLM execution, quantifying performance, energy efficiency and accuracy across various state-of-the-art models and showcases the state of on-device intelligence in the era of hyperscale models. Results highlight the performance heterogeneity across targets and corroborates that LLM inference is largely memory-bound. Quantization drastically reduces memory requirements and renders execution viable, but at a non-negligible accuracy cost. Drawing from its energy footprint and thermal behavior, the continuous execution of LLMs remains elusive, as both factors negatively affect user experience. Last, our experience shows that the ecosystem is still in its infancy, and algorithmic as well as hardware breakthroughs can significantly shift the execution cost. We expect NPU acceleration, and framework-hardware co-design to be the biggest bet towards efficient standalone execution, with the alternative of offloading tailored towards edge deployments.","sentences":["Transformers have revolutionized the machine learning landscape, gradually making their way into everyday tasks and equipping our computers with ``sparks of intelligence''.","However, their runtime requirements have prevented them from being broadly deployed on mobile.","As personal devices become increasingly powerful and prompt privacy becomes an ever more pressing issue, we explore the current state of mobile execution of Large Language Models (LLMs).","To achieve this, we have created our own automation infrastructure, MELT, which supports the headless execution and benchmarking of LLMs on device, supporting different models, devices and frameworks, including Android, iOS and Nvidia Jetson devices.","We evaluate popular instruction fine-tuned LLMs and leverage different frameworks to measure their end-to-end and granular performance, tracing their memory and energy requirements along the way.   ","Our analysis is the first systematic study of on-device LLM execution, quantifying performance, energy efficiency and accuracy across various state-of-the-art models and showcases the state of on-device intelligence in the era of hyperscale models.","Results highlight the performance heterogeneity across targets and corroborates that LLM inference is largely memory-bound.","Quantization drastically reduces memory requirements and renders execution viable, but at a non-negligible accuracy cost.","Drawing from its energy footprint and thermal behavior, the continuous execution of LLMs remains elusive, as both factors negatively affect user experience.","Last, our experience shows that the ecosystem is still in its infancy, and algorithmic as well as hardware breakthroughs can significantly shift the execution cost.","We expect NPU acceleration, and framework-hardware co-design to be the biggest bet towards efficient standalone execution, with the alternative of offloading tailored towards edge deployments."],"url":"http://arxiv.org/abs/2403.12844v1"}
{"created":"2024-03-19 15:49:32","title":"The Interplay Between Symmetries and Impact Effects on Hybrid Mechanical Systems","abstract":"Hybrid systems are dynamical systems with continuous-time and discrete-time components in their dynamics. When hybrid systems are defined on a principal bundle we are able to define two classes of impacts for the discrete-time transition of the dynamics: interior impacts and exterior impacts. In this paper we define hybrid systems on principal bundles, study the underlying geometry on the switching surface where impacts occur and we find conditions for which both exterior and interior impacts are preserved by the mechanical connection induced in the principal bundle.","sentences":["Hybrid systems are dynamical systems with continuous-time and discrete-time components in their dynamics.","When hybrid systems are defined on a principal bundle we are able to define two classes of impacts for the discrete-time transition of the dynamics: interior impacts and exterior impacts.","In this paper we define hybrid systems on principal bundles, study the underlying geometry on the switching surface where impacts occur and we find conditions for which both exterior and interior impacts are preserved by the mechanical connection induced in the principal bundle."],"url":"http://arxiv.org/abs/2403.12842v1"}
{"created":"2024-03-19 15:45:54","title":"Global-guided Focal Neural Radiance Field for Large-scale Scene Rendering","abstract":"Neural radiance fields~(NeRF) have recently been applied to render large-scale scenes. However, their limited model capacity typically results in blurred rendering results. Existing large-scale NeRFs primarily address this limitation by partitioning the scene into blocks, which are subsequently handled by separate sub-NeRFs. These sub-NeRFs, trained from scratch and processed independently, lead to inconsistencies in geometry and appearance across the scene. Consequently, the rendering quality fails to exhibit significant improvement despite the expansion of model capacity. In this work, we present global-guided focal neural radiance field (GF-NeRF) that achieves high-fidelity rendering of large-scale scenes. Our proposed GF-NeRF utilizes a two-stage (Global and Focal) architecture and a global-guided training strategy. The global stage obtains a continuous representation of the entire scene while the focal stage decomposes the scene into multiple blocks and further processes them with distinct sub-encoders. Leveraging this two-stage architecture, sub-encoders only need fine-tuning based on the global encoder, thus reducing training complexity in the focal stage while maintaining scene-wide consistency. Spatial information and error information from the global stage also benefit the sub-encoders to focus on crucial areas and effectively capture more details of large-scale scenes. Notably, our approach does not rely on any prior knowledge about the target scene, attributing GF-NeRF adaptable to various large-scale scene types, including street-view and aerial-view scenes. We demonstrate that our method achieves high-fidelity, natural rendering results on various types of large-scale datasets. Our project page: https://shaomq2187.github.io/GF-NeRF/","sentences":["Neural radiance fields~(NeRF) have recently been applied to render large-scale scenes.","However, their limited model capacity typically results in blurred rendering results.","Existing large-scale NeRFs primarily address this limitation by partitioning the scene into blocks, which are subsequently handled by separate sub-NeRFs.","These sub-NeRFs, trained from scratch and processed independently, lead to inconsistencies in geometry and appearance across the scene.","Consequently, the rendering quality fails to exhibit significant improvement despite the expansion of model capacity.","In this work, we present global-guided focal neural radiance field (GF-NeRF) that achieves high-fidelity rendering of large-scale scenes.","Our proposed GF-NeRF utilizes a two-stage (Global and Focal) architecture and a global-guided training strategy.","The global stage obtains a continuous representation of the entire scene while the focal stage decomposes the scene into multiple blocks and further processes them with distinct sub-encoders.","Leveraging this two-stage architecture, sub-encoders only need fine-tuning based on the global encoder, thus reducing training complexity in the focal stage while maintaining scene-wide consistency.","Spatial information and error information from the global stage also benefit the sub-encoders to focus on crucial areas and effectively capture more details of large-scale scenes.","Notably, our approach does not rely on any prior knowledge about the target scene, attributing GF-NeRF adaptable to various large-scale scene types, including street-view and aerial-view scenes.","We demonstrate that our method achieves high-fidelity, natural rendering results on various types of large-scale datasets.","Our project page: https://shaomq2187.github.io/GF-NeRF/"],"url":"http://arxiv.org/abs/2403.12839v1"}
{"created":"2024-03-19 15:43:16","title":"How Spammers and Scammers Leverage AI-Generated Images on Facebook for Audience Growth","abstract":"Much of the research and discourse on risks from artificial intelligence (AI) image generators, such as DALL-E and Midjourney, has centered around whether they could be used to inject false information into political discourse. We show that spammers and scammers - seemingly motivated by profit or clout, not ideology - are already using AI-generated images to gain significant traction on Facebook. At times, the Facebook Feed is recommending unlabeled AI-generated images to users who neither follow the Pages posting the images nor realize that the images are AI-generated, highlighting the need for improved transparency and provenance standards as AI models proliferate.","sentences":["Much of the research and discourse on risks from artificial intelligence (AI) image generators, such as DALL-E and Midjourney, has centered around whether they could be used to inject false information into political discourse.","We show that spammers and scammers - seemingly motivated by profit or clout, not ideology - are already using AI-generated images to gain significant traction on Facebook.","At times, the Facebook Feed is recommending unlabeled AI-generated images to users who neither follow the Pages posting the images nor realize that the images are AI-generated, highlighting the need for improved transparency and provenance standards as AI models proliferate."],"url":"http://arxiv.org/abs/2403.12838v1"}
{"created":"2024-03-19 15:42:46","title":"Opti-Acoustic Semantic SLAM with Unknown Objects in Underwater Environments","abstract":"Despite recent advances in semantic Simultaneous Localization and Mapping (SLAM) for terrestrial and aerial applications, underwater semantic SLAM remains an open and largely unaddressed research problem due to the unique sensing modalities and the object classes found underwater. This paper presents an object-based semantic SLAM method for underwater environments that can identify, localize, classify, and map a wide variety of marine objects without a priori knowledge of the object classes present in the scene. The method performs unsupervised object segmentation and object-level feature aggregation, and then uses opti-acoustic sensor fusion for object localization. Probabilistic data association is used to determine observation to landmark correspondences. Given such correspondences, the method then jointly optimizes landmark and vehicle position estimates. Indoor and outdoor underwater datasets with a wide variety of objects and challenging acoustic and lighting conditions are collected for evaluation and made publicly available. Quantitative and qualitative results show the proposed method achieves reduced trajectory error compared to baseline methods, and is able to obtain comparable map accuracy to a baseline closed-set method that requires hand-labeled data of all objects in the scene.","sentences":["Despite recent advances in semantic Simultaneous Localization and Mapping (SLAM) for terrestrial and aerial applications, underwater semantic SLAM remains an open and largely unaddressed research problem due to the unique sensing modalities and the object classes found underwater.","This paper presents an object-based semantic SLAM method for underwater environments that can identify, localize, classify, and map a wide variety of marine objects without a priori knowledge of the object classes present in the scene.","The method performs unsupervised object segmentation and object-level feature aggregation, and then uses opti-acoustic sensor fusion for object localization.","Probabilistic data association is used to determine observation to landmark correspondences.","Given such correspondences, the method then jointly optimizes landmark and vehicle position estimates.","Indoor and outdoor underwater datasets with a wide variety of objects and challenging acoustic and lighting conditions are collected for evaluation and made publicly available.","Quantitative and qualitative results show the proposed method achieves reduced trajectory error compared to baseline methods, and is able to obtain comparable map accuracy to a baseline closed-set method that requires hand-labeled data of all objects in the scene."],"url":"http://arxiv.org/abs/2403.12837v1"}
{"created":"2024-03-19 15:41:39","title":"AnySkill: Learning Open-Vocabulary Physical Skill for Interactive Agents","abstract":"Traditional approaches in physics-based motion generation, centered around imitation learning and reward shaping, often struggle to adapt to new scenarios. To tackle this limitation, we propose AnySkill, a novel hierarchical method that learns physically plausible interactions following open-vocabulary instructions. Our approach begins by developing a set of atomic actions via a low-level controller trained via imitation learning. Upon receiving an open-vocabulary textual instruction, AnySkill employs a high-level policy that selects and integrates these atomic actions to maximize the CLIP similarity between the agent's rendered images and the text. An important feature of our method is the use of image-based rewards for the high-level policy, which allows the agent to learn interactions with objects without manual reward engineering. We demonstrate AnySkill's capability to generate realistic and natural motion sequences in response to unseen instructions of varying lengths, marking it the first method capable of open-vocabulary physical skill learning for interactive humanoid agents.","sentences":["Traditional approaches in physics-based motion generation, centered around imitation learning and reward shaping, often struggle to adapt to new scenarios.","To tackle this limitation, we propose AnySkill, a novel hierarchical method that learns physically plausible interactions following open-vocabulary instructions.","Our approach begins by developing a set of atomic actions via a low-level controller trained via imitation learning.","Upon receiving an open-vocabulary textual instruction, AnySkill employs a high-level policy that selects and integrates these atomic actions to maximize the CLIP similarity between the agent's rendered images and the text.","An important feature of our method is the use of image-based rewards for the high-level policy, which allows the agent to learn interactions with objects without manual reward engineering.","We demonstrate AnySkill's capability to generate realistic and natural motion sequences in response to unseen instructions of varying lengths, marking it the first method capable of open-vocabulary physical skill learning for interactive humanoid agents."],"url":"http://arxiv.org/abs/2403.12835v1"}
{"created":"2024-03-19 15:41:16","title":"Embarrassingly Simple Scribble Supervision for 3D Medical Segmentation","abstract":"Traditionally, segmentation algorithms require dense annotations for training, demanding significant annotation efforts, particularly within the 3D medical imaging field. Scribble-supervised learning emerges as a possible solution to this challenge, promising a reduction in annotation efforts when creating large-scale datasets. Recently, a plethora of methods for optimized learning from scribbles have been proposed, but have so far failed to position scribble annotation as a beneficial alternative. We relate this shortcoming to two major issues: 1) the complex nature of many methods which deeply ties them to the underlying segmentation model, thus preventing a migration to more powerful state-of-the-art models as the field progresses and 2) the lack of a systematic evaluation to validate consistent performance across the broader medical domain, resulting in a lack of trust when applying these methods to new segmentation problems. To address these issues, we propose a comprehensive scribble supervision benchmark consisting of seven datasets covering a diverse set of anatomies and pathologies imaged with varying modalities. We furthermore propose the systematic use of partial losses, i.e. losses that are only computed on annotated voxels. Contrary to most existing methods, these losses can be seamlessly integrated into state-of-the-art segmentation methods, enabling them to learn from scribble annotations while preserving their original loss formulations. Our evaluation using nnU-Net reveals that while most existing methods suffer from a lack of generalization, the proposed approach consistently delivers state-of-the-art performance. Thanks to its simplicity, our approach presents an embarrassingly simple yet effective solution to the challenges of scribble supervision. Source code as well as our extensive scribble benchmarking suite will be made publicly available upon publication.","sentences":["Traditionally, segmentation algorithms require dense annotations for training, demanding significant annotation efforts, particularly within the 3D medical imaging field.","Scribble-supervised learning emerges as a possible solution to this challenge, promising a reduction in annotation efforts when creating large-scale datasets.","Recently, a plethora of methods for optimized learning from scribbles have been proposed, but have so far failed to position scribble annotation as a beneficial alternative.","We relate this shortcoming to two major issues: 1) the complex nature of many methods which deeply ties them to the underlying segmentation model, thus preventing a migration to more powerful state-of-the-art models as the field progresses and 2) the lack of a systematic evaluation to validate consistent performance across the broader medical domain, resulting in a lack of trust when applying these methods to new segmentation problems.","To address these issues, we propose a comprehensive scribble supervision benchmark consisting of seven datasets covering a diverse set of anatomies and pathologies imaged with varying modalities.","We furthermore propose the systematic use of partial losses, i.e. losses that are only computed on annotated voxels.","Contrary to most existing methods, these losses can be seamlessly integrated into state-of-the-art segmentation methods, enabling them to learn from scribble annotations while preserving their original loss formulations.","Our evaluation using nnU-Net reveals that while most existing methods suffer from a lack of generalization, the proposed approach consistently delivers state-of-the-art performance.","Thanks to its simplicity, our approach presents an embarrassingly simple yet effective solution to the challenges of scribble supervision.","Source code as well as our extensive scribble benchmarking suite will be made publicly available upon publication."],"url":"http://arxiv.org/abs/2403.12834v1"}
{"created":"2024-03-19 15:37:27","title":"Has Approximate Machine Unlearning been evaluated properly? From Auditing to Side Effects","abstract":"The growing concerns surrounding data privacy and security have underscored the critical necessity for machine unlearning--aimed at fully removing data lineage from machine learning models. MLaaS providers expect this to be their ultimate safeguard for regulatory compliance. Despite its critical importance, the pace at which privacy communities have been developing and implementing strong methods to verify the effectiveness of machine unlearning has been disappointingly slow, with this vital area often receiving insufficient focus. This paper seeks to address this shortfall by introducing well-defined and effective metrics for black-box unlearning auditing tasks. We transform the auditing challenge into a question of non-membership inference and develop efficient metrics for auditing. By relying exclusively on the original and unlearned models--eliminating the need to train additional shadow models--our approach simplifies the evaluation of unlearning at the individual data point level. Utilizing these metrics, we conduct an in-depth analysis of current approximate machine unlearning algorithms, identifying three key directions where these approaches fall short: utility, resilience, and equity. Our aim is that this work will greatly improve our understanding of approximate machine unlearning methods, taking a significant stride towards converting the theoretical right to data erasure into a auditable reality.","sentences":["The growing concerns surrounding data privacy and security have underscored the critical necessity for machine unlearning--aimed at fully removing data lineage from machine learning models.","MLaaS providers expect this to be their ultimate safeguard for regulatory compliance.","Despite its critical importance, the pace at which privacy communities have been developing and implementing strong methods to verify the effectiveness of machine unlearning has been disappointingly slow, with this vital area often receiving insufficient focus.","This paper seeks to address this shortfall by introducing well-defined and effective metrics for black-box unlearning auditing tasks.","We transform the auditing challenge into a question of non-membership inference and develop efficient metrics for auditing.","By relying exclusively on the original and unlearned models--eliminating the need to train additional shadow models--our approach simplifies the evaluation of unlearning at the individual data point level.","Utilizing these metrics, we conduct an in-depth analysis of current approximate machine unlearning algorithms, identifying three key directions where these approaches fall short: utility, resilience, and equity.","Our aim is that this work will greatly improve our understanding of approximate machine unlearning methods, taking a significant stride towards converting the theoretical right to data erasure into a auditable reality."],"url":"http://arxiv.org/abs/2403.12830v1"}
{"created":"2024-03-19 15:24:49","title":"Answer Set Programming for Flexible Payroll Management","abstract":"Payroll management is a critical business task that is subject to a large number of rules, which vary widely between companies, sectors, and countries. Moreover, the rules are often complex and change regularly. Therefore, payroll management systems must be flexible in design. In this paper, we suggest an approach based on a flexible Answer Set Programming (ASP) model and an easy-to-read tabular representation based on the Decision Model and Notation (DMN) standard. It allows HR consultants to represent complex rules without the need for a software engineer, and to ultimately design payroll systems for a variety of different scenarios. We show how the multi-shot solving capabilities of the clingo ASP system can be used to reach the performance that is necessary to handle real-world instances.","sentences":["Payroll management is a critical business task that is subject to a large number of rules, which vary widely between companies, sectors, and countries.","Moreover, the rules are often complex and change regularly.","Therefore, payroll management systems must be flexible in design.","In this paper, we suggest an approach based on a flexible Answer Set Programming (ASP) model and an easy-to-read tabular representation based on the Decision Model and Notation (DMN) standard.","It allows HR consultants to represent complex rules without the need for a software engineer, and to ultimately design payroll systems for a variety of different scenarios.","We show how the multi-shot solving capabilities of the clingo ASP system can be used to reach the performance that is necessary to handle real-world instances."],"url":"http://arxiv.org/abs/2403.12823v1"}
{"created":"2024-03-19 15:21:10","title":"FlowerFormer: Empowering Neural Architecture Encoding using a Flow-aware Graph Transformer","abstract":"The success of a specific neural network architecture is closely tied to the dataset and task it tackles; there is no one-size-fits-all solution. Thus, considerable efforts have been made to quickly and accurately estimate the performances of neural architectures, without full training or evaluation, for given tasks and datasets. Neural architecture encoding has played a crucial role in the estimation, and graphbased methods, which treat an architecture as a graph, have shown prominent performance. For enhanced representation learning of neural architectures, we introduce FlowerFormer, a powerful graph transformer that incorporates the information flows within a neural architecture. FlowerFormer consists of two key components: (a) bidirectional asynchronous message passing, inspired by the flows; (b) global attention built on flow-based masking. Our extensive experiments demonstrate the superiority of FlowerFormer over existing neural encoding methods, and its effectiveness extends beyond computer vision models to include graph neural networks and auto speech recognition models. Our code is available at http://github.com/y0ngjaenius/CVPR2024_FLOWERFormer.","sentences":["The success of a specific neural network architecture is closely tied to the dataset and task it tackles; there is no one-size-fits-all solution.","Thus, considerable efforts have been made to quickly and accurately estimate the performances of neural architectures, without full training or evaluation, for given tasks and datasets.","Neural architecture encoding has played a crucial role in the estimation, and graphbased methods, which treat an architecture as a graph, have shown prominent performance.","For enhanced representation learning of neural architectures, we introduce FlowerFormer, a powerful graph transformer that incorporates the information flows within a neural architecture.","FlowerFormer consists of two key components: (a) bidirectional asynchronous message passing, inspired by the flows; (b) global attention built on flow-based masking.","Our extensive experiments demonstrate the superiority of FlowerFormer over existing neural encoding methods, and its effectiveness extends beyond computer vision models to include graph neural networks and auto speech recognition models.","Our code is available at http://github.com/y0ngjaenius/CVPR2024_FLOWERFormer."],"url":"http://arxiv.org/abs/2403.12821v1"}
{"created":"2024-03-19 15:21:00","title":"A Physics-embedded Deep Learning Framework for Cloth Simulation","abstract":"Delicate cloth simulations have long been desired in computer graphics. Various methods were proposed to improve engaged force interactions, collision handling, and numerical integrations. Deep learning has the potential to achieve fast and real-time simulation, but common neural network structures often demand many parameters to capture cloth dynamics. This paper proposes a physics-embedded learning framework that directly encodes physical features of cloth simulation. The convolutional neural network is used to represent spatial correlations of the mass-spring system, after which three branches are designed to learn linear, nonlinear, and time derivate features of cloth physics. The framework can also integrate with other external forces and collision handling through either traditional simulators or sub neural networks. The model is tested across different cloth animation cases, without training with new data. Agreement with baselines and predictive realism successfully validate its generalization ability. Inference efficiency of the proposed model also defeats traditional physics simulation. This framework is also designed to easily integrate with other visual refinement techniques like wrinkle carving, which leaves significant chances to incorporate prevailing macing learning techniques in 3D cloth amination.","sentences":["Delicate cloth simulations have long been desired in computer graphics.","Various methods were proposed to improve engaged force interactions, collision handling, and numerical integrations.","Deep learning has the potential to achieve fast and real-time simulation, but common neural network structures often demand many parameters to capture cloth dynamics.","This paper proposes a physics-embedded learning framework that directly encodes physical features of cloth simulation.","The convolutional neural network is used to represent spatial correlations of the mass-spring system, after which three branches are designed to learn linear, nonlinear, and time derivate features of cloth physics.","The framework can also integrate with other external forces and collision handling through either traditional simulators or sub neural networks.","The model is tested across different cloth animation cases, without training with new data.","Agreement with baselines and predictive realism successfully validate its generalization ability.","Inference efficiency of the proposed model also defeats traditional physics simulation.","This framework is also designed to easily integrate with other visual refinement techniques like wrinkle carving, which leaves significant chances to incorporate prevailing macing learning techniques in 3D cloth amination."],"url":"http://arxiv.org/abs/2403.12820v1"}
{"created":"2024-03-19 15:17:23","title":"Dynamic Survival Analysis for Early Event Prediction","abstract":"This study advances Early Event Prediction (EEP) in healthcare through Dynamic Survival Analysis (DSA), offering a novel approach by integrating risk localization into alarm policies to enhance clinical event metrics. By adapting and evaluating DSA models against traditional EEP benchmarks, our research demonstrates their ability to match EEP models on a time-step level and significantly improve event-level metrics through a new alarm prioritization scheme (up to 11% AuPRC difference). This approach represents a significant step forward in predictive healthcare, providing a more nuanced and actionable framework for early event prediction and management.","sentences":["This study advances Early Event Prediction (EEP) in healthcare through Dynamic Survival Analysis (DSA), offering a novel approach by integrating risk localization into alarm policies to enhance clinical event metrics.","By adapting and evaluating DSA models against traditional EEP benchmarks, our research demonstrates their ability to match EEP models on a time-step level and significantly improve event-level metrics through a new alarm prioritization scheme (up to 11% AuPRC difference).","This approach represents a significant step forward in predictive healthcare, providing a more nuanced and actionable framework for early event prediction and management."],"url":"http://arxiv.org/abs/2403.12818v1"}
{"created":"2024-03-19 15:15:19","title":"Re-identification from histopathology images","abstract":"In numerous studies, deep learning algorithms have proven their potential for the analysis of histopathology images, for example, for revealing the subtypes of tumors or the primary origin of metastases. These models require large datasets for training, which must be anonymized to prevent possible patient identity leaks. This study demonstrates that even relatively simple deep learning algorithms can re-identify patients in large histopathology datasets with substantial accuracy. We evaluated our algorithms on two TCIA datasets including lung squamous cell carcinoma (LSCC) and lung adenocarcinoma (LUAD). We also demonstrate the algorithm's performance on an in-house dataset of meningioma tissue. We predicted the source patient of a slide with F1 scores of 50.16 % and 52.30 % on the LSCC and LUAD datasets, respectively, and with 62.31 % on our meningioma dataset. Based on our findings, we formulated a risk assessment scheme to estimate the risk to the patient's privacy prior to publication.","sentences":["In numerous studies, deep learning algorithms have proven their potential for the analysis of histopathology images, for example, for revealing the subtypes of tumors or the primary origin of metastases.","These models require large datasets for training, which must be anonymized to prevent possible patient identity leaks.","This study demonstrates that even relatively simple deep learning algorithms can re-identify patients in large histopathology datasets with substantial accuracy.","We evaluated our algorithms on two TCIA datasets including lung squamous cell carcinoma (LSCC) and lung adenocarcinoma (LUAD).","We also demonstrate the algorithm's performance on an in-house dataset of meningioma tissue.","We predicted the source patient of a slide with F1 scores of 50.16 % and 52.30 % on the LSCC and LUAD datasets, respectively, and with 62.31 % on our meningioma dataset.","Based on our findings, we formulated a risk assessment scheme to estimate the risk to the patient's privacy prior to publication."],"url":"http://arxiv.org/abs/2403.12816v1"}
{"created":"2024-03-19 15:12:56","title":"Knowledge and Data Dual-Driven Channel Estimation and Feedback for Ultra-Massive MIMO Systems under Hybrid Field Beam Squint Effect","abstract":"Acquiring accurate channel state information (CSI) at an access point (AP) is challenging for wideband millimeter wave (mmWave) ultra-massive multiple-input and multiple-output (UMMIMO) systems, due to the high-dimensional channel matrices, hybrid near- and far- field channel feature, beam squint effects, and imperfect hardware constraints, such as low-resolution analog-to-digital converters, and in-phase and quadrature imbalance. To overcome these challenges, this paper proposes an efficient downlink channel estimation (CE) and CSI feedback approach based on knowledge and data dual-driven deep learning (DL) networks. Specifically, we first propose a data-driven residual neural network de-quantizer (ResNet-DQ) to pre-process the received pilot signals at user equipment (UEs), where the noise and distortion brought by imperfect hardware can be mitigated. A knowledge-driven generalized multiple measurement vector learned approximate message passing (GMMV-LAMP) network is then developed to jointly estimate the channels by exploiting the approximately same physical angle shared by different subcarriers. In particular, two wideband redundant dictionaries (WRDs) are proposed such that the measurement matrices of the GMMV-LAMP network can accommodate the far-field and near-field beam squint effect, respectively. Finally, we propose an encoder at the UEs and a decoder at the AP by a data-driven CSI residual network (CSI-ResNet) to compress the CSI matrix into a low-dimensional quantized bit vector for feedback, thereby reducing the feedback overhead substantially. Simulation results show that the proposed knowledge and data dual-driven approach outperforms conventional downlink CE and CSI feedback methods, especially in the case of low signal-to-noise ratios.","sentences":["Acquiring accurate channel state information (CSI) at an access point (AP) is challenging for wideband millimeter wave (mmWave) ultra-massive multiple-input and multiple-output (UMMIMO) systems, due to the high-dimensional channel matrices, hybrid near- and far- field channel feature, beam squint effects, and imperfect hardware constraints, such as low-resolution analog-to-digital converters, and in-phase and quadrature imbalance.","To overcome these challenges, this paper proposes an efficient downlink channel estimation (CE) and CSI feedback approach based on knowledge and data dual-driven deep learning (DL) networks.","Specifically, we first propose a data-driven residual neural network de-quantizer (ResNet-DQ) to pre-process the received pilot signals at user equipment (UEs), where the noise and distortion brought by imperfect hardware can be mitigated.","A knowledge-driven generalized multiple measurement vector learned approximate message passing (GMMV-LAMP) network is then developed to jointly estimate the channels by exploiting the approximately same physical angle shared by different subcarriers.","In particular, two wideband redundant dictionaries (WRDs) are proposed such that the measurement matrices of the GMMV-LAMP network can accommodate the far-field and near-field beam squint effect, respectively.","Finally, we propose an encoder at the UEs and a decoder at the AP by a data-driven CSI residual network (CSI-ResNet) to compress the CSI matrix into a low-dimensional quantized bit vector for feedback, thereby reducing the feedback overhead substantially.","Simulation results show that the proposed knowledge and data dual-driven approach outperforms conventional downlink CE and CSI feedback methods, especially in the case of low signal-to-noise ratios."],"url":"http://arxiv.org/abs/2403.12813v1"}
{"created":"2024-03-19 15:12:11","title":"The Emergence of Hardware Fuzzing: A Critical Review of its Significance","abstract":"In recent years, there has been a notable surge in attention towards hardware security, driven by the increasing complexity and integration of processors, SoCs, and third-party IPs aimed at delivering advanced solutions. However, this complexity also introduces vulnerabilities and bugs into hardware systems, necessitating early detection during the IC design cycle to uphold system integrity and mitigate re-engineering costs. While the Design Verification (DV) community employs dynamic and formal verification strategies, they encounter challenges such as scalability for intricate designs and significant human intervention, leading to prolonged verification durations. As an alternative approach, hardware fuzzing, inspired by software testing methodologies, has gained prominence for its efficacy in identifying bugs within complex hardware designs. Despite the introduction of various hardware fuzzing techniques, obstacles such as inefficient conversion of hardware modules into software models impede their effectiveness. This Systematization of Knowledge (SoK) initiative delves into the fundamental principles of existing hardware fuzzing, methodologies, and their applicability across diverse hardware designs. Additionally, it evaluates factors such as the utilization of golden reference models (GRMs), coverage metrics, and toolchains to gauge their potential for broader adoption, akin to traditional formal verification methods. Furthermore, this work examines the reliability of existing hardware fuzzing techniques in identifying vulnerabilities and identifies research gaps for future advancements in design verification techniques.","sentences":["In recent years, there has been a notable surge in attention towards hardware security, driven by the increasing complexity and integration of processors, SoCs, and third-party IPs aimed at delivering advanced solutions.","However, this complexity also introduces vulnerabilities and bugs into hardware systems, necessitating early detection during the IC design cycle to uphold system integrity and mitigate re-engineering costs.","While the Design Verification (DV) community employs dynamic and formal verification strategies, they encounter challenges such as scalability for intricate designs and significant human intervention, leading to prolonged verification durations.","As an alternative approach, hardware fuzzing, inspired by software testing methodologies, has gained prominence for its efficacy in identifying bugs within complex hardware designs.","Despite the introduction of various hardware fuzzing techniques, obstacles such as inefficient conversion of hardware modules into software models impede their effectiveness.","This Systematization of Knowledge (SoK) initiative delves into the fundamental principles of existing hardware fuzzing, methodologies, and their applicability across diverse hardware designs.","Additionally, it evaluates factors such as the utilization of golden reference models (GRMs), coverage metrics, and toolchains to gauge their potential for broader adoption, akin to traditional formal verification methods.","Furthermore, this work examines the reliability of existing hardware fuzzing techniques in identifying vulnerabilities and identifies research gaps for future advancements in design verification techniques."],"url":"http://arxiv.org/abs/2403.12812v1"}
{"created":"2024-03-19 15:07:22","title":"Comparing Explanation Faithfulness between Multilingual and Monolingual Fine-tuned Language Models","abstract":"In many real natural language processing application scenarios, practitioners not only aim to maximize predictive performance but also seek faithful explanations for the model predictions. Rationales and importance distribution given by feature attribution methods (FAs) provide insights into how different parts of the input contribute to a prediction. Previous studies have explored how different factors affect faithfulness, mainly in the context of monolingual English models. On the other hand, the differences in FA faithfulness between multilingual and monolingual models have yet to be explored. Our extensive experiments, covering five languages and five popular FAs, show that FA faithfulness varies between multilingual and monolingual models. We find that the larger the multilingual model, the less faithful the FAs are compared to its counterpart monolingual models.Our further analysis shows that the faithfulness disparity is potentially driven by the differences between model tokenizers. Our code is available: https://github.com/casszhao/multilingual-faith.","sentences":["In many real natural language processing application scenarios, practitioners not only aim to maximize predictive performance but also seek faithful explanations for the model predictions.","Rationales and importance distribution given by feature attribution methods (FAs) provide insights into how different parts of the input contribute to a prediction.","Previous studies have explored how different factors affect faithfulness, mainly in the context of monolingual English models.","On the other hand, the differences in FA faithfulness between multilingual and monolingual models have yet to be explored.","Our extensive experiments, covering five languages and five popular FAs, show that FA faithfulness varies between multilingual and monolingual models.","We find that the larger the multilingual model, the less faithful the FAs are compared to its counterpart monolingual models.","Our further analysis shows that the faithfulness disparity is potentially driven by the differences between model tokenizers.","Our code is available: https://github.com/casszhao/multilingual-faith."],"url":"http://arxiv.org/abs/2403.12809v1"}
{"created":"2024-03-19 15:07:12","title":"Freshness-aware Block Propagation Optimization in 6G-based Web 3.0: An Evolutionary Game Approach","abstract":"Driven by the aspiration to establish a decentralized digital economy, Web 3.0 is emerging as the fundamental technology for digital transformation. Incorporating the promising sixth-generation (6G) technology with large bandwidth and space-air-ground integrated coverage, 6G-based Web 3.0 holds great potential in empowering users with enhanced data control and facilitating secure peer-to-peer transactions, especially in consumer electronics, through the utilization of blockchain technologies. However, 6G-based Web 3.0 is still in its infancy, such as ensuring block freshness and optimizing block propagation to improve blockchain performance. In this paper, we develop a freshness-aware block propagation optimization framework for 6G-based Web 3.0. We first propose a novel metric called Age of Block Information (AoBI) based on the concept of age of information to quantify block freshness. To make block propagation optimization tractable, we classify miners into five different states and propose a block propagation model for public blockchains inspired by epidemic models. Moreover, considering that the miners are bounded rational, we propose an incentive mechanism based on the evolutionary game for block propagation to improve block propagation efficiency. Numerical results demonstrate that compared with other block propagation mechanisms, the proposed scheme has a higher block forwarding probability, which improves block propagation efficiency.","sentences":["Driven by the aspiration to establish a decentralized digital economy, Web 3.0 is emerging as the fundamental technology for digital transformation.","Incorporating the promising sixth-generation (6G) technology with large bandwidth and space-air-ground integrated coverage, 6G-based Web 3.0 holds great potential in empowering users with enhanced data control and facilitating secure peer-to-peer transactions, especially in consumer electronics, through the utilization of blockchain technologies.","However, 6G-based Web 3.0 is still in its infancy, such as ensuring block freshness and optimizing block propagation to improve blockchain performance.","In this paper, we develop a freshness-aware block propagation optimization framework for 6G-based Web 3.0.","We first propose a novel metric called Age of Block Information (AoBI) based on the concept of age of information to quantify block freshness.","To make block propagation optimization tractable, we classify miners into five different states and propose a block propagation model for public blockchains inspired by epidemic models.","Moreover, considering that the miners are bounded rational, we propose an incentive mechanism based on the evolutionary game for block propagation to improve block propagation efficiency.","Numerical results demonstrate that compared with other block propagation mechanisms, the proposed scheme has a higher block forwarding probability, which improves block propagation efficiency."],"url":"http://arxiv.org/abs/2403.12807v1"}
{"created":"2024-03-19 15:07:08","title":"VisualCritic: Making LMMs Perceive Visual Quality Like Humans","abstract":"At present, large multimodal models (LMMs) have exhibited impressive generalization capabilities in understanding and generating visual signals. However, they currently still lack sufficient capability to perceive low-level visual quality akin to human perception. Can LMMs achieve this and show the same degree of generalization in this regard? If so, not only could the versatility of LMMs be further enhanced, but also the challenge of poor cross-dataset performance in the field of visual quality assessment could be addressed. In this paper, we explore this question and provide the answer \"Yes!\". As the result of this initial exploration, we present VisualCritic, the first LMM for broad-spectrum image subjective quality assessment. VisualCritic can be used across diverse data right out of box, without any requirements of dataset-specific adaptation operations like conventional specialist models. As an instruction-following LMM, VisualCritic enables new capabilities of (1) quantitatively measuring the perceptual quality of given images in terms of their Mean Opinion Score (MOS), noisiness, colorfulness, sharpness, and other numerical indicators, (2) qualitatively evaluating visual quality and providing explainable descriptions, (3) discerning whether a given image is AI-generated or photographic. Extensive experiments demonstrate the efficacy of VisualCritic by comparing it with other open-source LMMs and conventional specialist models over both AI-generated and photographic images.","sentences":["At present, large multimodal models (LMMs) have exhibited impressive generalization capabilities in understanding and generating visual signals.","However, they currently still lack sufficient capability to perceive low-level visual quality akin to human perception.","Can LMMs achieve this and show the same degree of generalization in this regard?","If so, not only could the versatility of LMMs be further enhanced, but also the challenge of poor cross-dataset performance in the field of visual quality assessment could be addressed.","In this paper, we explore this question and provide the answer \"Yes!\".","As the result of this initial exploration, we present VisualCritic, the first LMM for broad-spectrum image subjective quality assessment.","VisualCritic can be used across diverse data right out of box, without any requirements of dataset-specific adaptation operations like conventional specialist models.","As an instruction-following LMM, VisualCritic enables new capabilities of (1) quantitatively measuring the perceptual quality of given images in terms of their Mean Opinion Score (MOS), noisiness, colorfulness, sharpness, and other numerical indicators, (2) qualitatively evaluating visual quality and providing explainable descriptions, (3) discerning whether a given image is AI-generated or photographic.","Extensive experiments demonstrate the efficacy of VisualCritic by comparing it with other open-source LMMs and conventional specialist models over both AI-generated and photographic images."],"url":"http://arxiv.org/abs/2403.12806v1"}
{"created":"2024-03-19 15:06:53","title":"Contextual Moral Value Alignment Through Context-Based Aggregation","abstract":"Developing value-aligned AI agents is a complex undertaking and an ongoing challenge in the field of AI. Specifically within the domain of Large Language Models (LLMs), the capability to consolidate multiple independently trained dialogue agents, each aligned with a distinct moral value, into a unified system that can adapt to and be aligned with multiple moral values is of paramount importance. In this paper, we propose a system that does contextual moral value alignment based on contextual aggregation. Here, aggregation is defined as the process of integrating a subset of LLM responses that are best suited to respond to a user input, taking into account features extracted from the user's input. The proposed system shows better results in term of alignment to human value compared to the state of the art.","sentences":["Developing value-aligned AI agents is a complex undertaking and an ongoing challenge in the field of AI.","Specifically within the domain of Large Language Models (LLMs), the capability to consolidate multiple independently trained dialogue agents, each aligned with a distinct moral value, into a unified system that can adapt to and be aligned with multiple moral values is of paramount importance.","In this paper, we propose a system that does contextual moral value alignment based on contextual aggregation.","Here, aggregation is defined as the process of integrating a subset of LLM responses that are best suited to respond to a user input, taking into account features extracted from the user's input.","The proposed system shows better results in term of alignment to human value compared to the state of the art."],"url":"http://arxiv.org/abs/2403.12805v1"}
{"created":"2024-03-19 15:04:35","title":"DreamDA: Generative Data Augmentation with Diffusion Models","abstract":"The acquisition of large-scale, high-quality data is a resource-intensive and time-consuming endeavor. Compared to conventional Data Augmentation (DA) techniques (e.g. cropping and rotation), exploiting prevailing diffusion models for data generation has received scant attention in classification tasks. Existing generative DA methods either inadequately bridge the domain gap between real-world and synthesized images, or inherently suffer from a lack of diversity. To solve these issues, this paper proposes a new classification-oriented framework DreamDA, which enables data synthesis and label generation by way of diffusion models. DreamDA generates diverse samples that adhere to the original data distribution by considering training images in the original data as seeds and perturbing their reverse diffusion process. In addition, since the labels of the generated data may not align with the labels of their corresponding seed images, we introduce a self-training paradigm for generating pseudo labels and training classifiers using the synthesized data. Extensive experiments across four tasks and five datasets demonstrate consistent improvements over strong baselines, revealing the efficacy of DreamDA in synthesizing high-quality and diverse images with accurate labels. Our code will be available at https://github.com/yunxiangfu2001/DreamDA.","sentences":["The acquisition of large-scale, high-quality data is a resource-intensive and time-consuming endeavor.","Compared to conventional Data Augmentation (DA) techniques (e.g. cropping and rotation), exploiting prevailing diffusion models for data generation has received scant attention in classification tasks.","Existing generative DA methods either inadequately bridge the domain gap between real-world and synthesized images, or inherently suffer from a lack of diversity.","To solve these issues, this paper proposes a new classification-oriented framework DreamDA, which enables data synthesis and label generation by way of diffusion models.","DreamDA generates diverse samples that adhere to the original data distribution by considering training images in the original data as seeds and perturbing their reverse diffusion process.","In addition, since the labels of the generated data may not align with the labels of their corresponding seed images, we introduce a self-training paradigm for generating pseudo labels and training classifiers using the synthesized data.","Extensive experiments across four tasks and five datasets demonstrate consistent improvements over strong baselines, revealing the efficacy of DreamDA in synthesizing high-quality and diverse images with accurate labels.","Our code will be available at https://github.com/yunxiangfu2001/DreamDA."],"url":"http://arxiv.org/abs/2403.12803v1"}
{"created":"2024-03-19 15:01:19","title":"RelationVLM: Making Large Vision-Language Models Understand Visual Relations","abstract":"The development of Large Vision-Language Models (LVLMs) is striving to catch up with the success of Large Language Models (LLMs), yet it faces more challenges to be resolved. Very recent works enable LVLMs to localize object-level visual contents and ground text to them. Nonetheless, current LVLMs still struggle to precisely understand visual relations due to the lack of relevant data. In this work, we present RelationVLM, a large vision-language model capable of comprehending various levels and types of relations whether across multiple images or within a video. Specifically, we devise a multi-stage relation-aware training scheme and a series of corresponding data configuration strategies to bestow RelationVLM with the capabilities of understanding semantic relations, temporal associations and geometric transforms. Extensive case studies and quantitative evaluations show RelationVLM has strong capability in understanding such relations and emerges impressive in-context capability of reasoning from few-shot examples by comparison. This work fosters the advancements of LVLMs by enabling them to support a wider range of downstream applications toward artificial general intelligence.","sentences":["The development of Large Vision-Language Models (LVLMs) is striving to catch up with the success of Large Language Models (LLMs), yet it faces more challenges to be resolved.","Very recent works enable LVLMs to localize object-level visual contents and ground text to them.","Nonetheless, current LVLMs still struggle to precisely understand visual relations due to the lack of relevant data.","In this work, we present RelationVLM, a large vision-language model capable of comprehending various levels and types of relations whether across multiple images or within a video.","Specifically, we devise a multi-stage relation-aware training scheme and a series of corresponding data configuration strategies to bestow RelationVLM with the capabilities of understanding semantic relations, temporal associations and geometric transforms.","Extensive case studies and quantitative evaluations show RelationVLM has strong capability in understanding such relations and emerges impressive in-context capability of reasoning from few-shot examples by comparison.","This work fosters the advancements of LVLMs by enabling them to support a wider range of downstream applications toward artificial general intelligence."],"url":"http://arxiv.org/abs/2403.12801v1"}
{"created":"2024-03-19 15:01:18","title":"Learning Neural Volumetric Pose Features for Camera Localization","abstract":"We introduce a novel neural volumetric pose feature, termed PoseMap, designed to enhance camera localization by encapsulating the information between images and the associated camera poses. Our framework leverages an Absolute Pose Regression (APR) architecture, together with an augmented NeRF module. This integration not only facilitates the generation of novel views to enrich the training dataset but also enables the learning of effective pose features. Additionally, we extend our architecture for self-supervised online alignment, allowing our method to be used and fine-tuned for unlabelled images within a unified framework. Experiments demonstrate that our method achieves 14.28% and 20.51% performance gain on average in indoor and outdoor benchmark scenes, outperforming existing APR methods with state-of-the-art accuracy.","sentences":["We introduce a novel neural volumetric pose feature, termed PoseMap, designed to enhance camera localization by encapsulating the information between images and the associated camera poses.","Our framework leverages an Absolute Pose Regression (APR) architecture, together with an augmented NeRF module.","This integration not only facilitates the generation of novel views to enrich the training dataset but also enables the learning of effective pose features.","Additionally, we extend our architecture for self-supervised online alignment, allowing our method to be used and fine-tuned for unlabelled images within a unified framework.","Experiments demonstrate that our method achieves 14.28% and 20.51% performance gain on average in indoor and outdoor benchmark scenes, outperforming existing APR methods with state-of-the-art accuracy."],"url":"http://arxiv.org/abs/2403.12800v1"}
{"created":"2024-03-19 15:01:14","title":"Investigating Text Shortening Strategy in BERT: Truncation vs Summarization","abstract":"The parallelism of Transformer-based models comes at the cost of their input max-length. Some studies proposed methods to overcome this limitation, but none of them reported the effectiveness of summarization as an alternative. In this study, we investigate the performance of document truncation and summarization in text classification tasks. Each of the two was investigated with several variations. This study also investigated how close their performances are to the performance of full-text. We used a dataset of summarization tasks based on Indonesian news articles (IndoSum) to do classification tests. This study shows how the summaries outperform the majority of truncation method variations and lose to only one. The best strategy obtained in this study is taking the head of the document. The second is extractive summarization. This study explains what happened to the result, leading to further research in order to exploit the potential of document summarization as a shortening alternative. The code and data used in this work are publicly available in https://github.com/mirzaalimm/TruncationVsSummarization.","sentences":["The parallelism of Transformer-based models comes at the cost of their input max-length.","Some studies proposed methods to overcome this limitation, but none of them reported the effectiveness of summarization as an alternative.","In this study, we investigate the performance of document truncation and summarization in text classification tasks.","Each of the two was investigated with several variations.","This study also investigated how close their performances are to the performance of full-text.","We used a dataset of summarization tasks based on Indonesian news articles (IndoSum) to do classification tests.","This study shows how the summaries outperform the majority of truncation method variations and lose to only one.","The best strategy obtained in this study is taking the head of the document.","The second is extractive summarization.","This study explains what happened to the result, leading to further research in order to exploit the potential of document summarization as a shortening alternative.","The code and data used in this work are publicly available in https://github.com/mirzaalimm/TruncationVsSummarization."],"url":"http://arxiv.org/abs/2403.12799v1"}
{"created":"2024-03-19 15:00:53","title":"Introducing Combi-Stations in Robotic Mobile Fulfilment Systems: A Queueing-Theory-Based Efficiency Analysis","abstract":"In the era of digital commerce, the surge in online shopping and the expectation for rapid delivery have placed unprecedented demands on warehouse operations. The traditional method of order fulfilment, where human order pickers traverse large storage areas to pick items, has become a bottleneck, consuming valuable time and resources. Robotic Mobile Fulfilment Systems (RMFS) offer a solution by using robots to transport storage racks directly to human-operated picking stations, eliminating the need for pickers to travel. This paper introduces combi-stations, a novel type of station that enables both item picking and replenishment, as opposed to traditional separate stations. We analyse the efficiency of combi-stations using queueing theory and demonstrate their potential to streamline warehouse operations. Our results suggest that combi-stations can reduce the number of robots required for stability and significantly reduce order turnover time, indicating a promising direction for future warehouse automation.","sentences":["In the era of digital commerce, the surge in online shopping and the expectation for rapid delivery have placed unprecedented demands on warehouse operations.","The traditional method of order fulfilment, where human order pickers traverse large storage areas to pick items, has become a bottleneck, consuming valuable time and resources.","Robotic Mobile Fulfilment Systems (RMFS) offer a solution by using robots to transport storage racks directly to human-operated picking stations, eliminating the need for pickers to travel.","This paper introduces combi-stations, a novel type of station that enables both item picking and replenishment, as opposed to traditional separate stations.","We analyse the efficiency of combi-stations using queueing theory and demonstrate their potential to streamline warehouse operations.","Our results suggest that combi-stations can reduce the number of robots required for stability and significantly reduce order turnover time, indicating a promising direction for future warehouse automation."],"url":"http://arxiv.org/abs/2403.12798v1"}
{"created":"2024-03-19 15:00:39","title":"Parallel Gaussian process with kernel approximation in CUDA","abstract":"This paper introduces a parallel implementation in CUDA/C++ of the Gaussian process with a decomposed kernel. This recent formulation, introduced by Joukov and Kuli\\'c (2022), is characterized by an approximated -- but much smaller -- matrix to be inverted compared to plain Gaussian process. However, it exhibits a limitation when dealing with higher-dimensional samples which degrades execution times. The solution presented in this paper relies on parallelizing the computation of the predictive posterior statistics on a GPU using CUDA and its libraries. The CPU code and GPU code are then benchmarked on different CPU-GPU configurations to show the benefits of the parallel implementation on GPU over the CPU.","sentences":["This paper introduces a parallel implementation in CUDA/C++ of the Gaussian process with a decomposed kernel.","This recent formulation, introduced by Joukov and Kuli\\'c (2022), is characterized by an approximated -- but much smaller -- matrix to be inverted compared to plain Gaussian process.","However, it exhibits a limitation when dealing with higher-dimensional samples which degrades execution times.","The solution presented in this paper relies on parallelizing the computation of the predictive posterior statistics on a GPU using CUDA and its libraries.","The CPU code and GPU code are then benchmarked on different CPU-GPU configurations to show the benefits of the parallel implementation on GPU over the CPU."],"url":"http://arxiv.org/abs/2403.12797v1"}
{"created":"2024-03-19 14:54:41","title":"Morse Theory for the k-NN Distance Function","abstract":"We study the $k$-th nearest neighbor distance function from a finite point-set in $\\mathbb{R}^d$. We provide a Morse theoretic framework to analyze the sub-level set topology. In particular, we present a simple combinatorial-geometric characterization for critical points and their indices, along with detailed information about the possible changes in homology at the critical levels. We conclude by computing the expected number of critical points for a homogeneous Poisson process. Our results deliver significant insights and tools for the analysis of persistent homology in order-$k$ Delaunay mosaics, and random $k$-fold coverage.","sentences":["We study the $k$-th nearest neighbor distance function from a finite point-set in $\\mathbb{R}^d$. We provide a Morse theoretic framework to analyze the sub-level set topology.","In particular, we present a simple combinatorial-geometric characterization for critical points and their indices, along with detailed information about the possible changes in homology at the critical levels.","We conclude by computing the expected number of critical points for a homogeneous Poisson process.","Our results deliver significant insights and tools for the analysis of persistent homology in order-$k$ Delaunay mosaics, and random $k$-fold coverage."],"url":"http://arxiv.org/abs/2403.12792v1"}
{"created":"2024-03-19 14:51:01","title":"DDSB: An Unsupervised and Training-free Method for Phase Detection in Echocardiography","abstract":"Accurate identification of End-Diastolic (ED) and End-Systolic (ES) frames is key for cardiac function assessment through echocardiography. However, traditional methods face several limitations: they require extensive amounts of data, extensive annotations by medical experts, significant training resources, and often lack robustness. Addressing these challenges, we proposed an unsupervised and training-free method, our novel approach leverages unsupervised segmentation to enhance fault tolerance against segmentation inaccuracies. By identifying anchor points and analyzing directional deformation, we effectively reduce dependence on the accuracy of initial segmentation images and enhance fault tolerance, all while improving robustness. Tested on Echo-dynamic and CAMUS datasets, our method achieves comparable accuracy to learning-based models without their associated drawbacks. The code is available at https://github.com/MRUIL/DDSB","sentences":["Accurate identification of End-Diastolic (ED) and End-Systolic (ES) frames is key for cardiac function assessment through echocardiography.","However, traditional methods face several limitations: they require extensive amounts of data, extensive annotations by medical experts, significant training resources, and often lack robustness.","Addressing these challenges, we proposed an unsupervised and training-free method, our novel approach leverages unsupervised segmentation to enhance fault tolerance against segmentation inaccuracies.","By identifying anchor points and analyzing directional deformation, we effectively reduce dependence on the accuracy of initial segmentation images and enhance fault tolerance, all while improving robustness.","Tested on Echo-dynamic and CAMUS datasets, our method achieves comparable accuracy to learning-based models without their associated drawbacks.","The code is available at https://github.com/MRUIL/DDSB"],"url":"http://arxiv.org/abs/2403.12787v1"}
{"created":"2024-03-19 14:50:13","title":"Total Disentanglement of Font Images into Style and Character Class Features","abstract":"In this paper, we demonstrate a total disentanglement of font images. Total disentanglement is a neural network-based method for decomposing each font image nonlinearly and completely into its style and content (i.e., character class) features. It uses a simple but careful training procedure to extract the common style feature from all `A'-`Z' images in the same font and the common content feature from all `A' (or another class) images in different fonts. These disentangled features guarantee the reconstruction of the original font image. Various experiments have been conducted to understand the performance of total disentanglement. First, it is demonstrated that total disentanglement is achievable with very high accuracy; this is experimental proof of the long-standing open question, ``Does `A'-ness exist?'' Hofstadter (1985). Second, it is demonstrated that the disentangled features produced by total disentanglement apply to a variety of tasks, including font recognition, character recognition, and one-shot font image generation.","sentences":["In this paper, we demonstrate a total disentanglement of font images.","Total disentanglement is a neural network-based method for decomposing each font image nonlinearly and completely into its style and content (i.e., character class) features.","It uses a simple but careful training procedure to extract the common style feature from all `A'-`Z' images in the same font and the common content feature from all `A' (or another class) images in different fonts.","These disentangled features guarantee the reconstruction of the original font image.","Various experiments have been conducted to understand the performance of total disentanglement.","First, it is demonstrated that total disentanglement is achievable with very high accuracy; this is experimental proof of the long-standing open question, ``Does `A'-ness exist?''","Hofstadter (1985).","Second, it is demonstrated that the disentangled features produced by total disentanglement apply to a variety of tasks, including font recognition, character recognition, and one-shot font image generation."],"url":"http://arxiv.org/abs/2403.12784v1"}
{"created":"2024-03-19 14:45:17","title":"ViTGaze: Gaze Following with Interaction Features in Vision Transformers","abstract":"Gaze following aims to interpret human-scene interactions by predicting the person's focal point of gaze. Prevailing approaches often use multi-modality inputs, most of which adopt a two-stage framework. Hence their performance highly depends on the previous prediction accuracy. Others use a single-modality approach with complex decoders, increasing network computational load. Inspired by the remarkable success of pre-trained plain Vision Transformers (ViTs), we introduce a novel single-modality gaze following framework, ViTGaze. In contrast to previous methods, ViTGaze creates a brand new gaze following framework based mainly on powerful encoders (dec. param. less than 1%). Our principal insight lies in that the inter-token interactions within self-attention can be transferred to interactions between humans and scenes. Leveraging this presumption, we formulate a framework consisting of a 4D interaction encoder and a 2D spatial guidance module to extract human-scene interaction information from self-attention maps. Furthermore, our investigation reveals that ViT with self-supervised pre-training exhibits an enhanced ability to extract correlated information. A large number of experiments have been conducted to demonstrate the performance of the proposed method. Our method achieves state-of-the-art (SOTA) performance among all single-modality methods (3.4% improvement on AUC, 5.1% improvement on AP) and very comparable performance against multi-modality methods with 59% number of parameters less.","sentences":["Gaze following aims to interpret human-scene interactions by predicting the person's focal point of gaze.","Prevailing approaches often use multi-modality inputs, most of which adopt a two-stage framework.","Hence their performance highly depends on the previous prediction accuracy.","Others use a single-modality approach with complex decoders, increasing network computational load.","Inspired by the remarkable success of pre-trained plain Vision Transformers (ViTs), we introduce a novel single-modality gaze following framework, ViTGaze.","In contrast to previous methods, ViTGaze creates a brand new gaze following framework based mainly on powerful encoders (dec. param.","less than 1%).","Our principal insight lies in that the inter-token interactions within self-attention can be transferred to interactions between humans and scenes.","Leveraging this presumption, we formulate a framework consisting of a 4D interaction encoder and a 2D spatial guidance module to extract human-scene interaction information from self-attention maps.","Furthermore, our investigation reveals that ViT with self-supervised pre-training exhibits an enhanced ability to extract correlated information.","A large number of experiments have been conducted to demonstrate the performance of the proposed method.","Our method achieves state-of-the-art (SOTA) performance among all single-modality methods (3.4% improvement on AUC, 5.1% improvement on AP) and very comparable performance against multi-modality methods with 59% number of parameters less."],"url":"http://arxiv.org/abs/2403.12778v1"}
{"created":"2024-03-19 14:44:54","title":"Discover and Mitigate Multiple Biased Subgroups in Image Classifiers","abstract":"Machine learning models can perform well on in-distribution data but often fail on biased subgroups that are underrepresented in the training data, hindering the robustness of models for reliable applications. Such subgroups are typically unknown due to the absence of subgroup labels. Discovering biased subgroups is the key to understanding models' failure modes and further improving models' robustness. Most previous works of subgroup discovery make an implicit assumption that models only underperform on a single biased subgroup, which does not hold on in-the-wild data where multiple biased subgroups exist.   In this work, we propose Decomposition, Interpretation, and Mitigation (DIM), a novel method to address a more challenging but also more practical problem of discovering multiple biased subgroups in image classifiers. Our approach decomposes the image features into multiple components that represent multiple subgroups. This decomposition is achieved via a bilinear dimension reduction method, Partial Least Square (PLS), guided by useful supervision from the image classifier. We further interpret the semantic meaning of each subgroup component by generating natural language descriptions using vision-language foundation models. Finally, DIM mitigates multiple biased subgroups simultaneously via two strategies, including the data- and model-centric strategies. Extensive experiments on CIFAR-100 and Breeds datasets demonstrate the effectiveness of DIM in discovering and mitigating multiple biased subgroups. Furthermore, DIM uncovers the failure modes of the classifier on Hard ImageNet, showcasing its broader applicability to understanding model bias in image classifiers. The code is available at https://github.com/ZhangAIPI/DIM.","sentences":["Machine learning models can perform well on in-distribution data but often fail on biased subgroups that are underrepresented in the training data, hindering the robustness of models for reliable applications.","Such subgroups are typically unknown due to the absence of subgroup labels.","Discovering biased subgroups is the key to understanding models' failure modes and further improving models' robustness.","Most previous works of subgroup discovery make an implicit assumption that models only underperform on a single biased subgroup, which does not hold on in-the-wild data where multiple biased subgroups exist.   ","In this work, we propose Decomposition, Interpretation, and Mitigation (DIM), a novel method to address a more challenging but also more practical problem of discovering multiple biased subgroups in image classifiers.","Our approach decomposes the image features into multiple components that represent multiple subgroups.","This decomposition is achieved via a bilinear dimension reduction method, Partial Least Square (PLS), guided by useful supervision from the image classifier.","We further interpret the semantic meaning of each subgroup component by generating natural language descriptions using vision-language foundation models.","Finally, DIM mitigates multiple biased subgroups simultaneously via two strategies, including the data- and model-centric strategies.","Extensive experiments on CIFAR-100 and Breeds datasets demonstrate the effectiveness of DIM in discovering and mitigating multiple biased subgroups.","Furthermore, DIM uncovers the failure modes of the classifier on Hard ImageNet, showcasing its broader applicability to understanding model bias in image classifiers.","The code is available at https://github.com/ZhangAIPI/DIM."],"url":"http://arxiv.org/abs/2403.12777v1"}
{"created":"2024-03-19 14:44:45","title":"Automated Data Curation for Robust Language Model Fine-Tuning","abstract":"Large Language Models have become the de facto approach to sequence-to-sequence text generation tasks, but for specialized tasks/domains, a pretrained LLM lacks specific capabilities to produce accurate or well-formatted responses. Supervised fine-tuning specializes a LLM by training it on dataset of example prompts with target responses, but real-world data tends to be noisy. While many fine-tuning algorithms exist, here we consider a \\emph{data-centric AI} perspective on LLM fine-tuning, studying how to \\emph{systematically} curate the training dataset to improve the LLM produced via \\emph{any} fine-tuning algorithm.   We introduce an automated data curation pipeline CLEAR (Confidence-based LLM Evaluation And Rectification) for instruction tuning datasets, that can be used with any LLM and fine-tuning procedure. CLEAR estimates which training data is low-quality and either filters or corrects it. Automatically identifying which data to filter or correct is done via LLM-derived confidence estimates, to ensure only confident modifications to the dataset. Unlike existing data curation techniques, CLEAR is a comprehensive framework that can improve a dataset (and trained model outputs) without additional fine-tuning computations. We don't assume access to a stronger LLM than the model being fine-tuned (e.g.\\ relying on GPT-4 when fine-tuning GPT-3.5), to see whether CLEAR can meaningfully improve the capabilities of any LLM. Experiments reveal that CLEAR consistently improves the performance of fine-tuned models across many datasets and models (like GPT-3.5 and Llama2).","sentences":["Large Language Models have become the de facto approach to sequence-to-sequence text generation tasks, but for specialized tasks/domains, a pretrained LLM lacks specific capabilities to produce accurate or well-formatted responses.","Supervised fine-tuning specializes a LLM by training it on dataset of example prompts with target responses, but real-world data tends to be noisy.","While many fine-tuning algorithms exist, here we consider a \\emph{data-centric AI} perspective on LLM fine-tuning, studying how to \\emph{systematically} curate the training dataset to improve the LLM produced via \\emph{any} fine-tuning algorithm.   ","We introduce an automated data curation pipeline CLEAR (Confidence-based LLM Evaluation And Rectification) for instruction tuning datasets, that can be used with any LLM and fine-tuning procedure.","CLEAR estimates which training data is low-quality and either filters or corrects it.","Automatically identifying which data to filter or correct is done via LLM-derived confidence estimates, to ensure only confident modifications to the dataset.","Unlike existing data curation techniques, CLEAR is a comprehensive framework that can improve a dataset (and trained model outputs) without additional fine-tuning computations.","We don't assume access to a stronger LLM than the model being fine-tuned (e.g.\\ relying on GPT-4 when fine-tuning GPT-3.5), to see whether CLEAR can meaningfully improve the capabilities of any LLM.","Experiments reveal that CLEAR consistently improves the performance of fine-tuned models across many datasets and models (like GPT-3.5 and Llama2)."],"url":"http://arxiv.org/abs/2403.12776v1"}
{"created":"2024-03-19 14:43:52","title":"Is open source software culture enough to make AI a common ?","abstract":"Language models (LM or LLM) are increasingly deployed in the field of artificial intelligence (AI) and its applications, but the question arises as to whether they can be a common resource managed and maintained by a community of users. Indeed, the dominance of private companies with exclusive access to massive data and language processing resources can create inequalities and biases in LM, as well as obstacles to innovation for those who do not have the same resources necessary for their implementation. In this contribution, we examine the concept of the commons and its relevance for thinking about LM. We highlight the potential benefits of treating the data and resources needed to create LMs as commons, including increased accessibility, equity, and transparency in the development and use of AI technologies. Finally, we present a case study centered on the Hugging Face platform, an open-source platform for deep learning designed to encourage collaboration and sharing among AI designers.","sentences":["Language models (LM or LLM) are increasingly deployed in the field of artificial intelligence (AI) and its applications, but the question arises as to whether they can be a common resource managed and maintained by a community of users.","Indeed, the dominance of private companies with exclusive access to massive data and language processing resources can create inequalities and biases in LM, as well as obstacles to innovation for those who do not have the same resources necessary for their implementation.","In this contribution, we examine the concept of the commons and its relevance for thinking about LM.","We highlight the potential benefits of treating the data and resources needed to create LMs as commons, including increased accessibility, equity, and transparency in the development and use of AI technologies.","Finally, we present a case study centered on the Hugging Face platform, an open-source platform for deep learning designed to encourage collaboration and sharing among AI designers."],"url":"http://arxiv.org/abs/2403.12774v1"}
{"created":"2024-03-19 14:34:44","title":"Multispectral Image Restoration by Generalized Opponent Transformation Total Variation","abstract":"Multispectral images (MSI) contain light information in different wavelengths of objects, which convey spectral-spatial information and help improve the performance of various image processing tasks. Numerous techniques have been created to extend the application of total variation regularization in restoring multispectral images, for example, based on channel coupling and adaptive total variation regularization. The primary contribution of this paper is to propose and develop a new multispectral total variation regularization in a generalized opponent transformation domain instead of the original multispectral image domain. Here opponent transformations for multispectral images are generalized from a well-known opponent transformation for color images. We will explore the properties of generalized opponent transformation total variation (GOTTV) regularization and the corresponding optimization formula for multispectral image restoration. To evaluate the effectiveness of the new GOTTV method, we provide numerical examples that showcase its superior performance compared to existing multispectral image total variation methods, using criteria such as MPSNR and MSSIM.","sentences":["Multispectral images (MSI) contain light information in different wavelengths of objects, which convey spectral-spatial information and help improve the performance of various image processing tasks.","Numerous techniques have been created to extend the application of total variation regularization in restoring multispectral images, for example, based on channel coupling and adaptive total variation regularization.","The primary contribution of this paper is to propose and develop a new multispectral total variation regularization in a generalized opponent transformation domain instead of the original multispectral image domain.","Here opponent transformations for multispectral images are generalized from a well-known opponent transformation for color images.","We will explore the properties of generalized opponent transformation total variation (GOTTV) regularization and the corresponding optimization formula for multispectral image restoration.","To evaluate the effectiveness of the new GOTTV method, we provide numerical examples that showcase its superior performance compared to existing multispectral image total variation methods, using criteria such as MPSNR and MSSIM."],"url":"http://arxiv.org/abs/2403.12770v1"}
{"created":"2024-03-19 14:33:16","title":"Understanding the Factors Influencing Self-Managed Enterprises of Crowdworkers: A Comprehensive Review","abstract":"This paper investigates the shift in crowdsourcing towards self-managed enterprises of crowdworkers (SMECs), diverging from traditional platform-controlled models. It reviews the literature to understand the foundational aspects of this shift, focusing on identifying key factors that may explain the rise of SMECs, particularly concerning power dynamics and tensions between Online Labor Platforms (OLPs) and crowdworkers. The study aims to guide future research and inform policy and platform development, emphasizing the importance of fair labor practices in this evolving landscape.","sentences":["This paper investigates the shift in crowdsourcing towards self-managed enterprises of crowdworkers (SMECs), diverging from traditional platform-controlled models.","It reviews the literature to understand the foundational aspects of this shift, focusing on identifying key factors that may explain the rise of SMECs, particularly concerning power dynamics and tensions between Online Labor Platforms (OLPs) and crowdworkers.","The study aims to guide future research and inform policy and platform development, emphasizing the importance of fair labor practices in this evolving landscape."],"url":"http://arxiv.org/abs/2403.12769v1"}
{"created":"2024-03-19 14:32:28","title":"ContextVis: Envision Contextual Learning and Interaction with Generative Models","abstract":"ContextVis introduces a workflow by integrating generative models to create contextual learning materials. It aims to boost knowledge acquisition through the creation of resources with contextual cues. A case study on vocabulary learning demonstrates the effectiveness of generative models in developing educational resources that enrich language understanding and aid memory retention. The system combines an easy-to-use Dashboard for educators with an interactive Playground for learners, establishing a unified platform for content creation and interaction. Future work may expand to include a wider range of generative models, media formats, and customization features for educators.","sentences":["ContextVis introduces a workflow by integrating generative models to create contextual learning materials.","It aims to boost knowledge acquisition through the creation of resources with contextual cues.","A case study on vocabulary learning demonstrates the effectiveness of generative models in developing educational resources that enrich language understanding and aid memory retention.","The system combines an easy-to-use Dashboard for educators with an interactive Playground for learners, establishing a unified platform for content creation and interaction.","Future work may expand to include a wider range of generative models, media formats, and customization features for educators."],"url":"http://arxiv.org/abs/2403.12768v1"}
{"created":"2024-03-19 14:32:21","title":"Inter- and intra-uncertainty based feature aggregation model for semi-supervised histopathology image segmentation","abstract":"Acquiring pixel-level annotations is often limited in applications such as histology studies that require domain expertise. Various semi-supervised learning approaches have been developed to work with limited ground truth annotations, such as the popular teacher-student models. However, hierarchical prediction uncertainty within the student model (intra-uncertainty) and image prediction uncertainty (inter-uncertainty) have not been fully utilized by existing methods. To address these issues, we first propose a novel inter- and intra-uncertainty regularization method to measure and constrain both inter- and intra-inconsistencies in the teacher-student architecture. We also propose a new two-stage network with pseudo-mask guided feature aggregation (PG-FANet) as the segmentation model. The two-stage structure complements with the uncertainty regularization strategy to avoid introducing extra modules in solving uncertainties and the aggregation mechanisms enable multi-scale and multi-stage feature integration. Comprehensive experimental results over the MoNuSeg and CRAG datasets show that our PG-FANet outperforms other state-of-the-art methods and our semi-supervised learning framework yields competitive performance with a limited amount of labeled data.","sentences":["Acquiring pixel-level annotations is often limited in applications such as histology studies that require domain expertise.","Various semi-supervised learning approaches have been developed to work with limited ground truth annotations, such as the popular teacher-student models.","However, hierarchical prediction uncertainty within the student model (intra-uncertainty) and image prediction uncertainty (inter-uncertainty) have not been fully utilized by existing methods.","To address these issues, we first propose a novel inter- and intra-uncertainty regularization method to measure and constrain both inter- and intra-inconsistencies in the teacher-student architecture.","We also propose a new two-stage network with pseudo-mask guided feature aggregation (PG-FANet) as the segmentation model.","The two-stage structure complements with the uncertainty regularization strategy to avoid introducing extra modules in solving uncertainties and the aggregation mechanisms enable multi-scale and multi-stage feature integration.","Comprehensive experimental results over the MoNuSeg and CRAG datasets show that our PG-FANet outperforms other state-of-the-art methods and our semi-supervised learning framework yields competitive performance with a limited amount of labeled data."],"url":"http://arxiv.org/abs/2403.12767v1"}
